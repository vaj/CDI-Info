
Hello, my name is Shyam Iyer. I'm here to talk about smart data accelerator interface,  use cases, futures, and the proof points that are emerging for this new standard. I am the chair for this technical work group in SNIA,  also a technical council member in SNIA, and a distinguished engineer in Dell. You can reach me at this email, sdxitwgchair@SNIA.com.

So as part of this agenda, I will cover what SNIA's SDXI technical work group is doing,  some of the use cases that SDXI solves,  a look at some of the features that we're looking ahead,  some of the proof points that we have been collecting,  and end it with a summary and call to action.

So what is SDXI? SDXI stands for smart data accelerator interface. It's a SNIA standard for memory-to-memory data movement and acceleration,  which is one extensible, power compatible, and independent of I/O interconnect technology. In short words, it's basically a memory-to-memory DMA,  and it's independent of the bus that is being used to perform the memory-to-memory data movement. It was formed in June 2020. We are over 25 members, 100 individual members,  and to its credit, we have already released version 1.0 of the specification.

These are the list of companies that contributed to the specification. They contain the OEM vendors, CPU vendors, accelerator vendors, storage vendors, and OS vendors,  and we continue to attract new membership to this work group.

If you're looking for a much more detailed discussion on what SDXI specification 1.0 has,  please take a look at this YouTube video that I did at storage developer conference last year. Here's a QR code for that as well.

Let me cover some of the use cases where memory-to-memory data movement and acceleration  are beginning to become part of a system architecture,  and one of the reasons why SDXI is becoming one of the standards that you need to pay attention to. If an application wants to offload a lot of the buffer copies that it is performing today  to simply save on CPU or compute cycles, you might want to employ an accelerator for that,  and for that reason, you might queue a work descriptor, ring a doorbell,  and then the accelerator will go and perform those buffer copies,  and then once it's done, provide a completion signal. This is one example of an SDXI-based accelerator helping you with memory-to-memory data movement offload.

Another example is if you wanted to just persist the data and make sure that the data is moved from  the volatile memory buffer to a persistent memory buffer,  or if you wanted to bring back the data to perform computation on it,  and you wanted to move it back from the persistent memory buffer to a volatile memory buffer,  an accelerator can be very useful. SDXI plays a role here.

This is another example of where a user buffer in a virtual machine needs to be copied over to a user buffer in another virtual machine,  and in this example, an SDXI-based accelerator can very safely and securely perform a read from a user virtual machine's buffer  to another virtual machine's user buffer. So the standard needs to perform these type of virtualized buffer copies as well.

Another usage model is when you have different classes of memory,  you might want to tier them and promote or demode data between DRAM, CXL-based memory,  or memory behind I/O devices, or other kinds of high bandwidth or types of memory. And offloading this to an accelerator helps a compute to perform other meaningful activity  while it's offloading the tiering operation to an accelerator.

This is an example of where an accelerator may be useful to transform the data as it's being moved from a source memory buffer to a destination memory buffer. An SDXI-based memory data movement standard can be incredibly valuable in defining these type of transforms. Another example is when an accelerator may temporarily compute on the data as it's being moved from a source buffer to a destination buffer,  as defined by the application. An SDXI-based accelerator can be incredibly useful in performing these type of compute operations  as data is being momentarily transferred from a source buffer to a destination buffer.

And since we are in the world and age of AI, one may ask, does SDXI, is it applied to AI as well? And the answer is yes. You might have various kinds of data formats, intermediate data representations used in AI/ML data pipelines. A lot of the training and inferencing operations involve tensors in memory. Your tensors may be in host memory, GPU memory, wherever. You need to be able to perform different kinds of operations to do format conversions or other kinds of AI operations like quantization, scaling, matrix operations, etc. Having different kinds of vendor-specific accelerator operations weakens the overall total cost of ownership of a system. One possible solution could be a memory-to-memory data movement standard like SDXI, which can help perform data movement across different address spaces  and also standardize the outflows and the transformation, leveraging the same architectural interface.

And hence, you have this new standard that SNIA is developing that is one, you know, independent of where it's implemented,  whether it's in a CPU-based architecture, GPU, FPGA, smart I/O device. It enables direct user mode access to the application, cutting down a lot of the software context isolation layers,  providing the same architectural stability like instruction set architectures and security,  as well as being able to perform data movement across different types of memory, effectively tiering. And this allows us to leverage the same standard framework to innovate and to add additional incremental data acceleration features.

So looking ahead, we think that the SDXI technical workgroup is on good track to provide a lot of value. So the first among that list is the working group is working on a OS independent user space software library called libsdxi  to enable applications to be able to make use of this accelerator interface. The team members are also enabling efforts to ensure SDXI drivers become part of various operating systems. The team is also discussing some of the efforts to enable how can emulate SDXI in software to help a lot of the ISVs to get their software as hardware based implementations start showing up. We have been as a working group planning feature discussions around what should go into 1.1 and what should go into a plan 2.0 version of the feature specification as well. So you can think that the working group is thinking ahead and planning for features that can be easily incorporated into 1.1 and looking for much bigger or more complex features and planning them for a version 2.0 of the specification. So in that regard, we are looking at a framework for creating an operations which can be defined by vendors in which may not also be something that every vendor wants to implement in their accelerator. But the specification is working on providing a framework for different types of vendors to be able to innovate with their own set of definable operations. When SDXI discusses data movement between different address spaces, version 1.0 of the specification addressed the steady state in which a connection has been already established and then the accelerator performs a secure address space to address space data movement. In our planning right now, we are looking at a framework for brokering connections between different address spaces with the help of a connection manager. The work group is also defining new data mover operations like POSIX style memory operations, CRC, DIF, compression, etc. We are also looking at SDXI features that involve these type of memory data movers. For example, how to address memory that could be in a confidential memory and be able to move from one confidential memory region to another confidential memory region. And we are doing some other considerations as we go along this journey of improving the spec, improving future implementations as such. We've also been collaborating very heavily with the computational storage technical work group in SNIA called the SDXI+CS subgroup, which is defining new ways in which SDXI could be used in a computational storage architecture and also in architectures that involve NVMe based subsystems,  whether SDXI is external to the NVMe subsystem or internal to the NVM subsystem. In addition, SDXI has also found many use cases in CXL enabled architectures. And we are looking at all of these as part of the next set of activities that the group is going to be working on.

So it's not like we have been only doing specification. Some of the proof points are emerging. First, we have been getting recognized within SNIA as the group of the year, as a group that had a significant impact,  but also in industry trade shows like the Flash Memory Summit, where it got the most memory, most innovative memory technology award.

At the same time, SDXI is also showing up in conferences with specific PoCs brought to you by STM member companies. On the picture on the right, you can see William Moyes from AMD discussing an SDXI PoC at MemCon 2024 this year. And in this demo, what he's showing is an application, a sample application, a Hello World application that has used the libsdxi library  and is basically issuing all the SDXI descriptors to an SDXI hardware using its own sort of descriptor queue. And then it has allocated memory buffers for itself that the SDXI hardware is able to use and perform these type of mem copies. And here on this picture, you can see an example of a mem copy test being successful with the help of this PoC implementation. We expect this to be first of many to come and the ecosystem to continue to emerge.

So finally, I want to end with a summary and call to action that SDXI TWG is now working on specification 1.1 and beyond. We have a fairly nascent but growing software ecosystem. SDXI accelerators have a wide variety of applicable use cases, including in emerging areas like AI. So please come join the group and participate in this growing ecosystem. Thank you.
