
Good afternoon. You guys are the brave souls. Last session of a conference—you have to be persistent or being very friendly to me. So, we will use the time to have any type of questions and discussion. So, feel free to stop me anytime—we don’t have to wait for the last. So, I am Mohiuddin Mazumder. I work at Intel, but I am presenting on behalf of PCIe SIG, particularly showing the collaboration between PCIe SIG and SNIA. And the standard body, like OIF, is what is making it possible to develop cable solutions for PCIe technology.

So, in this presentation, I will walk you through. I know that all of you are very familiar with PCIe technology, but it’s still—for the benefit of all—we’ll go a little bit over what is the basics of PCIe technology, and then walk you through what’s the journey of cabling. And there has been some pause and some acceleration, and what’s driving those. We will talk—probably quite a bit—a little bit about the PCIe SIG, this new cable solution which is called CopperLink. By the name of it, it is copper-based mostly, but some efforts are ongoing, and we are also working on optical cable solutions. So, let me continue there.

So, in terms of PCIe technology, it is always trying to stay at least in power with what the need of the industry is. This is 950-plus member companies. So, it represents the industry requirements and so on. So, what it is doing—it is giving about new specifications about every three years. With, on the average, you will see that if you look at the timing, it’s between three and four. There was a long sort of pause, and most of it came from the challenges on the electrical side—what industry thought at that time could be done or cannot be done. But the necessity, I would say, drove many innovations, and that was the time when Cloud was really taking shape and was driving the bandwidth need. Since then, every two, three years, we are doubling, and it looks like it’s not abated at all—it’s continuing. So, it is a low-latency and high-reliability—and it’s a power-efficient interface—and we are seeing more and more data-integrated security. Those are integral features of the PCIe link. So, if we look at this signaling, we have done up to PCIe 5.0 NRZ signaling, and now we are going into PAM-4 signaling with PCIe 6.0 and 7.0.

In terms of the—the bandwidth, which is simply a function of the data rate times the width—so, set up a number of lanes times the data rate. So, that provides this whole metrics for various applications. Some certain applications will stay narrow and high-speed, others will go by wider and lower speed, and there will be data center-type of applications which will go maximize on both nodes. So, if you go by 16 and the highest speed, you’re getting a maximum bandwidth. And we are seeing in data centers—AI, ML—those types of applications truly driving those limits. And we will talk a little bit about—but most of you are very expert on that—this, simply, the fact that applications like ChatGPT-type of things are requiring—now we are talking about trillions of variables. And when you even try to solve those types of models and metrics, you just need a huge amount of memory and data bandwidth. And that requires that you are pulling in GPUs, CPUs over a certain radius, and that often goes beyond racks. So, that is what is driving the need that we cannot go with traditional infrastructure and platform architecture that would be a motherboard and card and so on. So, all of these needs have been evolving very fast in the last—I would say—five years or so.

So, on this one, I would not say infinite opportunities, but I would say I keep getting surprised how many places and how many ways PCIe technology is being used. I mean, maybe a year ago, a gentleman from MIT Lincoln Lab came to Electrical Workgroup asking for some clarification. And I invited the person to talk about what their applications are. So, they are basically trying to take Ethernet links and PCIe links to outer space applications. And that’s required some sort of consistency between PCIe and Ethernet and asking for some of those clarifications. And, of course, we know that immersion cooling and other things are taking some of the solutions inside liquid or under the ocean. And my excitement—my excitement about PCIe came when my son—when he was a sixth grader, he’s going to college this week—that when he opened his MacBook Air and somehow saw this PCIe storage device, he just ran to me, ‘This is what you work on!’ So, as a parent, that was my excitement that, okay, he at least found something, and he heard this word but never connected that he’s using them. So, so we actually see—and any modern car, like Tesla and all electric cars, have NVMe-based storage. So, PCIe is sort of everywhere, even though in your day-to-day life, people probably would not know what this is about.

So, with that little intro, let’s get and jump into some technical details—that’s what are the challenges, why we cannot do things that we have done for many years. As we go to higher data rates, take PCIe 5.0, which is 32 giga-transfers per second, and as the signaling—it’s 16 gigahertz. So, at 16 gigahertz, the copper—best quality copper—provides a certain amount of dB loss. So, what we have been trying to do in the PCIe SIG since PCIe 4.0—that, no matter what the frequency is, keep it dB per inch. That’s been my mantra. So, that way, we keep the PCB channel reach somewhat fixed—we can support the same thing. And what is really happening is keeping it the same reach is still not enough. Because when more compute elements—more GPGPU accelerate everything we put in—same system basically keeps growing. So, it’s going in the opposite direction. And then we need to connect box-to-box, and within rack—inter rack—top of the rack and rack-to-rack. So, all of those new types of architectures that require to make the full use of this compute power require new platform architecture. So, what you can see in this simple graph—if you look at the 16 gigahertz, even with the lowest materials, we are getting slightly lower than one dB, but it is becoming—typically, we used to use just vanilla FR4 traditional PCB—that will, in this frequency, about three dB per inch. And if it is a client product, that’s what there is still using it. So, because even an increase of one cent is not access allowed. So, they’re coming up with innovations to state. On the other hand, data center products must go and do the latest things. So, the PCB cost usually—um—in one generation goes a couple of X, and once it becomes volume, maybe it becomes 50% or two X more, but it still cannot provide those needs. So, that is why—what is driving.

And this was realized back in 2019-20—that the need for the industry to provide this type of flexibility and solution—PCIe SIG needs to step in and provide a better cable solution. And that’s when we started working on. But if you look at this slide, you’ll see that some of the cabling efforts started pretty early on with even PCIe 3.0. So, there has always been some applications—it was anticipated to be needed. And we had OCuLink, we have the PCIe Express External Cabling Spec, which is continuing from supporting PCIe 1.0 to—to 5.0, which is a fantastic job given that how many generations is speed up. And then we took over the Electrical Workgroup to look into solutions for 5.0 and 6.0—sort of leapfrogging that we did not do in the previous generation.

So, let me go a little bit over the timelines. So, if you look at—sort of—2003 to 2025, I have put some timeline there. The need for PCIe 5.0—when we used the so-called ultra-low-loss PCB material—means what used to be used in the defense industry. And I often joke that if—2013, 14 timeframe—if I would have proposed to any of our Intel product division that you should be using those types of material, I would be thrown out of the room, ‘You’re crazy!’ But it was pretty clear that people are no longer looking at just the PCB cost, because if you’re connecting some silicon which costs $50,000, the PCB cost going from 200 to 1000 is really not a big deal. So, the whole dynamics changed in the industry—that we just need to make the best use of the best silicon and interconnect at whatever solution cost innovation needs to come so that we make the best use of it. And that’s what is represented here. So, 2019-20 timeframe, we started looking into those solutions, and this year, the CopperLink—which is for 32-gig NRZ signaling and 64-gig PAM-4 signaling—has been published. And this is a joint work between PCIe SIG and SNIA, where we took all the good work that was done on the form factor, on the non-high-speed specification, reliability, and all of those, and combined that with whatever is needed to make it as a complete solution space. And from last year, we took up that we also need to standardize optical PCIe. So, what that does is basically almost give us a path towards not having any interconnect limit and have as much bandwidth as we can have. We’ll talk about that a little bit—a lot of work to do there.

So, this is just a quick summary of the External Cabling Specification. Sometimes, again, we have External Cable Specification for 5 or 6. So, this is the one that started in 2003 and continues. And as one understands, it was the storage which—even in PCIe 3.0 or earlier—needed those cable-based solutions because the topology was long, and it also enabled various types of architecture that storage needed. And we continue to support that. And this one—just, I think, recently got approved to have a Version 1.0 specification. And again, it uses SNIA SFF-8614—in the same spirit, PCIe has always been using various of the form factors that are well-defined in the SNIA and then make the best use of it for high-speed signaling in the PCIe.

This is a little bit of table summary. So, I do not expect—particularly in this late afternoon—to look at all the numbers. But again, in one place offline, you can review—gives you a little bit of history that when things are coming. So, again, focusing mostly—starting this year—we have accelerated the release of some of these specifications. And moving fast with optical to meet the demands that various of the technologies would be coming with PCIe 5.0, 6.0, 7.0—and that’s what. And again, I want to emphasize this collaboration. I mean, our colleagues here have—we have been talking offline and doing a lot of this technical work—but because both this standard body encompasses pretty much the industry representation, I think many volunteer engineers are putting their time to make sure that we are maximizing and building synergy between those works. And that, of course, as you saw, started all the way like 20 years ago and is still continuing.

So, in the next—probably—15 minutes, I will walk you through a little bit about the CopperLink specification. So, in this brand name that PCIe has created—for both internal and external specifications—and we will go through some details about those. 

So, the Internal Cable Specification is mostly—it is internal to the box. So, it’s connecting either chip-to-chip—they are located in the motherboard—or two different systems and cards. And it is addressing either the PCB limitations or giving more of a—more flexibility to build the platform architecture. On the other hand, External Cable is often box-to-box, but it doesn’t have to be long. We have seen that Google publicly showed their work where they are basically connecting a CPU and TPU sitting in another box, which are only 10 inches away. So—so it could be 10 inches, and there are other applications—which IBM—I mentioned their application goes to two-plus meters. So—so we will see all of those variations, and we have to consider in the specification—how do we bring some trade-offs when we define such a wide variety of applications?

So, this table—again—I’m not going to go and walk through probably every line, but the few points I want to highlight. So, in Internal Cable, we are taking SFF-TA-1016, which is often known as MCIO. That technology came from Amphenol, and Amphenol worked with the PCIe SIG to basically license those two to the broader industry. So, that is specification—by the time we picked up to do the additional work in the PCIe SIG—was already in Version 1.0. So, it was a pretty solid specification by SNIA. And that was what enabled us to accelerate the work. So, even then, I would say it does this two to three years because of driving—not the technical work—but the consensus across the industry. Because, by that time, already many companies are using those solutions with their own custom pin specification and so on. So, everybody making sacrifices and changing their course of action and agreeing to a common pin map—and all of those work is what we drove in the PCIe SIG collectively. And I really thank all of the companies for genuinely coming forward and making those negotiations.

So, what’s the objective of this CopperLink Internal and External Specification? So, when we started this workgroup, we had to discuss it very openly that—what’s the scope? Because otherwise, we cannot define what’s the timeline. So, scope was pretty clear that—as a cable assembly—is basically one channel sub-component of the whole end-to-end link. And when any particular company designs their own cable solution, it is straightforward for them to budget that—how much all other components get and how much the cable. And then they can negotiate that with the cable vendors to get their required things. How do you do that when somebody could be using a 10-inch cable versus a two-meter cable? And do you make this specification length-dependent? And all of those open questions came in the PCIe SIG. The way we drove those technical works—we’ll give you some examples that—how do we take this collective knowledge of this workgroup to say—what are the major applications, major drivers? And what characteristics will make minimum sacrifice on the system builders so that they can take this performance specification and decide what to keep for the rest? And that type of discussion and analysis helped us to do that. The other very important one is the sideband specification. Because, for example, SFF-TA-1016 has dedicated sideband pins. But various companies already use them for various purposes. And how do you—everybody then—agree that what should be the industry specification? So, I know that John, sitting here, drove some of those discussions, brought why they’re being used. And when people saw the data, the agreement did not become that controversial. But it still—I would say—took probably about a year to drive those agreements that—yes, these are the 10 pins, and this is how we are going to do it. So, similarly, pinout for the high-speed and sideband pins and also providing—as you go to such high speed—how do we make sure that we have truly accurate methods to do the compliance testing? So, we focused on some of those—we made some compromises—what should be optional, meaning that compliance testing is an informative spec. So, we expect the vendors themselves will take those best-known methods and do their tests themselves and show it to their customers. So, PCIe SIG will not slow down creating a compliance program.

So, in the next—probably—five or six slides now, I will take you deep dive into signal integrity, which is my favorite area of expertise. So, when we get into that controversial zone—that who should be making how much improvement in a full link—meaning that how much insertion loss budget the cable should be getting. And often the question I would be facing that—why not the motherboard does better than that? Why not the packages should do that? What is—why silicon is getting so much budget and not so on? So, that’s where we needed some agreed-upon topologies and do some analysis, bring those results in front of the group, and look at it together that—how much you want to sacrifice?　The other thing that is very important—as we have been mentioning—the one of the key value-add of a cable solution—that it is providing us 5 to 10x lower loss on a per-inch basis than PCB. So, which means that I can go probably up to 10x more inches compared to PCB. But the thing that often not talked enough is that—as soon as you bring a cable—it has to connect on-site, and the connector is not one simple cable. Usually, you have a paddle card—it goes from the raw cable and to the card, and it goes through vias, goes through pin field. So, I would quote Xenang once said, ‘Well, there are about 10 interfaces in between.’ So, when you bring those 10 interfaces—even though you are giving 10x lower loss—you are increasing reflection, and in those pin fields, you have crosstalk. So, you could easily get into a design situation where those effects at such high speed completely will wash out the benefit of loss, and you will have a failed solution. So, that was the key objective of the workgroup—that how do we bring agreement that—while we are getting the advantage of the lower loss—we are doing at a moderate cost and penalty because of those other issues?　So, this two topology—one on the left side, we are seeing typical Internal Cable topology. So, you can see there are three connectors—two connectors coming from the cable—and the External one is a little bit simpler. We are taking an example where maybe there are two retimers sitting into two boards, but they are going pretty far distant. So, we can do some of that activity there.

So, for non-SI people, I give this analogy—I mean, of course, I am here from California, so can’t avoid the beaches. So, if you think about it—if you think about reflection, what that is—is very similar to what—if you are in a beach where there are a lot of rocks, you will be seeing the white surf and a lot of that. And if there is a beach where it is calm and quiet, you will see very blue and nice waves—feels like pretty quiet. So, that’s exactly what it is for electromagnetic signals. More interfaces it will be going through means more impedance discontinuities it will have, and more reflection you will suffer—more crosstalk you will suffer. And how do you minimize the number of interfaces, and how do you control the characteristics of those interfaces—is basically, at the end, going to dictate how much benefit you get out of those cable assemblies as you bring them into new solution space.

So, next few slides—I am not going to walk you through all the numbers, but I want to give you a flavor. So, for example, in this three-connector topology, we are kind of highlighting some numbers in red. So, there would be variation of the lengths in the PCB because of—not all lanes or not all ports will have the similar length. You will have impedance variation in the PCB motherboard—impedance variation. You will have impedance variation in the package. And then you will have silicon variations on all of those. So, the often question that I face that—well—I mean, how do you know that the cable is the limiting factor? And maybe others could be more limiting—why are you not controlling them? And why are you not making PCB plus-minus impedance variation 5% instead of 10%? So, those are—we draw the industry experts that—yes—I mean, you can do that—technology is there. But it drives the PCB cost by 4 to 5%—5X. And then your trade-off with the cable solution would be a different ball game. So, those are our metrics.　So, we did various types of design of experiments. We tried to figure out what is an acceptable worst-case solution on everything except the cable. And then we make the cable is the variable to say—how much is acceptable?

So, I will give an example here. And to do that, we had to come up and develop some new methodology and metrics. And those metrics are contribution. And when I say ‘we,’ it’s sort of PCIe SIG members—I will specifically mention Lord, Semtech, Amphenol—some of the truly industry experts came up and borrowed some of the concepts from USB and Ethernet—and developed this metric called Integrated Return Loss. As if you are a sort of S-parameter expert, you will see that return loss and S-parameter is in the equation—there is a basically frequency-dependent spectrum function. So, it takes care of the bandwidth of the transmitter, bandwidth of the receiver, return loss of the cable assembly. And concept can be extended—and now has been extended—to other components like M-connector and so on. So, we can look at one number and say, ‘Okay, this number is good or bad.’ And how do we say whether it’s good or bad?

So, this is an example where we show the S-parameter of an Internal Cable assembly—that was designed for 32 giga-transfers per second. And x-axis is the frequency—so you can see gigahertz—usually we look up to 24 gigahertz—1.5x of the Nyquist. And this graph shows that I’m below the mask—lower is better—less reflection. And this is good. So, it is a lot more time-consuming and difficult to create a physical model of various amounts of reflection. So, what the previous formula does—it allows us to basically scale that and then generate the scaled S-parameters.

So, this slide—is that idea. So, IRL—Integrated Return Loss—was minus 24 dB was the original S-parameter that are derived from real physical connector cable assembly models. And once we have that, we have basically gone with this small signal variation type of work—that, okay, now go and change this IRL number by plus-minus 4 dB up or down and regenerate the S-parameters, and then put those into the full link analysis to see how things are changing. So, that’s the new methodology. And you can see—if IRL equals to minus 32 compared to the sort of the light blue one—everything is going down—meaning that it is getting better by 8 dB. If it is minus 20 dB, we just made things 4 dB worse in the other direction. So, we are controlling the reflection of this particular component. And then we want to see—how does the eye height and eye width—which is our margin—varies? And that will tell us what’s the sensitivity.

So, again—I mean—a lot of numbers here. But on the x-axis—this is a particular combination—as we showed in the original picture—we had sort of baseboard length and backplane length, add-in card length, trace impedance of the PCBs. So, we pick one out of a big DOE analysis that—this is the worst case. And then we fix all other parameters to the worst case. On the x-axis—on the innermost one—cable RL scaling—now we are just scaling it. So, what you can clearly see—at some point, there is not too much value-add by making it better—which means I can make the reflection almost negligible, but my margin is not improving much. On the other hand, after a certain point, it starts falling off very quickly. So, then it is basically driving some consensus that—we do not want to push the technology to a point where it becomes extremely costly and manufacturing-wise not feasible and still not get much value out of it. On the other hand, we do not want to fall on a cliff where—if I am violating it by a few dB, and I am immediately completely falling off the cliff. So, those types of analysis and agreement is how we define the spec. 

And you can see that same concept can be taken to define various such components. And that is what we are doing in defining—like—CEM connector response and so on. So, similar to return loss—which is basically reflection—crosstalk can be studied the same way. So, we took an example of the External Cable case. And the idea is very simple again—pick a certain worst case for that whole channel and basically set crosstalk to 0 for all components except the cable assembly. So, that way, we can look at that—if I vary the cable assembly crosstalk up or down from the physical model that we had. And the physical model—by the way—is again thanks to the vendors—they provided based on their PCIe 5.0 real design. So, it’s not made up or just a model only—it’s the real physical hardware they had. So, we took that, and then we varied them.

So, the similar idea—and we do those far-end crosstalk, near-end crosstalk, and all those details will not go over—but this one captures the idea. 

So, again—the red one is the physical model—we are showing S-parameter on the y-axis, and x-axis is showing the frequency. So, similar scaling technique that—if this whole crosstalk gets 5 dB better—so 6 dB is 2x—so if you go 6 dB and you improve it 2x—so if you go down, means you are reducing it by 2x—if it’s 6 dB. If you go by 6 dB on the opposite direction, you are increasing the crosstalk by that. So, that will tell us—for this particular channel—as we change the crosstalk—improve it or degrade it—how does the margin vary? 

I know that this is a lot of data, but one thing I will point out—if you look at on the right side again—so the main thing to look at—we say original is the physical model. If we improve it by minus 5 dB—means I am reducing the crosstalk—and minus 10 dB—I am reducing the crosstalk. If I make it worse—plus 5 dB, plus 10 dB—then I am making it crosstalk increasing.　So, again—it is pretty clearly seen what type of crosstalk is making the biggest difference. Because if the crosstalk is coming from near-end—so near-end crosstalk is defined when a receiver is getting crosstalk from a transmitter who is sitting nearby—so from that idea. And then far-end crosstalk is where the aggressors are both far from the receiver, but as the crosstalk keeps propagating—that crosstalk increases and affects this thing.　So, this type of analysis gave us two things—one is it gives us who is impacting how much, and also what is the point of not much benefit. So, you can see that—if we increase it by 5 dB—we are already starting to fall off very sharp. On the other hand, if we improve it by 10 dB on the other direction—it doesn’t change much. So, that tells us that—where is the optimal point that is practically doable by the vendors without blowing up their cost, but at the same time, it is a penalty system builders are willing to take? So, those types of—again—mutual work and analysis helped us to eventually define the specification.

So, in the next few slides—I’m going to go a little bit faster—um—mostly—again—as I have mentioned—the difficulty of this part of the work for the PCIe SIG was driving into an agreement that made technical sense and also—at the same time—something people saw the future extendability of—of those functions. 

So, for Internal Cable—it is—um—mostly a passive cable—there is no power—it’s a full crossover from side A to side B—and we went into some details that—there—this is the sub-team that focused on the sideband—came up with the idea that—we have sufficient pins—particularly for x8 and x16—that we define some baseline functionality that every option will have, and then some extended functionality that could be used for bifurcation—for other purposes—and—and that’s the key idea. 

So, I will not probably walk you through the pin map—but again—you will see that some of those sets—particularly anything—we did some color-coding here to show that—we will be using some of the gray color for the sideband and other blue and purple colors for high-speed signals—

So, the extended sideband pins are—again—for future extendability—but what we did do there—that—if—let’s say—those pins are used for an x1 PCIe sideband signaling—how those pins will be assigned—so there is no confusion if it is used—you—so—similarly—it could be—some of them could be used for USB 2—and then—we say—which one is transmitter pin, which one is receiver pin—so we tried to make those—and—and all of that came from a lot of those collective—I would say—expertise of the group.

Let me jump a little bit on the External Cable—so External Cable is not fully passive cable because External Cable needs—and they basically have a black-box cable management—initially—our goal was—it is mostly a passive copper cable—except we’ll have a 3.3-volt power for cable management EEPROM—late in the development phase—Lightelligence—another team—or likes—came—they wanted—and a VCC of 12-volt pin—so—so we kind of dedicated one pin—but if it may need to be active—it can be used—so that has been used—has been used because it has a power—so the high-speed pins are crossover—but the—the other pins are not because of the functionality—but the high-speed Flex IO pins are still crossover-type—yeah—so that x1 PCIe can be used as a sideband signaling—

Let me go—so these are just examples of pin maps—how it is done—so I’ll skip them—probably—I’ll now go towards a little bit talk about optical—so this—this is the one which mentions this compatibility and future flexibility—this is our CMIS work—is this coming—and through this—there is a memory map table in the specification that will go through some of the details of the interface—

So, overall—again—what are all these areas—most of us know that PCIe 5 is used in CXL—so many of the CXL full-memory applications—potentially—will be using—because this is just a physical element—various CPU-GPU applications—anything that are limited—mostly—either by PCB-based architecture—are potential users—and that’s a pretty wide, broad field.

So, on the optical—just—the Optical Workgroup started in last year—August. The high-level goal has been—the Optical Workgroup started in last year—August—the high-level goal has been said that—PCIe SIG will not take a particular position on which optical technology should be used—and all of those details—because there are a lot of innovations happening—a lot of things are well-established—others are coming—and PCIe SIG should try to just help make changes in the PCIe specification that basically standardizes optical implementation. So, that’s the approach we are taking—and we are keeping a lot of the traditional PCIe features—but adding that—RX detect is something different. If you want to send sideband signaling—you don’t have to have the dedicated pins or optical fiber—so all of that work is ongoing.

And the current focus on the optical side is based on two retimers. So, the picture on the right that you’re seeing—it’s basically—we call it kind of Optical Retimer—so the one that is facing towards the root complex and non-root complex—that part is traditional electrical—so everything is as if root complex and non-root complex doesn’t know that there’s an optical element to it. The ECN will provide all the recipe—what this new sort of Optical Retimer needs to understand—and then do the communication with the optical module. And so, the whole optical fiber—EO—and all of that—will be part of that—and that is hiding from this. So, this way—we can bring the optical standardization to PCIe 6.0—because 6.0 is already out there—products are—some shipping, some designing—it’s too late to make those changes. But PCIe 7.0—probably—will start making some of those updates into the logic—for definitions into the base root ports for the non-root complex and non-root complex—and then it will ease the transition that way.　So, this is a two-retimer-based ECN is right now—our target is the end of the year. And then we will be looking into—if we have no retimer—how does that optical get work? I mean—how do we decide how much electrical should be taking, how much optical should be taking—and those types of things? Yeah.

Thank you. So, that brings me to the end. And again—thank you so much for all your patience and great questions! But I’m open for any further discussion and questions.
