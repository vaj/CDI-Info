
All right, I'll get started. So, my name is Jon Hermes. I'm a systems software researcher at Arm in the Architecture and Technology Group. Our charter is to look up ahead five to ten years. We're trying to determine what, if anything, is going to need to change in Arm architecture or what disruptive technologies are kind of on the near horizon that we're going to have to deal with.

So, let me start with an easy statement: Disaggregated memory is very important. Everyone is aware that memory costs are dominating server costs. They're growing. They were growing before AI even took off. So, this is a large problem. We've looked at CXL-attached memory, and we see kind of two major benefits here. The first is that I'm able to pool my memory; I'm able to scale my memory separate from my compute, as opposed to having to do it all at the same time. And as you've heard from others today, there's this possibility of memory tiering, something that is a little bit farther than main memory but not quite as far as disk. And kind of the last supposition here is that this dedicated, disaggregated, far memory will be less expensive than host memory—not necessarily in a DIMM-by-DIMM way, but because we've simply run out of pins on the host. I can't add any more DDR channels, so I have to do something else to support the capacity that's needed. In this case, attaching CXL over PCI. So, in this future, we have two major questions: How are we going to design systems that can use this memory efficiently? And how is anybody going to program for these systems?

So, at the start of this year, we published a paper called "UDON," which was Understanding Distributed Offload Networks. And this was our first foray into investigating CXL-attached memory. We didn't have testing hardware; we were emulating this by using a two-socket system. The far socket had been slowed. We're imagining that it has a smaller core for power reasons. And, going across NUMA here, we're inducing a memory delay on every memory access. So, in this emulation setup, we wanted to figure out: how can we optimize throughput? How can we optimize performance?

Okay. And we want to optimize performance in two ways. We want to use as much of this far memory as we possibly can for cost reasons. And we want to minimize the runtime of the application. These two goals are in conflict with each other. So, you have to bias one way or the other. We ended up writing a memory compute placement algorithm that would try and not just do data placement, but also be able to do compute placement onto this CXL-attached memory, assuming that there is some small amount of compute available. We fed in AI models. The models themselves, as well as profiling data, and the output of this placement algorithm is an annotated AI model. So, each layer pictured here, you see that we are dissecting whether to place the compute locally or on this emulated attached memory device. We see the weights, the large static read-only data. Does that deserve to go into main memory, or does it live in this attached memory? Similarly with intermediate data. This is the partial compute results through layers. Should that live in main memory or should that live in this attached memory? We then modified TensorFlow to take annotated models. So, when designated, we would either put our memory in what we would call the host socket or this far socket. And similarly, we would move compute with thread affinity.

And what we found here, for this graph, let me explain it a bit. The X-axis is the total runtime. So, we want to be as leftmost as possible. And the Y-axis is how much host data is being used. We want that to be as low as possible. We want to use as much far memory as possible. That upper left point there you see, that's kind of the baseline. There's no attached memory. We get kind of optimal performance, but we're using, in this case, seven and a half gigabytes of host memory for this inference. We really want to get down into the bottom left. And using that placement algorithm, kind of biasing how much we care about the placement versus the runtime, we can make an annotated model that places some compute far and most of the memory far. In this case, we found optimal use looked something like it would run 20 percent slower, but virtually all of your memory would be living in this new tier, this attached memory tier. Anyway, the work that we had done last year and published at the start of this year, and all of this is assuming that I have one device and one host that's directly connected. What would we do instead if we had a multi-headed device, if we had multiple hosts that were all able to connect to a shared memory pool?

So we could use that same algorithm, the one that we had designed, and if each host is acting independently, nothing is shared, nothing changes. We would have the exact same findings. But in a multi-headed device, we have these two new benefits: we can reduce communication bandwidth and communicate through this attached memory. And similarly, we can start looking toward deduplication. That certainly, in the AI use case, if I have a lot of inference servers and they're all doing the same inference and they're all using the same weights—this is static, read-only data—there's no reason to have so many copies of it. Our kind of mental model here, we don't use hotel terminology. We call it a pod. But similar idea: all of these hosts, all of the ones connected, some small n, four or eight hosts or so, connected to this multi-headed memory device, they all act as one unit.

All right. So, talking about those communication problems, if you're familiar with primitives, this is called an all-to-all communication problem. This is an n-squared problem. If I'm able to group my hosts into pods, then I can potentially reduce this all-to-all communication cost by a factor approaching pod size squared. Okay. So, the graph down below is an estimation here. This is RDMA, which is to say, just every host directly, or a network connected to every other host, versus going through this multi-headed device. This is assuming a 500 nanosecond delay to multi-headed device. So that is your cost going through a CXL switch compared to, in the best case, something like a five microsecond delay across RDMA. And similarly, deduplication of any shared objects could be outright cut.

I wanted to focus on one specific use case for this—um, for AI in training. Very commonly, you're going to shuffle your inputs. Repeatedly shuffle your inputs. And all of this is so that you don't have an ordering bias, that your AI is not being trained based on the order that the training data was fed into it. Shuffling is an all-to-all problem. As I had just mentioned, this is an N-squared problem. This is very bad. The best, the current state of the art for shuffling, is ExoShuffle. There was a paper on that last year. And ExoShuffle was implemented atop Ray, which is a distributed futures or a distributed compute framework. If you're familiar with things like Spark, that's pretty similar as well.

And I do have to talk a little bit about Ray. Sorry, there's a lot of software here. So, Ray is this distributed compute framework. Each node, each hardware node, will run one Raylet. Raylets know how to speak to other Raylets. Raylets know how to start and stop tasks. They have a local object manager, which manages all of their semantic data. And they do their own scheduling. There is one point of centralization called the global control store. This is mostly just used for metadata. This is not in the critical path. Raylets communicate resource status to this GCS. And there is such a thing as a global scheduler, but it's not being used in ExoShuffle. So I can come back to it later.

Ray does write-only data; write-once, read-many. So, in a write-once pattern, the worker that does the task writes that object into its local object store, and that's its permanent location. That's where the original copy is located, the location of record. And anyone else can copy it if they want, but that's the original. Because all of these Raylets are doing independent scheduling, they first see if they can do this task on themselves. If not, they go out in a kind of locality-aware way. They look for the next neighbor who might be able to take that task. And if so, they'll complete it, and they'll write the resultant object to their local store.

So, we wanted to design a system that designed some way to force these objects onto shared memory, onto pooled memory in a multi-headed device. In Ray, there's not really an easy or obvious way to transfer ownership. As I mentioned, it's write-once. When the worker does the task, he writes it to his own local memory, and that's where it lives forever. The only exception to that is what they call fallback allocate, or memory spilling, or as everyone else would call it, swapping. So, if I've run out of memory locally, I can go to disk. I can go to this farther tier of memory.

So, we inject our changes here. We write a custom fallback allocator, or swapper, that knows about the shared memory device. Each Raylet then has a relatively small amount of memory, and most or all object creation is going to go onto this multi-headed device. Everything in Ray behaves as normal. We don't have to make any changes to its consistency guarantees. We don't have to make any changes to its scheduler. There's space there for each node to make local copies of a lot of the objects, but mostly they live in the multi-headed device. When we share objects between Raylets within this pod, they're then reading from the location of record, which is in the multi-headed device, which is nearby. And if you have to go to a different pod, then you are going over RDMA. Then, it is network access, even farther tier.

Looking forward, the next thing we want is we want those control levers, like I had mentioned with TensorFlow, where I'm able to pick and choose where the compute is running, where the memory exactly is located. I would like to be able to differentiate hot and cold data, kind of more traditional data placement algorithms. I would want to say that the local object store for the Raylets holds the hottest data, and the multi-headed device holds colder data. Similarly, I had mentioned that ExoShuffle does not use the global scheduler. This is a newer design for Ray. But we would like to use a global scheduler. And ideally, we would like to run the Raylet, on the network, on the multi-headed device. It has compute there. Not a lot of compute, but like in the Udon case, enough to take over the most memory, the most latency sensitive, and most bandwidth sensitive operations.

So overall, what have we found? Data placement and compute placement are both important. There are many papers, many efforts, on purely data placement. Those are good and important. But compute placement, we feel, has an important role to play here as well. The programming model for these devices, for all of this CXL attached memory, in our opinion, is the largest hurdle. It is difficult to figure out how much code is even going to need to change to support this new memory tier. A good example is something like Ray, a distributed computing framework. If there is a way to make generic changes there to support far memory usage, memory tiering, without having to change the application, that's golden. There are many millions of lines of code that we don't want to make memory location aware if we can avoid it. And then lastly, kind of the open problem, the things that I would love to talk more about. In this model, the host and the device were sharing addressing. They were using shared memory primitives. That requires full coherence. The coherency model, we feel, is a fairly open question. Full coherence may not be necessary. Similarly, computational offload, if I'm going across ISA from an x86 host to an ARM core on this device, or vice versa, it requires special care. You have to do something else. You have to either recompile JIT on the far side, something. And then I mentioned deduplication. We didn't have the time to look into that yet. We want to see if there is a generic way to do duplication here.

So, thank you all. I'm Jon Hermes. Anyone else that wants to duplicate these findings, I'd love to talk to you. Any memory vendors that want to test this on real hardware, I'd love to talk with you. And lastly, I'm always available in the CMS working group. Thank you. We have time for questions.

Can I just get one question in? John, you talked about collectives. Off we go. We'll get help if you do shared memory. You talked about all-to-all. Did you have a chance to actually run them and see what kind of bandwidth reduction we could see, or is it, at this point, just an idea?

You're talking about the collectives from a specific? I'm talking about collectives in a more general case.

OK, I see. You're talking about n squared, basically, all-to-all. With the shared memory, what number are you targeting? Does it become 2n? Or does it still remain n squared?

It's still  n^2 . It's still order  n^2 , but it's  n^2  in this case, reduced by a factor of 64, which could be enough to get the problem to fit, hit some QoS target.

OK, and any other collectives improvement—if you think from the near-compute perspective—all-reduce, or anything that you're thinking in this context?

All-reduce could be improved as well. I didn't have a collectives thing planned, but yeah.

Thanks.

Thank you.

Yeah, just a mini follow-up from the last point. If you could please give more color on what NMC ops were involved, precisely in terms of the arithmetic operations that need to be done at the ALU level.

Yeah, so in the UDON case, we were focused on near-memory compute and compute placement to these devices. In this model, that's a general ARM core, that's general-purpose compute. So, it's running these various ML model layers. So this is convolutions, matrix multiplications, things like that. They're not something. They're not just like an ad, necessarily.

All fixed point?

We tested different models. Some are fixed point. Some are variable integer.

Yeah.Thank you.

Yeah.Thank you.
