
Hello everyone, thank you for attending the CXL Consortium's Breaking Memory Barriers: CXL's Game Changing Impact on AI/ML webinar. Today's webinar will be led by Steve Scargall from MemVerge. If you have any questions, please feel free to enter them into the question box, and we will address them during the Q&A. With that, I will go ahead and begin the webinar.

Thanks for the warm welcome there, Nolan. Appreciate it. So, I have a fairly packed agenda for today, including the announcement of the recently released 3.2 specification. I want to kind of walk through the state of the ecosystem now that we're into 2024, kind of cover what we've seen over the past year and what we expect to see coming in 2025. Then, getting into the meat of the talk, we're going to talk about the importance of memory for AI and ML workloads and where we can store the data using CXL shared memory. The memory placement and movement policies; then we'll get into some new things like talking about dynamic capacity devices, now that they are real. And then some of the tools of the trade when it comes to benchmarking, particularly on the GPU side, again focusing on AI and ML workloads. And then I'll really get into the use cases and workloads that we've demonstrated in 2024 using CXL, specifically for AI and ML workloads. And then we'll end with a call to action and hopefully we'll have time for Q&A.

So, like I said, on the 30th of December 2024, so just a few weeks ago, the 3.2 specification was released. Links are at the bottom of this slide to the full documentation and to this announcement. I just wanted to highlight a few key things of the 3.2 version. First of all, we got CXL memory device monitoring and management. So, from this, we get CXL online firmware activation, and particularly interesting to me, and what SK and MemVerge have showcased at recent events, the hot page monitoring unit. So, this is the ability for the device itself to advertise, up to software, whether that be kernel drivers or user space, the hot areas within the device itself. So then, we can start to build more intelligent memory tiering and memory placement algorithms and solutions using this new telemetry. Additionally, we also get some RAS features, so the post-package repair, and there are some additional performance monitoring events for uh CXL memory devices, so that's in addition to the CHMU stuff, and then of course we got improvements to security with the TSP protocol.

So, let's talk a little bit more about the state of the ecosphere. Like I said, we're at the end of 2024 now, heading into 2025. So, I'll show you where we are for Linux, some of the devices, and some of the virtualization technologies.

So let me give you a quick update of where the Linux mainline kernel is and the QEMU virtualization are. So, along the top is a rough timeline of the CXL specifications lined up with the Linux kernel and QEMU version. So, we we saw that, both promotion and demotion were put into 5.18, 5.16 had the promotion code. Then we got weighted interleaving in the 6.9 kernel, more recently in the 6.12 which is the current kernel release as of the time of this webinar. We now have phase one of the DCD support, 6.13 and beyond we'll add some some more features and functionality for DCD, but it's great to see this feature is now being added. And both MemVerge and SK have showcased this running in in real hardware, and I'll show you that later on in the webinar as well.  On the QEMU side, 9.2 is is the recent release as of December 2024, that was mostly bug fixes, so no notable additions for CXL at least, but their roadmap shows that in 9.3 we're also going to get DCD device support as well, so at least maybe phase one of that. So, so yeah, everything's kind of moving forward here. Pretty much everything that's in the specification will be added to both the mainline and QEMU. So, early 2025 as we start to see new releases from from both sides, then you'll start to see the new features and continued development and bug fixes and performance improvements of previous generations of of the CXL specification.

Let's just cover briefly the three main use cases for how applications are going to access memory at scale in data centers using CXL. So, on the left, we have memory expansion. This is where we're adding devices into servers locally, and then we can optimally use either Linux TPP or weighted interleaving or MemVerge memory machine to do the transparent placement of memory. And then we get into moving that memory outside of the house, and then we get into memory pooling and finally memory sharing, where we're now able to access that same physical data on the same physical addresses for multiple hosts. Now, this does require that we have some kind of shared memory, either object or file system. Again, I'll cover those later on in the webinar. And the application itself is shared memory aware, which today would require some modification for the application code. But at least from from the data center perspective, the memory appliances could be direct attached or accessible via a switched fabric.

So this is my software architecture view of how Linux and the applications, whether they be modified or unmodified, will be able to access the CXL devices. And I’m pleased to say that as of today, we’re kind of pretty much well covered on the device types, anywhere from memory expansion—whether they be adding cards, whether they be E3.s—through to dynamic capacity devices, shared memory devices, persistent memory devices. And these can also be, you know, locally attached, external switch, direct attached, etc. Additionally, some of the vendors are also talking about adding device compression and additional telemetry that we can get out of the devices, and that’s in addition to the CHMU, so the hotness piece as well. So, so all of all of that is is definitely coming or it’s already been implemented. And then from the Linux side, pretty much everything we need is covered with the exception of the persistent memory drivers that would then allow us to to put on a file system like ext4, XFS, and access the data through the the block or file interface, or directly using FsDAX. But the new thing here is that we’re also, you know, been talking about file systems and object stores, particularly on the shared memory side, so we can do coherency from the software perspective while we wait for hardware coherency to arrive, and then above that, you know, applications can either use middleware such as the Memverge Memory Machine, we’ve got some software developer kits you can use, NUMA control particularly for unmodified applications as well.

And just to finish off this section, I've listed a long list of useful links. You're welcome to go there. A lot of them are updated regularly with new and useful information—both as Linux kernel and QEMU developers are talking. You can look at new patches, new features, etc.—so a very useful set of links there. You're welcome to follow them and ask questions to the community.

So, let's then talk about the importance of memory, particularly with the focus on AI/ML workloads.

If you're a developer, I'm almost certain you've probably hit this 'out of memory' error. And mostly the reason is because not necessarily of your code, but more on the training side of AI—I guess an inference as well—of how the models are using memory, particularly on the GPUs, right? And whilst you do have some control over that, really that comes down to how the models and applications are run/implemented. There's a great memory snapshot tool that's integrated into PyTorch that allows you to visualize the memory utilization, look at different areas, and things like that. So, the figure here is showing three training iterations, and you can see how the memory balloons—you're doing the forward and backward passes. So, a lot of this is why we need a lot more memory in our systems, particularly on the high bandwidth memory for GPUs, when doing training and inferencing.

When it comes to the memory requirements for training and inference, obviously, we're going to require hundreds of gigabytes if not terabytes of memory or more, both on the GPU side and on the host side. You know, some of these large language models are using a hundred thousand GPUs, so the data's got to come from somewhere, right? And with that, the training datasets often will exceed the available DRAM capacity of certainly a single node, if not a full cluster as well. So there are a lot of techniques, and I'll go into that in a minute, of you know, model parallelism and data sharding that try to help when when we're limited by memory. But additionally, it, we've also got to get the data from wherever it's stored; it could be in the cloud; it could be on-prem; it's definitely going to be some persistent storage, and we've got to get that through the training framework and into the GPUs where it's going to be processed, and then the results sent back to the application. Certainly, most AI operations are going to be memory bound both bandwidth and capacity, rather than being compute-bound. So there's this critical requirement for data transfer between the main system memory and and GPU. So because of that, the GPUs are often stalled or starving for for data as we're not able to move that data optimally between where it is and where it needs to be, and that's why we see the increase in the number of GPUs because once the data is in the GPU cluster, it often stays there so you can compute on it before you send the results back.

and here's just a relatively short list of a very much longer list of the tips and tricks, and techniques that I know that people are using to uh, kind of around the out of memory, and solving the memory wall problem. Uh, first and foremost, the one that I hear the most of, and it's the most expensive of course, is people are just buying the GPUs, mostly for the memory, not necessarily for the compute that's available on the GPU, so they're building these very large GPU farms predominantly for very high capacity, high bandwidth memory. Other tips include the quantization, so if, if you, you, if you can sacrifice some of the resolution of the models you can go from floating point 32 down to floating point 16 or BF16, and then if need be, you can go with the quantization of the models. You can go even further and go into the integer realm as well. Of course, this significantly reduces the amount of memory a model requires, but the sacrifice is that your your resolution, your results are not going to be potentially quite as accurate. There are also hybrid models where they do mixed precision training, so for certain parts of the model, they'll use a high precision and then for others, they'll use a lower precision as well. Again, this this should reduce the footprint requirements for for the model. Models don't fit inside of a GPU, and they typically won't, or they won't fit inside of a single host, maybe that has four or eight GPUs. Then there's a whole bunch of parallelism techniques where we can do model data tensor parallelism, and then then you can get into the distributed data parallelism and fully, fully sharded data parallelism where we're doing multi-node compute in a GPU fabric, and then there are other optimizations so we can take some of the data that would naturally reside in GPU like your KV cache and your retention caches and more, and and do what they call offloading, so we're moving that data from GPU back to the main system memory, and then we can access it there, do some CPU computing, or we can move it back into the GPU upon demand. Also, there's selective activation checkpointing so we can checkpoint some of the data and move that around as well. Using a small base model is also becoming beneficial now, particularly as the smaller models are becoming almost as accurate as the larger cousins. We can also trim some of the unused weights or least frequently used weights, certainly, and that's called pruning. We can also reduce the batch size or use a gradient accumulation, also called mini-batching to to help with the memory constraints here. The memory frameworks or the the frameworks rather, PyTorch and TensorFlow, they're also having additional optimizations put in, again mostly around the offloading side but also on compression and things like that. So as you're developing your application and doing training and inferencing, the frameworks actually give you quite a lot of tools to to play with that that help for your situation with with your hardware and optimizing costs. Um, and then lastly on the list, but by no means least is is low-ranked adoption. So this is kind of like taking a git diff of all the different weights, so if everybody's sharing the same base model, you don't have to train or update every model for every user. You can just take the deltas of the weights that uh, that they they require, store those, and then as they want to run their application, you can put that back onto GPU, let it run, and once they're done, you can move it back off. So you can actually put a lot of users on the same model simply by identifying the the differences between what they want in their model versus the base model, and that that's a huge optimization for for getting throughput as well.

So, where does the CXL fit into this? Well, we talked a little bit about this earlier, right? If we're able to add capacity to each individual node, not only adding capacity but also bandwidth as well, so we, we can start to use some of that as we offload uh some of the stuff from GPU back onto the main system. And we're able to move that faster, utilizing the bandwidth additions per call that CXL provides. Again, I'll show you that when it comes to the bandwidth tools. The DCDs allow us to accommodate varying memory requirements on the fly without having to pre-know what requirements you have. So, this might be useful in particularly an development environment or an inference environment where you might exceed the DRAM capabilities, but you still have compute available, compute, and GPU. So if we're able to allocate main system memory on the fly, we can bin pack those use cases and workloads more efficiently. On the shared memory side, of course, if our data lives in memory that is accessible directly from each compute node, then of course we don't need to move it around. And a lot of the data movement, particularly when data is in the cloud and you're doing compute there or you're doing compute on-prem or in the cloud or your data is in the other place, then we can eliminate all of that. And again, I'll show you some use cases and workloads that show a good improvement in this area. And again, offloading that KV cache, the LORAs, and other caches to main system memory can certainly benefit the GPUs as well.

So let's take a look at the in-memory data stores.

So today, there are two dominant solutions, MemVerge GISMO and FAMFS from Micron. Both aim to store data in a CXL shared memory environment. And we can do this in a CXL 2.0 era, so we don't need the back-invalidate just yet because we're handling the coherency in software. So, potentially not as optimal as hardware, but I think still, when we get into the 3.0 era, that we will need software driving the coherency in addition to some of the hardware coherency benefits that we'll get as well. But the main difference between GISMO and FAMFS is that GISMO is an object store, so you can access the data using object interfaces like put and get. FAMFS is a file system, so if you're used to file interfaces like you open, you close, you read, and you write, then you would choose FAMFS. Neither are currently persistent, just mostly because we're not dealing with persistent media behind the scenes. We're just dealing with memory that's accessible to us either through a direct attached appliance or behind a switch.

So, how are we dealing with memory placement and movement?

So there are two policies available today, one being latency focused, the other one being bandwidth focused. Latency has been around a little bit longer, so in the kernel it's known as the transparent page placement or TPP. And that has been available since 5.18 with the promotion and demotion code and MemVerge memory machine, of course. And the intent of this policy is to monitor and move pages based off of their temperature relative to others. So we're trying to make sure that the data that is most frequently used is available in the lowest latency tier. So in the example here, it will be moving the hot data to DRAM, and if we exceed the DRAM capacity, then the cooler pages can be moved over to CXL. They're still byte addressable, they're still accessible to the application. So as long as your hit ratio is mostly in the lowest latency tier, application performance doesn't suffer too much here. Over on the bandwidth side, this is the newer policy. On the Linux kernel side, both MemVerge and SK Hynix upstream this into the 6.9 mainline kernel. And again, it's been around for a while. It's been available in the memory machine for a bit longer. But here, we're not necessarily focusing on the temperature or hotness of the data. We're simply trying to load balance or stripe in terms of storage the data between the various memory devices that we have available to us. So in this example, DRAM and CXL. So you can specify the ratio, and the ratio would be determined by the performance characteristics of your DRAM versus CXL. You can specify things like 90:10 or 80:20 or 66:33, for example, dependent on the application requirements and, of course, your hardware setup.

Let's talk a little bit more about that on the weighted interleave side. So if you plot latency versus throughput, which is the chart in the top right there, you get this knee in the curve where if you start to saturate throughput, then your latency skyrockets. It's pretty obvious. It's been known for a while. So what we're trying to do by adding... we're adding more CXL devices to the mix. We're trying to move that knee in the curve as far to the right as we possibly can. So in the example below, if we have five devices, then I could set up potentially DRAM as being 50% of my weight. And then I would equally split the rest of the data across the five CXL devices. So 10% each. So if I was to write 10 pages, then five would be written to DRAM and then one to each of the CXL devices in turn. And then we'd come back around and keep this ratio. So that's pretty much how the weighted interleave works. It's based off of the number of page allocations, and then we just keep going around in a loop.

Additionally to this, so in 6.8, we have this thing called quality of service. So this is the ability for the device itself to advertise what its read and write bandwidth and latencies are. And then that can also be used when looking at the latency. Policy is, I'm probably weighted interleaving as well. So we're able to understand a little better about what the performance characteristics are of the device and devices in my system without having to do bandwidth testing or benchmarks on my devices, and then plug that into my latency or bandwidth algorithms.

And like the other section, I've added some references here. So you're welcome to go read more on the weighted interleaving and, of course, the kernel documentation is very extensive and very thorough. So it gives you lots of good information about how this stuff works and how you can configure it, and then how you can use it.

I mentioned earlier about how dynamic capacity devices are new and pretty cool, and they certainly are. So, let's talk a little bit about DCDs and how we can provision memory on demand without making any hardware changes.

So, like I said, you know, DCDs are thin provisioning of memory, meaning that they sit out there in a memory pool connected to one or more servers, and then on demand, runtime, we can then provision that memory to one or more hosts, allowing us to scale the memory capacity on demand, dependent on what applications are running there. So, you know, this is highly efficient for large data centers because you don't have to send technicians to the physical server to go increase or decrease the physical memory here. We can, we can do it on the fly from the keyboard. And that's exactly what we'll show in the demo here.

So here's the setup that we did. So we work closely with SK Hynix. SK Hynix kindly offered us Niagara. They implemented all the DCD features in the firmware. And then on the MemVerge side, we wrote the DCD agents. So these little things that kind of sit on the host and monitor the amount of memory that's currently being allocated. And the agents then talk to the composer, the orchestrator effectively. And between them, they can orchestrate. And request memory from Niagara if need be, or release it back to the pool. And that's just based off of a simple threshold mechanism right now. So if we see the trend is rising and you cross our threshold, we'll go and request more memory. And then when the memory is freed up and things like that, we can release it back to the pool. So the only additional piece here is that we created and wrote a memory allocation piece for Kubernetes using the new DRA or dynamic resource allocation feature—that's now beta in the 1.32 release of Kubernetes. And what this allows us to do is, traditionally in Kubernetes, when you start up the cluster, particularly the Kubelet on each node, it just looks at the number of vCPUs, cores, and amount of memory on the host. And that was then fixed for the lifetime of Kubelet. Since we have these dynamic capacity devices, that doesn't work for us because Kubelet would only look at the memory that was available to it at the start time, not runtime. So DRAs allow us to extend this capability for memory, and the focus was GPUs, of course, with heterogeneous composable GPUs. But we're able to use the memory agent that we wrote, and now we can tell Kubernetes—hey, you can request more memory, or the DCD agent, or request more memory when launching new pods, so that we don't run out of memory before we run out of compute. So that's what the next demo is going to show here.

So this is what we showed at Super Compute. So the topology diagram is on the top there with the Niagara in the middle and two Sapphire Rapids systems, each with a terabyte of DRAM, though for the purposes of this exercise, we don't use DRAM other than to run the operating system. Again, we've got Kubernetes with the DRA piece, and then the MemVerge Composer and the agents that run on each host. So what's happening here is: top left is the available capacity within Niagara. So Niagara offers 64 gig that is shared; they're available between both hosts. So lower means that we have used more memory, and higher means there is memory available. The chart lower left is the allocation of memory from Niagara to each of the two hosts. So that's why there's an orange and a blue line, one for each of the hosts. You can see that we're allocating and deallocating memory, again, more aggressively on the allocation and less aggressively on the deallocation. Over on the right hand side, the orange and the blue lines represent the number of pods that are running on each of the hosts. So the x-axis is the time, y-axis is the number of pods that we are currently running on each of the hosts. So you'll see that change as time moves forward. And then the duration of each pod is also shown there, along with how much memory each pod has requested. So either two or four gig in our example. So we just have a very simple script that starts a number of pods and randomly stops some and then restarts some, etc. So you can see that we've got some very long running jobs on both hosts, that's very short running jobs that kind of represent what a traditional Kubernetes environment would go through. And again, we're just using the memory from CXL at the moment, so we're not running any benchmarks here. Each pod is just running a very simple memhog just to prove that we are indeed touching the memory and using it. But the functionality of this is that DCD is a reality and it does work, which is phenomenal. So, yeah, really good to see and demonstrate this at Super Compute. So if anybody has any questions, happy to answer some more here.

But, so let's switch gears, and I want to talk a little bit about some of the benchmarking tools that I use when trying to understand the bandwidth and throughput characteristics of GPUs when data resides in main system memory, that is either DRAM or CXL.

So I know many of us are familiar with application benchmarking using CPU and CXL and DRAM, you know, tools such as Intel MLC. We've got some in the CXL bench and of course, you know, pick and choose your favorite application benchmark there. So that should be fairly obvious. What I wanted to focus on for this slide was the GPU side. So NVIDIA has a tool called NVBandwidth that was recently released open source in GitHub. And there's another one that was released a little earlier than that in the CUDA examples repository called Bandwidth Check. The effects really tried to do the same thing. That is to move data from GPU back to main system memory or main system memory back to GPU. So I've been looking at this from a CXL perspective. So the tools are there. The observability tools are on the bottom. Obviously, we've got Memory Machine X there. You can use Linux tiering and bandwidth features as well. But the matrix on the right is typically what I do. So if you've got one or more GPUs in a system and obviously one or more CPU sockets and CXL devices in a system, you can build out a very simple test matrix per socket. And there's more information on my blog that's referenced below there.

And here's just some examples of showing both tools, both showing that you actually get the same same result. I had to patch the bandwidth check on the right-hand side to get it to do what I wanted, but ultimately we're able to to show that, yes, indeed, we can get significant amounts of bandwidth through to this older, I think this was an A10 GPU. So, not a not a huge modern GPU by any standards. But the fact that you can do this now is pretty cool. The only thing I would say is that you probably need to learn a little bit more about CUDA programming. If you rely upon the host or the kernel to manage memory pages, then you'll take a slight penalty hit for that overhead. So, there's a thing called unified memory where the memory pages in the host, whether they be on DRAM or CXL, are actually pinned so that the GPU can do DMA operations. And whilst that does significantly improve the throughput that you get, the trade-off is that you can't do memory tearing because now the data is pinned and you can't move that regardless. So even if you identify data in CXL that's hot, you can't move it to DRAM anyway, but it does work with weighted interleaving. So if you're trying to get the bandwidth, which GPUs typically like, then, yeah, you can round robin in the data between DRAM and CXL. And then when the data needs to be moved, either from main system to GPU or vice versa, you're actually getting the benefit of the bandwidth there.

And again, with just enclosing of this section, there's the links to the CXL bench and NVIDIA. We are moving CXL bench from its current repository over to OCP. So we will rename it and we will move it, plan on doing that early 2025. So stay tuned for that. But if you do go to that link, we'll redirect you when it gets time to be moved.

So, let's talk a little bit more about actual workloads and use cases, those that we've shown through 2024.

So let's start with a popular RAG pipeline. MemVerge Micron did this and showcased it earlier this year. But the basic premise here is that a naive RAG pipeline, the bit on the left-hand side there, that when you're after you've embedded all your enterprise knowledge, all your PowerPoint docs and Word docs, et cetera, into your vector database, when you're asking questions about the data that you have embedded and stored in a vector database or maybe in a graph database. But it's very important that the time to retrieve that data is obviously going to be impactful on the time to get your answer. So what we found was when we were just benchmarking vector databases in general, we were seeing a 30, 50 percent improvement in response time. So lower P95, P99 latency and more queries or tokens per second effectively. So we were able to use that learning in conjunction with Llama Index's composable memory. And Llama Index is a very popular developer framework for building RAG pipelines, but LLM applications in general. And one of the cool features of the composable memory is that you can have multiple databases. So dependent on how you want to embed the data, you might have a vector database that stores some of your documentation for products or a different database that stores your legacy products or it might store some Wikipedia entries or something like that. Right. And the composable memory means that we can now query all of the databases with our query. We get back the data from each of those and then we can pass that to the prompt. So we were able to use CXL, both extending the amount of data that we could fit on a host, but also getting the bandwidth capabilities as well. So the efficient retrieval of those vectors. So the time to answer was improved. And then to help with longer context of our chat, we were able to store the answers in chat history. So as we asked the LLM more questions, we're storing previous answers so that then becomes part of the retrieval piece, meaning that if I do a question on Monday and then I ask a similar question on Friday, I should get back the same same result because I'm now using the output that I got from Monday in my answer for Friday, so nothing else really changed in this area. But again, we were able to see a 30 percent improvement in requests per second for each of the quadrant vector databases in this example. But you could you could probably pick and choose your favorite vector database here. And again, because I'm now storing my chat history, my context window is increasing as well, which is very important. So the the quality of my results is also improved.

Let's move on to TPCC, a traditional DBMS type of workflow here. For this example, we worked with Samsung with a CMMB. So this is a memory pooling scenario. And we were showing up to 60 percent higher transactions per second, and that that was also reduced our P95 latency by 40 percent. Again, this is not using anything relatively new. We're using some older Intel Xeons and an older kernel. But with our memory machine in latency mode, we got some fairly significant numbers here.

Moving on, we can talk a little bit more about MemVerge Gismo. So this is the object store that I talked about, earlier running in conjunction with Ray. And if you're not familiar, Ray is a great Python distributed workload kind of framework, very popular in the AI/ML world. So I can take what is traditionally a single-threaded application like and parallelize a lot of the work. And here, because we're using MemVerge's version of Ray that uses the Gismo interface rather than a traditional network interface, we're able to use that CXL memory for storing and loading our data. So in the case of a Spark Shuffle, instead of moving data around the environment and caching it locally, we're able to just pass the data by reference because the data already is resident in memory and it's accessible by multiple nodes at the same time. So with that, obviously, we eliminate the shuffle piece, which can be time-consuming in and of itself. And then loading the data because it's memory-resident, we're just using byte addressable objects effectively. So we can access the data natively in nanosecond latencies. So overall, we're able to kind of reduce the object spelling and data skewing that would typically occur with a traditional kind of network-based solution here. So replacing that local memory with shared memory means I don't need as much DRAM in my system because the data is all there in shared CXL.

And then, one of the more recent ones, Stable Diffusion. We worked with Micron on this one. So again, using Gismo. So Stable Diffusion, if you're not familiar, is a text-to-image model. So I can type in, you know, cute cats and things like that, and get some actual images rendered from that. So this diagram shows the the end result. It's the same actual diagram as what we were using. But instead of passing data and storing data over the network and in shared storage, we were storing data in the shared memory using Gismo. Again, this particular application was relatively unmodified because it was a Ray application anyway. So what we did was use the MemVerge version of Ray, didn't modify the code above it, and we were able to do everything from training. So that's the bit on the far left where we're passing hundreds of thousands of images of cats or dogs or whatever. We're training the model. We're storing the results and checkpoints on Gismo on top of the shared memory. And then as we go into the encoding phase and then we get into the distributed training and finally into the inference stage, all of the data remained inside of Gismo. So we're not moving data around.

And so if we take a look at the results here, this is just a screenshot from the Grafana dashboard that we put together. So, just to walk you through this, the top two rows are the cluster or the main system memory of each host. The column on the left is the results, what happened with GISMO or with shared CXL, and the one on the right is the comparison without shared CXL. So, you can kind of see, even without looking at the numbers, that the amount of main system memory required for the one on the right and the one without CXL is significant because we’re having to cache that data locally. The one on the left, we don’t have to do that because the data is in shared memory. So that’s why. The next row here is the shared memory or the GISMO capacity. So, you can see kind of the pyramid type stuff as we’re storing and freeing memory inside of GISMO. On the right hand side, there is no data because natively it has no concept of using main system shared CXL, sorry. The next rows are all about network. So, you can see on the left with CXL, there’s almost no network going on at all. It’s just a couple of packets floating across the wire as intercommunication. So, there’s a lot of activity between the workers on the right hand side. Of course, we’re having to move this data around over the network. Right? So, that’s why you’re seeing a significant increase in network overhead here. The next two rows, the first one is the GPU utilization. So, we’re not changing the workloads. We’re not doing anything else in that regard. So, that’s why the charts look relatively similar. And finally, the amount of memory used by each GPU. Again, we’re not changing anything there. So, that’s why the charts look very similar. But, you can kind of see the point that, you know, with shared memory, you definitely eliminate the network overhead. That’s for sure. There’s not much network going on at all.

So finally, the call to action, you know, don't wait, right? The future is now. CXL devices are available. If you go talk to your server OEMs or talk to device vendors, that type of stuff, then you can certainly get access to engineering samples, POC environments, etc. If you want to learn more about the CXL consortium, the CXL specifications, go to computeexpresslink.org. There's a ton of information there as well, including, you know, who's who in the CXL ecosystem. If you're—if you can't get hold of hardware, then definitely look at the QEMU because it has pretty much all of the features you need. It's not that hard to set up a shared memory environment using, you know, memory backed files or something like that in terms of connecting those to the VMs. I would say that it's not performance. I don't do any benchmarking in this, but if you're trying to understand the capabilities and features that are available, QEMU is a great place to start. And honestly, that's where the kernel community uses, right? So we're able to put the features into QEMU first, then the kernel community can use it, develop the features, make sure they work. They can do all the testing and UAT and then, you know, those patches effectively become upstreamed in the next release of the kernel. And there are plenty of communities around. There's a memory fabric forum. There's OCP. There's tons of other events that have happened throughout the year. Certainly attending some of the conferences, you'll get to see a lot of what's going on in the CXL ecosystem from pretty much all of the vendors. We host webinars like this one. So, yeah, I definitely encourage you to kind of reach out and look around and get more familiar with what's available now and what's coming in the near to far future.

So with that, thank you for your time. I think we've got a few minutes left, so we can open it up to Q&A, Nolan. Thank you.

Great! Thank you, Steve. With that, we'll go ahead and dive into the Q&A. If you have any questions, please feel free to submit them into the questions box. So our first question is: how can I start learning and testing CXL without access to hardware?

Yeah, that's a good question. And also, like I said at the end there, right? I mean, QEMU is a great place to get started. We're able to emulate a lot of different CXL device types, from from expansion. So if you just want one very large CXL device, you want many smaller ones, you can also emulate shared memory as well. In fact, there's a Docker image on the MemVerge website. We blogged about this as well. That makes it super simple with just a few commands. You can launch a Docker image that has a couple of QEMU VMs running. And we just use file back memory for the interconnect and sharing piece. But that gets you a long way towards where you want to be for sharing. So definitely, QEMU is the place to be. It's kind of the one that the kernel community uses for a lot of the development work. So as we're integrating new features, functionality, doing a lot of testing and bug fixes and everything's pretty much done on top of QEMU because the kernel community doesn't have access to the hardware yet either. So.

Great. Our next question, how do I know that CXL fits my application well?

Yeah, I guess that's a million-dollar question, right? I think, you know, first and foremost, I would say reach out. There's plenty of consortium members. That would include, you know, the memory vendors, the server OEMs, and the device vendors as well. And we'd be very happy to talk to you about that. You're welcome to reach out to me directly if you want to kind of a single point for that one; I can kind of point you in the right direction. But yeah, we definitely have a lot of learning under our belt and a lot of learning to do. So definitely understanding what your application is doing and where it's bottlenecking or where we could add value in terms of capacity or cheaper memory or, you know, shared memory, for example, memory pooling. We would be happy to have those conversations. And, you know, we're always looking for interesting use cases. And of course, killer applications with AI/ML being kind of the hot topic at the moment here, we'd definitely like to talk to anybody that's interested in working with us in this field.

Excellent. Our next question: Which GPU model number and how many GPUs did you use for the RAG Pipeline demo?

Yeah, for the RAG Pipeline demo that was done in our lab. So that was a single NVIDIA A10, so on the older side. We were not necessarily focusing on the performance, although we did definitely get some decent performance out of it, but we were focused more on the compute side. So, this is the retrieval part. That's my opinion where the magic is, right? So, being able to run the vector databases in DRAM and CXL using the weighted interleaving policy that we talked about. And then when you pass that data along, with a prompt into the LLM model, that's when it goes over to the GPU side. So, again, I was mostly interested in the compute/CPU side for this one.

Great. Our next question, in the RAG slide: When CXL memory sees a 30 percent improvement to avoid spills to disk, what is the direct attached DRAM memory size, or CXL memory size?

Yeah, for that particular example, I think we had like 512 gig of DRAM and we had, I think, 128 gig of CXL, so just a single CXL device. Since doing that, because this is one of the ones that we talked about earlier this year, we've—we've got access to servers that allow us to put more CXL devices front-loaded E3.x. And we're seeing scalable bandwidth and latency numbers with that. So the more CXL devices I put in, the performance is linearly scalable. So I wouldn't mind going back to this one and seeing what we could do with more CXL devices in this solution, because I think we could do a lot better than what we advertised here with a single device.

Excellent. Our next question, with a stable diffusion example: Did you measure image output performance with and without CXL memory? How much faster was it, if at all?

So we did not focus on that particular aspect of the stable diffusion model. The intent here was to show that we could do training on one system, store all of, all of the models and everything in effectively shared memory, and then access that same trained model on a different server. So we were not necessarily focused on the number of images we could generate, because I think, if I remember rightly, when we first started this, we had a single A100 on the training side and a single A10 on the inference side on the stable diffusion demo side. I believe we did get a second A100. So that would have improved the performance significantly. But again, it wasn't the focus of the demo there. It was mostly just showing that, yes, we can move data around or using the memory bus versus over the Ethernet, which is traditionally how things are done. And we're not; we didn't have anything stored in in kind of block storage either. So.

Gotcha. Great. So our next question, on your Kubernetes/DCD slides, what were the integration points between your composer, the K8 scheduler, and the underlying CXL Niagara system?

Yeah, so SK provided the kernel bits and pieces for us. So it's a custom kernel with all of the DCD features implemented, again, because at the time it wasn't wasn't upstream, and the patches are slightly different to to what you'll find in the open source, but they're very close to. And the Niagara firmware was also modified to provide the the features and functionality that we needed for DCD and CHMU. And then our piece was really just writing the kind of the user space agents that run on each of the compute nodes and monitor the memory utilization. And we had very simple thresholds that say, if if the memory utilization goes above, let's say 80 percent, then we're going to start adding more memory into that box. And on the Kubernetes side, we we wrote a DRA or a dynamic resource allocation piece specifically for memory. And this is this is a relatively new feature for Kubernetes. I think it was alpha as a version 1.26. And then the recent release, the 1.32 version now actually has it as a beta. So it's becoming a bit more of a mature technology. But yeah, the intent there is for AI/ML workloads. So you can write DRAs that allow you to look at GPUs, for example. So I guess if you look at Liqid's composable memory infrastructure, they're able to add and remove GPUs on demand. So that would be a great use case. And again, we kind of used it with DCD to make sure that the system and then Kubernetes, I should say, was aware that the amount of memory could change over time, which today, if you deploy Kubernetes with a stable version, it is not. You know, it looks at the amount of memory the system has when when the kubelet starts and that's a fixed value until you restart kubelet. So that was the integration piece. The composer was really just kind of the orchestrator on top, just making sure that the decisions were made correctly and making sure that we weren't going to request more memory than what Niagara could provide to us. So when the system gets full of pods, we do tell the scheduler that you can't schedule more more pods onto this node because it doesn't have any more memory available to it, so it's kind of an overarching visibility control plane piece to the puzzle.

Excellent. Our next question: How can I try MemVerge's memory machine or FAMFS?

Yeah, good question. So FAMFS is open source, provided by Micron. So if you Google for FAMFS, make sure you're looking at the I think it's called the CXL Micron ResKit GitHub repository, and you'll find it in there. It's initially architected and written by John Groves. So that's relatively easy to download, compile, and run. John's working on it pretty frequently, so hopefully, it'll be a good solution into other tools as well. And then on the memory machine side, feel free to reach out to myself or just go to memverge.com and down the bottom of the page, you'll find a request POC option in the footer, fill out that form, and we'll get back to you. I will say that we're mostly interested in enterprise customers at this time. Again, the hot topic is AI/ML. So if you have AI/ML workloads that you're interested in using with shared memory, then yeah, definitely feel free to reach out and have that conversation.

Great. Our next question: Does QEMU run/pass the CXL compliance test?

That's a good question. I don't believe so, although I'm not necessarily kind of tied into that piece. I think for the most part, the developers just take whatever's in the specification and implement it, and then the kernel team kind of takes that and implements the necessary bits from the operating system perspective. So it's more of a true implementation of the specification. I don't think we do all of the hardware testing that I know occurs, I think, quarterly with the hardware vendors.

Excellent. I think we have time for one more question. So, for our last question: Where can I find links to the details of these use cases?

Yeah, so I'm creating some blogs on memverge.com. I've got some blogs on my website as well. As part of OCP and the CXL consortium, we're also releasing white papers and solution briefs. Certainly, the device vendors and server OEMs are putting those together as well. So, yeah, kind of dotted all over the place. But a good place to start would be memverge.com, and then go and take a look at the CXL device vendor websites for sure.

Excellent. Well, that was our last question for today. So, thank you again, Steve, for leading the webinar.

Yeah, I appreciate it. 

Yeah. So we'll make the webinar recording available on the CXL Consortium YouTube channel, and it'll also be available here on BrightTalk. And we'll be uploading the slides to the CXL Consortium website. Great. Thank you, everyone, for your time today!
