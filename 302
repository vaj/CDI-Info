YouTube:https://youtu.be/0-zHObQrNVY?si=6AG2racMp9Zyu-ZM
Text:
Hi, I'm Jim Handy, and I'm here to make a presentation called "CXL is Exciting, but Where Is It Headed?" I'm actually the general director of Objective Analysis, a market research firm, and we produce reports on various markets, mostly memory-oriented, but we also do custom work for a number of clients.

So, the question that goes through my mind is, what is CXL really for? Way back when CXL first came out, there were a number of other standards: CCIX, Gen Z, and OpenCAPI, but all of these said that they were going to be used for maintaining coherency in large computing systems. But then we started hearing other things about how good CXL was going to be for eliminating stranded memory, and that's a completely different application. And so then along came expanding memory size. It seems like today we've got a lot of people who are looking at CXL as being a way to get really large memories that you wouldn't be able to get because of capacitive loading on the memory buses. Something else that CXL offers, that so far hasn't been a big deal, although with AI it will be, is increasing memory bandwidth. CXL introduces latency to memory systems, but it does that while bringing the potential of having huge bandwidth. You can run basically several DRAMs in parallel on several PCI lanes and get higher bandwidth than you would be able to with DDR. Something else that Intel really jumped in for back when it was still really pushing Optane was the idea that you could put persistent memory onto CXL. And you start noticing that none of these really have anything to do with each other. The persistent memory support requires for you to have a bus that can run at different speeds for reads and writes, and 3D crosspoint memory, what Optane was built on, took about three times as long to write as it did to read, whereas regular DRAM, which computers are generally built for, has the same speed write as read. And then there's an interesting one that IBM has been using with the Open Memory Interface. And that is to hide the difference between DDR4, DDR5, and whatever succeeds that, DDR6 maybe. And these differences, you've got certain processors that only run with DDR4 or only run with DDR5. You can't put DDR4 in a DDR5 processor and vice versa. And so what this does is it erases the differences. And that's something that some people see as a real boon because they can buy whichever module happens to be cheaper at the time and stick it on CXL. And be able to use that. And then finally, there's a sophisticated one that is really important to the large-scale computing community, and that's passing messages between different kinds of processors. That's going to percolate down into lower-level systems. And so the idea that you'd be able to have a CPU load the HBM in a GPU, or even just the GDDR that's on the GPU's board without having to go through an NVMe channel is very attractive to a lot of people. But also, there'll be other applications with GPU to GPU transfers and also CPU to CPU transfers. And that does actually tie back into one of these, and that is maintaining coherency. You can't do that unless you can maintain coherency between these different things. So basically, all of these are like separate dimensions, except for the first and last one. And so the question then is... And the reason why I put all these question marks on here is, which one is the one that's really going to propel CXL to greatness?

So let me, with that as a preface, go into the basics of CXL technology. What it does, the idea is, is that it's going to use the PCIe physical layer to give access to memory at memory speeds. And so I say, memory speed access over PCIe physical layer. That's something you can't do right now because the fast interface on PCIe is NVMe, and even the standard PCIe channel is interrupt-driven. You don't want to have interrupts on your memory transfers. And so, you'd like to have something faster. So CXL has stripped down what's involved in the PCIe interface, while still using the physical layer. That means using the electrical transfer of data between the processor and the peripheral, because it's a really fast physical layer. And then, it's just a question of whether or not you are using that to your best advantage and whether or not it's fast enough for memory. Something really cool about CXL is that it does support new architectures. You can have disaggregated memory, which a lot of people talk about: memory pools. Memory is the last thing to be disaggregated in the system. With virtualization, you were able to disaggregate processors. You no longer had a server that was dedicated to email and another one that was dedicated to your website, et cetera. You now have these virtual machines where there's a virtual machine that is your email and a virtual machine that is your website. And those virtual machines bounce around from server to server. Then later, storage had the ability to do that. Having attached storage rather than local storage allowed you to reassign the storage to different servers, and memory still ended up being very server-dependent. Now, you can have a pool of memory that's accessible by all the different processors or servers in the system. And that allows you to assign more memory depending basically on what the needs are for each of those. Pooled memory is also kind of another way of saying that is that you have all the memory in one place. And so then, whoever needs the most gets it temporarily, and then it will be reassigned once they no longer need that. Switches for memory. Processors are a very important part of all of this because if you're going to be allowing multiple processors to talk to a single memory pool or even to multiple memory pools, then you need to have switches that are going to be able to handle that. And that's all defined inside the CXL specifications. Shared memory is different from pooled memory. What shared memory is, is that you've got two processors talking to the same memory locations. Now, you have this great opportunity to have big crashes between these two processors as they're trying to get to the memory area. But also, you have the opportunity where processor A modifies a memory location in the shared memory, and processor B has a cache copy of the old contents of that memory. How do you make sure that processor B doesn't use that old cache copy, that it uses the most up-to-date copy? And that's something that the coherency protocols have been worked around to fix that. So that when processor A changes what's in the shared memory, then processor B automatically gets its cache invalidated for that memory address. And then finally, persistent memory. I talked a little bit about Optane, but Tom Coughlin and I wrote a report. He's the current president of IEEE, and he and I update this report every year. It's on the emerging memory market. And emerging memories are MRAM, resistive RAM, re-RAM, we call it, Ferroelectric memory or FRAM, and phase change memory or PCM. And all of those memory types are non-volatile. And so they're really cool from the standpoint of storage class memory or persistent memory, which Intel likes to call it and SNIA likes to call it too. That idea of persistence is something that can accelerate systems that use a lot of storage or frequently use storage, like banking systems, where you don't want to have a transaction get screwed up because the power failed halfway through the transaction. And so it is a concept that's going to happen. Optane didn't happen the way that it was supposed to, but it's not going away. And so with CXL, we now have something that, over the course of the next decade or two, is going to be able to support persistent memory. So, this is a really great spec. They put an awful lot of things into this specification.

And so, to give you an idea about... I don't want to talk about how difficult it is to build a CXL system, I decided I'd show the difference between the architecture, between a CXL DRAM module and an SSD. Some people call a CXL DRAM module CMM, or CXL memory module. So, this is what an SSD looks like: You've got PCI coming into a controller; the controller then distributes things that are to be written into NAND flash or reads the NAND flash and puts it back out on the PCI bus. With a CXL DRAM, you have almost the identical same thing. Instead of NAND flash, you have DRAM chips, and you can have a whole lot. And that's where you get the big bandwidth from. And that all goes into a controller, which is somewhat similar to the NAND flash controller in an SSD, except for the fact that it's driving DRAM, which means that a lot of times you're going to have much higher power dissipation in those controllers than you have in an SSD. And the CXL bus... The CXL bus that I show here is still PCI signal levels. And so, the companies that provide the IP for PCI are going to be the same ones that you go to to get the IP for CXL. So, it's all very similar.

If we look at what those look like in real life, then you end up on the left-hand side of this slide with an SSD. This is a relatively new Micron NVMe SSD. And on the right-hand side, you've got a... It's a CXL memory module. You can see it says "CMM" on it. And that's one from Smart Modular, who's really working hard on the CXL marketplace, trying to make a name for themselves there.

And if you look inside the CXL memory module, you see pretty much the same thing that you'd see inside of an SSD. All of those black chips that you see there are DRAM. What you don't see is a controller chip because it's on the flip side of the board. There's also DRAM on the other side of the board, too. But the controller chip would be in between... Well, it's in the bottom center of the board. And it's where you see all of that stuff going on. That's mostly decoupling capacitors and solder bumps. So, you can tell that there's a lot of noise, care for noise going on, but it's DDR. And so, you kind of need that.

So let's talk about what the users of CXL are going to want and need, because we do see some conflict in there. Google is...Google has come out with a paper relatively recently that kind of went against something that Microsoft Azure said. Microsoft Azure had something called the Pond paper, talked about a system that they called Pond that said that stranded memory was a problem that was adding about 7% to the cost of memory systems in a computer. Google looked at that, ran some tests of their own, and they said, "Well, our virtual machines are small enough and the resources are big enough in servers. So, we can pack them in really tightly. And when we do that, then there's not really any advantage because we never have stranded memory to begin with." So, you've got these two sides here: Microsoft saying it's important for saving money and Google saying, "Well, it doesn't really save anything."

IBM did something that Georgia Tech later tried out with CXL. IBM did it with the open memory interface and Georgia Tech did it with models of CXL system. But both of them believe that DDR is a poor answer, that it's better to have the memory interface abstracted so that you don't end up having to have the processor tied to a certain memory interface. And this is because, over the past several years, you've seen the number of DDR modules that can be put onto a memory channel has gone down from four to two to one. And so, in order to counter that, then the processor designers have had to add a lot more and more channels onto the processor in order to get the same kind of bandwidth. And that just consumes gobs of power. And as I said before, it ties you down to only using DDR5 with a DDR5 compatible processor. You can't use older DDR4 modules with it and vice versa. You can't use DDR6 with it. So both of them looked at that and said, "You know, this isn't something that we want."

But also, Georgia Tech and their models, they found that a lot of benchmarks were slower when the DDR was tied to the DDR channel than it was when the DDR was tied through a CXL channel. And that was because the cues inside, the memory cues inside the processor were getting backed up. And it's something that for some reason wasn't happening with a CXL system. Now, I take this with a grain of salt because it's a model. And I think until we see some real tested results, it could be that there's something that they overlooked.

AI providers. They really like to have huge memories because they're just processing a ton of data. And so if you can give the AI providers a whole lot more memory on a CXL based system, then you can on a system that uses standard DDR interfaces, simply because the DDR, you can only put so many DDR modules onto a server processor. Then, you know, the CXL, basically, you can go until you've overloaded all of the PCI channels. So it's a huge amount of memory that you can do that. And so they're all very excited that they'll be able to put significantly more memory on. They're probably going to put it on along with all the memory on the regular DDR memory channels. Um, but also the AI providers use an awful lot of GPUs and they want to be able to load the HBM on those GPUs faster, and CXL allows them to do that.

Hyperscalers really like the idea that any processor can talk to any other processor, no matter what kind of processor it is with, um, CXL. So, you know, this is a new way for them to be able to design memory meshes, memory, uh, fabrics that, that accomplish all of that and do share an awful lot of memory between them. Um, the other side of this is the small system guys. So, PC OEMs, they just don't immediately see any use for this. So, it looks like CXL is going to start probably first with the hyperscalers because of GPUs because of the any to any connections. Um, but, you know, and despite what Google said, you know Google saying stranded memory is not important, but we still want the big memory that CXL has. So, they're still gung-ho about, uh, using CXL, but the PC OEMs, they look at it and they say, "Nah, LPDDR is good enough for our small systems, standard DDR5 for, you know, our regular systems." And so we'll stick with that. And that's probably going to happen for a long time.

So, this is our CXL forecast. Um, it's basically for CXL memory modules, because we think that that's where the majority of the market is going to be in the near term. The switches were defined an awful lot later than the CXL memory modules, and still, CXL memory modules are not that widely available; but over time, and as we see, um, CXL gaining in popularity, we're expecting to see servers in data centers, um, using an awful lot more CXL. And so, we're expecting revenues for CXL memory modules to hit about $3.4 billion by 2028.

The long-term impact of this is rethinking system architecture. There, people are going to be looking at the way their systems are configured, and they're going to say, "Okay, we can disaggregate memory. What can we do with that?" And so, you'll probably end up finding systems that have smaller memories, large systems of smaller memories on the server, but with very large memories out in a pool somewhere shared among processors. We'll also see a lot more mesh networks being used. You know, right now that's, uh, um, you know, high-performance computing (HPC) kind of an approach to things, but it's probably going to be much more widespread in data centers.

And then, this idea that things are memory agnostic—that they don't need to worry about whether they're DDR4, DDR5, they don't need to worry about whether it's DRAM or whether it's Optane or whether it's FRAM or whatever it is, that's going to be used there. All of those are things that are going to attract people to CXL plus the very large memory. And then, you know, I say better memory bandwidth and size, which, you know, I was talking about how the size is great. And the bandwidth—it can be great because you can put a whole lot of chips on that CXL channel, but you get worse latency because you have that CXL controller talking to the DRAM instead of having the processor itself talk to the DRAM. There's not extra latency really on the processor side because the processor is going through its, um, chips in the, the processor chips, internal, uh, PCI controller to be able to manage that. And so that's going to take about the same amount of time as it would take to go through the DDRIO.

But, um, when you get over to the CXL memory module, then you're going to have a controller standing between the CXL channel and the DRAM. And so that's going to slow it down. Um, what I'm expecting to see over the long term is that people will get smart about how they use CXL and they're going to configure their software, first of all, to manage near-term, near memory closer or differently than far memory, and near memory would be the DDR that's inside the server. The far memory is what's on the CXL. And so colder data will go into the far memory, hotter data into the DDR. Um, and, uh, that, and, you know, also the fact that it's got this huge bandwidth, there will be programs that separate out the high bandwidth part versus the highly random part and work those against each other.

Um, a lot of the same kind of thing happened with SSDs where, uh, early on SSDs were being used for all storage inside of large systems. And then later on, people started adopting an architecture where the hot data went into the SSD and the cold data went into a hard drive. And so that kind of an approach is going to start being used with memory too. It's just that the software hasn't evolved, and it might take a decade for that to happen, but those design rounds will happen.

And this is just a pitch for a report that we've got on it. It's called 'CXL: Looks for the Perfect Home.' We just came out with it a couple of weeks ago, and it tries to cover all of the perspectives of CXL, talking about where it's useful, where it isn't, what the demand drivers are for CXL DRAM modules, and what the opportunities are outside of DRAM. And then we give this forecast, like the one that I showed in the chart earlier; that report can be purchased. You just enter your Visa code online, and you can get a PDF today if you'd like to, by going to the address below. Which, um, you know, I'm sure that, uh, you can look at the Objective Analysis website and find it there, or, you know, you'd probably find it here too.

With that, I want to thank everybody for, uh, uh, going through this presentation with me. I hope you've learned something, and Objective Analysis would like to work with you on refining your CXL strategy. So, do reach out to us if, if you have any questions that we can help out with answering. Um, I'd like to thank Frank Berry, uh, for, for hosting this, and, um, I'm available for questions after the presentation is over. So, uh, you know, if you'd like to ask any questions, now's your time.
