
Hello, everyone. Thank you for attending CXL Consortium's Exploring  CXL Use Cases and Implementations webinar. Today's webinar will be led by CXL Consortium member company  representatives Oren Benisty from UnifabriX,  Ahmed Medhioub from Astera Labs, and Matthew Burns from Samtec. Now I'll hand it off to our presenters to begin the webinar.

 Thank you very much. And today, we're going to explore the CXL Use Cases  and Implementations.

So this is Oren Benisty from UnifabriX. And I will go over the main use cases for memory pool.

But I will start before discussing the memory pool. I will discuss the memory wall. And what is the memory wall? What we can see in this slide is a presentation of IBM  at the OCP Summit a few years back, probably 2021. And what we can see in the orange bar  is actually the memory channel bandwidth. And specifically, the memory channel bandwidth per core. So in the green bar, we can see that the number of cores  increase. So we had a 2012, I believe it was two core. After that, four core. And today, we are going to 64, 128 cores. While we are discussing cores, we  can see the bandwidth on the orange bar. We had two core with two channels. After that, we had four core with two channels. It means that the bandwidth was about half. And after that, we are seeing the number of cores increase,  while the number of channels to the memory  actually is getting flat. And the bandwidth that we can see on the orange bar  means that the amount of bandwidth per core  is actually decreasing. So memory pool is coming to address two issues. First of all, when we have fewer channels,  it means that the bandwidth are not  increasing at the same pace as the number of cores. And the second issue is the capacity. It means that the amount of memory  that we can have per channel is limited. So we need to increase the number of channels. Or we need to find another solution, external solution. The external solution come in the form of memory pool  that will provide more bandwidth and more capacity  to the system.

Moving to the next slide, over here, what we can see  is meta presentation at the Memcon event last year. And they presented their challenges,  and specifically the challenges for implementing AI. And on the red color, we can see on the upper right-hand side  memory bandwidth. They showed memory bandwidth as the limitation  for large language models inference. On the green bar, we are seeing the limitation  as memory capacity, which is the green color, which  is the recommendation engine inference. And all in all, we will look on the left-hand side,  and we'll see the model size that increasing  from gigabyte to terabytes. All of this can be addressed by the memory pool. Now, memory pool is a tool that can provide more memory that  is not dependent on the memory channel of the CPU  and can provide it externally. Now, when we are looking specifically at AI models,  the AI models increase their size. And when we are looking at today GPT-4, for example,  this is a terabyte size of memory. And not all systems have this capacity. Using a memory pool, we can provide more memory  to every host.

Moving to the next slide, this is  the market or the use cases that can  be addressed by a memory pool. And a memory pool that we can see over here,  first of all, on the bottom side,  we can see private cloud and public cloud. Public cloud and private cloud both providing instances. Today, when we are buying instances,  there is a specific memory attached to each core. So each representation of memory and core are correlated. In the future, when we'll have memory pool,  we are able to provide memory as a service. It means that we are able, as we're  increasing the number of cores, we can also, per core,  increase the amount of memory. This will be a new type of memory,  and we'll discuss it in the next slide. On the upper right-hand side, we can see GenAI. GenAI, we discuss it. We need more memory to hold the large models. The large models specifically require terabytes of memory. In a memory pool, we can hold terabytes of memory. And by that, we're enabling to hold single instance  with large models, and by that, provide specific implementation  to each use case. Other use cases can be HPC, number crunching,  like national hubs in the US. They require a lot of memory per compute node. Animation studios, for example, they  need to render lots of videos. So when you're having like 8K video,  you require lots of data to hold in the memory in order  to render the full video. Let's discuss medical drug discovery. Medical drug discovery, let's take, for example, DNA. DNA of a person is about a terabyte. You need to compare those. You need to build a full capability in order  to discover and to create matching  between DNA of different people. And by that, you require lots of memory. And usually, DNA sequencing is a memory-stranded application. Data analytic in high-frequency trading  is having the same issues. You are having lots of data. You need to match between different databases. And by that, you create memory-stranded on the CPUs. All those applications that we can see in this slide  actually creating memory-stranding on the CPUs,  or the GPUs, by the way, that we'll  show it in the next slides. And those are the type of use cases  that memory pool based on CXL will address.

In this slide, what we can see is the connectivity. So CXL is actually the magic. The magic happens when CXL is connected between a memory pool  and the different hosts. The different hosts are running standard Linux  with standard drivers. It does not require anything. It means that we are using CXL connectivity between the memory  pool that we can see max memory is the product. And the different hosts are running standard Intel or AMD  processors. It can be with or without the GPUs. CXL create the connectivity between the pool  and the different hosts. The magic about CXL that it creates coherency  between the pool and the hosts. It means that each host can see a memory  as it is in the box itself. The connectivity today is CXL 1.1. Next year, it will be CXL 2.0. It support both CPU and GPUs when GPU will support the CXL. It's a standard 2U box, very similar to a top of rack  switch. Next to the top of rack, you can populate it with a memory pool  and provide between 4 to 32 terabytes. It means that those 32 terabytes can  be shared across multiple nodes, whether it's 2, 4, 8, 16,  depends on the architecture. The architecture today is limited to single system. But in the future, we will support multiple boxes  in CXL 3.1. CXL 3.1 will be a fabric-based, and we  will be able to connect multiple nodes. It means that we can scale the system to 256 terabytes  across multiple racks. The memory bandwidth can be scaled up  to 512 gigabyte per second. And the whole system is capable of providing those 32 terabytes  or, in a multiple rack environment,  256 terabytes at very high bandwidth. Memory pooling and adaptive sharing  create the opportunity both to provide the memory  from a single memory pool to a single host. But we can also provide simultaneously  the same memory to multiple hosts. It means that multiple hosts can address the same memory  addresses. And with that, we can make the system more efficient. An interesting part, we can use any media in our system. It means that we can use DDR4, DDR5. And in the next presentation from Astera,  you will see that they are also presenting EDSSF. It means that it's CXL-based memory. We are also able to provide NVMe. It means storage class memory, like Intel  had the Optane memory. So we are also able to present our memory as an Optane. It means that we are able to provide  more capacity and persistency. It means that we are taking the NVMe  and presenting it as a memory. And we're having a backup because the NVMe  can be persistent. So the system is already being deployed. It means that we already have customers  that are buying the system. And they are experimenting to get performance, capacity,  and bandwidth. Connectivity. Today, the connectivity is based on both optical and copper  wires. And in the next presentation, Samtec  will present an optical connectivity  between the memory pool and the different hosts. Bottom line, the system is available. And if you would like to try it, you  can reach me out in this chat.
 
This slide is actually showing the future. In the future, as we are saying, the max memory  is a single box, is a single appliance that is  connected to different hosts. In the future, we'll be able to connect multiple max systems. It means in a multiple rack environment,  we are able to have multiple max systems  and connect to hundreds of nodes all together. So this is where we're going, connecting CPUs to GPUs,  providing memory from a max memory to a multiple host. This environment is very, very attractive for GenAI and HPC. It means that we are having hundreds of nodes connect  to many max memories that are sharing  their memory between each other. This, of course, will be managed by a fabric manager that  can see all hosts and all the max memories  and share the max memory across all nodes.
 
Now, you are probably asking the performance. So over here, we actually have five different benchmarks. Those benchmarks stretching from GenAI inference belt. And we showed that we increased the instruction  per second by 150%. In HPC, we have run HPCG. And we got 131% floating point operation per second. And those actually benchmarks can go on and on. And we have multiple benchmarks that  show both HPC, AI, data analytics, number crunching. And we showed that by increasing the amount of memory,  we are able to increase the performance of almost  every workload in every system. If we'll go back to the first slide that I showed,  the number of cores is increasing. And it's not increasing at the same pace as the memory. By taking a memory pool, we are able to provide more memory. And each core will have more memory. And by that, we'll get more performance. So the bottom line memory pool is here. It will provide more memory to every system. And you're more than welcome to test it.

My contact details are over here. So you'll have the presentation. And you can send me an email. And with that, I would like to hand over to Astera.

Thank you, Oren. Hello, everyone. My name is Ahmed Medihioub. I'm a product manager at Astera Labs,  focusing on our CXL type 3 devices.

To start, I wanted to take a step back and talk  through CXL as a high-speed, high-capacity interconnect  for CPU to device or memory connections. As it is built on top of the PCIe physical and electrical  interface, it includes a PCIe-based block input/output  protocol, CXL.IO, used typically for discovery, configurations,  and interrupts, as well as new cache coherent protocols  for accessing system memory. That's CXL.cache and device memory CXL.mem. The CXL protocol defines three types  of devices that support different combinations  of CXL.io, CXL.mem, and CXL.cache protocols. The type 3 devices specifically support both CXL.io  and CXL.mem. They are typically memory expansion devices  that allow host processors to access CXL device memory  coherently through CXL.mem transactions  or memory expansion, pooling, and sharing use cases.

So some of these use cases where we  see significant impact of CXL type 3 devices  fall into three primary camps--  memory expansion, memory pooling,  which Oren has went through earlier in the presentation,  and memory sharing. In the memory expansion scenario on the left,  the CPU is connected to multiple CXL expansion cards via a x8  or x16 link, expanding the total system memory  capacity with additional DIMMs connected through the CXL type  3 controller. The memory pooling use case in the middle  depicts a scenario where two hosts connect  to a CXL attached pool of memory via a type 3  controller with two CXL x8 links,  allowing for a memory pool that can be statically  or dynamically allocated between processing units based  on demand. And lastly, memory sharing use case  where it shows host 1 and host 2 connected to a CXL module  via two x8 links, enabling both servers  to coherently access and utilize a common shared memory  connected behind the controller.

So the question here, what does this  look like in terms of server infrastructure? Typical architecture for memory sensitive applications,  some that Oren went through earlier in his presentation,  as the ones you see on the left, for example,  we use 64 DIMMs via a four socket system. The challenge here is that you're typically  buying or over provisioning more than what you would need  to run your application. For example, different extra CPUs, backplanes, drives,  power supplies, et cetera. Versus on the right, getting one dual socket CXL box  with 8 x16 memory expansion cards,  for example, in two DIMMs per channel configuration  that allows you to provision the same total number of DIMMs,  64 DIMMs. The value here is the ability to add more DIMMs  without having the need for more CPUs,  optimizing thus the full utilization of CPU cores  while lowering both CAPEX and OPEX.

So the Astera Labs Leo CXL Smart Memory Controller  is a type 3 CXL 1.1 and 2.0 memory controller  or memory expansion. And that's the E-series offering,  as well as pooling and sharing, which is the P-series offering. The Leo CXL controller offers high speed memory access  developed through strong partnerships  with leading CPU vendors, such as Intel, AMD and Arm. It is hardened through multiple interop efforts  with different hosts and a growing list of DDR5  10-part numbers from all three major memory vendors, Micron,  Samsung, and SK Hynix. The Aurora A1000 add-in card provides a vehicle  for seamless evaluation and deployment of CXL memory  expansion and pooling that has gone through extensive CPU  and memory interoperability and performance  testing with different vendors within the CXL ecosystem,  improving resource utilization, scalability,  and reducing total cost of ownership. So the Leo CXL Smart Memory Controller future  proves data centers, preparing them  for the next generation of memory intensive applications,  including AI and memory databases,  analytics, and others that were shown earlier by Oren.  

Moving on to this slide here, I wanted  to sort of highlight the problem statement  that this slide describes. So looking at the left side, the volume deployment of CXL  is highly contingent on multiple factors and requirements,  such as applications and workloads  that need to be optimized to leverage CXL-attached memory. CXL-attached DIMMs are part of the memory subsystem now. So reliability, accessibility, and serviceability  are very important. Confidential compute, as well, is a strong security driver. Also, management stack and telemetry  on both out-of-band and in-band need  to be ready and aligned between different providers. And additionally, as I mentioned in the previous slides,  CPU and DIMM interoperability are  crucial for seamless integration and operations. So Astera Labs has developed a comprehensive platform solution  stack to support these requirements,  featuring Cosmos, which is short for our connectivity  and system management optimization software. That covers all the platform APIs, modules,  and embedded software running on our silicon. This stack integrates seamlessly with end applications,  commonly developed device--  community-developed device drivers, as well as  OS kernel and BMCs from different vendors,  and the underlying software-defined architecture  for the silicon itself, enabling, thus,  a robust and efficient CXL ecosystem. This integrated approach ensures that the CXL ecosystem  is fully enabled, offering a reliable and scalable solution  for modern computing needs.

So to present the full view of this comprehensive portfolio  of CXL solutions, first shown on the left of the slide  is what is currently the most common application. That is CPU direct-attached CXL memory expansion case,  commonly used via CEM or EDSSF. We also provide a solution for short-reach CXL-attached  memory, usually used with cabled solutions like MXIO,  depending on where the expansion module is located with respect  to the motherboard and the channel loss of that connection. This is essential to unlock backplane and JBOM designs. And earlier this year, we released our new retimer smart  cable modules that unlock active copper, PCIe,  and CXL connections of up to 7 meters,  and our optical modules for up to 50 meters. This has the potential to enable new architectures,  like shared and pooled memory, or node-to-node, as well as  rack-to-rack CXL connectivity. So if you'd like to have access to our Aurora DevKit  or any of our products, feel free to reach out to me  directly, and I'll pass it to Samtec  for the next part of the presentation.

Thanks, Ahmed. I appreciate the good introduction. Appreciate, Oren, too. You guys did a nice job setting this table here. So the next phase that we want to consider  during our webinar today is actually  looking at the physical layer of implementation  that a lot of the trends and use cases and topologies  that Oren and Ahmed referenced. How is that actually affecting system architectures  or system topologies? So there's a lot of interest in the market space for optics. So the question we're asking is, CXL over optics, why the need?

And this is just Samtec's perspective,  but it does align with some of the things  that were already presented during the webinar. Obviously, when looking at how disruptive AI has been,  models are getting bigger, compute requirements  are getting bigger, memory requirements  are getting bigger, exponentially so. We're also seeing a rise in disaggregated computing  within the data center, high-performance computing,  supercomputing, and the like. So when you look at these major trends within the ecosystems  that CXL plays in, there's a need for external cabling. And when you look at the use cases,  we could spend an entire webinar detailing all the use cases  here. But in general, we see three requirements or reach  requirements when it comes to cabling. In the rack, typically, you have to route external cabling  from your chassis 2 meters, plus or minus. If you're going rack to rack, the industry  and a lot of the standards bodies  have coalesced around 7 meters of length. And then when looking at clustering,  whether that's memory clustering or GPU clustering  that we're seeing, again, because of all the disruption  of AI, longer distances, signal reach,  may be required of 10 meters or more. So the question comes up, if I need to reach 2 meters,  if I need to reach 7 meters, if I need to reach 10 meters,  what medium makes the most sense? And there's several industry efforts  that are underway to answer those challenges. For shorter distances, depending upon protocol,  there's passive DACs. We'll talk about that briefly during our presentation here. For longer distances, maybe 3, 4 meters, 5 meters,  there's retimed active electrical cables. PCIe SIG sees the need for this. They've recently released the latest Copperlink cable  specification, which really presents use cases for PCIe  Express cabling, PCI Express interconnect over Twinax cable,  using industry standard MCIO interconnect  for inside the box, and then industry standard CDFP MSA  solutions for outside the box. That supports both PCI5, PCI6, or in the CXL world,  CXL2, CXL3, 3.1. And then for some of the optical solutions  for going longer distances, again,  for clustering rack-to-rack, top-of-rack, or even longer  solutions, you need optical. For the data rates that we're talking about,  32 gigatransfers per second, PCI5, 64 gigatransfers  per second, PAM4, there's only so far  that you can go with copper. So it really implies the need for longer distances  that optics are needed. When we talk about optical transceiver MSAs,  there's any number of form factors  that the industry is looking at, OSFP, OSFP XD, CDFP,  which we talked about, SFP, SFP+. But are the optical transceiver MSAs the only form factor  that is possible for routing PCI Express, and more importantly,  CXL over optics? And from Samtec's perspective, we  would argue that there are other options.

What do we mean by that? Well, this is a highly simplified graphic  that illustrates, in a data center  architecture or an AI system architecture,  where you can put the optics. A number of the solutions that we talk about  will typically put the optical solutions  in some sort of MSA cage with some sort of optical cable  off the front panel, right, at board edge. FPP stands for front panel pluggable. So that's where you're going to see your OSFPs, your OSFP XDs,  your CDFP solutions, et cetera, et cetera. On the other extreme, when looking across the data center,  not just CXL, next generation data rates  are looking at 224, 224 gigabit per second PAM,  4 per lane or per channel. There's this thought process of CPO co-package optics. And then Samtec's perspective, and where  we focus in our optical solutions  is on board or mid board optics. We feel that in terms of ease of design,  the density of signals that you can get,  signal integrity, and the like, that on board optics  provides a path forward for a lot of the system architectures  that we've talked about for CXL memory,  for desegregated computing, AI clustering, and the like. That's not to say that on board optics makes sense  for everything, and that's not to dismiss  what's being done with front panel pluggables,  because those are standard and will have their place. But as we go through this presentation,  for some CXL use cases, using that mid board optics  for point to point communication, memory clustering,  memory architectures, memory servers,  these types of approach can be very easily implemented.

How do we do that? Well, Samtec, without going into all the product details,  just want to hit a few of the highlights. Samtec has a growing family of mid board optical transceivers. And the illustration here shows that by placing  the optical transceiver directly next to the ASIC in the system,  GPU, FPGA, memory controller, whatever the case may be,  you simplify the PCB layout, the signal trace from the ASIC  directly to the optical transceiver. In the electrical domain, you're literally  going from the ASIC pad through the PCB  to the on board PCB connector. And then entering the optical engine  within that small PCB form factor. So with these on board or mid board optical transceivers  that Samtec offers, we can support up  to 32 gigabit per channel right now doing  the electrical to optical conversion. That supports PCI 5. We're working on PCI 6. We'll talk about that here in just a second. We also have the smallest footprint,  which allows for highest density on board systems. We'll actually show a picture of this solution  here in just a moment. More importantly, our optical transceivers  are protocol agnostic. Samtec has been designing, manufacturing,  implementing mid board optics for the better part of 15  years. We have a long history of supporting PCI express  over optics, which we'll talk about here in just a moment. And by extension, that lends itself to CXL over optics. We'll actually show some of the proof of concepts  that we have routing CXL over fiber as well. Something else that's advantageous about our mid  board optical transceivers, they're  very easy to attach and remove within a system. There's no through holes. It's surface mount. So you can take advantage of standard manufacturing  processes. From a protocol standpoint, we've  worked with customers to route ethernet over optics,  InfiniBand over optics, fiber channel over optics,  specifically for this webinar, PCIe, CXL, and more.
 
To delve into this just a little bit more detail in terms  of PCIe/CXL over a physical layer optical interface,  we actually have a family of optical transceivers  finely tuned for the PCIe infrastructure  that CXL sits on. So we have a family of PCI express focused optical  transceivers, which comply with the physical layer  and electrical signaling defined by PCI sig for PCI express,  enabling you to do E to O and O to E over 100 meters. We currently are working on PCI5. We expect to have a PCI5 optical transceiver mid-board available  by the end of '24. What's nice about these optical transceivers,  too, is that they're scalable, though flexible. We can support any configuration,  whether that's a x4, a x8, or a x16. We've been able to show these in CXL solutions. Looking at the DSI performance, we've  seen error rates of better than E to the minus 12. So that's better than what's defined by PCI sig. But more importantly for the audience on our call today,  we've also been able to support CXL memory, CXL cache,  and CXL I/O functionality over these PCI express  focused transceivers. So not only do we have the physical layer in Connect,  but we can put the CXL protocol on top of that  to show CXL over optic use cases.
 
 As a matter of fact, the first time  that we showed our CXL over optics proof of concept  was at Supercomputing 2023 last fall. Basically, what you see on the illustration  here on the right-hand side, we're  using an AMD Genoa platform, which has native CXL support. Sam Tech designed a PCI express form factor adding card  with our PCI express/CXL over optic transceivers. You can see on the right that we started with a x8  configuration because there's two modules. This actually is x4 because you can see one transceiver  here and another transceiver that can plug in. And then we accessed via an endpoint a CXL device. And while we don't show it on the illustration,  we're able to access memory over 100 meters of fiber  using some standard disaggregated memory point  to point solutions. If theoretically you throw the CXL switch in here,  which that's one of the next things we  want to start working on that enables memory switching  from host to multiple endpoints or from endpoint  to multiple hosts as well. So we're really excited about the work  that's been ongoing. We're starting to engage customers  that are interested with this type of platform. And we're starting to see the advantages of CXL over optics  moving forward, moving from proof of concept to reality.

 In addition to the optical transceivers that  support both the PCI express signaling and the CXL protocol  over the PCI express, we are designing  a family of industry standard form factor PCBs,  which make enabling this functionality easier  for the end market user. So the current card that was in the illustration  was a PCIe CEM AIC supporting PCI 4 x8. We now have a solution that supports x16. We're working on, as I mentioned earlier,  a PCI express 5.0 CXL 2.0 optical transceiver that  will drop into the same footprint. Our plan is to have that available  by the end of the year. The advantage of these add-in cards, highly configurable. Just as we can with the optical transceivers,  these add-in cards can support x4 x8 x16 configurations. Do it on the same card, a single x16 or a single x8,  a dual x8 or dual x4 or quad x4. As we showed from the previous proof of concept,  these can also--  the same PCB can work both on the host side or a target. And we're also looking at and developing  other standard form factor cards such as OCP NIC, E3S--  E3.2T, M.2, and similar for enabling CXL over optics  as these system architectures continue  to evolve due to AI and disaggregated computing.

We talked about the density that Firefly  offers in terms of its small form factor. Here we show in the circle there just one of our x4 Firefly  transceivers next to two QSFPDD form factor MSAs. So we see about a 4 to 1 space savings  on the PCB using a mid-board optical transceiver  versus front panel pluggable. Again, we're not trying to say that the mid-board optics works  in every application. It doesn't. But for applications where density, signal integrity,  CXL operability is there, our solutions  provide an alternative to some of the things that  are already available on the market.

So key takeaways from our portion of the webinar--  CXL over optics is a reality due to the disruption of AI  and disaggregated computing within the data center, which  we mentioned. We have the industry's most comprehensive portfolio  of mid-board optical modules, mid-board optical transceivers  for next-gen system architectures. And then there's some information  that you can look at. You can go onto our website to get more information  on our optical transceivers. You can email our optics group. And we're willing to engage on the applications  that we've been talking about. So with that, we'll turn it back to Nolan. And I think he'll lead us into our Q&A.

Great. Thank you. And thank you to all of our speakers for presenting today. So we'll go ahead and dive into the Q&A. If you do have any questions, please  feel free to submit them into the questions box. The first question that I have is for Oren. What are the expected latency numbers for max memory? Where are the performance improvements coming from?

OK. So latency, it's a good question. Latency, actually, we are looking at the NUMA hop. It means that when you're looking at the system today  in a dual socket, you have memory  attached to the CPU number 0. You have memory attached to CPU number 1. And we're looking at our system as the next hop. It means that assume that you have like a four-socket system. And the CPU number 2 or number 3, they have attached memory. And when we are showing the memory pool,  it behaves very similar to a multi-socket platform. The improvements, again, improvement  is a very good question. Everybody asks how the improvement is coming to play. So the improvement of the memory pool  is actually by providing more bandwidth. It means that the CPU have, whether it's Intel,  they have eight channels. Or in AMD, you have 12 channels to the memory. Assume that you have another channel to the memory. So the CXL is actually providing more bandwidth to the system. The other improvement is coming from the capacity. When system is designed, it's designed  for maybe an average type of application  that consumes specific memory capacity. But in a stranded environment, it  means that you need more memory. So assume that you have in your server,  let's say, 2 tera, which is expensive. But actually, you need 4 tera. Dynamically, in a memory pool, you  can get extra 2 tera of memory from the memory pool. So you use it for a specific or for a limited amount of time. And then you return this memory to the memory pool. And other system can have it. So bottom line, providing more capacity and more bandwidth,  providing the extra performance that  is shown in the benchmarks.

Great. My next question is for Ahmed. What are the best practices and challenges for benchmarking  and evaluating the performance of CXL solutions  for different use cases?

Thanks, Nolan. I think I'll start by best practices. And I think what Oren has covered predominantly  is latency measurements. I think understanding of what the target is. And as you mentioned, one NUMA hop  versus what are the key factors that  affect that measurements and choke points in different ways  from DDR speed to width of the CXL link  and that do affect that measurement. From also to add stress testing, understanding and evaluating  CXL as a solution in memory pooling or expansion scenario,  but under various loads to understand the behavior in C  where bandwidth versus capacity makes sense. And then focusing all that in terms of baseline comparison  from bare metal performance between different types  of memory, but also comparing it to traditional models  in these different use cases. To finally sort of understanding how properly benchmark  workloads to closely mimic the actual deployment  stack and actual use case. Now, the challenges here is the complexity  of some of these CXL systems. And I think right now we're in better place  in terms of availability as well. It's also that understanding that CXL  is an evolving standards and there is still  more work to be done. Benchmarks are also sort of evolving  to catch on to how to take advantage  of different tiering systems, memory tiering models  and leverage this new tier of memory. And then we have to sort of get over the integration  from a challenge from all the way down to the stack  to the application in terms of manageability,  but also as I mentioned, different ways  and efficient ways to tier memory. So I think those are the overall best practices and challenges  I can see, but I'm sure if Oren and Matthew have more  to add to the subject.

I think you covered it. Thanks.

Great. My next question is for Matt. Does the Samtec optical cable solution  include PCIe clock, sideband signals, and 3.3 Vox?
 
Yeah, it does. One of the questions that we get a lot  when talking about physical implementation  is how do you handle the sidebands? And that's available within the solution that we have. So that's actually one of the benefits of the approach  that we offer is it's quite elegant,  it's quite simple to use, it's quite easy to implement  from a design standpoint. And then with the ability to route the protocol,  the CXL protocol over the optical PCIe link,  it works quite nicely in a number of use cases. So yes, the short answer is yes to that question.

excellent. This next question I'll share with Oren,  but feel free to jump in, Ahmed or Matt. Can you share more information about the readiness  of CXL technology?

OK. So CXL technology currently, we already build a system,  we're already providing it to customers,  we already have a few use cases that are using the CXL. So it means that the technology is alive and kicking. From a hardware perspective, CPUs from Intel, AMD  already support it. Intel support it in the previous generation  of the Sapphire Rapid and today support it  with the Emerald Rapid. Next generation, of course, will support it as well. So it means that on the CPUs, we're  already coming to the second generation that's supporting  CXL. And because of that, it will be more deployable  in the coming years. But we're already having system with our customers,  so you can start testing it.

Yeah. I would just add too, we've been to a few industry events  over the last couple of months and several of the key players  in the memory ecosystem, not to mention anyone specifically,  but several of the memory players  within the ecosystem have introduced  solid SSDs, memory appliances, and other solutions  of the like. So I couldn't agree with Oren more that CXL is real. We're seeing adoption and we're seeing the pieces for--  whether it's the hyperscalers or equipment manufacturers--  to really build systems that leverage the benefits  and features of CXL.

Yeah, and I think the one thing I'll add on top of what Oren  and Matthew said is, in terms of standardization and compliance,  the CXL consortium and work groups are way ahead. And we're now planning the fourth and soon fifth test  event with multiple vendors coming together. And we can see the ecosystem growing.

Excellent. My next question is for Ahmed. Does the Leo platform provide academic support? If so, how can we access it for development and evaluation?

Yeah, so we do have an offering for research. And we encourage and we would like to partner  with any initiatives. So please feel free to contact me directly. We're slowly rolling out also an interop lab. And hopefully, maybe in the future,  be able to have access to racks in here in our lab. But until then, we can definitely  collaborate with sharing hardware. So please feel free to contact me directly. And we can work on that.

Excellent. My next question is for Matt. What's the time frame for PCI 5.0 and CXL 2.0 and PCI 6.0  CXL 3.1 optical CXL interconnect from Samtec?
 
That's a very detailed question, but a good one  and pertinent to our topic. We plan to have the PCI 5/CXL 2 optical transceivers available  late '24, early '25 at the latest. We're real excited about that. And we've had some development hiccups, but stuff happens. And then we're targeting PCI 6/CXL 3.x later in '25. So that's our current roadmap. Obviously, that's subject to change. But that's our plans for right now.

Great. This next question is for Oren. Performance storage systems claim low latency,  optimized for the AI market. How does the memory pool based on CXL  address the latency challenge?

 So the latency challenge is basically physics. So in some cases, we need cable. So cable costs latency. And within the system, we are behaving like a NUMA node. So when you combine the third NUMA node and the cabling,  you are getting the latency. Some of the companies provide services like memory tiering,  or they can support different latencies over the memory. And by that, they're overcoming the latency issue. So latency is a challenge, but we've  seen lots of mitigation software that can address it. So it's there, but it's a solvable problem.
 
Great. My next question is for Ahmed. And again, feel free to jump in, Oren or Matt,  if you have an answer as well. How do you foresee CXL impacting the design and deployment  of in-memory databases and other AI applications?

I think I can think of three major points. And I think Matthew and Oren will add to those. First, resource efficiency and some  of the points we discussed earlier  in terms of memory sharing, CPU utilization, all come to play. Lower overall latency is the second one. And I think this is a crucial part in terms  of optimizing the overall performance of these systems. And most importantly, I think what most of the value prop  here is highlighted is enhanced memory capacity and bandwidth  that allows these type of systems  to cache bigger, much more data, and hold a larger  amount of data in memory versus the old models.

Yeah, I would agree with Ahmed. To me, it's scaling. One of the biggest challenges that the industry is  facing right now is how do you handle AI models? And I know it's--  I feel like you're kind of beating on the drum,  but it's saying the same thing over and over again. But it really is true. The AI models continue to get bigger. They continue to get larger. And CXL is going to be one of the tools that  is available to help handle that,  to increase efficiency within the system,  to lower latency, to the cache coherency, obviously,  from a system level. And as CXL3 becomes available natively on the chipsets  and more of the infrastructure becomes available,  I think it's just a matter of time  before the market adopts it more and more.

I would like to add to this. We have seen NVIDIA GPUs that are already  having their own proprietary solution using NVLink. CXL is an alternative to the NVLink. We've seen AMD already presented at some of the events  that their next GPU will support CXL. And we have seen the whole market moving to a memory pool  based on CXL because this is not a proprietary solution. It's an actually open solution. The interfaces are standard. Everything is fully available, all the information. So you will see many vendors. And when you are having many vendors, first of all,  integration is easy. It's never easy, but it's doable. And second, when you are moving to a standard system,  the price are going down. So you'll have low-cost interfaces. You will have many solutions from many vendors,  both on the memory, on the interfaces,  on the platform itself, management, fabric management. So all those will drive the CXL-based solution  to be more commodity. And specifically for AI, this is very important  because you know how much AI system costs these days.

Excellent. I think we have time for one more question. So this one goes to you, Matt. Can you expand on what use cases may use optical CXL  interconnect?

Yeah, I would-- that's a $64,000 question. I think that as the optical transceiver solutions become  more readily available, as there becomes more of an industry  standard, the use case is really going to define the adoption. Obviously, there's a lot of optical interconnect  within the data center when it comes to GPU clustering. I think we've talked earlier about how  the advantage of using CXL for scaling and for latency  and for a lot of the use cases that are coming out of AI,  it's only going to drive adoption for CXL over optics. CXL has a ton of benefits, chip to chip,  on a PCB within a system. But to scale it using optics up to 10 meters or more,  I think is really going to drive it. For now, we've been able to use--  Samtec has been able to use CXL over optics for type 1, type 2. We're looking at type 3. We're really anxious to see how we can use optics in a CXL 2.0  solution when it comes to memory pooling or across a CXL switch. So I'm probably getting a little ahead of myself. But I think that the adoption of CXL over optics or CXL optical  interconnect is really limitless and will  be adopted more and more, just given some of the design  challenges or technical challenges  that we see with AI system models  and segregated computing.

Great. Thank you all. And thank you all for your great questions. It looks like we're all out of time for questions  at this point. If we did not get to your question,  we'll be developing a webinar recap blog  that will address any of your unanswered questions. Thank you to our speakers for hosting the webinar. The webinar recording will be made available on the CXL  Consortium YouTube channel. And the slides will be available to download on our website. Thank you.
