
Okay, here we are going to talk about hotness tracking in Composite Systems and I have Durgesh from NVIDIA and Larrie from Rambus and Samir from Microsoft. We have been talking about this in OCP CMS group and one good thing that has happened recently is now it's getting discussed in CXL consortium so they are looking into the details of how it has to be done. So this presentation is a little bit high level.

Okay so let's first start with the problem statement here, right. What we are really trying to do. We all know that CXL memory is useful for bandwidth expansion, capacity expansion, but it also comes with some additional latency adder and there are a lot of software applications that would like to have one view of the memory and it's better if hardware does some trick under the hood where you'll get unified memory with the same characteristics. So the other two statements on the problem is in the cloud environment, right, there is a run-to-run variation between the VMs and we'd like that not to appear. And the third thing is CXL gives us an opportunity to try new memory technologies such as DDR4 or emerging memory and that looks attractive but at the same time it brings different characteristics and how do we hide those using some techniques and that's the proposal here which we call hotness tracking.

So again going back to how CXL memory will be used, in software as a service where we have the total control over the software stack, we will be able to deal with two memory regions more effectively but in the cloud service where it is a single memory region, we prefer to have one unified view of the memory and if there are any techniques under the hood that we can use, that will be useful. So that's the overall thought here.

So the proposal here is to have a small widget or an IP you can call that can go inside a CXL memory controller, and CXL memory controller is an endpoint here, and the controller will track all the hot pages that are on that CXL memory module. And this widget is actually providing all the information to the software, and it is software's job to make a decision what to do with it. So the proposal here is we are building this hardware software interface, that's the number one thing, and without that software will not be able to use it, and the thought here is that we will provide these hooks and then software will look at these hot pages and it will make a right decision whether to move this memory from a CXL memory to a local memory.

So to get into the details of that, I will give it to Larrie. But just before that, there was a lot of debate whether this is an endpoint problem or it's a CPU problem and we looked at that and we found out that there are a lot of good things if it can be done on the CXL memory controller and that's why we are proposing this to be done on a memory controller.

So I will skip and go ahead.

Thanks Samir. So when Samir proposed this problem over a year ago, it was a little hard to wrap around because I'm a hardware guy and somehow software is going to do the right thing. So really what we have right now architecturally is the MMU and the MMU is very well suited for figuring out what is cold data. There are a lot of papers out there about this is cold and literally if it's in the CPU and it's not being used, it's technically cold and the MMU is going to tell you that. Now when we started adding CXL, we started bringing in new levels of hierarchy within the system here where now you kind of got like colder memory and kind of NUMA connected memory where the latency parameters are different. It's one of those things where it's really easy to push things down the hierarchy because the MMU is going to be really good at doing that. The question is how do we pull it out of that hierarchy? How do we say, well in reality this data should be closer to the CPU and it's all about again placement. It's a kind of a strange double-sided problem where if it's really hot, it's going to actually live in the cache hierarchy. So we're looking at workloads where the data is not cold but the data is not smoking hot that it's in the cache and trying to identify that from the traffic profile. The whole purpose of my presentation is to present the problem statement to the hardware guys because there's a certain amount of innovations required. So I'm not here with the right answer. I'm just saying this is sort of the envelope you should probably think about. To me it's a RAS feature because when you start putting these systems together with memory controllers, ultimately someone's going to complain to you. They're going to like, it doesn't work very well. This is just one of the tools. It may not be applicable to the software problem that Samir is talking about but from a hardware point of view it's a bit of a defensive RAS feature where we can identify bad behaving data flows and work through it.

So on that note, it's really about looking at the edge of the distribution of what we would say accesses per page. It is a massive amount of data. You cannot do just a simple basic view of track everything because we're talking about there's millions and millions of pages. You're being accessed every nanosecond. You can't build a circuit to track everything. So you're looking at the tail. Just like cold data is looking at the tail on one side, hot data is looking at the other. Now there are existing algorithms. It's funny when you start talking to the architects, everyone wants to build counters and little LRU caches and stuff like that but really there are existing algorithms out there. SSDs do it today. They sort of understand that well an LBA is being accessed more often than another LBA. They're changing their behavior. Telecom QOS, even Rowhammer at DRAM, it's a fairly defined problem that many people have thrown multiple solutions at. The hint I have is think about filtering out anything that's low access and then start collecting statistics to build off the top of that.

The performance goals, it's kind of like CXL. It's got to go either really fast or really slow. Some people would like from, if you're NUMA style, you want to bring hot page tracking to the millisecond resolution because again it's a swapping style operation where you identify something that's hot. It should be closer. Part of the swap infrastructure pulls it from the colder data hierarchy to the faster direct attached memory. You're kind of looking at that millisecond response time where if you look at a computer today it does swap a few thousand pages per second. Then you get to the people where seconds, if we deploy CXL there's a lot of existing applications out there. They don't want to sit there and manually work out what is hot within their memory map. They just basically says let's do some long tracing, figure out what's hot and then we'll fix our memory map. You've got this wide bound of performance. Because you're on the tail, you're kind of looking for a few thousand candidates. Not every memory segment or memory page can be moved. Things like PCIe operations can tack down or lock a page in place. It may be really hot because it's getting DMA traffic but you can't move it because it's getting DMA traffic. We have this tail that we're looking for. We've got a performance goal and then we're looking for solutions. The resolution 4K is perfect but again it's a big data problem that you're trying to do on a very inexpensive device with as little memory as possible.

We currently are working on, we've gone from simulation, we're kind of working, just kind of hoping you have better data here. We came up with a simulation model where we said well cores are kind of poisson distribution. If you sum that up that's a gamma distribution. The simulations where we took a gamma distribution of page access per page, we threw it at our algorithm. It works fairly well. We're trying to now map this into hardware and kind of keep going. But again, I'm sure there's a lot of smart people who will come up with different ways of doing this but technically it's still the same problem statement. What's interesting is a highly exponential response. If you look at the red which is the response and it overlaps the threshold just a little tiny bit, that's actually a very hard region of false positives that's hard to get rid of. It's really about getting rid of, keeping your false positive rate down. Your solution gets a little peaky in that regard. But this is an adaption. I'm from storage so I know SSDs and everything. So we took something from SSDs and started to warp it. When you get into it you'll understand.

But where I'm starting to have a problem architecturally is the consequential action because then it gets back to software. There are this feedback of the controller coming back and who does it talk to. If it's a closed solution as Samir says, they can do anything they want. They can put it in their hypervisor and start working with it. Arguably the OS would be nice to tell. It's like, 'Please, if this is hot, please don't put it in this memory segment or the application.' There is no right answer here but until we actually give the hardware tools to the software guys they're kind of blocked from innovating. We kind of got to get a hardware platform in place. But it's literally that control loop is where we're going to send it to. Samir just started the working group within CXL and that's where I expect that solution to be solved. One of the limiting factors, I talked about millisecond response time. Really right now the CXL memory mailbox has a time constant of a second or so. It's really a management interface. It's not going to support any real-time reporting of hot page information. If we want to go there we have to figure out how to close that loop.

Finally call to action. I don't know, do you want to talk? Well for me it's like explore on the hardware side of hot page tracking and hopefully we can start publishing some of our results.

We have written a white paper about this. It will be released soon. But this is an example of really community coming together. We have CPU, GPU, NVIDIA is represented here. We are users, hyperscalers, and implementers. And Rambus and companies like that will play an important role. Actually implementation is the hardest task. So I really want the whole community to come together. And this is not just a hardware problem or a software problem. So we need something like this through OCP. So please participate and try to promote this and get into CXL also. I think it will be ECN to CXL introduced just yesterday. And that will give you more details about it. So I'm very happy that it's moving in the right direction.

Hi, I'm from Marvell Semiconductors, Marvell Technologies. So how much do you see this kind of data which is warm, which is neither -- if it is very hot, it will be in the cache. If it's not that hot, then it is okay to be in the CXL. How much of data and workload fall into this category? What is the percentage of data in Azure or any kind of hyperscaler environment that fall into this category?

Yeah, I think it's at the end of the day use case dependent. I cannot generalize across the board. Some use cases it will happen more. And also it also depends on what's the ratio of the memory between the main local memory and the CXL memory. So all these factors are there. So Larrie, you want to add?

There's a number of papers out there. Google actually had one where they talked about their really cold infrastructure, right? And they made statements basically as Samir said, the ratio of latency. So how close cold memory starts to look like regular attached memory. It increases their ability to get into their colder memory. So it is very application dependent, right? It really comes to, you know, like are you trying to save space on the direct attach or are you trying to keep stuff out of SSDs, right? So that's what makes this kind of a hard problem to look at right now because we know architecturally there's this whole of how do we pull stuff out of this tier. The MMU TPP work that people are working on, you know, they're getting the cold side. That's been proven for a number of years. But it's pulling up. That's still a bit -- it's a bit researchy right now. But we kind of need it to sort of build the whole sort of complete ecosystem of control.

It's a good question. And the main thing which we observed is if you move the hard data sitting in CXL into the near memory, it's good because now it's in the near memory. But if you let it sit there, the most challenge you see is it's getting warm. And then you are at the fence whether you move it or not. So that has to be solved. But there is a decent percentage of data, especially in microservices, which starts getting from hot to warm. And that's a tough call.

By the way, there are other technologies being deployed. And please look at Hot Chips where Intel introduced something called tiering using CXL memory.
