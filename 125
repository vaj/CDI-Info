
Okay. Actually the hardest and the most stressful part is to get this connection going. Otherwise it's absolutely a blast to be here and sharing our perspectives with you and getting the OCP community involved and addressing some of the challenges. All of the challenges that the previous presenters made absolutely resonate and we don't want in this space for standards to be equal to slow, right, not in the AI space. We need to move standards along and make sure that we make a difference in the technologies that we really need to enable for AI. So I'm going to present some Microsoft use cases.

So my basic outline is to first of all appreciate the challenge of AI training systems. So it's really the context of AI training and how challenging connectivity problems are in this context. At a system level there's absolutely a tremendous amount of complexity, lots of components, lots of multiple interconnects. There's cost involved both CAPEX and OPEX-wise, energy efficiency related costs, but absolutely hardware costs as well, and reliability. It was really interesting and encouraging to see that all of the presenters here mentioning reliability because any time you see systems of this complexity you're going to see some serious reliability problems and they in turn present increasing costs. So bringing it down to interconnects, the problems are exacerbated by not only the back-end network but the presence of multiple interconnects. So a bit differently we're going to take the perspective of an individual accelerator, a fairly powerful accelerator and the system right around it and the interconnects around it. So I'm not only going to emphasize the back-end network in this talk but also what happens around with respect to other interconnects because there's a constant balancing act when you design systems and future accelerators between back-end networking connectivity and the rest of the connectivity around an accelerator. So when we envision an approach, a North Star approach, obviously we need to really drive and get to the best energy efficiency possible for target reaches and bandwidth. But the other thing that we want to try and particularly take OCP and the rest of the industry consortia's help is in really driving a degree of homogeneity in the multiple interconnects that are there because all of these interconnects are getting into many, many terabits of bandwidth around an accelerator. So how do we move forward along all of those vectors, all of those interconnects at the same time?

So first just stepping back, Microsoft and AI, lots of excitement around the large variety of applications that can be enabled. And larger the models, it does matter. They do increase fidelity. They do create exciting user experiences. So these larger models are pretty exciting. But they are here to-- they present unique challenges. And the challenge is very apparent when you see the model size growth. So just between 2020 and 2022, a 30x increase in the size of models. But if you look at all of the other component level technologies, whether it's computation, performance per watt, interconnect capabilities, all these other capabilities, HBM capacity, there are tremendous technological advances. But they still represent about 2 to 3x increase in capability. So there is not a capability at a component level that is singularly capable of bridging this gap. So that's where it is critical for us to be able to scale and scale extremely efficiently. We actually are looking for breakthrough in scaling efficiency, not incremental improvements in scaling, not just scaling alone. It is that breakthrough in efficiency that we are really seeking. And this scaling problem is not totally new. It's well understood in supercomputing realms. And it is really fascinating that for all of our user experiences, we are now dependent on a supercomputing class capability. It's not just a few scientists and concerns about how we might irritate scientists. But it's really about all of us and our user experiences and how we depend on such a powerful capability.

So let's zoom in into the connectivity around an accelerator. So to me, again, as we look forward, it's not about just thinking about the back-end network, but what do we build around an accelerator? How powerful is it getting? What does it need to connect to? And we have a little taxonomy here. And the taxonomy basically is accelerator to accelerator connectivity, both from a remote connection standpoint. So that's synonymous to back-end connectivity, right? The way others and everyone have been talking about. There's also relatively local connectivity. So this is in the immediate vicinity of a particular accelerator. And the thing about local and remote is that, there is a gray area. How local is local? Is it the next accelerator or is it a whole rack? So there's that tension between what is local and what is remote. And clearly something like when you connect 20,000 nodes through switches of any sort, then you're talking about remote and back-end networks. But the local network and the local connectivity, you can expect that to sort of grow a little bit. There is also accelerator to CPU connectivity through PCIe. And we should anticipate PCIe Gen 7 class connectivity. And 16 lanes of that is about 2 terabits. So that's getting into terabits as well. And there's accelerator to memory connectivity. For the most part, accelerator to memory connectivity, we think about immediate short-reach connectivity to HBMs. But we now need to think in terms of how we may expand beyond a package, right, and to expand memory. So there's a tremendous amount of difference that memory connectivity can make. So there's always an exploration about how do you go beyond a package and add memory capacity and memory bandwidth as well. So in summary, this is a complex system. Several components and interconnects. Four plus types of interfaces, some proprietary and some industry standard. So where do these go? Where does this -- so this is a generic picture of a typical system. It's an example. But where do these interconnects really go? Where does this system at this level go is a question that we want to be able to address. And that in turn should drive the right connectivity solutions as we go forward.

So when you focus on the interconnect problems, the broader problems across all of these interconnects is the scaling, the cost of scaling at an increment -- in increments of 10 terabits. So now you're talking about N times 10 terabits, right? So the aggregate bandwidth is going to depend on the solution, the time frame, the variety of workloads, et cetera. But across all of these solutions, we must come together and figure out how do you drive an increments of 10 terabits where N can be any number, perhaps pretty large. And this is per accelerator, right? The energy efficiency problem, you do want to get down to those low single digits of picajoules per bit, right? Something like 3.2 is super if you can get there. Heterogeneity of interfaces, right? So this is where having multiple interconnects, independently making progress on them, right, without synergizing them proactively is problematic. They all need to come together in the same system. They may all need to come together in the same accelerator, right, as we project forward. So you don't want to think of them as completely independent tracks. You want to proactively converge them as much as you can. Now, these are broad problems, but there are individual trends against every potential connectivity use case. So the remote connectivity use case, there's a serious scale-out cost. I think that's a fairly familiar use case. What is concerning is how it's bifurcated completely from any sort of local connectivity. So the definition of local is interesting as we go forward. We want flexibility. We don't want local to be just connecting up with copper, a few GPU nodes or accelerator nodes, right? It's got to scale a little bit. And accelerator to CPU, so there's a potential disaggregation story there where a pool of accelerators is connected to a pool of CPUs, and we can't assume that your CPUs are just right there and you can just connect them with copper using some PCIe. And accelerator to memory, so this is where I mentioned that expansion beyond a package gets really interesting.

So let's anticipate some system trends. System trends, whenever you see a tremendous amount of complexity, involves classic architecture techniques like integration and disaggregation, right? So whenever you see complexity, you try to tackle the complexity with some degree of integration. So integrate what is reasonable to integrate from a CPU package and die area perspective and power perspective. But you also try disaggregation, and that modularizes systems and can simplify the overall system complexity. So disaggregation of CPU resources is a good example. Integration of the scale-out network controller capability is important in the sense that just because you're scaling out, it doesn't mean that you have to depend on a very complex smart NIC and multiple of them in every accelerator box to be able to scale out to the large Ethernet-based or InfiniBand-based clusters, right? Some degree of integration there should be anticipated, and that means that your accelerator device directly communicates and scales out, has the capability to scale out to the larger network. And the memory expansion capability, which could potentially be very similar to local accelerator to accelerator connectivity, and you could envision fairly low latency and high-rate switches that connect not only accelerators locally but also to powerful high-bandwidth memory devices. So now when you think about bringing these capabilities together in a future accelerator, we are now shifting the problem from a platform perspective to the accelerator. The accelerator may have to take the burden of how to support this integration, how to support the disaggregation. So now you've shifted the problem of heterogeneity into an accelerator.

And this is a problem that really creates a very challenging design space, because on a per-interconnect basis, per-use case basis, now you have to figure out what is the nature of the electrical interface, what is the nature of the optical interface. So this is the problem that we should be very concerned about, because it's very hard to fork four different tracks in our large community. The industry is large, but we are still a pretty tight community. We know of a handful of industry standard bodies. It's very difficult to go and progress four of them at the same time along all of these dimensions. And there are obviously more dimensions when you actually get into the design of these solutions. So convergence becomes important, first to minimize silicon and package complexity. But that convergence actually helps you with respect to the system as well.

Now convergence is one thing, but of course we worry about energy efficiency. As we think about converging interconnects, let's also set a target in terms of how energy efficient do we want to be across these interconnects. So once again, we are trying to take the perspective of all four interconnects at the same time. And you can see that as we-- you can do a fairly straightforward baseline. Even the baseline assumes some relatively aggressive assumptions around picajoules per bit per interconnect, but it adds up very quickly. It adds up primarily because of your back-end connectivity and any memory expansion, but the other components are pretty significant too. So it easily adds up to 200 watts or so per accelerator device when you start to pull these things together. So the desire, the target is, can we really get down to 2 to 5 picajoules per bit? It is challenging, but it is not a complete-- there's at least some good prototypes and some data points out there that show that there is some line of sight to building it. So we must invest in them, understand them, and figure out how to drive them to a standard. So we want to get that down to below 100 watts with two technologies that offer 2 to 5 picajoules per bit.

So when you pull both the convergence story and the energy efficiency story together, what would an accelerator look like at a package level? So let's envision an approach to connectivity where you are doing-- you're supporting multiple interconnects out of an accelerator, but you're also driving a high degree of convergence. So the first element of the North Star here is that you have uniform reach, uniform short reach electrical interfaces around the chip, around an accelerator. You're driving uniformity. So imagine that you're supporting four different types of interconnects and all of them in a single accelerator. That's hard to do. You're driving some uniformity. You are associating specific protocols. If you really need to differentiate protocols, you're associating protocols with that connectivity using separate protocol chiplets. And you're associating optical to electrical or electrical chiplets to actually extend the connectivity beyond the package. So this is a-- it's a generic solution. It's an example, but it's something that we want to envision together and drive the ingredients of it. The benefits are across the board. So first, you're maximizing the benefits of optical connectivity. You're getting the best possible energy efficiency along with the bandwidth and reach. And then you're simplifying accelerator silicon on packages. You're simplifying the system because of the uniform approach to all of the different interconnects. And you're also simplifying the enabling in the sense that our community and the standards don't have to worry about driving four in parallel with relatively serious urgency. But we can focus on perhaps two. So significant convergence is possible.

So finally, calls to action. So from an OCP standpoint, a tremendous amount of synergizing is possible. If you look at all of the CXL activity and the other composability stories, it's possible that we can develop synergies with general-purpose compute disaggregation use cases. So that's something that OCP can and various folks here can help us identify. Let's focus on one or two standards to cover the four-ish different types of connectivity. We do need methodologies. I think there was a lot of emphasis on how do you estimate the cost? How do you drive the cost down? How do you actually estimate the cost for some of these forward-looking technologies? What does it buy us? So we need good methodologies that drive the cost down. We need to approach something like $0.05 per gigabit. That looks like a stretch, but we need to try and get there. Reliability and fit metrics, not very solid methodologies as to how do you start with something like $200 and drive it into some single-digit level of fit. What are the right methodologies to work on? And finally, the coordination across multiple forums. Anytime you say interconnects, easily five, six forums are involved. There are some new ones out there that include UEC and UCIE, and I think we really need to coordinate and look to OCP to help us coordinate those standards. Thank you.
