YouTube:https://youtu.be/6D4FIZmkA88
Text:
So, Frank called me a few weeks back and he said, "This is going to be, what did he call it?" I have a hard time to say. It was kind of a quick session where we can just go introduce ourselves. I just blanked out on the word that he had used. But essentially, I get to quick dates, or I forgot what the word was. So, what was that? Speed dating. There you go. That tells you how out of touch I am with the dating scene. And I got confused. I said, "Okay, I got to create a couple of profiles." So, what you'll see today is going to be, I'm going to use two profiles. One is going to be something which I'll talk about OCP, and that's my hat. And then the second one is going to be, I'll speak of Meta, and we'll go through that. And it's speed dating. So, I have 15 slides. I don't think I'll finish in 15. So, I'm going to go quickly. And I have a very high baud rate, as you can imagine. So, please feel free to ask me questions later. Stop me anytime if you want to. And we can try to get through as soon as we can. 

So, what we'll walk through is basically, as you can see, we'll keep switching back and forth between two things. One is, what is this software-defined memory work that we are doing in OCP? And then, how does it relate to what at Meta we see from a hyperscale needs perspective? And we'll go find out what memory hierarchy means, what the solutions out there today are, where we think there are the gaps today. And we are the end users, so we really look forward to smart implementations in the industry for solving our problems. And we will share. These are pain points, but we're really looking forward to seeing what the solutions are coming down the pipe, and we'll get to that. Then we'll jump back again to OCP and say, "What is the software-defined memory group doing? What are the use cases and where are we in the status?" This is, of course, community work. And thanks, Samak, for the shout-out for the OCP work. We are really excited to see this use case growing. CXL is seeing a lot of good opportunities for us. So, we want to see the community get involved to contribute. And I'll talk a little bit. I'll talk a little bit about that. 

Back on my Meta hat, let me connect it back with why we think this is going to be important. I mean, when you look at Meta or all our applications that you see, which is Facebook, Instagram, or Snapchat, no, sorry, WhatsApp, all of these applications run on our data center infrastructure and rely heavily on what we put in there, what performance we can give, what kind of growth we can give. And all of these components need to continue to grow. And all of these components need to continue to grow. And all of these components need to continue to grow. And all of these components need to continue to grow. And all of these components need to continue to grow. And all of these components need to continue to grow in capacity, but at the same time with performance while keeping the balance of the TCO, total cost of ownership. So, all of these components are important when you put CPUs, what NICs we put, what SSDs we put for DRAM. And as we look at this for presentation, let's focus on DRAM or memory because, as Charles said, memory is a significant part of our infrastructure, significant part of our server TAM. 

So, let's go and dig a little bit deeper. From memory perspective. Let me see there are three aspects you can look at it. How much capacity we put, what kind of performance we expect, what tradeoff applications are ready to do, and what options do we have. And eventually how does that translate to the TCO. As you can see, there is a significant gap growing between the capacity that we land and the bandwidth need. Applications that need performance need to continue to land more and more memory because they want to achieve that higher performance. And applications that need more capacity also tend to need to land more and more capacity, but they will get a lot of bandwidth that they may or may not be able to use. So there is a discrepancy there right now, the way the capacity and the performance keeps growing. And at the same time, as we keep on seeing the DRAM die sizes and the design points growing, you will see the bandwidth per gigabyte is also not necessarily keeping in line with the compute performance that is growing in our platforms. So there is a trade-off that is required for optimizing the capacity that we land in data center where the choices are relatively limited. Power is another important aspect for us. Anything that will land at a hyperscale, the design really is, everything is around power. Even if I say I want to ignore CapEx for a moment, to land that gear for availability and keeping up with the demands of hyperscale, we have to keep basically making sure that how much power the data centers are consuming, how much capacity can I land. And memory is a significant part of that power contributions too. So as we look at increasing the bandwidth and the performance as well as the capacity for DRAM, we want to make sure that we can achieve the kind of power goals that we have for the platform and the capacity. And all of this always adds up to the TCO. The capacity that we land and we use, of course, is going to talk about how much CapEx we are actually effectively using. And the power that we are using, of course, is going to go into the OpEx part of it. So there is a need for optimizing the memory configurations for our applications, especially from the Meta's point of view when we are looking at it as a private cloud. We understand our applications and we know the applications that we talked about before, which translates into the databases they use, the data warehouses that they use, the kind of application backend that they have for serving the ads with the training and inference infrastructure. All of that we can optimize appropriately if we have the right components available and make sure that we have a balanced system. 

So when we look at memory, there are different skews. You try to maintain as small number of skews as possible to make sure that we achieve the economies of scale. So you will say for this much of computer, I need this much of memory. And what kind of memory I need? The memory you can look at as a variety of options you have today. It can start from caches that sit inside the processor. It could be HBM that is connected to your GPUs or DRAM. And then you go on and on and on. And you can have a tiering that goes even up to the block devices which hides the memory. And then you can have a tiering that goes even up to the block devices and block access and make it look like memory. So you have all those tiers available. And then different applications use different tier. As we go through the remaining part, we'll see that the DRAM tier, right now this is just one monolithic tier that as the CPU grows to the next generation, it tells you what DRAM you can put. If I'm on a particular CPU, I'll go to DDR4. Next generation, I'll go to DDR5. CXL brings up an opportunity where we can start decoupling and then make sure that you have the cool solution that we want, which is what all people are excited about. 

So when you look at this kind of a trade-off, let's look at different use cases. The slide that I went through quickly, you had databases, you had different training and inference applications. All of these have different needs for how much performance and bandwidth, performance and capacity is needed. For example, if you look at caching use cases, they can achieve, their goal is basically to make sure that they can continue to deliver higher queries per second while making sure that data is available for a given amount of time. This is typically the case. This is typically going to be driven by saying how long I hold the data. That means you're going to be driven by capacity if you want to deliver the performance out of that DRAM. If you look at databases, databases really try to deliver how much storage I have on that, how much capacity I'm storing on those platforms. But to enable the performance out of that SSDs, you need memory. Your block caches, mem tables, all of those have to sit in the DRAM. So amount of memory enables you to achieve better efficiency on the platforms to deliver the vendor capacity, which is your storage capacity. Inference, I'm picking one of the AI ML use cases. This is another thing which actually it's important to make sure that your accelerators and your GPUs are all constantly fed by the information that they need to continue to basically work on, which basically means they are going to be driven by, and we have a little bit more details. I'm going to go through that a lot more. These AI models that actually are driving this inference infrastructure, they are really scaling much faster as compared to the underlying memory technologies, especially from the capacity perspective. And so this is where I think for all of these, tiered memories start becoming a very interesting solution, where I can have different tiers of memory, which goes and allows me to scale the performance and bandwidth needs independently. 

Let me step back a little again and say, let's double click on the inference part of it. So the AI is a significant part of our infrastructure, growth as it is true for most of the people out there in the infrastructure. 

Let me go into the specific workload requirement that we see driving this. There are two aspects that we see. As the AI continues to grow in the performance need, there are interesting charts from two of them that you can see. The left side chart, what it shows, is basically how many petaflops. It's a very compute-intensive workload. So petaflops that these models are continuing to drive is increasing. The line looks linear, but if you watch carefully, the y-axis is logarithmic. So the performance is actually going exponentially up. So one thing is basically to, if you want to keep these models fed, you need to have enough amount of data that you continue to feed on it at that kind of performance level. By that performance, I mean, of course, it's a compute performance, which drives up need for memory capacity. On the right-hand side, what you see is basically these models are getting complex. The number of features that you feed into is also growing dramatically. In the same way, actually, this is also another chart that shows a significant growth into the amount of memory that you need just to maintain the number of features. And both of these things are multiplicative. So to feed the accelerators that are actually consuming data fast, you need to have enough data to feed them. And the data that is being fed into these accelerator machines is also pretty large. And so the overall data that gets into this machine to make sure that you're optimally utilizing this, is a very large amount of memory, which starts becoming an interesting problem for solving at a platform level. That you're going to max out of the amount of memory that I'm going to have, and this is where CXL can become a really interesting solution for us to enable beyond that. 

But let's dig a little bit deeper even more. Let's look at what kind of needs are there, what kind of trade-off we are doing. So connecting it back to the point I was making, that there are ways applications understand what do they need. When they mean, I need performance, at what point do I need, what is my code and what? I think Charles made a point about Microsoft's paper which talks about saying that there's a certain amount of memory that perhaps is not being used, or a certain percentage of memory that perhaps can trade off the performance. On the DLRM, by the way, we have a paper published out there, I'm not sure whether the link is there in the paper on my presentation or not, but we will definitely add it there. So deep learning recommendation models, which is what the DLRM stands for, what we see is that bandwidth perspective, if you look at the top right, top right chart, you have bandwidth going up, and what you can see is basically bandwidth goes up as you come closer to the processor or the GPU, where your accelerator memory requires to have the highest bandwidth, but relatively smaller capacity. But total memory basically requires, which can come close to the kind of memory that you can get it from a storage device. So I can use very large capacity, which perhaps does not need a lot of performance, but then there's a small amount of capacity that needs to be very close to the accelerators and that requires very high bandwidth. So that's one way of looking at it on that type. However, it has a different latency characteristics. If you look at those three, it's a reverse chart of the top, you have the storage on the top, that's a high latency that storage device, like a block device, provides. And the bottom is basically you have your main memory, which is the lowest latency that you have from the compute perspective. So what we see is basically for accelerators, the memory latency, the requirements are tighter and they're very close to main memory. They could be lax, they could be lower than the main memory, but it is not as the higher latency that block device provides can be problematic for us. So as we can see that, so we need something in between. We are ready to go with the bandwidth that storage devices provide. However, the latencies need to be pretty close to the memory. 

So as we start looking at this, basically now I'm going to skip. There are a few things a lot more details that we can come, why essentially we are making the same point that, you know, hey, if I had used TLC NAND flash, what's my problem? That I can use the capacity that TLC NAND flash provides, but from latency perspective, my requirements are much stringent and that's what creates the challenge. 

And we need something that comes somewhere in between DRAM and NAND. So if you remember previously, what we had drawn was basically between HBM and NAND, there was one big pool which said DRAM. But we really need something in between. These applications really do very well from performance perspective, especially latency perspective, if I can get a load store interface because it's much more transparent way of utilizing it. Otherwise, I need to create a layer which maps between DRAM and NAND, which again, we do that in some of our applications and we definitely can use in some applications. But on these specific applications, many applications, what we see is that there is a breed of memory required in between that allows us the right trade-off, which stays close to the DRAM from load store perspective, cache line read-write perspective, allows us to scale this linearly from performance perspective, allows heterogeneous. By heterogeneous, I mean you want, this is what the CXL brings into the table, that I should be able to imagine basically different types of DDRs. For example, DDR5 and DDR4, different memory technologies to be combined. Today, they are combined together because that's what CPU would come out with. That's the DDR performance I'll do. But I should like to have the performance trade-offs, capacity trade-offs that different technologies bring in and connect it together in our system if I had to memory layer in between a traditional DRAM and a NAND SSD. And at that point then, I have software capabilities to make sure that what sits where, what sits into my native or near memory, and what sits into my far memory, which again could be composed of different technologies. I do have small details in there. Some of the applications which really want bandwidth memory, in the sense they do need high bandwidth, there is going to be higher bandwidth need and their capacity needs are relatively lower. It will go into some of the applications. For example, we talked about which applications really want to scale for lower, higher capacity than we have local, but they're ready to live with the higher latency, but their bandwidth requirements are high. Reverse of inference applications like other web-based here and other ad skiers, et cetera, that we need. Capacity memory is what I focus most of our discussion so far. This is where you want to have memory-like interface, higher capacity and lower latency relative to NAND devices, but relaxed latency as compared to native DRAM. 

Let me kind of try to map it back into the different SKUs that we'll see. And this, you can map it. You can map this back into the OCP servers SKUs that you might see. And this is a representative of what you'll see in the industry also. You'll see compute servers, storage servers, training servers and inference servers and different technologies and where we see. What we see is basically HBM is primarily, of course, used in training and inference area. CXL plus DDR has a potential for us to enable the inference use case much more significantly because that allows us to go much higher into the capacity and applications will be ready to make the kind of trade-off that we think that can be offered for such capacity memories. 

Okay, I'm going to come to the last part where let me try to wrap up with where we are. So let me just give a background of SDM, software-defined memory, where we came about. What we see is basically there's a large ecosystem that slowly needs to come up from hardware perspective that offers CXL solutions that allow us to have this kind of a capacity and performance trade-off for DRAM and a new breed of solutions that come from the hardware side. Similarly, there's a whole amount of work required. Multiple people talked about it today. Charles talked about it. We had VMware talk about it. And, of course, we had AMD presentation also. There's going to be significant amount of software functionality that requires to enable such kind of tiering across different tiers of memory. And all of this gets combined together for what applications we'll be using, how they will use it, and what kind of benchmarks can be defined so that anybody who brings these novel ideas to the table can really demonstrate the value by using some consistent benchmark. So that's what the SDM community is working on right now. As we look at the next-generation OCP servers to bring forward with the CXL enablement, we want to bring this CXL hardware contribution, software contribution, and make sure that they're tightly coupled together with the hardware-software co-design functionality, methodology, using common knowledge and common culture and common benchmarks. So we have... A few people have been working on it together. We created... We started this work as a part of what you call as a future technology initiatives within OCP. This is more of a forward-looking body of people who will look at saying that what's coming down the pipe and how do you make sure that we enable that into OCP. So that's what we started almost one and a half years back in OCP. This group is right now focusing on three major work streams, in a way. One use case is basically focused onto the virtualized servers, database and caching is the second stream, and the third one is basically AIML and HPC. We have had multiple people who have brought forward their hardware and software and use cases as well as benchmarks. You have the names of those. Those are the people who have come forward together. We have vendors now. All of us, we have heard that vendors are getting ready for demonstrating their products. We will see perhaps today and tomorrow more here at FMS in the exhibition area. We have multiple end users who have already brought forward their use cases, and we are looking forward to basically bring this force together as we get into the OCP, global OCP coming up in October and see what does it mean, how do you go together, do we need to have a space, a regular space for making that as a contribution that we can bring in from hardware-software perspective as a co-design. So that's where we are. So this is kind of a shout-out for everybody to say that please do, and shout-out for OCP also, please do join us actually when we start talking about whether we should, how should we take this effort forward and enable the very thriving ecosystem, which is important for Meta, for all hyperscalers and the whole industry. We are actually end users for the technologies, so we rely on the smart people and smart product companies to make the products, so definitely we look forward to having a very healthy ecosystem for us. And that stops my pitch. Thank you. Thank you.
