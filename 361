
My name is Jason Molgaard. I am a Storage Solutions Architect with Solidigm. I also have a number of different roles within SNIA, including co-chair of the Computational Storage TWG, and co-chair of the Technical Council. One of the activities that I've been working on with some of my colleagues up here is a joint collaboration between Computational Storage and SDXI. SDXI is another TWG within SNIA, and Shyam Iyer is the chair of that group. And so we have been exploring what does it look like to combine these two technologies together. Last year, Shyam and I gave a presentation at SDC, and we didn't even get through half of our slides before we started getting peppered with questions. Why would you do this? What are you thinking? You guys are crazy. What's going on? And so we thought, OK, well, this year, instead of having slides for 50 minutes, we'll have half as many slides, and then we will jump into a panel discussion so we can talk about it. We want your input. This isn't just Q&A. It's a discussion. All right, so with that, I'm going to have each of our panelists introduce themselves. And we can start with Mats right here at the end.

Yeah, I'm Mats Oberg. I'm doing signal processing for data storage, and machine learning as well as computational storage at Marvell. I've also been very involved in SDXI plus computational storage work within SNIA, and computational storage work within SNIA as well.

So we have memory in the networking devices. You have memory and storage, CMB, SML. You have GPUs that have large amounts of memory in them. And as you've heard, you know, throughout today, or this morning, we have CXL tiered memory that is coming into the system. We're also kicking around ideas of putting larger amounts of high bandwidth memory into systems. The GPUs are exploring this now. A lot of GPUs have HBM already integrated. We're kicking around some ideas of possibly putting it into the CPUs. So I guess the point here is, memory is just not concentrated in DRAM. It's all over the place.

So one of the things we have to start looking at is, OK, now we have lots of memory tiers. If you look at a contemporary memory pyramid, you've got the CPU on top with the registers. You've got multiple layers of cache. You could have high bandwidth memory. You have your main memory. And then everything going down to CXL. We have various flavors of persistent RAM. We have NVRAMs, stuff like that. So with all of these different pools of memory scattered all throughout the system, we really, really need to have an efficient data mover that can move data between these different tiers and also do transformations on that data. You don't want to have to rely on the CPU doing everything. So that's one of the reasons we're into this. So with that.

Thank you, Bill. So I think he laid out a great picture of how the systems are evolving. And they've been evolving for a while. And we've been seeing those trends. And that's one of the reasons that we, early on, in the office of CTO at Dell, we had been doing a lot of POCs on how to accelerate workloads and applications. It came down to, how do we actually do this in a more holistic way? So that's why we started working towards a specification and came to SNIA, where this community of storage, memory, software developers can look at these from various use cases' point of view. So the standard is a memory to memory data movement and acceleration standard. But it's also extensible, meaning that if you have the right framework, you have the right software investment done, you can keep extending it for any kind of memory or acceleration operations that you want to fit in. And it's also independent of where you implement it. So a CPU could implement it, a GPU could implement it, a SmartNIC DPU could implement it, an FPGA could implement it. And it's also architecture independent. And that's kind of what makes it very unique, in the sense that when you have proprietary implementations that are making sure that they bind you to an instruction set architecture or something that only works with their software stack, then you need something that you can rely on, not just for making it vendor neutral, but also for the fact that when things go wrong, you need very architected states. You don't want to go rely on specific software vendor or proprietary hardware vendors' specs to know where the hardware failed when you were trying to do an acceleration operation. And that's why we're doing this in a very holistic way. This is also meant to do address space to address space data movement, whether that address space is within a version machine, in a user space, or in some other device's memory. And it's also independent of the IO interconnect technology, which means that as long as your memory structures are adhered to the standard, you could be moving the data using PCIe, you could be using CXL, you could be using a proprietary interconnect, or some other bus that you want to design to make your data more efficiently. But the standard thrives where, in terms of launching it, completing it, in the extensions that you can derive, and the common software that you can write to leverage the right kind of APIs. We have released 1.0, so you should be able to take a look at it and see it for what it is in 1.0. But we are also working towards 1.1. And a big focus of that has been developing a user space library that can be leveraged by a variety of applications. Now, having said that, user space library is not the only way for your application to live in. You can also be a kernel mode application that is trying to move data or transform data within the privileged software use case.

These are the different use cases, and it's an eye chart in the sense that it's just giving an example view of how you can look at it. Starting from the left top corner, this is a simple use case of a data moment or a memcopy offload. If you had an accelerator to perform this kind of a mem copy for you, you would not be spending your time doing multiple buffer copies in your software stack. The second to the right after that is the persistent memory to volatile memory tiering use case, which means you can push your data to persistence using this accelerator interface and then bring it back from persistence. And it will just work because these are just memory pointers. The third one on the top right is a virtualized data moment use case. Which means that if you wanted to just move your data from a virtual machine buffer to another virtual machine buffer, you have direct access to this accelerator interface. And now you don't have to perform multiple buffer copies from a guest to a hypervisor and then a DMA buffer before you go do the same thing on your way back. The bottom left picture is about how do you use this interface to tier memory, whether it's CXL memory, high bandwidth memory, DRAM memory, persistent memory. Now you just need an architected data mover that is doing this kind of tiering while you can focus more on processing on data in near memory. And then the one in the middle there is about how do you do transformations? For an example, if you are just doing a memory copy, but while you're doing the copy or a data moment, you wanted to compress the data or you wanted to encrypt the data, you want to calculate the CRC on the data, all of these are data transformation kind of operations or calculations or computations that you want to perform as you're moving the data. This is the framework that allows you to do all these type of operations and now you have a standard to leverage.

If you want to know more about this, I do have another talk later today. While I go into a little bit more detail, I don't have so much time as in about an hour, but still, we will try to focus on some of the details of the specifications and how you can make use of it, and what we are going to do in 1.1 and beyond. That's very important as the group starts focusing on the various use cases that the membership brings us. With that, I would like to hand it over to Jason.

Thank you, Shyam. Shyam gave a little bit of an overview of SDXI. For those of you who are in my presentation in the previous hour, I apologize. Some of this is slightly repetitive, but I'm going to keep it very succinct. For those of you who are not familiar with computational storage, I just want to give a little bit of an introduction. The definition that SNIA has come up with for computational storage is computation coupled to storage, offloading host processing or reducing data movement. There's two elements there, offloading and reducing data movement. It isn't necessarily that we're eliminating data movement entirely, and it depends on the workload, without question. That's what we're going to talk about in our session when we get to that point. On the right-hand side, I've drawn this very simplified picture that shows a host with a fabric connected up to a drive. This happens to be a computational storage drive that I'm depicting here, where you've got this large teal-colored block of compute resources. I think the key is it's a drive at the end of the day, and as much as it's got device media or device storage and it's got a storage controller in addition to that compute.

So SNIA has defined three different architectures for computational storage. The computational storage processor on the left, this is a device that doesn't have any device storage but is able to offload the host and perform computations. So this would be a great candidate for you need to move data into it, perform the operation, and then you move the result out. The computational storage drive in the middle, this is the more detailed picture of the picture I just shared on the previous slide where it's more of a traditional drive that also has compute resources. And then lastly, the computational storage array on the right, which is an array that you'd be familiar with, with multiple drives that has computational storage resources.

So this is more detail on each of those little blocks within that teal colored computational storage resource. This is blocks that was listed on each one of those architectures. I am not going to walk through this just given our time, but if you can go get a copy of the slides and if these terms are unfamiliar to you, then here they are. And you can take a look at them offline, and happy to meet afterwards as well and explain any of it.

So with that, what is this subgroup and what are we doing? So we are a collaboration between the computational storage TWG and the SDXI TWG, and we are exploring how could we combine these two technologies. And we've got kind of three objectives that we set for ourselves. Number one is to develop a unified block diagram that imagines a combined computational storage and SDXI system and architecture. And I've got a couple of slides that can kind of address that here coming up in a moment. Develop use cases for SDXI based computational storage devices. And we've got some use cases that some of our other panelists will discuss coming up. And then, thirdly, to consider if enhancements to NVMe are necessary to enable this combination. This activity is still very much in its infancy, and we just have nothing ready at this time. No conclusions to share yet, but it is an area that we are investigating. All right.

So if you were in my talk in the previous hour, or if you were in our talk last year, you may have seen this picture. So share it again. This is one of the early diagrams that we had prepared early in the process of this subgroup where we've got two different computational storage drives along with two different hosts and then a CXL pool memory over on the right-hand side. And basically, we're showing with the different red and green arrows instances of where you may need to move data. Where SDXI could help do that. So the red arrows are showing where your computational storage function or your CSF or your CSEE would be the producer. That would be where you'd have your SDXI instance and it would initiate the data transfer. And then the green arrows are showing where the host might initiate a transfer. And I think that one of the very interesting aspects of SDXI, yes, is the fact that as it's moving the data, it can do transformations. And that could be part of the solution for your computational storage that you want to do, right? If you want to bring data in and you need to do an operation on it, but you first need to decompress it, the SDXI could do the decompression as it's moving that data into your drive, and now your computational storage engine can do whatever else you need to do with that data, whatever function you want to perform.

All right, so this next diagram, there’s a left and a right half, and it kind of depicts some of our additional thoughts on having the SDXI internal and external to the NVM subsystem. The left-hand side is showing SDXI external from the NVM subsystem, so this could potentially be in the host, or most likely would be in the host, and the host. The instance of that SDXI could be used to initiate those data transfers that were in green on that previous slide. The right-hand side is showing the SDXI in the drive, so you could actually have the drive initiate the data transfer to itself or to another drive and perform that data movement to get it over to a different computational storage drive, performing a transformation on the way potentially.

All right, so with that, we've kind of talked a little bit about our first objective, which was to develop high-level diagrams, so I'm going to turn it over to Mats, who's going to talk about one of the use cases that we've been considering.

Yeah, thanks, Jason. So obviously, compression has been used for a long time; in particular, audio and video is very common. It would be too hard to send any data if we didn't do that, but of course, there are all different kinds of compression algorithms for which you need to standardize. For example, YouTube video needs to use a standardized compression so that the user can actually see it. But for more kind of other compressions, for example, if we're talking just memory compression, which is basically a way to save cost of memory, there's really not a standard for this. And in most use cases, it doesn't really matter if you just compress the data and decompress the data. That's not that important. But now imagine that we could actually compress the data on one device, and as we move it to another device, we move it in a compressed format. And for that, of course, we would need a standard so that we can decompress it on the other side. So we think it's important to get some kind of compression, and we think there, as actually SDXI can be really helpful. So the transform in SDXI, we can, for example, compress the data, and that's what we focus on on this slide. And then we also look at that compression itself is maybe one of the more common use cases that's been discussed for computational storage. I mean, why do we need computational storage? Yeah, compression is definitely a use case there. So here's one way where we can relate. Okay, let's try to tie the two together. So SDXI data movement, the transform to data that does the compression, and with the computational storage, we implement it this way. And as I mentioned earlier, I'm in the version of 1.1 is working towards standardizing memory to memory transformations. So of course, compression, as has been mentioned, is one of those. But obviously, we have many other transforms that can be used, and many of those, for example, CRC computation can also be used as part of computational storage. So with the help of this SDXI transformations, these can be the computational storage functions or the CSFs. So basically moving data from the internal memory to another internal memory and compressing, transforming them, the movement. So with that, I want to hand it over to Fred, who will talk a little bit more about NVMe compression.

Thank you, Mats. So clearly we have to be able to make this usable from an application, have an interface to get to the device. We have the SDXI committee working on that within SNIA. But we'd like to be able to hook this into NVMe directly to find a way to make these operations available in that kind of environment. So what we've taken here is the compression use case. And you can see that we have our host with its memory, which has the data in it. And we transfer that data out to the device, maybe to the CMB, and that device, maybe to the SLM memory in that device, using a, a new project that NVMe is working on TP4184. So we get that information out in the device. So now we can do the computations on it. So why is that interesting? Well, host memory gets beat on a lot to you. Think about what happens when you have to do a compression. You suck in the memory into the CPU, you run the algorithm on all of that data, and then you send it back out again. If you're going to do a protection information calculation, but, before you store that information, you've got to pull that memory back in again. You've got to run the algorithm to do the protection information, save it back out to memory. Again, that memory is getting a lot of reads and a lot of rights to do these calculations in the CPU, in those 128 or 256 cores, however many you have, all those cores are really beating on that memory. So if we can get some of that calculation done outside of the main memory, we get it done in the device. Then we want to take all of that data that we're operating on and get that moved out to the device so that we can operate on the data locally. We can free up more bandwidth for that memory to do other useful things. So in this case, we have the data in the CMB. We have a compression engine within one of the computational functions that operates on that data. And then we can use the operations within NVMe to save that data directly from the device, from the CMB into the, to the persistent storage device. So you can think about just the opposite as well. The decompress, we pull it out of the persistent storage into the local storage within the device. We perform the decompress operations and then transfer that back to the main memory so that the application has access to the raw data. So we've thought about this particularly for compression, but there are other use cases as well. Mats mentioned the PI calculations that I was just talking about performing that on the way as that, that transformation is happening as that data is moving to the device. There's encryption algorithms that devices perform today as the data moves into the device. The self-encrypting drives do the encryption before they store it. They also do it the de-encryption as it comes off of the persistent media on the way into the host memory. So are there other cases where encryption would be valuable and useful to have the encryption performed in the device itself before you, ship it over a network or something like that? So there's other possibilities other than just compression or decompression. There's other algorithms that can be run in the device. Raid algorithms, you know, can those kinds of algorithms be used as the data is being moved in and out of the device rather than having to fetch that data multiple times to run those algorithms? You think about a complex, you know, 20 or 30 device raid set where you've got to run multiple parity algorithms. Can you run those all at the same time on the same fetch of the data to produce different output buffers? Can that data be stored using the peer-to-peer transfers that we'll be talking about next so that we can reduce that overhead? And with that, I'll have our next speaker come up and talk about peer-to-peer.

In the peer-to-peer use cases, basically, as I described earlier in the talk, you know, we've got memory scattered everywhere and through a lot of peripherals. We've got it. We have it both inside the CPU, external to the CPU, uh, could be CXL, it could be in the GPUs. So it's really inefficient to try and think that we're going to, uh, basically just have all of the data that comes in and out of the system flow through the CPU and through the DRAM main memory. So one of the things we've been looking at is some of the peer-to-peer use cases. And one of the good ones is either having data come in over a network. And, in, uh, the case of AI/ML, have it transferred from the network controller directly to the GPU or have it, uh, move directly from the network controller into a persistent memory like, uh, an NVMe drive. So we could use the controller memory buffer as a way of dumping data from the networking device into the storage device. And, you know, getting back to, the original point of this discussion, to do that, you really need a standardized data mover. So that's where SDXI comes into this, um, mix of, we need a standardized data mover that allows us to do some of these peer-to-peer, uh, uh, use cases. Uh, CXL is another one. We could dump, uh, data directly from, uh, the networking device into a CXL memory and bypass the, uh, the DRAM, the DRAM main memory.

All right, so let me just kind of wrap up our presentation portion and then we're going to get into the discussion. Um, so you know, SNIA is developing both SDXI and computational storage. These are two different, um, technologies that are both part of the SNIA accelerate pillar. Um, and therefore they, they kind of actually have a natural way to blend together. We suggest, we think that that the SNIA initiatives can help with NVMe workloads and data acceleration. And we think that a system view is definitely important, uh, to find the right fit for your various data acceleration and compute requirements, especially as we start thinking about AI. I mean, that's obviously the hot topic of your presentation. Doesn't mention AI, then you probably need a new one. Um, so it, you know, but, but without question, there's a lot of data involved in AI. We got to, you know, it's gotta be moved. You've got to find the right compute. You've got to find the right compute solution. Uh, and, and so is it going to work for everybody? No, but it certainly is going to help, um, with, with some of the ideas that we've been exploring and, and through that collaboration, you know, maybe we'll come up with something really, really innovative. So if anyone's interested, certainly, uh, encourage you to come join us. We're always interested in having new people to give us new and fresh ideas.

So with that, now we're going to move into the panel. Um, I think that I will come around with one of the mics for anybody who wants to, to ask questions. That way we don't have to try to repeat each and every question. Um, does anyone have a question that they want to ask first?

Hey, how would this compare to like Intel DSA or other programmable DMA controllers that already exist?

I can take a bite. Ah, so as far as I know, this is the only standard architected data mover, and you know, we were very clear that there, this is not something rocket, you know, it's really novel in the sense that it isn't, you're not the first to get a data mover solution in front of an audience, right? Everybody has had worked with DMA memory to memory, you know, switches have had it. The problem is, everyone comes up with their own drivers, their own, you know, uh, nuances about how to program them, how to operations complete, what kind of error codes they have. Uh, how do you do telemetry with that? Uh, the challenge is, as we are in this accelerated world, how do we integrate all these different solutions, uh, so that we don't keep doing the same kind of investments again. And I think to, from a comparison point of view, implementations are going to be implementations. Performance is going to be based on whatever the implementations do in terms of juicing out the performance. The standard provides a way for these different implementations to come together and, you know, bring in their, their, their novelty in terms of the use cases.

So, yeah, you, there is a possibility that APIs could become common over a period of time, uh, as different vendors can look at our open standard. We have released 1.0 and so, you know, vendors are free to look at it and start modifying their APIs, and over a period of time they can start converging. But, uh, just from a comparison, they're just two different, um, you know, one started with a proprietary stack in mind, and you know, this is more open.

Can I just ask a follow-up? Um, so I think part of the reason why the situation is as it is because many of these data movements are application or system specific. So, um, you need to know about the data layer layout. You need to know how your data is compressed and stored, etcetera. Um, and that's why these solutions existing are so proprietary, I think. So, is it possible to generalize that to have like this open standard that, that handles all of that for all the,

Yeah. So for example, the, there is an effort within the SDXI group to create a library called the libsdxi library, and you know, there are applications that can make use of this. For example, just giving an example, if say Pytorch wanted to use this just to accelerate an AI application, then what do you vendors do today? They bring in their own library and flick, create a plugin and a framework to it, and that's how they try to accelerate it. Even they're not going to create a new Pytorch framework because it makes, takes a long time to create a framework. So these kinds of libraries can plug in into these places and still solve the specific problems that specific workloads use. Uh, the way we are doing it here is that it's, uh, we have something called definable operations. So if a vendor wanted to come and say, 'We want this new cool operation that we want to define,' but it doesn't make sense for a whole bunch of other people to standardize on it. The standard provides a framework for it, so you can still leverage the investment you made in your software, but you can bring in your own innovation, and that's how we think that, you know, collectively there may be more innovation spawned by this effort than just doing it separately.

Thanks.

My question is, uh, do you have feedback—feedback from the software architects working on that library—coming back into the standard? Because I see, always, not always, there's been disconnects like in computational storage between the software versus what the hardware implementers, and so, I was just curious how is your feedback loop from the software side?

So there are actually multiple software efforts happening. The computational storage also, uh, TWG is also working on some software APIs. Uh, uh, the SDXI group is working on a libsdxi effort, which we think can be kind of layered below a computational storage API. But having said that, the libsdxi can exist on its own for other applications that don't need a computational storage API on its own. Uh, there are driver efforts happening, uh, where kernel more applications like, you know, NUMA page migration, or, uh, uh, trying to zero fill memory for hypervisor. Those are all separate kind of API use cases. Uh, the libsdxi itself hasn't been released yet. It's happening within the SNIA group right now. Uh, they, we are going to be releasing hopefully this year. Uh, and that will trigger a lot more discussions from the ecosystem on, you know, where they think they would use it. Uh, but having said that, even the OCP group wants to come and partner with us. And they can't do it because we haven't opened it. So there are multiple discussions in play here and it, it's just a matter of time and how these come together.

Another point on that is, we would love to have more software guys come in and give us feedback. So, if you know, some tell them to come talk to us.

I, I think, just a quick additional comment yet. Users should always come in and join us. Because without the users, we're not sure we're doing the right thing.

Uh, so one of the speakers talked about a compression, uh, for memory compression algorithms, but there's already quite a few domain-specific ones, as a previous question, uh, mentioned, but for video with you, VP nine and HEVC and audio and so forth, would you want to supplant that or is this an additional to that or

It would be additional to that, when the existing kind of, if you look, for example, video compression is something you want to do relatively close to the source. But if you look at more compressing on kind of more or less live data, that, that's where we want to use, use this type of compression. And what should be done at the source should be done at the source.

Okay. Thank you.

Well, if I could just add a little bit, right? Uh, in the SDXI group, when we try to standardize a compression algorithm, we would, we're taking a few examples, right? And the examples just serve as a purpose of saying, “this is how you could compress. If you were moving your data from a source buffer to a destination buffer.” And that doesn't mean that it encompasses all the compression algorithms that exist out there, right? It's a pattern that we are saying, “here's an example, here's an example of a pattern that you can use if you were trying to do compression right now.” If vendors have new algorithms that they want to bring it to this SDXI framework, they can start with just registering a GUID for themselves for that operation group. And then they can define their operation on what the semantics are on how, how to enqueue the work and how to complete the work, which is again, the framework allows for you. So it really depends on where you want to innovate. Do you want to innovate on how do you enqueue the work, complete the work, which is mundane because it has to be the same way that you want an accelerator to perform a compression job for you? But your innovation can come in how actually you implement the algorithm. And that's where a standard like this will help.

How does the consistency synchronization model look like? Is it, is it still everything completely CPU managed, or could a GPU for instance, also trigger a data movement operation itself?

Um, yes, you, so you can imagine GPU kernels also being able to do this to achieve the consistency because at the end, we really think about it as producers and consumers. So the producer, for an SDXI data movement operation, could exist anywhere; just like in the examples that they were giving, it could exist within the NVMe subsystem. So potentially, firmware or even hardware could produce the memory structures required to affect a data movement operation or a data acceleration operation, and you would still be able to achieve the memory consistency, and you could do the peer to peer, so you don't really have to depend on the host doing that. Just the context. It's important what triggers this operation; that may be host driven, but you know...

well, I mean, as soon as you have multiple writers, you're running into consistency issues unless you have some capability to enforce that only one is modifying the data. So I was thinking about how you ensure that if you now have multiple entities in the system that can modify data and not only the CPU, essentially,

But if you're working with, with common data that both CPU and say devices need to coordinate, then you build the, you know, the consistency model of negotiation and coordination between those two. But the standard does... so the standard doesn't tell you what kind of higher level protocol you need to have for multiple producers to coordinate with each other. Having said that, multiple producers are certainly possible, and you know, there is a basic level of consistency provided if they are sharing the same kind of context. But if they are in a separate context, then you need a higher level protocol for you to coordinate amongst themselves.

So, this is one of the questions that came up last year. Doesn't data movement violate the definition for computational storage?

Well, depending on what you mean with 'definition,' but it's kind of not moving the data. Well, if you want to compute on the data, most likely you have to move it. If that is from the computational storage you often talk about from the flash memory, move it from there to compute on it. But in computational storage, we talk more about really near storage compute. So not moving the data from the storage to the host and the computational storage is then, it can offload the host. And so I don't think it really violates the definition. I don't know what the rest of you think.

My reaction is the same as, "You can't compute on data unless you move it somewhere where computation can be performed." So it's part of computation is getting the data where it needs to be, and having the computes performed on it.

I think we have instead of having four different data moves, moving it once and more efficiently to do the right kind of computation is definitely more efficient. So, it's like an oxymoron in the sense you're reducing the data movement, but you're doing it at the right place so that you can do the actual compute.

I haven't kept up with computational storage, but I remember there was no killer app, right? There was no generalizable enough computation to mass produce, right? And has that shifted now with AI or ML? Is there something different now that computational storage will succeed now versus not succeeding 10 years ago?

So I would say that I think there are a lot of applications for computational storage out there, but I don't know as I can say there is a killer app. And is AI going to be the reason? I think it's going to be another tool in the tool bag for AI. I think that there are things that could be done in the drive. That help offload the host in order to provide the acceleration needed as we're doing those AI type functions. But I'm not sure that this is the thing that's going to make computational storage the thing for everybody yet. I still think edge type applications are very, very relevant for computational storage. And especially as AI kind of pushes down into everything, right? It's not just... Yeah. It's not just, you know, interacting with a chat bot or ChatGPT or anything like that, right? It's coming to everybody's phone, as we know, whether you want it or not. And then it's going to eventually get to all your other edge devices as well. And so having the ability to have some additional compute capability near that data, I think, could be very, very relevant, very important. Does anybody else want to add to that?

Sorry. Well, first of all, there's a killer app on this device. So, do we need a killer app per se, or do we just need a lot of use cases? But I think to kind of bring in... and this is probably... before, it's maybe CXL will be using kind of... I mean, is it computational storage, computational memory, or near data compute? So, I think maybe with kind of CXL-based storage, maybe more towards that.

I mean, if you're kind of generally thinking about computation in the various devices, various architectures are going to continue to increase and just... If you have silicon out there and you're not able to use it, then you're already making it worse for your TCO models anyway. Does that mean... I think every niche use case is going to be adopted by everybody? Probably not. But then, when you start having a lot of these use cases and patents, you start getting common silicon, that it becomes very pervasive. And now, it becomes all of a sudden, "oh, yeah, it's almost natural that you have to be using that." Otherwise, your performance is going to suck. So, I think it's a case of evolution there to me.

Yeah. One of the things I didn't show in my diagrams is I showed that memory is being spread everywhere across the system. It's the same for compute. There are compute cores all over the place. I mean, the GPUs have massive compute cores. The networking devices have 8, 10, 16 ARM cores. Storage devices, same thing. We... You know, as much as I would love to have every piece of compute done in the CPU, it's not realistic. And we need to take advantage of all these scattered compute cores and, you know, put them to work.

Would you consider a sort of a key value store like a RocksDB interface as something viable for such a use case or that's too complex?

Putting my hat on from many of my previous lives, I know that some proof of concepts have been done in that regard, but I'm not making that statement in my current employment. So, just...Those thoughts are out there.

So, everyone, we're at the end of our session time, so thank you so much for joining us. We really appreciate the interaction.
