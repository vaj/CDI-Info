YouTube:https://www.youtube.com/watch?v=pX2kCoDtgBM
Text:
Hello, everyone. My name is Ravi Gummaluri. I work for Micron Technology. I'm part of the system architecture team. Today I'm going to talk about memory expansion with CXL-ready  systems and devices. Anil and Saimak have touched upon how important  it is for the memory vendor to enable the ecosystem. I'll talk about how we are working  with various ecosystem partners and enabling various CXL  solutions here.

 Before I talk about CXL expansion memory and solutions,  I would like to touch upon the memory demand and scaling  challenges and how CXL memory expansion is  solving those challenges. I'll specifically talk about capacity expansion solutions,  how database applications are getting benefited out of it,  and there are bandwidth expansion solutions,  AI and HPC workloads, how they can benefit out of it,  and finally, conclusions and next steps.

So we all know there is a huge demand for memory,  especially for data center applications. It's 26% year-over-year increase,  but we expect that it will scale with AI applications. And the second figure shows that CPU performance and memory  performance. Memory performance is not really able to scale  with CPU performance. Memory latencies are improving around 1.1 times every two  years, but processor speeds are doubling. So there is a huge gap between memory performance and CPU  performance. And in the third figure, we see that memory capacity per core  is not really able to scale with the core count increase. And if you see, the memory is 50% of overall server cost. So how do we solve the increased memory bandwidth capacity  requirements and reduce the TCO?

 So CXL memory expansion is one of the solutions  but it is the best solution today,  where you can solve bandwidth expansion, capacity expansion,  and solve the TCO problem. So if you see in the right-hand side, the memory paradigm  or the memory hierarchy here, there  is a huge gap between main memory and SSDs. CXL is going to fill this gap with various solutions  that we have today. CXL expansion cards are not really  a cache-line granular, and they are CPU-less NUMA nodes. They're hot-pluggable memory. This is the major advantage where with the main memory,  if the DIMMs are down, you need to reboot the machine  or to plug out where there is no hot-plug support. But with CMMs, what we have today,  they're hot-pluggable memory, and you can replace them  without shutting down the server. So this is one of the major drawbacks, what main memory has  and what we are seeing in the next few years. What main memory has, and it is being solved by CXL. And CXL comes with various form factors. Today, we have E1.S, E3.S, and future, we will have E5.S,  and we have add-on cards. So it supports various form factors today. And it can be interoperable with various memory types,  like DDR4, DDR5. Like Anil said earlier, you can have a DDR5  on the host with RDIMMs, and you can have CXL module  with DDR4 behind CXL. And they can do interop between both of them. So this enables us with various media types can be behind CXL,  and it can work with the CXL host. The major applications for CXL expansion memory  are like the capacity expansion. It's a no-brainer. We are adding more capacity to the system  by adding a CXL card to the system. When we attach a CXL memory module to the system,  it enables as a second-tier memory,  and it exposes to the operating system as another tier  to the system. Now, how this tier is being exposed to the applications  and OS is dependent on the solutions  that you're looking at. In a capacity expansion solution,  this second tier can be exposed as an app transparent,  meaning app doesn't know how many tiers are there,  whether a CXL memory, another tier is being attached. It just knows that it should require to allocate a memory. But it is the OS or the user libraries  which takes care of this allocations  by having multiple tiers in the system. So apps are not required to modify,  or it doesn't require to be aware of how many  tiers in the system. They can run as it is and just ask for memory. In the app application managed way,  the applications are aware how many tiers  are there in the system. And then they can allocate a particular memory allocation  in a given node. Example of libnuma. You can use libnuma to ask for a particular NUMA node  you want to allocate a memory. And further applications can be modified or optimized  in order to make use of various tiers in the system. One of the example is libmemkind. This library can be used by applications  in order to make use of various tiers in the system. When we have a CXL switch and a Fabric Manager,  Fabric Attached Memory, then we are adding another tier  to the system. This will have higher latencies, but you  are adding more capacity to the systems. By CXL switch, you can scale the number of CXL modules  that you can connect to the system  and have a higher capacity to the system. So this is one of the major use case of the CXL memory  expansion, where capacity expansion use cases  are very important. The second important use case is CXL memory bandwidth expansion. We need to interleave between the DDR, local DDR,  and the CXL in order to achieve higher system bandwidth. There are various ways today that we  can do for interleaving between DDR and CXL. There is a hardware-based interleave. There is a software plus hardware-based interleave. And there is software-based NUMA interleave. I'll talk about these interleaving solutions  in the next slides.

So we did, with the CXL-ready platforms  and our Micron CZ120 module. In this example, we have connected our CZ120 modules  to AMD Bergamo, where it has 128 cores. And it has 12 memory channels on the host platform  for each socket with DDR5 supporting DDR5. Overall, you can see that 768 GB of capacity  can be added to the system. And what we are plugging in is we are plugging four CXL modules,  Micron CZ120 modules. This is giving a capacity expansion. And you can see the system capacity has increased by 135%. Micron CZ120 ACMM module is a CXL 2.0 spec compliant. It's a PCIe Gen 5 x8. It has various features. It has a low latency, a RAS security features we have. And we have two DDR4 channels behind the CXL module. So in this system configuration, we  have tried various workloads and analyzed  how CXL memory expansion is giving overall system  performance.

And then one of the examples that we have analyzed  is TPC-H. So this workload, basically, we  have run a 3 TB of data set with the 3,000 scale. We have enabled on the local DRAM versus the DRAM plus CXL  solution. And we have analyzed what are the bottlenecks that we have  today with the workload and how CXL memory is  solving those bottlenecks. So if you see on the left-hand side,  if we are running the TPC-H workload, which  is a 3 TB database with 3,000 scale,  that database, when we run on only DRAM, which  is fully populated with 12 channel DDRs with 64 gig RDIMMs,  we can see that it's performing around the number of threads. As you see the number of streams,  if you scale the performance of the local DRAM fully populated  versus the DRAM plus CXL modules,  we have connected four CXL modules. With this, you're increasing the system capacity to 1.768 GB. With this increased memory, what we have observed  is for a single stream TPC-H workload, we see 23% benefit. For eight streams, we see 96% benefit in the performance. And for 20 streams, we can see around 300 plus performance  here. And on the right-hand side, if you see,  there are various graphs with CXL and 64 gig RDIMM  with 96 gig RDIMM. How CXL is actually compared when  we scale the capacity from 64 gig to 96 gig in the RDIMMs,  12 channel RDIMM, still the performance of CXL  is giving you 1.66 times better than the 96 gig RDIMM  on the host. So clearly, we can see that there is a performance boost  with adding additional capacity to the system. And we analyzed further why we see the performance benefit  with CXL additional capacity is if we don't have a CXL memory  today, there are lots of I/O stalls. Because the local DRAM channel capacity is not enough,  it has to go to the SSDs. And there are lots of I/O transactions  with the higher latencies and page out paging. We see a lot of I/O stalls happening. And that is the major point for degrading the performance. When we add CXL memory to the system,  we are adding additional capacity and memory  to the system. It is reducing the I/O stalls. It is reducing the data movement between SSDs and memory. This is bringing down the I/O stalls. And that is improving the performance here. So in overall, CXL can provide better performance  for capacity-intensive workloads.

Coming to the second use case with CXL,  when we talked about CXL use cases,  we talked about capacity expansion. The second one is bandwidth expansion. For bandwidth expansion, we have to interleave between the DRAM  plus the CXL memory. So there are various ways. One of the ways, hardware heterogeneous interleave,  where your system address map is chunked  and it is a flat memory model. And the system address map is chunked between your local DRAM  and CXL. DRAM can be 8- or 12-channel DDR. And an example is 4 channels. If you have 4 CXL modules you are connecting,  then the whole system address space  is chunked between local DRAM and CXL. This is easy to configure at the BIOS time. But it has lots of cons here. Its example is kernel cannot manage this memory allocations  because it is chunked. The system address map is flat. The kernel cannot manage this. And it hides the NUMA topology from the OS. So the applications cannot have a flexibility  to ask for allocations on particular nodes. It's a fixed configuration where application  wants to vary some any configurations. It cannot vary. Applications can be sensitive to various parameters  like latency, bandwidth, and capacity. But this is a fixed configuration  and we cannot tune those applications. And the major other drawback from this heterogeneous  interleaved hardware is the CMM capacity  to be ISO capacity with the DRAM. Meaning if you have a 64 RDIMM on your module, on your host,  your module also need to be ISO capacity with it. Meaning your CMM also need to be 64 gig capacity  because you're interleaving the system  address between DRAM and CXL. So having a higher capacity on the CXL  cannot get benefited out of this. You have to be ISO capacity with your RDIMMs. So this restricts the capacity on the CXL. And some of the capacity expansion use cases  cannot really benefit out of this.

The other way to achieve the interleaving between local DRAM  and the CXL is a hardware plus software based interleaving. A sub-NUMA clustering can be formed in the host. Meaning you have a 12 channel DDR when  connected to AMD EPYC here. You can subdivide this 12 channels  into sub-NUMA clusters where each cluster can  handle three RDIMMs. And you have four NUMA nodes you have created. Instead of 12 channel one NUMA node,  we are dividing that into four channel NUMA nodes. And you have CXL, four CXL cards connected as one NUMA node. So instead of two NUMA nodes, local one and CXL,  we have divided the local into four numer sub-clusters  and created four NUMA nodes and one CXL attached to the system. Now we can interleave with numactl and software tools  that we have today. We can interleave the page allocations  between these five NUMA nodes. Because local NUMA nodes are being split from one to four,  you have a better performance on the system. And CXL attaches for the--  once you have a cold pages, that can be really used  from the CXL point of view. So in this mode, the NUMA topology is enabled. The earlier hardware heterogeneous interleaving,  the OS is not able to control the page allocations. There is no NUMA topology. And here in this topology, you are enabling NUMA topology. Kernel has managed the memory. It overcomes the capacity limitations  imposed by hardware interleaving solution. The cons is, again, it is a fixed configuration here. Once you configure this at the boot up time,  you cannot change the number of NUMA clusters. And you cannot really tune each application for this workload.

So the other way for interleaving  between the local DRAM and CXL is a software way,  where if the application wants the 100 pages  to be allocated, you can have a flexibility  that you can ask that 80 pages can be allocated in node 0. And you can ask for application to allocate 20 pages  from node 1. So you can interleave in various ratios between local and DRAM. This interleaving gives a benefit  to the overall bandwidth of the system. And this is scalable. It's not a fixed configuration. According to the application, if you  have various read/write ratios in the applications,  according to your ratio, you can tune your page allocations  in the system. So this is NUMA topology enabled. OS can handle it and overcomes the capacity limitations  imposed by hardware interleaving. And this is another flexible configuration,  like in your software plus hardware configuration  we discussed earlier, has a limitation  of fixed configuration. Here, we don't require to have a fixed configuration. It can be scalable. According to the application needs,  you can scale the interleave between local and the CXL. The major cons is it is applicable to direct attach  today. But as you see when you have a switch and fabric attached  memory, this kind of interleavings  is very difficult to manage. But there is a lot of work which is  happening on the consortium. Even we are working with these kernel patches. We are trying to publish along with MemVerge  and other partners in the industry. And Micron is driving towards a solution, software interleaving  solutions for overall bandwidth improvement of the system.

 So with the software interleaving solutions  that what we have today, we try to analyze  a workload which is LLAMA inference workload. Here, on the left hand side, you can see the system  configuration. Earlier, I have showed on the AMD Bergamo  how we have analyzed a capacity expansion TPC-H workload. In this example, I'm taking the Intel Xeon platform  where we have eight channels local DDR5 running at 5600. And we have 6 mm Micron cards which are connected  to Intel Xeon platform. On this system configuration, when we run an MLC workload,  by varying the read percentages in the MLC workload,  we can see the system performance improving  to 25% and 100% read workload. And when we come to a 60/30 or 60/40,  by interleaving DDR plus local at a 70/30 or 65/35,  we can see almost a 45% improvement system bandwidth. So this example tells us by varying various read/write  ratios, you can see a sweet spot which  is coming between 70/30, 65/35 according  to the application needs. So if my application is a 70/30 workload  where I have 70% reads and 30% writes,  I can choose a sweet spot where it might be around 65/35,  meaning 65 I can choose pages to allocate on DRAM, and 35 on CXL. That gives an additional bandwidth of 45%  to the overall system. On the below, we have run the same application  on a real workload, LLAMA. And we do see that it is improving 22% when  compared with the running on the local complete fully populated  local DRAM. The number of tokens outputs are also improving. This proves that by interleaving between the DRAM and local  in a software way, you have a tunability in the system  as well as it is improving the overall system bandwidth. And a lot of applications like AI applications,  which are sensitive to bandwidth,  really get benefited out of this bandwidth expansion use cases.

So I'm just coming to the conclusion  that before we started the discussion,  the memory expansion can provide a solution  to increased memory bandwidth and capacity requirements. We have seen that software interleaving between DRAM  and local is actually giving a better performance  to the overall system. And it is getting benefited by AI and HPC workloads. And similarly, we have seen the capacity expansion  where the database and data analytic applications are  really benefiting out of this memory capacity expansion. For the next steps, I think we have  understood that without changing the applications today,  we are able to benefit the capacity expansion  and bandwidth expansion. But their applications can be further tuned or optimized  to know various tiers in the system. And page allocation algorithms can be developed around it  to improve or boost the performance further. And it can also take advantage of various media  characteristics. Today, we are not exposing what is the media characteristics  to the application to take advantage of it. It can be further enhanced with those media characteristics  and memory tiers. In the CXL memory pooling and fabric attached memory,  we can help further in defining various memory tiers  to reduce system TCO. So in a given CXL ready 2.0 platforms today  with Micron CZ120, we have analyzed  the benefit of memory expansion, bandwidth expansion,  and TCO, overall system TCO.

I'll just touch upon the introduction introducing  Micron CZ120 memory module, which  is delivering capacity, bandwidth, and flexibility. This comes in two SKUs, 128 GB and 256 GB. It can increase the capacity of overall system up to 2 TB. The bandwidth is 36 GB for a 2 is to 1 read/write combination. And it comes in E3.S2T form factor  with a x8 in the front end. This is an industry standard form factor  for broad deployment. So any questions for me?
