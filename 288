YouTube:https://www.youtube.com/watch?v=zq0htf7Hu_8
Text:
Hello, my name is Michael Abraham. I am with Micron Technology. I've been there for quite a number of years, and I'm a director of product management focused on the Compute Express Link memory modules.We're going to spend a little bit of today talking about the compatible memory modules, CXL compatible memory modules, and talk about some of the workload opportunities that we see with them as we move forward.

All right, today we're going to do this by talking about some of the data center challenges we have, spending a little bit of time looking at the memory storage hierarchy, which seems to be a favored thing to look at.And then we'll look at specifically at the CZ120 memory module and then some of the recent demos that we showcased in the FMS 2024 conference that we had.And then we'll drill into one of those looking at a Llama index with some RAG improvements.And then finally wrap it up with how to enable CXL in your company.

All right, so in terms of data center, we've been seeing a lot around AI, and it has a need for a lot more memory in terms of being able to do its thing.Now, we've got, there's more than just AI, of course, but we do bucketize a lot of the use cases into several categories.One of them is, there's typically an need for higher memory capacity.We see this in things like in memory databases, software as a service, especially AI training and inference.And we expect that this is going to continue to grow as time moves forward.And there's sometimes a point where we just don't have enough DRAM in a server itself.And so, the question is: Do we have an ability to add additional memory capacity and hopefully cache or tier memory off of the main memory to avoid, you know, long accesses to disk, whether it's SSD or hard drive? And also, bandwidth is another one that we look at. This is a different type of application. Oftentimes, things that require real-time processing really benefit from that additional bandwidth, and especially as we've seen in a virtualized environment, things like bandwidth per core requirements for processors then become quite important to be able to offset more memory bandwidth than what's just available by DRAM RDIMMs alone. And then, finally, we want to deliver these features in a TCO type of benefit. One of the beautiful things that people are looking for is the ability to have just the right amount of memory based on what they need to meet those capacity and bandwidth requirements, and based on their workloads. And so, that is a key focus of data centers today.

So when we look at what we have today, we refer to that lovingly as the memory storage hierarchy.We start with our fastest memory on top, which today is the high bandwidth memory or HBM.It tends to be very low latency and very high bandwidth, but it also comes with a pretty cost associated with that.And so that's a key focus of data centers today.We're limited in terms of how much we can put into a server based on, and oftentimes it's paired with a GPU. And so we're limited in terms of how much memory we can put in there.We do have more of the DDR memory in general available. This has been the mainstay workhorse for many, many years for servers in particular. And we're able to get today in the one to two terabytes per server of this main memory, essentially to make that happen.And then we have our fast storage and capacity storage and archival storage.A lot of the SSDs live in the fast storage arena. Capacity storage oftentimes moves to hard drive and archival is typically something like tape. So in this particular case, what we're trying to do is we're trying to bridge the gap between main memory and this fast storage.Anytime that we can get rid of a file system and be able to execute, you know, from a variable to a type of approach as opposed to a file type of approach in a programming or application model, we have the ability to reduce latency and make things happen. 

So we still may be introducing a tier, but direct attached CXL is basically attempting to bridge that gap. We are able to provide additional capacity in a memory-like access. And then it also is able to have additional bandwidth that's not directly available.So it's independent of the DDR memory that's in the server.

All right, so I wanted to introduce to you the Micron CZ120 memory. This is the first to market memory. So we've passed our quality release. It's actually in general availability right now.And we had announced it a year ago in 2023 and we've executed hard on this to get it ready to go and it's available for order.So, we have this module in two capacities, 128 gigabyte and 256 gigabyte.With the 256 gigabyte and eight of them per server, we can support additional up to two terabytes of incremental server capacity.This is a reasonably fast memory. It operates about 36 gigabytes per second. And so in this particular case, versus a 12-channel, 4800 megatransfer per second, RDIMMs in a server we have the ability to to augment that server bandwidth by roughly 34%, just 34%.And CXL does run today on a PCIe interface.It's from a physical perspective.So we talk about oftentimes about it as like a PCIe Gen 5 x8 compatible.Though CXL runs as a protocol that's unique on top of that, basically allows 64-byte cache line access.All right.And then there are a couple different ways to get CXL memory.The way we focused on is an industry standard E3S2T module with a x8 interface.And one of the advantages of this type of memory is it is able to slot into the front of a server.And you can add as many of them as you have CXL lanes for.So we expect that this is going to be a pretty easy way to consume and to add memory to a server.

All right. So, in flash memory, or it used to be Flash Memory Summit, now it's the Future Memory and Storage Summit. This year, we actually showcased three different types of demos for CXL. We worked with a number of partners for those. I'm not going to go into all three of them today. We'll focus on one of them. But we are looking at two of them that are related to AI, and one of them really is the memory storage. They are related more towards pooling and sharing. So, we had a LlamaIndex with RAG. And in this particular case, we partnered with MemVerge on that. We'll go into this in a little bit more detail. But basically, we're improving RAG performance by 30% by adding CZ120 to that application. In the case of inferencing, looking at the Intel neural with a Llama2 type of environment, environment, we're actually able to show a 23% inference gain by adding CZ120 memory modules to that. And then with RocksDB, in a pooling and sharing application, we were basically able to increase the memory capacity per PCIe lane by a factor of 10x to a total of 5.5 terabytes of additional memory. And in this particular case, we are looking with FAMFS at the ability to share that memory across multiple VMs and be able to make it available to enable new opportunities and workload capabilities. So, it should be a pretty fun one as that one continues to grow in the future. All right, so let's zero in on the LlamaIndex with RAG. We're going to spend a little bit of time on that one.

In this particular case, we're going to use the LlamaIndex with RAG. In this particular case, we are taking a look at adding a warm tier of CXL to an Llama inference server. We do that. The server, I'll show you how it's built in just a moment, but basically, we have the amount of hardware on the bottom. We run the OS on top of that. MemVerge adds their memory machine to this, and that basically provides intelligent tiering of data between hot and cold and has the ability to optimize workloads based on latency or bandwidth, and then we get from here into once we have all that baseline set up, then we basically focus on the LlamaIndex framework and the inference server on top of that. So we have three main functions that we provide with this. The vector DB index, we'll look at that in just a moment, which basically stores a lot of the RAG details. We have the ability to add a chain. We have the ability to add a chat history to this, to be able to increase the chat history so that it is able to provide based on context of previous questions, or prompts, or queries to be able to provide better answers, and then embedding vectors as well.

So, this has been an interesting collaboration with a number of companies. We built this on top of a super micro platform. It's using the Intel Xeon Gen 5 processor. And from the DRAM perspective, we've been able to do a lot of things. So, from a slot perspective, we loaded it up with Micron DDR5 RDIMMs, 64 gigabyte, eight of them at 5600 megatransfer per second, for a total of one terabyte in that server. It's actually, it should say perhaps, eight RDIMM channels. And so from a slot perspective, there's 16, with two DIMMs per channel. So, let's see here, for Micron CXL at the CZ120, we've added four of our 256 gigabyte modules, for a total of additional one terabyte of memory. And from a bandwidth perspective, we've got the rated down from an application perspective down to about 26 gigabytes per second. So, we're adding about 104 additional gigabytes per second. With the NVIDIA A1 GPU being the baseline for this one as well, in terms of how it's set up, that's our Tensor Core GPU. Essentially, it allows us to do a lot of the inferencing. All right. We built this on top of a number of applications on top of Red Hat Linux. And in this case, we're looking at Kernel 6.8 RC5.

All right. So, here's what a typical RAG pipeline looks like. Today, it's got a query that comes in. It does some retrieval. And if there's some additional augmentation that can take place, we're going to look at that. And then we basically bring that in at the retrieval point. We put together the prompt, and then we throw it through the model, and then we get the answer out when it's all said and done. Our goal was to basically see if we could improve this pipeline by doing a couple of things. Number one was, is it possible to add in more than one vector or database? If we are able to increase the amount of composable memory we have for LamaIndex, could that potentially improve the quality of response? The second thing that we were doing was, we've got some feedback now between this answer and this chat history. And the goal is to see if that chat history can basically improve the results coming back from the vector database to retrieval at the end of the day to be able to improve answers in general. So, we wanted to look at this in terms of how we set it up to see what the benefits could be. We think that, overall, it's a faster capability but also a better set of answers.

And on the list: resources to use of the GPU as well so it can be used for other things. Alright, so in our particular case, the way we looked at it, based on the setup we had on the previous slide—two slides ago—we are basically showing that with CXL, we're able to improve RAG performance by roughly 30 percent. And there is some tuning of the interleaving ratio required to optimize performance in this particular case. This is based on a nine-to-one interleaving ratio between DDR memory and CXL memory, and that was measured at a 99 percentile latency. So, it's pretty straightforward. We think in the future as well that there's going to be some additional benefit as we add pooling and sharing benefits to this. So today, this executed you in roughly, uh, you know, one terabyte versus one terabyte DRAM only in this particular case. Um, when the future we believe that we could expand this, um, with pooling and sharing to roughly, uh, six and a half terabytes total between the DRAM and CXL, and that would give us the opportunity to add additional vector databases to overall improve the model in the future. Alright, and then, uh, one of the other big things that happens is this: now the RAG models are basically contained in memory, so it reduces the amount of time we have to spend going to storage and improves the overall speed of the system as well. Particularly as we scale in the future to larger vector databases or more of them.

all right so with that we do have some future exploration that we're looking at of opportunities for CXL. This is showing several of them that are RAG based again and of course with the other two demos that we had at FMS we have additional opportunities we're exploring right now.

We do have quite a bit of a body of data that we've been assembling in terms of how to get your hands-on support for CXL. You have data sheets, a lot of electrical and thermal models, all the things that would be needed for designing CXL into your server, and we're also working on putting together the ecosystem partners who can assist in that and system level design. So, if you'd like to learn more, we have more at www.micron.com/CXL, and if you enroll in our technology enablement program called TEP, then we have the ability to answer your questions and to provide the data you need in order to make CXL happen. So, with that, I think we're in pretty good shape.
