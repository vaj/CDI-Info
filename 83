
All right. Good afternoon, everyone. Thanks for attending. So we're going to talk about CXL Fabrics. As Frank mentioned, I'm with Rambus. I'm the director of systems architecture there. I'm also co-chair of the manageability subteam and the fabric subteam in the CXL consortium, which is why I'm the one talking to you about fabrics today.

Let's start off with kind of an overview of what we set out to accomplish when defining CXL Fabrics. So there are a couple of key use cases that we wanted to go after. I'm sure you've heard before that memory disaggregation is kind of following a similar trajectory to what happened with storage in the past. So these disaggregated composable systems are really a target use case. We see that as the future for memory deployment in data center and enterprise, where you have a pool of hosts, device, and memory resources. And you can dynamically compose a hardware system from that. So that was definitely an important priority for us to be able to address. The second is scale-out systems, so high-performance computing, machine learning, analytics, these systems where you have a lot of nodes collaborating together on one larger work set. And they're exchanging data constantly. Is there a way that we could tighten up that data exchange, have it a little bit more closely tied to the memory media where the data is living? Ultimately, the goal was to add some capabilities into the spec that allowed CXL to expand from just a single node. I mean, that's a very important-- memory expansion is a very important use case. But we wanted to see if we could get it to span the rack or potentially a number of racks. There's a trade-off in terms of communication efficiency on the space that you allow for this. We decided that 12 bits for addressing was where we wanted to stop that. That was a pretty small ID space in terms of overhead size, but still allowed you to scale up to-- it allows for 4,000 IDs. Exactly who's going to consume an ID is going to depend on your system architecture. But we targeted for that. We certainly wanted to scale beyond the tree-based topology limitation that came along with a lot of the great architectural stuff that we were able to borrow from PCIe for this. Tree-based topologies, when you're scaling out, can be pretty problematic. So we needed to enable that. And all of this, we needed to ensure that it did not compromise node-level properties. Because as I've said already, that single node use case, just memory expansion within a single server, that's very important. So we didn't want to slow that down or impede that in any way.

To address the composability piece, we allow for endpoint binding from anywhere across the fabric. So the host is going to only ever see up to two layers of standard switching. A layer switching directly attached to it, which we call the host edge. That's its entry point into the fabric. And when necessary, a second layer for a downstream edge. And all of the intermediate fabric links and switches, all of that complexity in hardware topology is obfuscated from the host. It just needs to worry about what looks to it like standard PCIe switching on up to two levels in a tree-based fanout. And this allows us to reuse decades worth of software on the host for navigating this topology. So if you look in the center there, a physical topology with your host and your endpoint, multiple hosts, multiple endpoints spread out throughout the fabric, the logical view, when the host goes out and explores its architecture, it's only going to see up to two layers of switches.

In terms of scale out, we've defined something called global fabric attached memory, or GFAM. And this is a highly scalable memory pool. So instead of-- some of you may be familiar with the technologies in CXL that allow you to take single node memory and share it to one or a few more nodes, this is a complete redesign of that, approaching it from the perspective that the memory is highly shared. So I mentioned the 4K ID space, a hypothetical limit. If you can imagine 2,000 hosts accessing a common memory pool of 2,000 GFAM devices, that's the scale that this has been designed to. Now, all of that is-- that memory pool is a very small number. All of that is-- that memory pool is accessed through a structure in the host edge switch called the Fabric Address Segment Table, or the FAST.

The way that this was facilitated at the transport level was through the introduction of port-based routing. So this is a new flit mode introduced in 3.0, where an ID from that 4K space is included in the transactions. So that within the Fabric, we can start routing by ID, and we don't need to worry about address collisions when you have multiple hosts. They don't need to share an address space. Everything's routed among IDs. So every transaction is going to carry a destination PBR ID or D-PID, and as needed, a source PBR ID or S-PID will be provided. So if you can imagine, a request is going to have a D-PID and an S-PID, but a completion only really needs a D-PID. So this is what allows the inter-switch links to carry traffic from multiple virtual hierarchies. So I showed on the previous diagram, we had two hosts that at some point are sharing Fabric links. And then they may have addressed that memory to the same region. And that's fine, because we're not routing based on address throughout the Fabric, which is something that you would do in a conventional CXL or PCIe switch. But in PBR, it's all using these PBR IDs.

So I want to spend some more time talking about the routing model. We're also going to touch on some new content about access control mechanisms. So this is how we manage that new ID concept and conceal those details from the host. So if you look on the far left, you've got a host connected to, as I said, the host edge PBR switch. And color-coded here, we have the FLIT format for those links. So the host is operating in the legacy CXL mode, what we call host-based routing, or HBR. It doesn't actually know it's talking to a CXL Fabric. And that's by design. We don't want it to have to comprehend that complexity. At the host edge, that PBR switch is going to convert the transactions from HBR format into PBR format. And so with the fast, it's going to, based on the address, look up the destination PBR ID or use other concepts of the binding information it's been programmed with if it's targeting an endpoint as opposed to GFAM space. And then once we've made it past the host edge switch, all of those intermediate PBR switches, they just deal with DPID. They don't really care about the host. They don't really care about the endpoint. They're just routing traffic based on ID at that point. So the access control mechanisms that we've defined to ensure that this is only happening in an appropriate way is the host edge port has to be configured to access a particular DPID. So as long as we can trust that switch, we can trust that it's not shooting traffic out to a device that it has not been explicitly configured to access. When we get into the switch, it's a little bit less granular an access mechanism. But a routing path also needs to be established. So in very large fabrics, it's possible that you can't reach anywhere from everywhere. So you would need all those intermediate PBR switches to be configured to route properly to the DPID.

If the traffic is bound for an HBR device-- so maybe that's an SLD, a type 1, type 2, type 3 device. Maybe it's a PCIe endpoint. Maybe it's an HBR switch, so a non-fabric switch with however many endpoints are connected to that. That's going to be routed through the switch to what we call the downstream edge. And it's going to be the responsibility of the downstream edge to convert that PBR transaction back to HBR format. The access control mechanisms here kind of mirror what's happening in the host. That downstream edge port needs to be explicitly configured for that binding path to be established. It's going to look at the source PID. That's going to give it an idea of which host is making the request. And it will have a configuration knowing whether that host has been configured to access the particular DPID.

The GFDs, however, they sit within the PBR domain. They support the PBR flit mode. So the traffic's going to be routed directly to the GFD or the GFAM device in PBR mode. It's the GFD's responsibility to translate from that particular host's HPA into the DPA or the media address internally based on the S-PID. So the access control mechanisms in place we have here are that GFD needs to be configured to allow traffic from that S-PID. Any capacity behind the GFAM device, access to that capacity is managed using the dynamic capacity device framework, the DCD. So it's got kind of two levels of control there.

So what this looks like inside the GFD is capacity is assigned to a particular DC block. And it's assigned on that level of granularity. And those blocks are assigned to one host group. And hosts, which in this case you can think of as S-PIDs, hosts can be assigned to one or more host groups. So when the transaction hits the GFD, the GFD takes a look at the S-PID. It figures out the group access vector, as it's called, which is a bit vector indicating which host groups that S-PID belongs to. And then it's going to compare that to the group ID that has been assigned to that particular dynamic capacity block. And that's how the GFD controls access to the media.

I've also included a little cheat sheet here showing all the different layers of translation and access control. So starting from host physical address to-- first, we comprehend the GFAM interleave. And the GFAM interleave, or it's not necessarily interleaved. If it's interleaved, it uses the interleave decoder table to determine a D-PID. Or it's a straight lookup if it's not interleaved. So that gives us a D-PID in the switch host edge port to access a particular HPA. So that gets routed through the fabric based on DPA. When it hits the GFAM device, the GFAM decoder table is a table with an S-PID lookup and an HPA value. That is used to normalize that HPA to a DPA. Then that gets translated into the DC region. The GFAM devices also have the concept of a device media partition, which is an overlay on DC region. The device media partition uses the memory group table to determine that host group ID associated with the S-PID. And that will determine whether or not you can access-- that S-PID can access that DPA. And then you're out to the DRAM interleave and eventually to the DDR bus. So there are a lot of different address points and access control points.

OK, very quickly on fabric management architecture. This has been covered very well today already. So I won't spend a whole lot of time discussing it. But all of these shared resources, it's not appropriate to have an individual host in control. So that's why we rely on the fabric manager for those pooled resources. So the responsibility is the fault of the fabric manager in a true PBR fabric scenario. On initial power up, it's got to go out and discover the hardware topology. It's got to initialize all the devices. And then it's going to start doing composition. So it's going to start binding devices to host domains. It's going to configure access control mechanisms. It's going to configure the routing through the fabric. It's responsible for inter-switch link management. So a host owns and manages its link to the host edge. A host owns and manages the link from a downstream edge to an endpoint that's been bound into its domain because those links are exclusive to that host. But all the other switch-to-switch links, those are shared resources. So you don't really want one host going in and resetting a link and taking a whole bunch of other hosts down. So inter-switch link management falls to the fabric manager because it understands those shared resources, who's using them. GFD management as well. GFDs are, by definition, a shared resource. They are not bound on their own to a particular host. So the fabric manager is responsible for that. And then any unbound endpoint, there's not a whole lot that you would need to do with an unbound endpoint. But if there is any endpoint management required, that would fall to the fabric manager.

So there were a lot of core concepts for PBR fabrics introduced in 3.0. Still a lot of work to do. In the 3.0 spec, you will see a number of references to future specification. Mahesh Wagh this morning referenced 3.1 publicly. So I'm not going to have to pretend it doesn't exist. It's coming. And we've defined a lot of additional capabilities and frameworks. So you can look forward to seeing that in the 3.1 release. Details on PBR switch management, GFD management, mechanisms for host-to-host communication, device-to-device communication, and cross-domain traffic. There's a lot of very interesting fabric capability coming up. And I'm really looking forward to presenting that to the community.

Call to action is get your hands on the CXL specification, take a look, and then join us in the consortium. The power of the consortium is the diversity of members. The software developers, the server vendors, the component vendors, the CSPs, the hyperscalers, everyone has their own perspective. And it's very useful to be able to deliver a good holistic architecture, getting all those different viewpoints. So please take a look. Join us in the Fabric sub-team. And then if you want to see some of the activity going on on developing support for CXL and its management in the Linux kernel, I'd encourage you to join the Linux kernel CXL mailing list. All right, that's everything. Thank you.
