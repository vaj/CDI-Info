
Sorry for the little delay. So, as far as the agenda is concerned, I'll give a brief background about how we came up with this framework proposal and talk about it. Then, Bhanu will talk about how the framework looks in implementation and how it works in practicality, and then we'll end with a call to action.

All right, so as we talked about in the previous session during the updates, we started with, like, three workload categories and three very specific use cases last year, focusing on virtualization, containeration, AI, and then caching. This year, we decided to cast a wider net and include more categories of workloads, and within those, also focus on other specific workloads such as Cassandra, Spark, Redis, which you'll hear about during the rest of the sessions. And what that meant was, we need to now also, if you go to the next slide,

Figure out what sort of CMS solution actually works for that specific use case. Oh, sorry. Stay on this side. Because it's important to identify like one CMS solution or just connecting a CXL device is not enough. It's not like one solution will work for all the workloads. So, we wanted to make sure that we come up with some way of identifying the right CMS solution for the right use case, and more importantly, the right value. Because, at the end of the day, we are here not just for fun and just doing a science project. We actually want production deployments. We want to see the ROI and the TCO benefit for whatever CMS solution goes into production.

So, with that intent, we started looking. OK, how do we solve this problem? We have a wide variety of workloads. The resource requirements for each of these workloads is very different. But, if we came up with a checklist of questions that are most commonly asked when we come up with a solution design, it starts with, like, I won't go through the entire list, but from the very basic, like, will it even work on CXL-attached devices? To the more complex, like, OK, I have my workload working on a CXL device. Now, what sort of tiering do I need? Or what sort of ratios of CXL DRAM do I need? And so, if you create a work load out of all these questions, it boils down to three main things: performance, latency, and bandwidth. And that's not surprising; if you look at memory, those—performance, latency, and bandwidth—are the key performance attributes.

And with that, we decided to anchor our framework along those two dimensions. So, if you look at this particular chart and you start looking at your workload requirements, you start profiling your workload, which I'm assuming is on DRAM, and you're looking to migrate that on a CXL-attached device or a combination of both. This sort of latency-bandwidth decomposition of your workload, by quantifying using benchmarking and profiling, you can figure out where your workload lands in this particular latency-bandwidth chart. And based on that, your workload could end up being extremely memory-bound and extremely latency-sensitive, or it could be memory-bound and more bandwidth-sensitive, or it could be neither. And that means you're basically running in a core-bound workload, which also has its own use case for a CMS solution.

So once you have your workload decomposed, analyzed, and profiled on the memory requirements in terms of latency and bandwidth, the next step is to now figure out what sort of CMS solution really would work, or will provide value, if you're looking at optimizing for, say, $1 per gigabyte or $1 per perf, or whatever your success criteria is. And just at a high level, if your workload is memory-bound, you may want to start looking at a CMS-native expansion device, because pooling by design will incur a higher latency. So, you might not want to start with a CMS pooling solution, not that it's precluded, but just to give our users some guidelines on how to go about CMS solution design, you may want to start with a CMS-native expansion for a memory-bound workload.

So, the end goal is to sort of marry your workload memory requirements, in terms of latency and bandwidth, to the attributes that the CMS solution provides. Then, come up with the right design points and sort of right-size the solution so that you get the ROI benefit that you're looking for for the CMS solution. So, with that, I will hand it off to Bhanu, who will actually go over some examples of how this looks in practice.

Good morning, folks. So, I'm going to talk a little more about the nitty-gritty of getting things done. So, basically, what we're looking at is, if you're all trying to compare solutions—that is, you versus me, me versus him, him versus her—so we all got to talk the same language. If you don't talk the same language, we are in trouble. And then we'll keep talking past each other. And the goal at CMS is essentially to expand the scope, not to limit the scope. So, next slide.

So, one of the things that we could look at over here is, okay, so one such reference framework could well be something like this, right? We use a single-socket system. We essentially have attached CXL, and then we have DRAM, and we have a whole bunch of things. So, this allows us to assign workloads easily to the CXL-attached memory on one socket, or two sockets, or three sockets; in this case, just a single socket. And we could reduce any kind of variability that will arise when we make the system more complex. So, this is one such reference framework we could look at. Thanks.

So, over here, for instance, we could, for instance, say, "Hey, you know what? I want to just look at CXL-attached memory, look at DRAM, have two comparisons: one that says, 'How does this run on pure CXL?' and one, 'How does that run on pure DRAM?'" So, what we have—there are lots of GitHub recipes out there—and what we have tried to do in this particular thing that MemVerge is contributing, we are calling cxlbench, which is a set of application-focused applications, and we are trying to get this, and hopefully we can get people to expand this, and here we can talk exactly the same language. Okay, so again, over here, we can talk about what's happening on the device versus what's happening on an application. Next slide.

So, the next thing that we also got to look at is, we have to really talk the same metrics. If you're talking different metrics, you know, again, we are talking past each other, and again, we want to standardize. Talk the same metrics, get something going. So, say, for instance, at the operating system level, you can talk about dstat, you can talk about sar, you can talk about iostat. Those are common metrics, and those are common metrics frameworks at the operating system level. At the device level, for instance, we have a whole bunch of other tools from different vendors. You have perf from the Linux community. You have Intel VTune. You have AMD's uProf. You have PEC, you have Intel PCM, and so on. All of this gives us visibility of the entire system. Now, the whole point of getting the right metrics is to make sure that, A, we select the right solution, and, B, we have to right-size the solution.

So, what I'm going to do going forward is take a couple of small examples, one that looks at the device and one that looks at an application. Okay?

So, the first piece is, of course, I'll talk about Intel MLC. That's the Memory Latency Checker. I mean, considering where it started from, I can say MLC has really built up and has become the go-to tool as far as understanding the devices to get the fundamental understanding of both bandwidth and latency. So, again, over here, what we are talking about is, we are talking about just running MLC, we have an expander, and then we have the, of course, DRAM. Now, what MLC allows us to do is simply look at this in terms of what's happening as far as the device is concerned. So, the two parameters we go off against are latency and memory. Next slide.

So, basically, this is how we did the setup. We said, 'Okay, single-socket system. We'll run MLC on the socket, and then we'll go to DRAM, we'll go to CXL, and also, MLC gives us a way of interleaving the two.' So, we look at some results from the two.

So, we scaled MLC by slowly increasing the load on the system, starting from a few cores, ramping it all the way to the top. And at each point, we measure the bandwidth, we measure the latency, and now, basically, we've plotted it. So, now, as we plot the curve, fundamentally, you can see how CXL has decent enough bandwidth to start with, with low enough latency, and then suddenly, it hits a knee, and then latency just goes through the roof. And as latency goes through the roof, there's a corresponding fall in bandwidth. Now, there's another line. The yellow line over there is what only DRAM gives us, okay? So DRAM gives us a certain amount of bandwidth, a decent amount, a low enough latency as we increase the load. Now, the interesting thing is, on this particular system, what happens if you have a 90-10 mix of interleaving? So, 90% of traffic goes to DRAM, 10% goes to CXL. Now, suddenly, we see a nice bump in the bandwidth. Latency is comparable, okay, to DRAM; you can see the bad effects of CXL latency are really reduced to the point of being nothing, okay? So, this is one way of looking at the system and saying, "This is how the device, and this is how the system, gives us certain device characteristics that we can use going forward." Now, with any new attached device, this characteristic will change. So, this system will allow us to say, "Hey, what is my targeted goal, where do I need to go, and so on." Next slide.

So now, again, over here, if you look at this as a bandwidth curve, okay? So we just plotted the latency bandwidth curve that we had, that Vikrant had shown earlier. You can see 100% CXL has high latency, and 100% DRAM has a certain bandwidth, and then you follow that up with saying, "Hey, if you go all the way across, this is where we get in terms of the interleaving.

Now, the other application we look at is TPC-C scaling, okay? So, we are trying to keep things equivalent. Now, again, the thing that we have contributed into the cxlbench—you can change the parameters as you want, and you can go forward. In this particular case, keeping things equivalent, making sure everything fits inside CXL, and then we are simply doing a comparison between the two.

Next slide. So, this is how the setup is done. It's a two-socket system. One socket has the CXL expander, and that's where the database server runs, and then you have the load driver on the second socket. That's driving the load to the database. In this case, what I'm showing you is just a very, very simple comparison between what happens between DRAM and CXL.

Next slide. Now, again, please note that this is one experiment with one particular set of characteristics and one particular set of parameters. In this particular case, what we see is that we are using pure CXL. We are getting approximately 80% of the performance as opposed to the DRAM. However, our latency has also increased by 20%. Okay? So, again, depending on where you're trying to attack the solution, you either say, "Hey, you know, I can use a lot of CXL or I can use a lot of DRAM," or I preclude one or the other. And then set my design point.

Next slide. So, now, while we were doing this at one of these performance points, we said, "Okay, let's see what's happening on the system." Okay? Now, so this is approximately a case where we are looking at the application metrics. The first two say, "These are the application metrics." And then, the last four actually give us the system metrics out of PerfSpec. Please note, on this particular system, there is no CXL-specific data available. Hopefully, this resolves itself as we move to Granite Rapids and Turin systems, where I do know Granite Rapids opens this up, and I hope Turin does too. So, again, this is a way of looking at it, talking the same language, and making sure that we get there.

Next slide. Okay? So, basically, a call to action. Over the past couple of days, what I've seen is a lot of people have said, "I have run workload X. I have run workload Y. I have run workload Z." And this is what the result I'm getting, and this is how things are moving forward. Yes, I understand a lot of these are in the... There's lots of GitHub recipes. However, there is no single way of putting everything together. Everybody runs across similar kinds of difficulties or ease of bringing things up. Let's all collaborate. Bring this up so that everybody has an easy way of comparing workloads, running workloads, and moving forward. Basically, it's a collaborative effort. Let's make sure that we all collaborate and get things moving forward. The next thing that we want to do is please contribute. Okay? So, we at MemVerge, we started by saying, "Let's create a simple thing called cxlbench." We've contributed multiple recipes to multiple workloads over there. As we start bringing up more and more recipes, we'll keep contributing more and more over there. Hopefully, if not cxlbench, maybe there's something that you bring up and say, "Hey, this is my way of doing cxlbench benchmarks." It's there. We can modify it. We can do whatever it is that we need to do. The last thing that I want to ask is that, you know, we need out-of-the-box telemetry for all of these devices from both the CXL and CPU vendors. So, like I said, hopefully Intel and AMD have solved the problem with Granite Rapids and Turin. That gives us visibility into understanding the latency and bandwidth characteristics out of CXL. However, I also want to talk to people who are creating the CXL Direct Attach memory, the switches, the pooled memory, the fabrics. There should be a way for us to grab any kind of important metrics that's coming out of your system. And with that, I really will say thank you very much. And hopefully, we will see a lot more of you in the workloads group. And hopefully, we see a lot more contributions into a way of moving CXL forward. Thank you.
