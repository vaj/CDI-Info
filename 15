
Hi, everyone.Thank you for joining.My name is Vince Hache.I'm Director of Systems Architecture for CXL Data Center Products with Rambus.I'm also co-chair of the Fabric subteam and the Manageability subteam with the CXL Consortium.Today, I'm going to provide an introduction to CXL Fabrics, a new interconnect technology introduced in the 3.0 version of the CXL specification.

So we'll start off with a high-level overview of CXL 3.0, some of the key features that have been added there.And then we'll talk about some of the transport-level details introduced in 3.0 that enable fabrics.We'll talk about the routing model used inside a CXL fabric and then the fabric management architecture for configuring, initializing, and managing a fabric when it's been deployed.And then we'll talk about some of the new technologies and changes that will be coming in future versions of the specification.

So CXL 3.0 was released last year.It had a lot of new advances and additional capabilities built in, fabrics obviously being a big portion of that.It had a lot of new features for improving the sharing of memory between hosts and also pooling, where memory is put in a large common pool and divided up among multiple hosts.An important analogous feature to sharing is coherency, making sure that when more than one host is accessing a common set of memory, changes from one host are notified in another host so that it can invalidate cache entries.And then peer-to-peer so that accelerators can-- things like accelerators can access CXL attached memory as well.In general, there's just a lot of new capabilities targeted increasing the scale of systems and optimizing resource utilization.

If we look back over the last few years, there have been a lot of advances in CXL 3.0.And this material is being covered in much more depth than a lot of other great sessions during this conference.So I won't spend a lot of time here, but you can see the rate at which new capabilities and features are being defined in the specification.

So let's talk about CXL Fabrics.What use cases are we trying to support?What was the motivation?What were the design goals for introducing this technology?So one of the key sorts of systems that this capability was looking to enable are disaggregated composable systems.So systems where host resources, devices, memory resources, they're all pooled in a connected and interconnected set of hardware.And then dynamically as needs arise, a system is composed.A host, some devices, and some memory are assembled together, and you've got a host built.The other sorts of use cases this technology was looking to address is scale-out systems.So very, very large system of many hosts all working on a common task, like we find with high-performance computing and machine learning, some analytics work jobs.The idea was to build into the spec the capabilities required to take CXL out of a single node, a single host.I mean, there's a lot of benefit.There are a lot of great advantages that I'm sure you've all heard before of enabling CXL-attached memory in a single node.But now we wanted to get groups of hosts connected, maybe a full rack, maybe a small number of rack.So that was the goal.We're never going to hit a scale like Ethernet, because frankly, with memory access, you don't want to incur the latency impact that a scale at that size would result in.But certainly going beyond just a single rack.What's really driving this limitation is in the trade-off between scale and performance impact for the additional addressing capabilities required to hit that scale.12 bits was chosen as the size for an addressable ID.So that defines the ID space that results in 4,096 IDs.There are different requirements for ID assignment depending on the actual interconnect, the system topology of the fabric.And so the exact scope of that rack will depend on the use case and the system topology.But that's the scale that one is looking at when defining a CXL fabric.One of the big limitations that the consortium was looking to move beyond was tree-based topology restrictions that one would find in, for example, PCIe.So CXL fabrics were designed to be able to employ much more advanced and different sorts of topologies.And a fundamental guiding design principle through all of this was that fabrics are only required for some use cases.We did not want to compromise node-level properties, meaning anything that was done in the specification to facilitate fabrics was to be done such that if you were not a fabric, if you were just a host in the single box with CXL-attached memory, there were no performance penalties incurred for that sort of a deployment.So this is all added above and beyond that.

So here's how CXL fabrics address the composability type use case.They enable endpoint binding from across a fabric.So we've got two pictures on the right.There's a physical topology.And then there's the logical view from the host.So if we look in the center, we can have any manner of interconnect.You can have hosts and endpoints hooking up to switches that can be connected to intermediate fabric links, intermediate fabric switches, any sort of physical topology in between them.But the host is only going to see, at most, two layers of just standard switching.And those two layers represent what we call the host edge switch and the downstream edge switch, assuming that the downstream endpoints are attached to a different edge switch.So if we look at the yellow host 0 topology, there's one endpoint that looks as though it's connected to the host edge switch as well.And then all of the details of the physical interconnect and all of the fabric links in between, in this physical topology, switch 0 and switches 1, 2, and 3, all of those details are obscured out.And that's represented as a logical link between the uppermost switch presented to host 0 and the lowermost switches presented to host 0.So the reason that this was an important goal was that it enables reuse of existing host software.When this host goes and discovers its topology, configures all the endpoints, configures all the switches, in this specific deployment, no changes are required to facilitate CXL Fabrics.On the back end, the management of the physical topology, the management of the fabric itself, there's additional work there.But in terms of what that host needs to be capable of, it's all the same sorts of configuration and management capabilities that would be used for just standard fanout.

For Scale-Out, an additional technology called Global Fabric Attached Memory, or GFAM, was added.Now, this is a highly scalable memory pool.I mentioned we have 4,000 IDs.So you can imagine a system of around 2,000 hosts, all simultaneously having access to a memory pool of 2,000 GFAM devices, or GFDs.So this is a massive pool of memory that can be simultaneously accessed.So any host can access any of the 2,000 GFAM devices.And any GFAM device can be accessed by any of the 2,000 hosts.So it really helps large-scale, multi-node systems work together without spending a lot of effort on copying memory back and forth.The way that the hosts access this GFAM space is through something called the Fabric Address Segment Table.That is a flat space of memory in host address space, where the memory span is broken up into equally-sized segments.And each of these segments is configured to correspond to a particular GFD.So it's just doing address-based access.And that is being converted into fabric accesses across a broad scope of memory devices.

So why don't we take a look now at the transport-level details that have been defined to facilitate this?And we'll start off with a review of what has been named host-based routing.And that's the term that's been used in CXL 3.0 to cover the CXL 1.1 and 2.0 transport protocol definition.So all of the transactions that CXL users to date are familiar with, where read and write requests are address-routed, that's been called host-based routing, because the addresses are specific to a single host.Now, the implications of that is that links within a topology are restricted to a single virtual hierarchy, a single host domain.Because different host domain, the addresses can be aliased.So there's no way to resolve routing otherwise.So this big address field in the transaction shown on the right here, that's what's used for routing purposes.So obviously, something needed to change to be able to support traffic from multiple hosts passing on individual fabric links.

And so what's been defined is a brand new FLIT mode in 3.0 called port-based routing, or PBR for short.And in this mode, transactions are routed by that ID that I referred to earlier, which is called a PBR ID.Now, the transactions carry a destination PBR ID, or a DPID.Now, that's present in all transactions, so the transaction can reach its target.And on an as-needed basis, a transaction will carry with it a source PBR ID, or SPID.That's only where required.An example there is that a request will carry a DPID and an SPID so that the target of the request knows who originated the request and can use that SPID value from the request for the DPID for the completion.Now, this is how we're able to get inter-switch links, those switches inside the fabric.That's how we're able to allow them to carry traffic from multiple virtual hierarchies.Now, this is supported only in 256-byte FLIT mode.And you can see on the right the additional fields that have been added to the transactions for SPID and DPID.

The way that a link will know which FLIT mode to enter into is through the exchange of symbols 12 through 14 during alternate protocol negotiation.So each end of a link will advertise whether they are PBR FLIT capable.And if both ends are capable, then it's specified that they shall automatically enter into this PBR mode.

So let's take a look at the routing model and how things are passed through a fabric.So we have a simplified view of a fabric here showing the different types of elements that can be involved in a fabric deployment.And we can see that there are a number of elements that can be involved in a fabric deployment.And in a fabric topology.If you'll recall, I stated at the start that we did not want to compromise node level properties.And an implication of that is that hosts do not understand, nor shall they understand PBR.Hosts operate in an HBR mode.So all host requests passing through a fabric are going to begin in HBR format.And it's the responsibility of that host edge switch to convert that HBR format to transaction into PBR format.And what's involved there is a bit of address decoding to figure out the target of the request and translating that into a DPID and inserting that in the transaction.Once that host edge switch has performed that conversion, it's pretty straightforward for the intermediate switches.They just route based on DPID.Routing throughout the fabric is something that is configured by the fabric manager.So there's a lot of flexibility in how that is done, which facilitates those various topologies I referred to early on.So the intermediate switches are going to route based on DPID.If the target is a GFD, well, GFDs have been defined exclusively for fabric mode.So they operate in PBR FLIT mode.So transactions will be routed directly to a GFD in that PBR FLIT mode.All other endpoints, so the HBR based endpoints, they will connect through what's called the downstream edge switch.And the responsibility of the downstream edge switch is to convert into HBR mode.So the types of devices that can connect into a CXL fabric are SLDs, MLDs, even just PCIe endpoints.It also supports the connection of HBR switches.So there can be full non-PBR, non-fabric capable switches hanging off one of these edge ports.The implication there is that during binding time, the entire HBR switch is assigned to a single host.But that still offers a lot of flexibility.

OK, so I made brief mention of the fabric manager.Why don't we take a bit of a closer look at the fabric management architecture?So we have this topology on the right of a large, arbitrarily configured fabric.There is an entity managing all of the common resources.So the fabric manager is responsible for the initial discovery and initialization of all of the devices, generating an inventory.And once it has that, it can do composition.It can start binding endpoints to hosts and constructing these virtual hierarchies.That also involves enabling the access control mechanisms that are in place to decide which hosts have access to which parts of the GFAM space.All of the inter-switch links, they're shared among the virtual hierarchies.So it wouldn't be appropriate for hosts to manage them.So because they're a shared resource, the fabric manager is responsible.Same is true for GFDs.They are a shared resource.And then, of course, any unbound endpoint-- so endpoint 4 in this example-- has not yet been assigned to a host.That will be managed by the fabric manager until such a time that it's assigned to a host.Hosts get to manage the resources that are exclusive to them.So that includes the edge links and any bound endpoints.

OK, so we've taken a look at a lot of interesting new technology introduced in CXL 3.0, but we're not done yet.The consortium's hard at work adding additional features and capabilities.So in future specification releases, we'll have a more fully-fleshed definition of fabric management, including how PBR switches are managed, all the details on management of GFDs.There are also efforts in place to define host-to-host communication paths so that hosts can access one another's memory directly.Device-to-device communication-- basically enabling peer-to-peer transactions within a fabric.So if you could imagine an accelerator device having direct access to GFAM space.And then cross-domain traffic-- finding a way to get traffic from one host virtual hierarchy, as we call it, one host domain, into another host domain and how that can take place.And that's the content for this session.Thank you for sticking around.I encourage you to use the chat feature to ask questions.And please take a moment to rate this session.Thank you all.
