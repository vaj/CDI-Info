
Good morning, everybody. I'm Bill Gervasi with Wolley. We're a company that developed CXL and PCIe intellectual property, and then moving into product development. So, just flew up from Southern California. I'm not used to this freezing cold weather up here in the Bay Area, so if I shiver a little, you'll understand why.

Let's take a look at the history of how we got where we are. For about the last thousand years or so, we've had this basic architecture where processors had memory on one side, they had IO on the other side, and there was no way to really bring those worlds together. PCIe did have the capability of doing memory expansion, but it was really not optimized, and so this led to a situation where these resources basically could never be combined in interesting ways.

And so, what I wanted to do is highlight what I think is the underestimated importance of CXL development: that CXL is more than just a bus. Right? But it uses PCIe as its fundamental PHY and its protocol. However, what CXL brought to the party that we didn't really have before was the ability to combine these worlds—with communications, IO, memory—with all of this stuff brought into one consistent interface. This enables a new way of thinking about how we're going to partition our system resources, and that's what I wanted to explore in this talk: a way of taking these ideas.

and combining them in something we call NVMe over CXL. NVMe over CXL, in its simplest way of looking at it, is you take the idea of an SSD, you take the idea of a CXL memory module, and you combine them. You make a dual-function device that has NAND and it has DRAM. Now, what can you do with that once you have that combined interface? Well, it turns out it's one of those things, like, it's like an onion, or maybe a better analogy is a parfait because everybody loves a parfait. When you start examining the layers of what this brings to the party, it's actually very interesting.

So again, from the conceptual level, it's really very simple. From the conceptual level, you have NAND behind one interface, called NVMe. It looks like an SSD to the system. You have DRAM that's on the module, and it looks like a CXL memory module or a host-directed memory, and when you have those combined now, you can start manipulating the information between them in ways that really weren't feasible previously. One way of looking at NVMe is it is a cache protocol. NVMe does a block transfer typically four kilobytes from NAND out to some system memory. That is, and then, and then you're going to do evictions by writing it back out to the NAND media. That is a cache protocol, and so what we're going to do here is with NVMe over CXL, we take that controller memory buffer that was incorporated into the NVMe protocol and all the way back at revision 1.2, and that controller memory buffer is now going to be located in HDM space. That simple change that we're going to be working with the NVMe organization to standardize in a future version of NVMe is a critical part of optimizing those interfaces and turning it into a virtualized interface. Optimization by putting the controller memory buffer out in HDM space is now instead of doing four-kilobyte transfers across your system fabric and through your host processor, you can now just grab the flits that you need. You can grab 64 bytes or 256 bytes at a time instead of that four-kilobyte block, and this optimization is essential because when you break down network traffic, when you look at how much data is actually used out of those 4-kilobyte blocks, turns out that on average you're only using a hundred bytes. A majority of the accesses are just things like table lookups and key-value information which don't require all of that data. So why are you moving that 4-kilobyte block across your system fabric, interrupting your host processor interrupting the main DDR that's connected to that processor when all you needed was a hundred bytes? So that's where we're coming from is that philosophy that you leave the data where it is until you need it. With that kind of optimization, then we can expand this whole management scheme that looks like NVMe, but it also looks like HDM into a virtual HDM expansion of system memory.

So the number of problems that get solved by this, one of which is the roofline model. We've seen a lot of presentations already here at SDC, and we'll be seeing a lot of them over the course of this week. We, one of the problems with that roofline model is once you saturate your memory, once you run out of memory, you have nowhere to go. And when you spill out to NAND memory or to a hard drive, when you have to go to that block transfer, you're interrupting the system. And we saw, for example, I sat in on the MemVerge talk this morning, and the MemVerge guys showed how you hit the wall and you can lose 80% of your memory when you hit that wall. By raising the amount of virtual memory in the system, we can avoid that. We can raise the roofline up so that you can get more system performance. Similarly, we're going to look at the traffic reduction. This is something that I think doesn't get enough attention: is that systems are busy; there's a lot of information that's flowing through the fabric, and the fabric managers, as you know, they're still evolving over time. So anything we can do to optimize the fabric is going to have a big impact on system performance at the macro level, but even at the sub-assembly level. Today's accesses, when you do that four kilobyte transfer, it goes to the CPU, which means that it's draining the CPU resources, and the CPU and its attached DDR should be considered sacrosanct. You don't want to interrupt that resource if you can offload it elsewhere. So this kind of goes into that computational memory concept of: leave the data manipulation out there where the data is, instead of interrupting the CPU to get that work done. The next thing is uh persistence. We've been talking about persistence a lot over the last 15 years. We went through those Optane years and so forth, but one of the things that the Optane era opened our eyes to was the value of persistent memory, and it gave us that exploration into the DAX model of expansion. That persistent memory is one of those things that I think is going to be a low-hanging fruit that we are going to see evolve over time and become essential because what it does is eliminate that overhead of checkpointing. And from the system profiles I've seen, we're looking at seven to eight percent of system performance and seven to eight percent of system power are in these checkpointing operations that fundamentally add very low value to the system, but they, they add that level of security that's needed for high reliability transactions. So these functions are all now going to be addressed in one simple way that is consistent with itself. But the general philosophy that we come from is: let the host decide when the data is going to be moved around.

So from a high level perspective, uh, from the application layer, the idea is just to use the existing software infrastructure. Don't require changes to the software. You have multiple interfaces that you can use. NVMe, you have DAX, you have HDM interfaces. Don't interfere with those. Instead, write the drivers such that all of these APIs operate transparently to the underlying hardware, and that underlying hardware is an NVMe over CXL module that has DRAM and NAND and optionally can have an energy source for doing persistent memory backup like an NVDIMM-N used to do.

And all of these modes want to be available simultaneously because no single application and no user is going to have exactly the same usage profile that every other user has. So all of these things can be operated simultaneously. It's simply a resource management question that we let the end user decide. So, for example, when you need a controller memory buffer, you just go grab it. You use the standard memory allocator to define that CMB. Now your NVMe commands, when they need to access that CMB, uh, redirect it over into the NAND space through the RAM that's on the module. You can also come along just a block of memory that's used as a HDM. And this is your standard volatile HDM. When power fails, it goes away. But the idea here is, again, don't waste your resources. The stranded resources, uh, of memory that can't be used for anything else is something you also don't want to do. If you put 16 gigabytes of DRAM on this NVMe over CXL module, and you're only using a quarter of that for your CMBs, well, you can freeze up the rest of it to be added to the overall memory pool. So you can come along, allocate as many of those as you want. However, like we were just talking about, some applications want persistence. You want to have that energy source that backs up that critical data, and this might be a part of your SSD operation. It might be a part of your key store tables. It's whatever you want it to be. Again, let the host decide. They come in, allocate a block of memory, and now it becomes persistent. When power does fail, the baby algorithms will come in, allow for that, energy source to keep the system alive while the persistent memory is backed up to NAND, and when power is restored, that data comes back, it gets validated, and the system continues on as though power had not failed. However much of that memory you want allocated as persistent, again, is just flexible and decided by the end user. And then there's the virtual HDM. If you put eight terabytes of NAND out there, why not be able to advertise it as eight terabytes of expanded memory? And so that's what the virtual HDM is going to do. It's going to allow for these blocks of memory to be allocated as expanded memory. And then when, of course, you want to do an eviction, you write that out, and now you can reassign that same memory block, on the fly. So this is not that different in conceptually from how Samsung came up with semantic SSD, memory semantic SSD. However, there's one very significant difference in that they did their algorithms on the controller chip. We don't want to do that. We want to keep the controller chip dumb and simple and cheap. And so what we do is instead push this up to the host. Now that has some interesting effects. Also saw some talks this morning from UC Santa Cruz about how you can do the allocations. And this opens up the door for letting the host do allocation in a much smarter way because they know what's coming down the pipeline and can predictably do the cache algorithms using the standard NVMe command set. So again, there's no software change, that needs to occur. The memory allocator simply issues the NVMe command read to fill a block. And when it wants to do an eviction, it does an NVMe write. So very, very simple standard APIs, no new software development. And then the end result is of course the bottom line. This is a much cheaper solution than having 8 terabytes of DRAM. If you can have this kind of solution space, you can reduce the cost of your system very significantly, perhaps up to 90% cost reduction by having this memory expansion. Now, I'm not going to say that it doesn't have an impact on latency, but you're going to see that there are a couple of factors that CXL brings to the party that makes that latency disappear or become less significant. And we'll go into some test data that we've come up with. But as part of that story, the other thing to keep in mind is that CXL is full duplex. Read and write channels are operating simultaneously. And that you'll see also has some effects on improving the performance of the system.

So that, drilling down that high-level picture one level further, here you can see where the pieces fit together. The memory allocation system, the blocks in orange are the ones that we're developing and we'll make available then. You can access this through the global memory allocator, which gives you standard memory access. We can tie in the persistence functions and the baby backup and restore standards into there. So that gives us the DAX mode interface. You can similarly talk to this thing through a standard file system access or block memory accesses as well.

So we've built this thing. We showed this at FMS last month. And so go through the specs on this a little bit. I don't want to dwell too much on it. The things I wanted to focus in on though, is that this is not really state-of-the-art hardware. With the DDR module, the DRAM is just DDR4. And it's not even running at full speed. It's like DDR4-2000. The SSD, we simply plugged in an M.2 SSD. So this means that you're taking double the latency hit that you would normally take in a native implementation of NVMe over CXL. So you're taking the PCIe and CXL latency to get into the FPGA. And then from the FPGA, you go retransmit it over another PCIe interface for the M.2 controller. So we're taking a lot of different latency hits on this. So fundamentally what I'm just saying is that this is obviously not an optimized solution. However, it gives some interesting insights into what we can do at the system level. And that's what I did want to zoom in on.

Here's an example of the impact of that traffic reduction. So what we're seeing here is that if we compare a couple of these lines here at a time, for example, if you compare the just running on an idle machine, so it's the only application running, and you're going to just run a standard SSD. Compare that to an NVMe over a CXL SSD, you get a very slight performance improvement. And this is to be expected, because you're not doing the system traffic. You're not interrupting the CPU at all. You're letting the DMA happen between the NAND and the DRAM without interrupting the CPU. So, I mean, that's cool. You never want to turn down free performance. But the thing that's interesting, that I really wanted to zoom in on, was now let's take that same configuration and throw a bunch of background threads that are running. When you have that system very, very busy, now you are having to interrupt the CPU every time you want to do a standard NVMe transfer. And that's where, when you run on a busy machine, you see a 53% drop in system performance. Now let's focus in on the NVMe over SSD. We got that little bit of extra performance, that's nice, but now when you run it on a busy machine, the traffic is remote. And so you're able to only take a 25% reduction when you heavily load that system with other applications that are running. So this is really where the big gain comes from, is that you're reducing by half the performance hit when the system is busy. So that indicates the value of reducing the traffic on the fabric and through the CPU.

Next thing was, what's the impact of expanding the memory footprint? And so in this situation, we just tried a few configurations. We have more simulations and tests that we'll be running. But at least the early results on an unmodified Redis database is that we compared what happens if you just have 8, 16, or 32 gigabytes of DRAM as a baseline performance. Then we wanted to take and compare that performance to going through the FPGA to that DDR4 memory. And so, as you would expect, you see a decrease in performance. And it's actually not as bad as I thought it would be. And again, this is possibly due to the full-duplex nature versus the half-duplex nature of native DDR. But the added latency is going to impact you somewhat. Now, let's take and virtualize it. And so we see a nice increase here. And so this is where you see where you get this nice improvement in overall performance, when, when you have the combination of a fairly reasonable amount of DRAM out there and then a subsequently large block of NAND that you can see get a nice performance improvement over the DRAM-only configurations. And this makes sense. Bigger memory footprint, you can fit more of the data in memory. So here we're seeing roughly by increasing memory capacity by 4x, by increasing the memory capacity 4x, we get a nice doubling of performance. And again, driving that point home, that this is using NAND as the memory expansion instead of DRAM. So it's a whole lot cheaper than if you were to buy the equivalent amount of DRAM.

Next one is: Memory performance based on doing compression. So with memory compression, the idea here is to increase the number of threads that you can run simultaneously. And so there's a ton of data across here based on how much compression you're expecting to get and so forth. But the bottom line metric really is that you're able to get, with the memory compression, the same performance as DRAM by expanding that footprint because you can run four times as many of the threads. And so with the additional compression threads running, that's where you're getting that trade-off of, sure, the performance is going to be lower for any one thread, but the fact that you can run multiples now allows you to have an equivalent performance at a much lower cost.

So this is our traditional picture that we've lived with for centuries of how memory transitions occur. We see that DDR3 transitioned to DDR4, transitioned to DDR5. Everybody has seen this picture, right? But what's the reality? The reality is, there's not a single transition that occurs. Instead, what we see is that traditionally, the DDR3 to DDR4 transition happened in data centers first. They're always the ones that can afford the new technologies, that need that additional incremental performance boost from each technology, and they're the ones that drive the multiple changes in the market. Desktops, notebooks, and eventually SSDs and automotive, they all adopt these technologies as they move forward. And expect that the same thing is going to be happening with CXL. One of the visions that we have is that CXL needs to have this same roadmap. CXL needs to move out of the data center, it needs to go into workstations, desktops, potentially even notebooks, and eventually to cars as well. This progression of these technologies are what's going to drive these technologies, bring the prices down. Right now, it costs a lot to build a chip, an ASIC these days. We need to amortize that over a much larger market. So I encourage you guys to be cognizant of the things that we're doing here. We really need to move beyond just the data center market as well.

Has anybody heard of artificial intelligence? Has anyone not seen at least 20 articles on artificial intelligence in the last week? How many of them read articles on artificial intelligence while I was talking? Yeah, it's a pretty hot topic. Now, traditionally, the S-curve for artificial intelligence and for all new technologies, it tends to be seven years. Seven years from the early adopters, and then you have this hockey stick of mass adoption, and then finally the market saturates. We've been looking at these S-curves forever. Artificial intelligence is not following that S-curve. The artificial intelligence S-curve, and this is from some work that I just am in the process of with the United States Department of Energy, these S-curves are more like 18 months. In 18 months, what did we do? We went from 64-gigabyte language models, and then the LLM jumped up to 80 gigabytes, and then it jumped up to 240 gigabytes, and next year they're projecting 1.7 to 1.8 terabytes for the LLM. That kind of treadmill is not something that the industry is very good at keeping up with, and AI is not stopping. I think we all acknowledge that it's only going to keep going on that treadmill.

and so we need to take a look at how to deal with that and deploy these technologies in a time frame that is also hitting us from the demand side. At the same time that we're needing more memory, we have a particular problem in the industry that DDR5 is about to go from two DIMMs per channel to one DIMM per channel. There's a proposal called the MRDIMM or Mr. DIMM that's going to maybe help offset that, but it has its own potential problems, like you have to keep this thing at above 50% hit rate on two adjacent cache lines that happen to span across a rank boundaries. There's going to be a lot of challenges here, but the bottom line is that we're about to lose 50% of our memory capacity. Or you're going to run DDR5 slow, in which case you're not going to get the performance gains. So we don't want the industry to be facing this, and we've gotten Dell Technologies as our co-sponsor for some of this work. Hopefully by the next time I meet with you guys, we'll have two other OEMs that are approving this, and moving forward on taking these technologies and these memory expansions to the other client systems, the workstations and so forth. The idea that you can upgrade your systems now. You can add a new technology like NVMe over CXL to expand these footprints and give innovative new ways of approaching AI. The problem with an 18-month S-curve is that that's shorter than the three-year depreciation cycle for new equipment. And you're not going to throw your equipment away halfway through its usage cycle. You need to redeploy it. So having this concept of CXL expansion go to these new systems is going to be a great way to approach the problem of these short S-curves, do upgradable systems, do upgradable memory to address these problems. So the way I'd like to look at this is, I'm reminded of back when I was at Intel in 1992 and working with Jim Pappas on the development and deployment of PCIe and USB. And back then the argument was, we need to put PCIe into all the systems in the world in order to give the industry a place to innovate. And I remember everybody asked back then, well, what are you going to put in those slots? But if anybody's old enough to remember the 1992 PC, it had no sound. It had no network adapter. It had no graphics. Things were like Hercules. And... All these other cars. They didn't even have modems. You remember what a modem was? Does anybody even remember the horrible sound that a modem used to make? All those things ended up on PCIe. And, it exploded the industry with innovation. It allowed us to come up with all kinds of stuff that could go into those sockets. I think we're in that same position now. I think AI is going to be one of those factors that not only drives software innovation, but it's gonna drive hardware innovation and make us think of new ways to adapt the systems.

So that's the basis of the talk. Essentially, what I wanted to just introduce you to was that this thought process that we could do some new things with CXL, because of that ability to merge memory and other technology, storage and everything else, into a common protocol, is a way of looking at virtualizing the system. And so we've developed this concept of NVMe over CXL that combines storage and memory in some new ways. It can operate in lots of different methods, and we don't want to make them exclusive. These things can all be operated simultaneously, and it's always going to be the host that directs when and where the resources are going to be used. I showed you some data. This is data that indicates that reducing the traffic on the fabric and through the host CPUs is going to lead to significant performance gains at the system level, and that expanding the memory footprint is going to be an essential part of addressing the needs for artificial intelligence. And it can do it in a low-cost way. The idea of using NAND as a main memory has long legs, and it allows us to do new, lower-cost ways of expanding the system. And then I want to drive home that message that in order for CXL to be successful at the data center level, we need to reduce its cost and power envelope, and the way to do that is to expand its use into applications beyond the data center. And that's it.

Some questions?

I mean, that is a great question. The question was, do I expect all memory to sit behind CXL? Multiple answers. The primary answer is going to be no. But as I said, one of the things I do is work with the DOE on energy efficiency. How much data is actually used? And there's an embarrassing formula. Just recently, in fact, last week, the Department of Energy released our report out there. It's open for RFI. RFI closes the 30th of this month. So if you want to read those and respond, this is the time to do it. So. Moving data around and not using it is the big crime. If you, like I said, we, out of every four kilobyte block, on average, 100 bytes are used. Take that same concept to DRAM. For a 64-byte cache line to get filled by a CPU, it needs to go out to a memory module, which is 10 chips across, each with a one kilobyte page size. And it's a destructive read. When you read the data out of the core to the sense amps, you have to pre-charge it to get it back. So there's your interesting math. 20 kilobytes of data movement for a 64-byte packet. That efficiency is 0.025%. 100 bytes out of a four kilobyte, that's 3%. So SSDs are actually more efficient than DRAM in terms of data used over data accessed. But that's kind of where the rubber is going to hit the road. I don't see HBM going away. Why? Because HBM has a two picojoule per bit energy footprint. So even though you might be wasting some of your accesses with the HBM, at least you're only burning two picojoules when you do it. So adding up the number of picojoules, I think DDR, and a DDR RDIMM is somewhere around 8 to 10 picojoules per bit. And then CXL memory is about another 10 plus the memory. If you can integrate CXL with the memory, that helps the picojoules per bit a lot. We might be able to make CXL memory on a power basis be competitive with DDR, or maybe even better than DDR. But the latency needs to have the offset of the full duplex channel. I don't think there's a simple formula yet. I don't see it completely replacing DDR. But we have also seen that IBM did some great work with their differential DIMM, if you recall. And the differential DIMM did replace the DDR interface with a serial interface. So I think there may be something in that direction that kind of blends those worlds.

So the question was, what about the fault tolerance? And so, and fault isolation? Yeah, it introduces another set of complications. The RAS work that's being done in the CXL group and the RAS that's going into the CXL memory controllers addresses some of that, but in terms of it going offline/online, that I don't have an answer to off the top of my head.

Any other questions? 

Which is both a good and a bad thing, right? Yeah, field replaceable unit is a strong selling point for some things, as is hot plug.

Any other questions? All right, thanks. I'll give you back some of your time.
