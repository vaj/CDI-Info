
Good morning, this is Charles Fan. I'm co-founder and CEO of MemVerge. Today, I'm going to talk about how to deliver your own state-of-the-art enterprise large language models. 

As we know, starting about November 2022—a little more than two years ago—when we first got to know ChatGPT, little did we know how much they were going to change the world. Over the last two years, the API services from the leading frontier labs doing generative AI work, like OpenAI and Anthropic, have really built a growing business of offering large language model queries through the APIs. OpenAI is getting over $3 billion of revenue in 2024, and Anthropic is exceeding $1 billion. A lot of the enterprises are starting to explore killer apps in how AI could increase the productivity and competitiveness for their firms. Over the last few months, we are witnessing an advancement of open-source AI models that now can rival the performance and the cost of the state-of-the-art proprietary models. Meta open-sourced LLaMA, an open-source model, a few months ago. Mistral is another competitive open-source model. And in more recent weeks, we are hearing the news of DeepSeek, which can train a performance model similar to GPT-4 at one-thirtieth of the cost, and uh this can be deployed anywhere uh as they have opened up the weights of the model, and this really makes it possible for the Enterprises to deploy open source models in their own environment, and this can allow their own proprietary data to be merged with the model within their control without compromising data privacy or security. Uh so this is great development for the uh acceleration of adoption of AI for the enterprises.

So, how does an enterprise do it? This is a simplified picture of how the workflow works to deploy an enterprise AI system. You do have the API-based inference API at the bottom left, and these are the ChatGPT service, Anthropic service, Google Gemini service, etc. And then you have a model registry where many of the open-source models are available from places like Hugging Face, etc. Now, within the enterprises, you could be using a public cloud like AWS, Google, or Azure. You could also have your on-prem data center being revamped to house the GPU-centric computing clusters. So, this could be a multi-cloud infrastructure setup with both on-prem and public cloud resources. But they are all within your own security domain. They’re either in the virtual private cloud or within your own data center behind your firewalls. And now, these models can be downloaded into the virtual private cloud, and they can be deployed for model serving to serve the inferences. So, some of the queries from the AI applications can still go out to the API services, but others could be going to these open-source models that are deployed within the enterprises. As we mentioned, many of the enterprises have terabytes or even petabytes of proprietary data, and they are everywhere—in databases behind SaaS services or in unstructured data, in files and backups, and so on. Now, these data can all be mined for the value that they have, and they can be fed into the open-source models through a process called fine-tuning to embed this information into the model itself. And one of the trends in my discussion with John Rhodes this morning is that we are going to see more and more of those specialized models within enterprises, which is taking the open-source model and then combining it with some of the specialized knowledge from the proprietary information. And then combining it with some of the specialized knowledge from the proprietary information, and for that model to be the best model for this area that’s useful for this particular enterprise. So, there could be a process where you download the open-source model, fine-tune it with the proprietary information, and then deploy the fine-tuned model in your own environment. And there could be more than one of such models being deployed. They can be working independently, they can be working with each other, and they can also be working in conjunction with the large-language model delivered through the APIs. And this can be guided through this gateway process. And now, there is another interesting area, which I call "persistent context" here. This is actually a big area, and there are various forms of it today. For example, one of the popular forms of persistent context is something called RAG. It’s Retrieval-Augmented Generation, and the idea is you can take the proprietary data, vectorize them, and keep them in some kind of databases—often a vector database—and they can be queried by the application as they are sending out the query to the large language model. And they can generate the context that’s needed in order for this query to go with the pertinent information from the proprietary pool before it can be processed by language models. In the future, this can take on more forms. The historic data that has been generated by LLMs through past interactions—and this could be from various agentic AI applications—can be remembered by this persistent context. So, in some way, it serves as the long-term memory of the enterprise AI subsystem. And this can remember it, and this can be remembered by this persistent context—the proprietary information. This can also remember the past interactions between the applications and the large language models. And there will be multiple models; there will be multiple applications. So, this becomes a common shared memory space that various applications can take advantage of. And this can also be used to further fine-tune models as the enterprises see fit. So, all these things together can create a very organic enterprise AI subsystem. They combine the state-of-the-art in large language models with the rich proprietary information and have the two combined together to increase the productivity and competitiveness of the enterprises.

So, as this is being deployed, it needs a good infrastructure to run them. And as we know, the infrastructure has been going through a fundamental transformation as well, as AI and generative AI are changing the world. In particular, the architecture of the infrastructure is moving from an x86-centric world where the compute is driven by x86 CPUs and DDR memories. The connectivity is through TCP/IP networking, typically on Ethernet. And the data is stored on network-attached storage systems that are attached to this TCP/IP networking. Now, this is moving to a new world where, while all three components will continue to exist, there’s emerging a more GPU-centric computing capability with high-bandwidth memory as the primary memory for this compute. You’ll have new fabrics interconnecting these new processors, including NVLink from Nvidia, UALink, and three standards driven by AMD, Broadcom and others. And you have CXL and Ultra Ethernet and various technologies that are meant to provide high-bandwidth connectivity between these AI processors. And there will be new memory-centric data platforms that are connected to this fabric that are there to serve these GPU-centric computing farms for the AI workload. And all together, they form the new AI data center that’s going to be the underpinnings where much of the workflow we showed in the last slide will be run.

And now, this will call for a new layer that will optimize the mapping between these workloads and the AI data center architecture. So, this layer is AI infrastructure software. They will schedule and orchestrate the workload and allocate the right resources—both compute, networking, as well as data platforms—to support these workloads. And they can do so dynamically. And they can migrate workloads as they need it; they can preempt workloads as needed. And they can protect workloads when there are faults in the area. And this is an emerging area that’s critical to serve the enterprise AI needs. And this is where we at MemVerge have been focusing on.

So, at MemVerge, we are building a software stack we call MemVerge.ai. This is an enterprise AI infrastructure automation software that aims to help enterprises stand up GPU clusters and deploy, fine-tune, and auto-scale their large language models. And it has a few interesting components to this software. It’s built on top of Kubernetes. And it’s fully taking advantage of the auto-scale capability that’s enabled by the Kubernetes platform. And we have enhanced the scheduling capability of Kubernetes with a GPU scheduling and job orchestration layer. This can effectively allow multiple users, multiple projects, and multiple departments to collaborate and to share a single GPU cluster. And we can allow them to be guaranteed certain resources they need to run the job, and they can borrow resources when their needs go beyond what they have reserved, and they can also preempt other workloads when they have higher priority to a particular resource. And this can be done seamlessly by this scheduling layer. And we also have a unique technology that we call “transparent checkpointing” that is tightly integrated with our GPU scheduling capability. So, when a higher-priority workload needs to preempt a lower-priority workload, we can use the transparent checkpointing to save the state of the lower-priority workload so that it can be restarted from the point when it’s interrupted whenever their resources become available. So, this saves the work to avoid the cases where it needs to restart from the beginning. And transparent checkpointing is an independent module that can be integrated with a scheduler, but it can also be useful for many other use cases as well. And what we are also building are three key components on top of it, as was shown in the workflow slide that I introduced a couple of slides ago. We’re going to allow the operation person for a large language model to be able to easily download, deploy, and auto-scale any open-source models within the firewalls of an enterprise. We can also make it easier for you to apply your proprietary data to fine-tune the model that you have before you deploy them. And we are also building a persistent context layer that can serve as a long-term memory for your agentic AI platform. So, this all together forms the MemVerge.ai software stack that can make it much easier for enterprises to serve the various users within the enterprise that can take advantage of large language models. In a sense, it’s an “easy button” for you to take advantage of the state-of-the-art in large language models and deliver the last mile to the users who need to use them. And it provides an effective alternative to using the public API services, where, in this case, you can keep all the proprietary data private.

Okay, so now, let me dive into some of the components to give you a bit more detail. In particular, let’s go into the GPU scheduling and transparent checkpointing side.So, the key driver of it is that we are noticing there is a trend today that enterprises are actually buying more GPU servers into their own data center environment, and they are also using multiple GPU cloud service providers to complement that capacity. And those are often delivered in committed, reserved resource levels.However, these tend to be siloed today. So, there are often multiple departments in an enterprise, and each of them will get their set of GPUs, and those GPUs are not easily shared with the others. So, when they are using them, great. If they are not using them, these are sitting idle. And if they are using more than what GPUs they have, it’s difficult for them to get more GPUs. So, all this results in low GPU utilization, as shown in this Waste and Bias survey, where about a third of those surveyed are seeing a utilization less than 15%. And if you add the between 15% and 30%, about half of those being surveyed are seeing the utilization under 30%.And that’s a lot of valuable resources, you know, wasted and not being used.

So, what we have built is a "GPU-as-a-service" software that can essentially pool all the GPUs together between all the departments within the enterprise. And each of the departments can claim reservations of these GPUs, and they are guaranteed the resources that they reserve. Now, at the same time, they can borrow from the resources that are either allocated or allocated but not being used at the time. So, they have their reserved resources; they can also go to borrow additional resources. Now, if they need to use their reserved resources, but someone else borrowed their resource, they can preempt them—meaning they can stop those workloads and start running their own workload. And that’s where our checkpointing comes in. And we can protect the preempted workload, so those workloads can be resumed as soon as the resources can be found again. And this software also provides ample telemetry and observability features that allow you to see the exact usage of the resources and the performance of the workload on those resources. We support various workspaces that can be integrated into various IDEs, such as Jupyter Notebooks or VS Code, so that these jobs can be launched directly from those IDEs. And we can be running on on-prem as well as on the cloud service providers’ resources. And all this is fully integrated together and works seamlessly to maximize the utilization of GPUs while having minimal impact on the user experience from different departments. And we can handle the billing across the departments. And this is like establishing an internal spot market where the resources are available when they are not being used, and now they can be taken back if the people who have higher priority need them to be used. So, this can help the enterprise enable a private "GPU-as-a-service" for the enterprises.

So, here, let me give an example. You have three projects: Project A, Project B, and Project C, colored blue, green, and orange. And each of them has a reservation of four GPUs. Now, when Project B needs to use more than the four GPUs it reserved, it will see that Project A is not using all the GPUs in reserve. So, it would preempt—it would actually borrow or burst into those two blue GPUs and run the green workload.

Now, if Project A now needs to use those GPUs, it will take a snapshot of the green workload and will find other free resources. In this case, there are two orange GPUs that are available, so those workloads will be moved through the checkpointing process to these orange GPUs. This will free up the GPUs to allow Project A to use its blue GPUs. So, by combining the scheduling and transparent checkpointing, we can maximize the utilization while minimizing negative impact to any of the workloads that are running on the system.

So, this software is available for a test drive. We are announcing a pioneer program. You can sign up here by scanning the code, and we’d love for you to give it a spin to see if it serves your needs and to be the first step to help you stand up your GPU cluster and run your big large language models in your environment. Yeah, so more information can be found on our website, www.MemVerge.com, and we look forward to working with you and to helping you deliver the state-of-the-art large language models. Thank you.
