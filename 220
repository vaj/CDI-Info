
Good morning. It's great to be here. We are going to talk about one of the major workstreams,  one of the subprojects within OCP. As you guys know, OCP is the place to integrate a lot of great work  from outside technologies. One of the major efforts around OCP integration has been the composable  memory systems. Today, with Reddy, I'm here to report on a lot of good work that's been already done  within the OCP community. We are part of the server project that is under OCP workgroups.

So we will cover a little bit of history of composable memory system and the journey we have gone through  to get to this point. There are a lot of wonderful activities. They invite you to participate.

As I said, one of the technologies available to the community has been interconnect technologies  such as PCIe or CXL. The team took a look at the technology that's available and decided that  composable memory system is a type of thing that a lot of people were interested in. A group of people got together and came up with use cases for CXL as a fabric interconnect technology  and eventually arrived at a number of workgroups and use cases that defined the work that we want to do. When you challenge smart people, they normally come up with very good results. Some of the work that we have to do  will integrate with effort outside of OCP. The team a minute ago talked about server ready, for example,  from AI-ML or DMTF that we have, CXL, JEDEC, PCI-SIG. A number of different technologies bring solutions  that will integrate within OCP community. 

One of the concepts that's available to us came from CXL  specification enabling disaggregated computing. Once we disaggregate computing, software guys can get together  and reintegrate them, compose them in flexible solutions. For example, CXL offers direct attached CXL  controllers to the CPUs or GPUs or accelerators. It offers methodology for a memory controller to have  multiple ports so it can be connected to multiple hosts, therefore provides memory pooling and memory sharing  capabilities for efficient data movement and increased utilization of resources. Switch fabrics are  possible through PCIe techniques, so we could have much larger systems using multiple layers of CXL  switches within a fabric that's defined by, for example, CXL 3.1. Then once we have multiple switches,  we can consider a number of leaf nodes, devices that are connected that can be assigned or reassigned to  different processing elements. And of course, as part of large systems, not only usability, serviceability,  bandwidth, latency, and reliability are all important factors that the CXL community is defining. So in other words, CXL is not just a physical layer, although it is relying on a well-known PCIe physical  layer today. Just so that people know how it is and work with it immediately, but it has multiple other  layers that the community of software and hardware people are getting together to develop the entire system. So as an example, a physical layer can be remapped into UCIe. CXL protocol runs on UCIe, but the rest of  the ensemble of work that's being done can be transferred to other physical layers as well.

So again, as I suggested, a number of companies, now we have more than 35 people get together on Fridays  to talk about composable systems, and Reddy is leading that group. It is great that we move into that. But ahead of that, I maybe put a little plug on other work streams that are going on within OCP. We have hardware-based solutions around DCMHS, the data center ready modular hardware systems. And there's another work group that is relating to interconnect, extended connectivity work stream  that's based on PCIe and photonics interconnects. And we have hardware management that deals with  all aspects of resiliency and security of these modular systems. And then composable memory system  is a work stream that we were going to jump into now. Reddy? Please.

Thanks, Siamak. I will go through the history of how we started this project within OCP. And then towards the later part of the slides, I will actually talk about the -- I will go through  a little bit of technical detail on what we have done so far. Predominantly covering the CMS  architecture part, and then there are a couple of work streams where I wanted to touch upon what's  happening so that you kind of get a theme of what we are focusing this year. Hopefully it will  interest you enough for you to actually join the CMS subproject. So CMS -- we started this  initiative back in 2021, three years ago, as part of the FDI initiative. It's called software  defined memory. So the theme was let's take a look at what we need to do in OCP within the  context of software defined memory. Define what the right scope should be. So within a year,  we ended up actually graduating into the server subproject, as Siamak was mentioning. And we  actually started off with what are the key use cases we should be focusing on for the  composable memory. And then how do we actually gather the broader industry, you know, the  contributions into OCP. As you can see, last year we had a significant presence in the OCP  global summit. So quite a number of contributions that actually shaped up. We have one  specification which is actually contributed by Google and Meta. It's a CXL memory expander  specification for hyperscalers. It's basically looking at the requirements for the memory  expansion capability. We also delivered four white papers. This includes use cases,  workloads, memory access tracking, and then obviously the architecture part. So the  architecture basically lays the foundation work for what we do within the context of CMS. So  we wanted to do something very quick last year. And then of course, you know, almost 15  companies or so actually participated in the OCP. It used to be called Experience Center. So it's basically innovation village demos. So quite a bit of interest from the  community, significant amount of progress last year. Going into this year, our focus is  really looking at, you know, identifying the specific set of use cases that we really  want to target. This includes the hyperscaler infrastructure as a service type of use  cases that's, you know, virtualized, containerized, and bare metal. And then also AI and  HPC, you know, the fabric-based use cases as well. 

So we have five work streams that  are actually focusing on different facets of the CMS scope this year. These work streams  got kicked off probably like maybe three months ago, a very early part of this year. So we partitioned the scope into five different work streams. So the composable  workloads is essentially looking at what are the set of workloads that we really need  to focus on to drive composable memory solutions into. And then we have data center  memory fabric orchestration. So without having an end-to-end solution, there is not a  whole lot you can actually do. So fabric manageability is a very critical component. I will go into the, you know, one click down on the fabric manageability architecture. And then AI and HPC systems and fabric, we wanted to have a very specific and dedicated  focus for AI side of the, you know, workloads. Specifically looking at the fabric and  then how do we actually compose the memory for GPU-centric or accelerator-centric type  of use cases. And then the computational programming is fairly emerging. Essentially,  you probably might have heard about near-memory computing, computational storage that  SNIA actually was driving -- has been driving for quite some time. We needed to do  something similar for, you know, memory as well. Essentially look at how do we, you  know, have the compute element closer to memory as opposed to, you know, farther away  from the memory. So it requires evolving, you know, programming models and lots of,  you know, architecture patterns around it. And the last one is actually we want to  have connectivity with the universities and research community as well. 

So those are  the five work streams that we currently have in flight. So real quick, the focus for  this year for each one of these work streams is composable workloads, define the  workloads first, the critical ones specifically. That includes AI and HPC as well. And then have case studies around it. So how do you take an element of what Siamak was  talking about, direct attached memory expansion capability, multiported or pooled or  shared type of, you know, implementations and adapt it for specific use cases that can  really benefit from. So that's really the goal of case studies. Data center memory fabric orchestration, our goal is to essentially have an  architecture spec. This includes the API, you know, stitching together the end-to-end  API flows. And then on the AI side, we want to actually have AI-specific  architecture patterns for composable memory and then have a white paper  specification around it. Computational programming, the team is currently  focusing on one white paper that essentially talks about why computational  programming is very important and go through different programming models. And then we'll probably end up making more focused effort next year.

There are foundational contributions that need to happen besides the work stream,  you know, targeted focus areas. So the memory expansion capability that I talked  about as the base specification that covered only the DDR media. Now, how do we  take storage class memory? How do we take NVM and expose that over CXL? What  are the key requirements that we need for that type of, you know, device that is  being exposed through CXL? We need to essentially look at those details and have  a specific set of specifications for it. Then we are also looking at more of, if  you look at the switching capability that we actually need to work on, there are  not many CXL switching vendors out there that are readily available for us to  actually build these solutions, build the specifications and white papers. So we  are looking at what is the best way for us to expose that through OCP experience  center locations to see if we can actually do something more interesting for the  broader community. 

So going into the details, high-level logical architecture,  it's a very, you know, very high-level cartoon picture. So we look at--there are  compute elements. Compute could be CPU-based or GPU-based. And then you  essentially have memory. It could be near memory or far memory. So the way we  characterize the near memory in the CMS subproject is anything that is directly  reachable, connected to the socket, specific CPU socket, or it is a remote CPU  socket, or it is a CXL, you know, memory expansion capability, that is basically  one NUMA hop away from a latency perspective. It's almost as good as going to the  other socket memory. Those are all characterized as near memory. Far memory  essentially includes anything that, you know, is a little bit more latency, you  know, oriented, higher latency. And things like CXL buffer behind the CXL switch  is one example, right, for the far memory. 

Clicking down on the logical  architecture. So this architecture is supposed to include pretty much the broad  spectrum of use cases. So it starts off with direct attached memory expansion,  you know, functionality, whether it is single or multi-logic devices. And then you  have multi-ported devices that are actually be, you know, that need to be covered  as well, where you are essentially connecting not just one host but multiple hosts. And then you have the fabric-based CXL, you know, pooled solutions, pooled or  shared solutions. So we need to include that in the scope. And then the emerging  one is really the CXL memory enclosure with alternate transport. We just left it  as something that there is cheese down the path, but we have not really focused on  that element. And you probably will see some focused efforts, you know, later on  starting this year. Stitching together all these elements is essentially the data  center fabric manager. And we need to worry about implementing that end-to-end  to be able to say truly composable solution. 

So here is the work stream focus for  the CMS workload, you know, work stream. It's essentially, this team is looking at  pretty much broad spectrum of workloads, starting with the general compute. This  includes containerized microservices as well as virtual machines. And then you have  the storage-centric, analytics-centric, as well as AI and HPC type of workloads. The team is essentially focusing on what are the set of benchmarks, how do you  fine-tune the benchmarks, and contribute some software elements to make sure  everything is working end-to-end and have the case studies around it.

 So here is the data center fabric manager, you know, architecture clickdown. What you see is you essentially have the critical component is really the  centralized data center memory fabric manager. This needs to actually work in  tandem with the CXL-based media that is actually sprinkled across in the data  center using the CXL fabric manager agent. It can be in-band agent, it can be  out-of-band agent. So out-of-band could be through BMC. In-band is essentially  using CXL.IO through the host, you know, specific agent-based interaction. Our plan is to have Redfish-based profiles as a starting point for the  integration and then essentially use the CXL-based lower-level constructs,  which is essentially the fabric manager APIs, which is CCI-based interface in  the switch, to be able to actually stitch together the entire end-to-end  solution and make sure we can actually deploy this using Kubernetes framework. So what it means is now you have a full-blown Kubernetes-based memory  orchestration that we really want to showcase in the global summit timeframe.

This is the work in progress. One of the key contributors in the CMS for the  data center memory fabric manager is Jack Habits Lab. This team is actually  working on building a concept POC. We won't have enough time to actually cover  this. So currently -- they recently posted the -- developed the POC code. So there are two building block elements in this one. One is the command line  on the left-hand side, essentially using that as a way to control the switch,  you know, the orchestration flows. On the right-hand side, you have the CXL  switch emulator, because we don't have working CXL switch with fabric  management capability for us to actually do something more interesting. So we are starting off with the emulation capability. So the team actually  built an emulator that we currently have the POC code that -- you know, to  showcase how these end-to-end flows actually work for the data center memory  fabric manager. And there are links down for the blog as well as the code  that is contributed in the GitHub. I strongly recommend you guys to watch the  YouTube link as well. That essentially has the details behind on how the demo  actually works in the POC code.

The call to action, please do join the CMS subproject. There is a lot of  interesting stuff that is happening in the community. Like I said, there are  emerging tactical focus as well as way forward-looking, you know, focus. Computational programming, computational, you know, revolving around the  memory is a big deal. So there is quite a bit of interesting stuff. I  strongly urge all of you to actually join. We do meet weekly on Fridays, 9  a.m. PST. And it's a very lively community. There is a lot of interesting  debates and discussions that do happen in these meetings. So I request you  to join. If you are looking for additional information, if you go to the  OCP CMS wiki folder, you have all the details around it, including the call  recordings for the prior calls. Thank you. I guess we have almost one minute for Q&A. There are specific  questions.

So again, this is a community to get together and get things done. It  is a hardware, software, and security and management all co-designed. The  first aspect of what we have been trying to do is first do no harm. If  software expects certain things to work, although hardware changes, our job  is to abstract it away. But it takes effort. Zach told us that we can put  things together in a PowerPoint and call it done. But it takes many, many  people, many, many hours and perhaps years to get it done right. But at the  end, we push the envelope. If it is too hard, we stop and find another way.And we try to put things where they belong. That's why within the OCP  community, we have hardware teams do things in a partitioned way, in a  modular way. And then we have software and management teams try to put it  back together. And we need your help.
