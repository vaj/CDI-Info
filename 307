
Apparently, right. Okay, hello everybody, I'm Hannes Reinecke. I'm working for SUSE and, well, doing various things related to storage and working with Linux, developing on Linux, and everything. So today, the talk is "How to Mess Up Your NUMA Topology with CXL."

So, um, and if you thought that NUMA stands for Non-Uniform Memory Access, you are sadly mistaken. NUMA was the second, the first king of Rome, obviously. So it's actually named—no, it is not; it is Non-Uniform Memory Architecture.

So, um, in order to make—um, to introduce you into the difficulties, I'll have a short introduction into NUMA itself. Like, how did we even end up here? So, why is that a problem? I mean, this whole thing has been there since ages, so where's the problem? So, the thing is that the original design of NUMA, of computers, was actually quite simple. So, you have your CPU up there, then the North Bridge, and North Bridge talking to the memory and high-speed interfaces, and then the South, South Bridge doing the actual I/O to PCI, and then the lower-level I/O there. So, all quite easy, all straightforward, not a problem whatsoever. You can just code against this, and this is what the program was, and also what UNIX was originally designed to work on. So, you have your CPU, which is different from the memory. You have your I/O, which is again a different thing, and that is pretty much still the basic design goals of UNIX: I/O is different from memory, is different from the CPU.

So, but then, one figured, "Ah, you know what? We're having this bus there—frontside bus. Anyone who knows, um, so," which turned out to be not a clever idea because that meant that all I/O, everything you do, have to cross the bus. And what's more, it will actually block access to other instances or other devices on the bus. So, the more traffic you shove about, the more you throttle the other devices, which is not a bad idea. So, they switched from the bus to a point-to-point topology. That's what PCI did; that's essentially the big difference between PCI and PCI Express, where PCI is a bus system, and PCI Express is a point-to-point, star topology thingy. So, and um, to make that even faster, they said, "Alright, you know what? Going via a different chip is actually getting quite expensive." So, just move everything into the CPU, move this PCI controller directly into the CPU. Then, we're having low latency and are more efficient talking to the PCI devices there. So, but still, nothing really material and nothing really changes from the programming model. You still have a CPU, which is different, you have your memory, you have your PCI buses on PCI devices, and so on and so forth—all good.

But then, physics struck, and they found that they couldn't crank up the frequency as much as they wanted. So, essentially, they found that on the Intel design, four gigahertz is a pretty hard boundary. Getting around that is more quite complicated because you're burning a lot of power, and you need to reel a lot of cooling to go beyond that. So, in the end, they decide, "Ah, no, it's just too much effort; just leave it at four gigahertz, right?" But if you do that, the CPUs don't get any faster, so what is your selling point then? How would you sell the next generation of CPUs? "Oh, you know what? We just stick more CPUs on it. Alright, what could possibly go wrong?" So, and that's what they did. Suddenly, you're not having one CPU but two, or four, or eight, or something. So, that is okay, you could, but then you found, "Ah, you connected the PCI directly to the CPU. To which CPU?" For that to make sense, we actually need more than one interrupt, ideally one interrupt per CPU. That I/O is that each CPU knows, "Oh, there has been I/O which I need to worry about." So, you suddenly not only have multiple CPUs but also multiple interrupt vectors per device. Okay, so that makes things getting more complicated.

And then they found we can't put infinite chips on the CPU chip because, well, it's getting too large, too unwieldy, and we need to change the design. And they said, "Well, we could just put not just one CPU chip, but another beside it. So," and then have an interlink between them, "and then we can just duplicate the entire infrastructure because, well, that's what we do anyway. Okay, good." And then, suddenly, so, you now find, yeah, you still have a CPU and everything, but the latency you need going from CPU to the connected memory is vastly different than the latency you need to go from the CPU, across the QPI link, to that CPU, to that other memory. And that is essentially why they came up with NUMA (Non-Uniform Memory Access), because that's precisely what it is. And, um, so it then means that, well, depending on where, on which CPU, on which socket the code is executed at that time, and where the memory, which that code has been allocated, is located when the thread is running at that time, you get different latency results. If you have a process being created on the first CPU and memory being allocated on the memory near to the first CPU, and then the thread is being scheduled over to that other CPU, well, suddenly the performance goes down, with you none the wiser. Why on earth is it now, is the process no slower?

So, what shall we do then? How do we handle that? And that's how they—where they came up with the NUMA topology, or rather, the um, the NUMA setup in the kernel, which means, "Alright, we just model whatever the hardware do." So, we introduce NUMA nodes in the kernel and have memory locality that we can tell the memory allocator, "Alright, I'm now running on that NUMA node. Please allocate memory on that NUMA node. Okay? So, yes." And to help with that, we just, um, there's an HPI table called the HMAT table, which essentially gives you the so-called NUMA distance, telling you, "Right, how far is the memory I'm trying to reach?" And so, that now is an example from one of my systems where, in older ones, you can see, "Alright, so if you go from NUMA node 0 to node 0, you have a latency of 120. If you go from NUMA node 1 to 0, you have a latency of 194." Keeping in mind that this is not going from node to node, but rather from memory attached to that node to memory attached to that other node. Because really, it's memory transfer speed which you're measuring. You don't really measure the transfer speed within the CPU, because how would you measure that? And you can also see, if I'm going across the NUMA interlink, the latency increases by quite a lot. Roughly symmetric here. And if you're being within the NUMA node, you're actually being quite good. There's a slight discrepancy here, but this is because on NUMA node 0, there's lots of other things happening. Which, like housekeeping. All the... Hardware setup stuff, interrupt vectors, you name it, are all on CPU 0. That's where the skew is coming from. Alright, good.

So far, so good. That means that we now have not a symmetric programming model anymore, but rather a model which is split into two sides, or rather one in the number of NUMA nodes. Which means... Now, as we have NUMA nodes, then we also should look at the thread scheduler to ensure, alright, so what do I do with that thread? I mean, if it has allocated memory on that node, it should possibly stick on that node and not be scheduled over, because that would then just harm performance. And of course, you can then go a step further and if, like some modern chips do, also have some internal latency changes. So, like if you are running on a multi-chip module, say, where your CPU chip is not one chip but rather several welded together, then you suddenly have different latencies within the NUMA node and so you end up with something called sub-NUMA clustering, where even within the chip there are different latencies between each node.

So... Which means that our nice simplified model... That, well, we have that here, we have a CPU talking to memory, and then we have the I/O. underneath. It doesn't really hold anymore, because we have a CPU here, a CPU there, and then we have to go from that CPU to that CPU to that device, and so we have to do something with the program model.

So, that is especially important if you want to do I/O., because then you have everything you need to come from memory, over the CPU, down to the actual device. So... Alright. Originally, yeah, everything's easy, everything's one go; I can just do whatever I like. And with the MSIX, well, ideally, we have one vector per CPU, which means, alright, I can just align per CPU, and, yeah, I'll be good. I just have... I mean, I just set per CPU interrupt vectors, and then I'm pretty good with performance. Yes. Okay. This, you can do.

But... It turns out that that is not sufficient, because it still means that you just have... The program is just one big blob talking to the I/O. Well, no, it's not the I/O. We're having now per-core I/O. And in order to reduce the lock latency or reduce the lock contention, I really would need to adjust my programming. Okay. Our model to take into account, oh, incidentally, I'm now running on several CPUs. And I need to be aware that I'm running on several CPUs, otherwise the lock latency will go through the roof. So, I really have to redesign my program to be properly multi-threaded and to ensure that the program runs local to that individual CPU to take advantage of the multi-threaded capacity of that chip. Okay. And that is what we did for the I/O. That's the infamous block multi-cube ones, which did precisely that: we split everything up to be running local on the CPU, with that preserving the locality and reducing the thrashing and increased lock latency.

So. And then we can map it directly. Okay. With the interrupt itself, we have the entire I/O flow neatly aligned between memory, cache, CPU, and I/O, and everything looks very nice. Everything's good.

Well, yeah, nearly. Because if we now move to NUMA, we have the same problem, or we have a similar problem. Because as they move the PCI controller into the CPU, it now means that the PCI device is actually connected to a specific CPU. And you might have aligned your model nicely to be local on the CPU, which is nicely aligned for one NUMA socket. If you have two NUMA sockets, well, as it so happens, a single PCI device can only be connected to a single core. Unless you have a multi-headed PCI device, but, duh. Anyway. So, for that one, we are nicely aligned, but here, no, we are not. We are aligned from memory to CPU, but then have to go via the interlink and down to the MSI devices. Not very clever.

So yeah. So, we need to look at the NUMA again, at the NUMA node. And we need to, again, split the model to make all the allocations NUMA aware, to make everything NUMA aware, that we know. All right, we are now split. And that one side is aligned, but the other side is not aligned, which is good. Now we know, hey, we're not aligned. But really, if it's running here, it's running here. So, what else are we going to do? And what's more, there's no good way out. We could sort of restrict I/O to that size on the grounds that it will be more efficient. But if we do so, we just leave out half of the available CPUs, which will be a performance impact combo, whatever. So, not a good idea.

And what's more, what's really funny, you can even measure these impacts. This is with the RAM driver, which I modified to allocate. Okay. This is the NUMA node. And the, what's that, IOPS. So, the lower line is the default, if you don't do anything. And the upper line is if you actually allocate the NUMA node and restrict the measuring of the process on the NUMA node, the node they are allocated on. So, it has a measured impact. That's just memory-to-memory traffic. So, no I/O involved, just memory-to-memory. And we really measure things to figure out, yeah, we really want to take NUMA effects into account.

So, and if that weren't bad enough, now we have CXL.

As you might know, CXL is, well, actually in Gen Z done right. It all started off with a proposal slash idea from HPE—or from HP, at that time called "the machine"—which suggested that you would have a converged interconnect between CPU, memory, and IO, such that the interconnect is the same for everything. And that means that you can have distinct CPU, memory, and IO nodes, and you can put them together in any way you like. At that time, it wasn't really that feasible because the hardware simply didn't. It didn't do it, or was far too slow to do anything sensible with. Then, it got rebranded to Gen Z. And then, after certain iterations, it ended up being CXL, which in the end is just the PCI dialect. So, CXL runs on top of PCI, and those various things to the PCI was to be whatever CXL wants. So, they have defined—okay, there are three types of devices. Type one is cache coherent devices, which are typically things like GPU accelerators, crypto offload stuff. Then, there are cache coherent devices with memory, GPGPUs, and things, and non-coherent devices, which are memory expanders. And then we have different protocols on these: CXL.io, it's essentially the PCI protocol; then we have CXL.mem, it's a memory protocol—well, guess what?—to talk to memory expanders. And then there's CXL.cache, which is the cache protocol. You need to talk to type one or type two devices.

So, the big question, obviously, is what to do with these things. Currently, the canonical example—or, to be frank, the only example—is memory expanders. Because you can put normal DRAM memory on a CXL card, put that in your system, and voilà, you have more memory. Well, not voilà, after certain iterations and stuff, then eventually you will have some more memory, but yes, it works. There are designs and prototypes for actually doing switches, CXL switches, such that you can have cables from your machine to that switch, which then connects to something else, such that you have some sort of CXL fabric. And the idea with that one is that eventually you can arrive with something called device pooling. That is, as you have a switch, that device connected to the switch might be accessible from several computers, and not just one. But that is really a future topic. There are, to my knowledge, no devices available, not even prototypes doing that. So, at this time, it is just a normal PCI hierarchy, a normal PCI tree hierarchy, where you have a single device talking, or a single node talking to the connected devices. But even that is bad enough.

So. Because then. Well. Then they found if you have memory things, that memory clearly will have a different latency than your system memory. And they say, "Oh, not a problem. We just introduce another NUMA node and treat this as a CPU-less NUMA node. Because it will have a different latency to that memory, and as we are measuring memory latency anyway. Well, it doesn't really matter whether or not you have a CPU on that node. So just make it another NUMA node, that will be fine. What could go wrong?"

So, what the internal memory model in Linux does to handle NUMA memory allocation is a memory tiering. So, they have your internal table—that's basically an example of an HMAT table, again, giving you the NUMA distances. And there you see, all right, there you have a complex here, which is where you have your onboard, which is 10. It just has a distance of 10. The adjacent CPUs have a distance of 20. And if you go here, then you find, all right, now I'm on a different complex, because here, again, I'm having different latency that should be 20, like, anyway. Which means that you have your node here, NUMA node 0 here, and these are the CXL devices. They have a higher latency, clearly. So, going from CXL to NUMA to node 0, say, CXL device 2 to NUMA 0 will have a distance of 30. So, all nice, all symmetric, all good. And then you can have some, and then they implement something like demotion, which means, what does happen if I can't get memory, or what should I do with the memory which I've allocated? The idea is that you want to free up fast memory and move the fast memory to this slow memory if you don't need it. So, that means if you have allocated memory here, and find that it's unused for a longer period of time, you just move it to the associated CXL node to the slow memory. So, memory from node 0 will get demoted to node 2. Memory from node 1 will get demoted to node 3 to free up memory on the fast tier. Okay, which is a design one can do. I mean, this is essentially what caching does: moving memory back and forth. So, yeah, you can do it. But it sort of relies that you have a noticeable difference between CXL and main memory for that to even work properly.

So, and then I got actual hardware. Not this machine.

Which actually, where on to which I actually could put two CXL devices, two CXL memory devices. So, that was one of the Intel prototypes, the Grand Rapids server with two sockets and two CXL cards. And to figure out what this is about, I had to look at the HMET table.

And it looked like this. Is it? What? I mean, I have two NUMA nodes. Two physical sockets. All right. And I'm ending up here with something like 14 NUMA nodes. Oh, hell, that's a lot. Looking at so, okay, yeah, weird. Okay, fine. Then looking closer, I see, all right, good. So, I'm having apparently one complex up there. The noted A1. Where I'm having the same latency. Okay. So, I'm having the same latency. Then there's another complex here, A2, also having the same latency as A1. So, that's another complex. And then there's one here, which again has the same latency. Distinctly higher than that one, but still the same one. And then there's another one here. Okay. That is a weird thing. But oh, what do I know what Intel did? So, they would have their reasons why they did that. Okay, fine. But still, it's roughly symmetric. So, yeah. That's fine. But this is the latency. And then I had to look at the bandwidth.

And the bandwidth said this. I said, "What?" So, you have your A1 complex here, which is fine. I have a bandwidth of 256 gigabytes. Fine. You're having A2, 256 gigabytes. Okay. Fine. And then you're having this one here, 64 gigabytes. What on earth did you do? And same here. And what's more? All of these have the same latency. What on earth? So, I found the only reason, the only explanation I could come up with was that, well, apparently, here's a pretty hard boundary. And the connection between here and there is, well, slightly too small; they only got 32 gigabytes from one side to the other. Gods. What did you do? All right. But I said, "No. Let's see. Let's do some real measurements." Because these are just the pre-programmed tables, what basically the BIOS tells me. So, I'd say, "Right, do some measurements, some real-life measurements to see, all right, what is the actual latency we're getting?"

So, that's what I got. So, I found, ah, I only got six NUMA nodes. So, all the other weird NUMA nodes are some whatever, spare nodes or whatever, something. I'm not seeing them. That's fine. So, and this is basically what I expected. Okay. Good. So, again, I'm having a complex over up there between a zero, one, and two, which has, yeah, roughly a latency of 125, 130 nanoseconds. Okay. Yeah. Good. And I'm seeing the same over here. Three, four, five, also having 125 nanoseconds latency. Oh, yeah. That sounds good. And then I'm having of NUMA access, which have a latency of 530. Okay. Yes. It is a bit of a lot. But okay if they want to. Okay. Good. And same here. Three, and five. Okay. 530. Good. And then, looking at the CXL nodes here, six and seven are the CXL nodes. I am having a latency of 300 nanoseconds for the optimal case, meaning if it's directly connected to that NUMA socket, and 600 if it's not connected so the 600 is about expected because it's the highest latency I got in the system. That's fine, but 350 is about halfway between the optimal and the off-socket access that was not what we planned for. Okay, so I'm actually more efficient moving memory from socket zero to CXL than from socket zero to socket three. Curious.

Okay, and then I had a look at the bandwidth, and yeah, that is actually what I had expected. So, having 800 of 84, 85 megasecond, gigabytes, which is a far cry from the proposed 256 gigabytes which I should be getting as per HMAT table. Okay, and then compared to around 97, 79, 80 gigabytes per second. Okay, yeah, so less, but not noticeably less. So, okay, fine, and roughly symmetric between the two NUMA nodes. Okay, good. And I'm having far lower bandwidth for the NUMA nodes for the CXL notes, so that's 52 gigabytes for the optimal case and 26 for the non-optimal case. Yeah, that's about what I had expected.

But what on earth? I mean, yeah, CXL has a higher latency scores on the same socket; that was about expected, but it has a lower latency if you have a non-optimal access. That was unexpected and not really what we planned for. And the bandwidth is okay. It is slightly odd that the BIOS values, the HMAT values, are way off what I actually measured. So, I hope this is due to that being a prototype and that the real things will be better. But still, it will be awkward. But what again was it with memory tiering? And how can we do a reliable tiering here?

If we are using our classical model, we are just looking at the so-called NUMA distance, which essentially means latency. So, if we are going by latency, we would have a fallback that we are first trying to allocate on NUMA node 1, failing that on 2, then on 3, and then on CXL before going to the other NUMA nodes. I am not sure; is that a good idea? Is that really what we want? And similar for the other NUMA nodes. So, if you are starting off with NUMA node 4, we will be trying to allocate on that node and then going over to the CXL B device. So, really, really, really?

And because the real problem here is that we are only looking at the latency when doing all the NUMA distance calculations, we never look at the bandwidth. But, as it works perfectly if the bandwidth and the latency are in the same relation relative to the other nodes, then it is okay, because then it does not really matter. But if you have a different relation when looking at the bandwidth or looking at the latency, well, all your calculations are actually wrong. Because, what does it mean? Um. So. But especially in the CXL case, the bandwidth is consistently lower than the connection to the other nodes, whereas the latency is actually better in some cases. So we might be tempted to use the CXL node for memory allocation, which only works if you don't do lots of memory transfers. As soon as you start doing large transfers, your performance will suffer. It certainly will not be the best NUMA placement, but what would be the best NUMA placement in these situations? We don't really have a good way of measuring the bandwidth; we don't even take bandwidth into account. And what's more, even if we do, do we need to take the combination of latency and bandwidth? Do we need to have some additional measurement? Do we end up with some sort of two-dimensional fields? Depending where we are, we take this or that. So what shall we do here?

And is that one bad enough? We have more.

Because all the latency we're measuring is really just the memory-to-CPU latency, and we sort of assume that I/O connected to the CPU will always have the same latency, we don't even have a measurement for I/O latency distinct from memory latency. We just assume everything will be the same, which is okay, but again, only valid if you can make that assumption. As soon as you have to go across NUMA nodes, or even when speaking to CXL devices, that might no longer be true.

So, if you're having everything nicely aligned here, that's fine; but, in that case, you always have to go via the interlink. So, what do you do here? You could just route it through the interlink, but then this half will suffer and this will be optimal. And so, there again, as I mentioned, there's no really good, good way out. So, we don't know what to do there.

And if you now look at the—no, right—so what you can do, though, is to go to a fully symmetric solution where you have not one PCI device but two PCI devices. So every node has the same set of I/O attached. And then, you still, then you can continue doing parallel programming because then you can just ignore the off connections and just say, "Alright, we're not using them, so we're only doing I/O to the devices directly attached to that NUMA node." And that's what we implement; that works quite well if you have a parallel, such a parallel thing that you can do. That might not always be possible.

Because if you have CXL, then suddenly, that might be a bridge. And you are talking to an entire CXL fabric in the back, which you can account for memory because then you get another NUMA node. But this one, yeah, it is there and it has some latency, but we can't even measure this. So, how can we come to a proper placement? How would we know? Especially if, say, these two devices talk to the same hardware in the back. How can we come to any idea what would be optimal I/O, or even optimal programming or optimal placement for the thread? It doesn't really fit the, because we, so it doesn't really cover the NUMA model as such. So, there's currently no good way how we can program for these kind of setups.

There's one solution which I try to implement: just don't do it at all. And just look at the resulting latency. Then, basically, have an I/O schedule based on the resulting latency, and move I/Os back and forth depending on the actual latency. Which is—has the advantages that, well, you don't really care about the actual underlying hardware. But the disadvantage clearly is that you always have to look at that; you always have to calculate the statistics, which tend to be quite a bit of effort because, you know, it's statistics; you need a lot of data to come to a proper conclusion. And you introduce quite a lot of additional computing, which you would not need to do otherwise.

So I did some experimenting there, which was okay-ish, like so. Yeah, I'm seeing reasonably good results where I'm, for the, with the latency-based one. It's actually getting reasonably good results. Not that spectacular, but good. And I mean, random writes, for some reason, suffer, but then yeah, I mean, it's random writes. It's of course, as I'm tracking the latency, it will always lag behind because I have to calculate based on the previous values. So clearly, for random I/O, that will suffer. So yeah, hmm. For sequential, it's fine. But again, even the sequential improvements are not that spectacular. So, I don't really know if that's the best of ideas.

So, or we need to come up with another set of measurements to allow us to do a proper placement, especially now that we might have the problem that the latency is not the only solution, and that latency is not a reliable measurement for calculating the NUMA distance. So, really, we would need to look at how we can calculate a proper distance, how we can take the distance and the bandwidth into account to come to a proper NUMA placement. But this is something which, really, to my knowledge, no one even had looked at, because everyone else had just worked with either QEMU emulations or some prototypes where all measurements were off anyway. So, I do hope that we will get a discussion going at Plumbers later this week to see how we can drive this forward and come to some sort of solution. Right?

And with that, 'Tada! I'm done!' Thank you for your patience and your attention. I hope [something might be missing here] and I would be open to questions. Blasted everybody, very good!
	
So, if we treat the entire memory hierarchy as an internet, could we draw inspiration from all the internet academics?

Which sense?

In the sense that, in networking, we have both latency and bandwidth all over the place, all the time, so surely that would be a well-researched topic.

We might be able to look at things like CodAL, or something which tries to attempt a similar thing here, but, um, I'm not sure whether it's directly relevant because in networking, you always have the option just to not do it—i.e., just drop the packet if you, whatever, don't feel like it. And, um, that is not possible really when working with memory or I/O in this case, so it's not directly translated, but yes, it's a good point. So, we might want to look at that. Yes, clearly. And I mean, lots of research topics, you know.

So, does the kernel currently have any interface, like for user space programs, to say, 'I want my memory to be allocated on the same NUMA node'? Is that possible?

Not directly; you can restrict your program to run on specific cores, and with that, on specific NUMA nodes. As the memory allocator already tries to allocate memory locally, that then will imply memory being allocated on that NUMA node. But that is a very well-circumspect way, really. So, we don't really have a direct role in influencing that. Just by influencing the NUMA placement of your program, you can influence the NUMA placement of the resulting allocations.

All right, thank you very much for your attention, and if you have further questions, come to see me or in afterwards, whenever. Thank you.
