
Okay, so we're going to jump into the next presentation here. We're going to focus on the shared infrastructure,  specification under DCMHS. Same speakers, so if you're new, I'm Rob Nance with Jabil,  Paul Artman from AMD. We're going to co-present this topic.

So yeah, we previously viewed this slide. This is just the building blocks necessary  to build these modular servers. We're going to focus on the red box over here. So the shared infrastructure is a fairly new specification we're  adding to DCMHS to define how to put together multi-node systems.

And the timeline, as I discussed earlier,  for the MSIF specification, we're targeting .75 revision  by the end of this quarter. And we're looking to have the 1.0 version of the specification  by Global Summit in October of this year.

So just some words on kind of our philosophy  and how we're putting together the specifications to define how  to use MHS to build a multi-node system. We're not trying to define the components on the left-hand side. We're not trying to tell you how the chassis looks,  what the cooling and power looks like,  whether even you have a chassis manager. We're not focusing on that. We're focusing kind of on the interfaces. This slide is talking about the high-speed interfaces. We are assuming that because we're talking  about MHS HPM boards that we're going to keep  with the primary high-speed interfaces of PCIE and CXL. And so that will be the primary high-speed links  between the nodes and the shared elements,  which will primarily be I/O and network,  GPU cards, accelerators, et cetera. This one node here, we -- to kind of simplify things,  we don't allow node-to-node or element-to-element communication  directly without a switch. So for instance, if you want to talk between two nodes,  you would talk between -- you would talk from one node  to a shared element, which would be a switch,  and from that shared element back to the other node.

And on the management side, we --  if you look on the right-hand side of the slide here,  you can see we've got redundancy in every interface. And so we've doubled up the PESTI, the 2 wire I2C,  I3C, all of the management interfaces we're doubling  up for redundancy. All we're saying here is that all of the nodes and all  of the shared elements will have this type of pinout  with redundant management interfaces. We're not saying that it even has to go --  as we're showing here, we're not saying that it even has to go  to a chassis manager. You can design the system however you want, but at the node  and shared element level, this is what you can expect  on the interface side.

I've said this before, but kind of showing that --  kind of one of the applications. What we saw probably, you know, six or seven years ago,  M-DNO was there first with a multi-host,  so sharing a network card. They were sharing a network across four hosts. We've seen more desire to share management  and other things across multi-hosts. So this is an example of multiple nodes plugging  into a shared architecture, the ability to use existing DNO  into plug-ins that shared our infrastructure going forward. But we see a lot of interest in the industry. What we tend to see is a movement  from two-socket to one-socket, and then there's a goal  to use more of those resources, management,  network and share those resources. So this is kind of a way to interconnect  to get those shared resources going forward.

Some other examples, there for a long time,  there was two U4 nodes. Those were really dominant in the industry. We saw those in both cloud and HPC. We've seen four U, eight U chassis, but the main idea is  that you have a chassis that takes multiple nodes,  could be shared power, could be disaggregated, but you want  to have some elements that are sharing information. And this is really about that interconnect and sharing stuff  between the systems and how we define that interconnect.

One thing we're kind of avoiding at this point is tackling is,  how is this cooled, how is the power managed? Really, I mean, that's an implementation at this point,  is it an air-cooled system structure with fans embedded  in the node, embedded in the chassis, is it liquid cooling,  is this M-CRPS versus an ORv3 power supply? We're really trying to not get into those level of details  because we're not getting alignment  across multiple customers. Everybody's got a different approach, but we're trying  to at least on the interconnect  and the signals get a kind of approach to how we do this. And, you know, this is still .7 versus DCMHS,  which is well-established. So this is still early in the kind  of where we're going to this in terms of the final product.

Just to kind of show, you know, this is a good example  of where the Meta guys basically put together their own,  before the MSIF, obviously we haven't got the spec defined  yet, or to 1.0, but they essentially did their own shared  infrastructure design. And just a quick note  about how they chose to do it, and we're trying to keep  with this theme of we're not going  to define how you need to put these together. We're just defining the interfaces. So the light green board is a board that plugs into the HPM. They essentially put a small BMC on that board,  and then in the chassis there's a DCSCM. And so the DCSCM communicates with the local BMC  for every node, and they facilitate the management  that way, and other things about the system. But just a word that this is one example  of how you can put together a shared infrastructure system.

So on the power side, you know, I mentioned we're not trying  to define power. We are, we're focusing on a couple of key areas, though. This slide is really showing that the bright red arrows are,  are just indicating that for your power distribution board,  what all these elements are plugging into to provide power. We're trying to keep that board as simple as possible,  but we are saying that you have to have current limiting  circuitry on that board for every element,  whether it's a node or a shared element. And then obviously there's a few other things we'll have to,  you'll have to have I squared C fan out on that board, etc. But the key point of this slide is that just we're trying  to define as few things as we can about the power distribution,  but still have a safe system. So that's how we're focusing on this one.

This is just an example connector. We are working with some vendors to define connectors  that could be used in these multi-node systems. It's kind of hard to read, I know, for you guys. But on the left-hand side, we've kind of mapped out the pin  out of all the signals that we need  into this Amphenol connector as an example. But on the left-hand side of this, there's the primary  and the redundant management interfaces  that we were talking about. And then on the right-hand side, that's where all  of your high-speed I/Os is residing. So we have successfully fit this pin out into a few connectors  and we're working with those vendors. 

And another kind of philosophy we have is we're trying  to define for the key elements of the system,  this slide is talking about clocks, resets,  and that kind of thing. We're trying to define a default state. So for these, for clocks and resets, we're defining  that the nodes are defaulted as the drivers of those  and the shared elements are the receiving. But once you bring the system up, you can negotiate  to have this reversed. If you want to drive your clocks and resets from one  of your shared elements and not your nodes, you can negotiate  to do that, but we have to find a way to negotiate. So our default state is  that the node is driving all these signals. And I won't run through it, but we do offer, you know,  ways to bifurcate from x16 down to x8 and x4 for all  of these interfaces as well.

So what are the updates for, since the Global Summit 2023,  we really formed the MSIF workgroup, sub-workgroup,  came out of the 2022 OCP Global Summit. We've been working on the specifications since then. We got .5 release was at last year's OCP Global Summit 2023. I've already talked about kind of our philosophy here. We're just trying to define the interoperability there. And then the last bullet here, like I've said a couple  of times, we're, we have a goal for ourselves  of a 1.0 specification for the shared infrastructure  by Global Summit 2024. 

All right, and similar to before,  we have a call to action here. We've got all of these specifications, you know,  are posted out on the OCP website. You can just look under DCMHS and get most of this info. Feel free to reach out to any of us if you have any questions. Yeah, I think that's it. So yeah, any questions on that? All right. Sounds good.
