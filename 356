
So, welcome everyone. My name is Heiner Litz. This is Andrew Quinn. We are faculty at UC Santa Cruz, and we're going to talk about how to optimize CXL memory pools and CXL memory in large-scale cloud data center systems.

So, just a little bit of background about ourselves. Andrew and I, as well as three other faculty, lead the Center of Research in Systems and Storage at UC Santa Cruz. We do research on storage systems with about 15 PhD and MS students. We have some great sponsors; many of them are here today that support our work. They support the students, and we also closely collaborate on many different research topics. A big one is CXL. We have a number of students working on CXL; we're going to talk about it today, and then many other data center storage systems related research topics. And so, there's some high-impact research that came out of our center like the Ceph cluster file systems. We have many publications at the top-tier venues in systems and, well, produce a lot of quality, highly trained students. So, if you're interested in collaborating with us, please reach out after the talk.

Okay, so what's the problem that we're going to talk and tackle today? It is memory cost, right? So, if you look at this slide over here showing the cost per bit for DRAM, how it has evolved over time. And we all used to Moore's Law. So, we want to see that straight linear line giving us a 2X, say, cost improvement every couple of years. Then, we unfortunately see that here in the last decade that has almost plateaued off. So, we are down now to a 2X density increase in a decade, in 10 years, which we used to have in maybe three years or so. And as you can see, market plays a much bigger role on the DRAM price than the actual scaling that we get from Moore's Law.

So what do we do about it? We just heard in the previous talk that CXL memory pools can substantially address this issue by reducing cost by about 50%, or so, depending on your system configuration and your needs. So, introducing CXL into the memory hierarchy in our systems is a great way to address memory cost and get us back on that scaling trajectory. It's an open standard, so it allows us and many others to innovate. And we can also add additional capabilities like offloading computation close to CXL memory, which can give us even more and better TCO and performance improvements. So that's all, right? But there's a number of challenges to overcome. One is CXL is generally slower because it requires a different signal transmission scheme. The heterogeneity introduced by it, by now having to manage two different tiers or types of memory, potentially even more in the future, increases complexity. And it now really requires software and hardware to work together.

So I'm going to take the first part of this talk. We'll talk about how to utilize CXL to improve data center TCO, and then Andrew will talk about how to use CXL memory for cluster scope job scheduling.

Okay, so what's memory tiering? Just one slide. We have a CPU running multiple applications. We have a memory over here, and a hybrid memory pool that consists of a fast tier, say DRAM, and a slow tier, CXL. We could add even more tiers to this memory system, maybe remote CXL or something like that. And now you need to decide when you're running your applications how and where they should allocate their memory from. Right? So how much, what percentage of DRAM and what percentage of CXL memory should they use? And that depends on the application requirements, but also of the other applications being co-scheduled onto the same machine. And we can extend this problem to other resources as well, like last-level cache, right, which is also can be partitioned now on modern machines and assigned to applications. And that can be considered as just another memory tier, essentially. So the goal here is to reduce costs via memory tiering while maintaining similar performance.

That's not an easy task, right? So, on the left here, we have our different memory tiers. They give us a certain level of performance and some cost. And then we can run our application under different, say, CXL to DRAM memory ratios. And what we can see is that as we increase the amount of CXL memory, our execution time generally will, or excuse me, if we increase the amount of DRAM compared to CXL, our execution time will go down, and our unit cost will go up. Now, lower execution time is good. If we have high execution time, well, we can offset it by buying additional instances, right? So if you need certain requests per second, well, you can buy more systems with lower performance to make up to the faster one. So the question now is, where's the sweet spot here, right? Which configuration should we choose to give us that optimal performance per TCO? And so, while the unit cost is linear, the execution time is not because it heavily depends on workload characteristics. Some workloads work fine on CXL. Some others don't. Some need a specific portion of their data in DRAM, and the rest can be offloaded to CXL, often with negligible performance impact, right? But that's different for each application. So finding, for every application, the optimal layout and ratio is hard. The search space is large, and it becomes even larger as we add new tiers to our memory systems.

So, here's another motivational graph. What we did over here was we determined the total execution cost, so performance per TCO, under different CXL to DRAM memory ratios for a set of applications. The baseline here is at 1.0, meaning that is a DRAM-only baseline, right? And now, when you explore all different CXL to DRAM ratios, you get one that performs best. That's our max configuration, and you get some configurations that perform worse, you know? And so, as you can see, there's a huge gap here in performance per TCO, you know? So, picking the right setup of how much CXL versus DRAM you should pick has a huge impact on your TCO.

All right. So that was the user perspective, right? So you want to find the optimal CXL to DRAM memory ratio to give you the best performance for the lowest TCO. What about the data center cloud service provider's perspective? They are interested in packing density. They want to run the most or the largest number of tenants, workloads, applications on the fewest number of servers because that optimizes their revenue. So let's assume you have two application configurations, right? With a, like, about roughly the same amount of CXL and DRAM. And here we have a configuration which uses a lot more CXL than DRAM memory. And we have these two available servers in our cloud.

Well, then the data center provider would be interested to fit the second configuration because it nicely goes into one of the existing servers. They don't have to spin up a third one. So, what's interesting here is that this scenario dynamically adapts CXL pricing. So, CXL price is not only determined by the physical cost of what you buy in terms of capex for your CXL or DRAM system. It's also determined by the dynamic current availability of any of the resources, right? So, with the spot pricing, cloud model, CXL memory could, for instance, go even lower than what its physical cost is, because at the current point in time, we have more CXL available than DRAM, no? So, that introduces another complexity that CXL DRAM pricing is no longer fixed. It depends on the dynamic availability of those resources.

All right. So where does our work come in and how do we contribute? We are not the first working on that memory tiering problem. There have been a number of prior works, which generally depend on black box machine learning models. They feed in a set of workloads, try to predict their performance under different CXL DRAM ratios, and then advise you to choose that particular configuration. That's fine, but it has a number of difficulties or challenges. First of all, because it's black box, it doesn't really tell you why a configuration is better than another, and it's really hard to debug and being sure that the proposed configuration is actually good for you. In our case, TMC, we will devise a white box model which explains to you why a particular configuration is better than another. Even more important, I think, is that it also provides data layout hints. It not only tells you the best DRAM to CXL memory ratio, it also tells you what data structures from your applications should be allocated on DRAM and which others can be migrated to CXL, minimizing the performance overhead or impact. It provides you the ability to predict the performance. It doesn't only tell you, 'This is the configuration you should choose,' but instead it tells you, 'If you choose this configuration, then you're likely to see that particular performance,' and that allows you to run what-if analyses, right? So you can, for instance, say, Okay, let's change the CXL pricing now based on some dynamic parameters. What would the optimal configuration be in that case?

Okay, how do we do it? This is our system, so it's based on profiling. We run a particular workload for which you want to determine allocation hints and optimal DRAM to CXL memory ratio. We measure, and that is special about our system, a lot of performance monitoring counter information based on the data structure level. So we use a custom memory allocator. To essentially intercept every `malloc-free` call, if you want, so that we know what memory ranges are utilized by which data structures. And then we can correlate the performance counter measurements, which refer to hotness, but also memory latency, and everything that we can measure using modern performance counters. And we correlate them to individual data structures, so for instance, this application may have a hash map, a linked list, etcetera, and we can, for every sizable data structure used by an application, determine these performance characteristics. And then we put that all into our model, and we get both the memory allocation, the ratio, the optimal ratio of DRAM, CXL, as well as the placement hints. Which type of data structure is used, which type of data structure should we place into which memory area?

A little bit more information about how our system works. So, we need to perform three profiles of the same application under different configurations. So, basically, you have a spectrum of infinite design space of how much DRAM and CXL memory we want to use. We need three profile configurations: one is all DRAM, another one is all CXL, then we have a third one that is a mix of the two. So, three profiling runs are sufficient to inform our model and extrapolate it so that we can predict the performance for all other DRAM to CXL memory ratios. All the information we gather can be collected with like Intel Precise Event Based Sampling counters. And we feed all of that into our model. And from that, we can determine the ratio and the allocation or placement hints.

So for instance, one example application here that has three data structures where we measure the memory access rate, size, and a lot more other performance counters, which I'm not showing here on that slide. We will then first of all predict performance given different CXL to DRAM memory ratios. And we also will advise which of the data structures are the most critical performance-wise that should be placed into DRAM and which one can go into CXL memory. And this is two different configurations that is suggested by our system. And it shows in this configuration, these two data structures should be allocated in CXL, the others in DRAM, and vice versa. We can inject the information that is produced by the model back into our allocator, and so then the allocator can choose by knowing which data structure should go into which one of the two segments or tiers. It can automatically allocate the memory in there.

Okay. Some quick evaluation numbers. So, over here, we compare the prediction performed by TMC in terms of runtime performance for a given workload against the ground truth, which was obtained by exhaustive search. So, for the ground truth model, we evaluated each and every DRAM to CXL configuration with a granularity of a single percent. So, we swept basically over 100 different CXL to DRAM configurations, measured, ran the workload under this configuration, measured the actual execution time. That's the red curve, and uh, the blue curve is the performance prediction based on these three profiling runs determined by TMC. The closer these two curves overlap, the better because it means that the prediction accuracy is high. And so I picked four workloads here which have rather different performance behavior. Right? So, some are not affected basically at all of whether they run on CXL or DRAM. Some completely disagree with that. They want to run on DRAM. And then we have some which have these hotspots where some data needs to go to DRAM, and the rest can be offloaded to CXL. And independent of the memory characteristics of these four applications, we show rather good accuracy by predicting well how the workload will perform.

All right, so that was performance prediction accuracy. We also looked at the operator, the cloud operator side, and at resource efficiency. So here, what we do is we want to optimize packing density. And what we show over here is that our approach, TMC, improves packing density by about 17% over a naive baseline, which we'll just put whatever on the machine greedily that fits. And we also show that the effect on the user is minimal. So we get a 25% or 17% in average, 25% over here, improvement in packing density, and the execution cost only slightly increases for the user. So that may be acceptable for the user to get this small performance hit because the cloud provider can now basically offer the same resources 70% cheaper.

So again, very good utilization here. And then we also looked at search cost. How long does it take when we do exhaustive search, trying out all different CXL DRAM combinations? And again, TMC substantially reduces that search space because it only needs to profile three configurations, and then it has all that it needs.

Okay, so to summarize, TMC provides a performance model that can accurately model how a particular CXL to DRAM ratio will affect application performance. It not only gives you that ratio, but also allocation hints, which can be integrated in an allocator scheme to really make it easy to use. And we show really good performance in terms of accuracy, search cost, and the increase in resource efficiency. And with that, I would suggest we push questions to the end and I'll hand over to Andrew.

So we're gonna hone in on maybe a slightly different, similar, right, we're still in the data center, but slightly different types of workloads. So the next sort of part of this talk is going to focus on job scheduling in particular. So job scheduling is a key facet of many of our computer systems, right? So we use job scheduling in cluster management, in data analytics frameworks, and then in a number of machine learning frameworks as well. And efficient scheduling is really crucial for our large data centers. Because at scale, right, even seemingly small improvements can lead to gigantic cost savings, right? One percent is huge when you're operating at the scale of some of these systems.

So let's go through an example of what job scheduling looks like today. Here we have a really simplified data center. We're imagining I have these two machines, and I'm only thinking about two types of resources, just CPUs and memory. And what happens is every job that shows up has some requirements for those resources that it needs to use. So for example, in job one in this case, it needs, and I've sort of normalized these numbers to some mythical world, right, where I need 0.2 CPUs and 0.4 memory. So that's referring to 20% of a machine's CPUs, and then 0.4, so 40% of some machine's memory. So a job scheduler has to assign that to one of the machines in my cluster, according to those resource requirements. And so then when another job arrives, my scheduler also assigns that job, right, according to their resource requirements and the amount of resources that I have available. And then a third job arrives, and I assign that to yet another machine, right? So you get the point. At some point, though, there's going to be some job, or there's likely to be some job that arrives, and you'll note if you kept track of all of the sum so far that J4, unfortunately, doesn't fit anywhere. Because although actually there is 0.9 available memory in this cluster cumulatively, it doesn't exist on any one machine. So we have this issue, right? Yes, we have enough memory, I've bought all the memory I needed to support these four jobs, but I haven't quite managed to pack things in correctly such that I can actually leverage that memory. And in this particular case, there's no way for me to repack them. This isn't an algorithmic problem. This is just a fundamental issue in the space. And the issue is fragmented memory. So that's the name we're going to call this particular problem. It turns out that people have done studies and they found that a lot of jobs face scheduling delays for one reason or another in these clusters. And it's as much as 50% experience some scheduling delay. They're not always super long, but there's usually some period of time where a job arrives, and it'd be great if I could run it right now, but I can't.

Okay, so the question is, is CXL just the answer to this problem? And I'm going to try to articulate in this talk that it might be. So here's what I did. So I imagine the same cluster, and I actually allocate the same amount of memory, but this time rather than putting half my memory in machine one and half my memory in machine two, I put a quarter of my memory in those two machines, and then I put half my memory in this mythical CXL pool that's shared across both of the different servers. So everybody gets to access the CXL pool, and then both of my machines get to access their local memory. So I'm imagining the same trace of jobs here. And J1, okay, I assigned it actually, if you look back, exactly the same. And J2, this time I have to assign to machine two. And then where the kind of magic happens is J3 can split its memory allocation across its local memory as well as its remote memory, right? So that's what's happening here. And then J4 can actually run because I left myself with enough memory to fit J4 in space, right? So J4 no longer has any scheduling delay. Voila, fine.

Okay, so what we're imagining then is this type of cluster, right? Where I have two memory pools that are connected to some collection of servers, and I broadly refer to each of these kind of sections as a pod. And I have this scheduler that assigns, as jobs come in, machine jobs to one of the servers that are attached to some pool, and decides what their allocation is going to be. So the advantages are, as we saw on that previous example, that I can have less scheduling delay, and because I have less scheduling delay, perhaps I have lower cost. I might be able to run my system, for example, with slightly less memory. I don't have to buy as much memory. The disadvantage, right, is that jobs run more slowly, right? J4 in the prior example was running entirely in CXL, and there's a good chance that running entirely in CXL is going to make J4 run slower. So the question is, can we kind of weigh these advantages and disadvantages and actually come away with an end-to-end win when we try to run this? Now, there are also a number of research questions that we have to answer in order to make this thing a reality. The first is how should I actually configure this data center? How big should each of these pods be? In other words, how many servers should I have in each of these pods such that I still receive a benefit? And then another question is maybe how should I deploy my memory? In the example, I put half my memory in pools. Is that the right split? Should I put less memory than that in pools? What does that look like? And then the other question is what do I do about this scheduler thing? I just imagined this mythical scheduler that was going to solve all my problems, but how do I actually build that? So to answer these two questions or to address these two questions, we first built a cluster simulator that's going to let you search the space of possible configurations, and we'll go through that in today's talk. And then we developed a couple of schedulers where essentially we took existing state-of-the-art cluster schedulers and we looked at how we would add farm memory or CXL memory awareness to those schedulers, and so we incorporated that.

In the end, we wound up getting massive wins over state-of-the-art in certain scenarios. So, in fact, in one of the configurations that we thought was reasonable with one of our schedulers, we saw up to a 30x improvement over what people might have today.

Okay, so what is our gigantic cluster scheduler? That's what I'll start with today. So your scheduler takes as input a workload trace. So you can imagine that as either a real-world or maybe a synthetic trace that represents the types of jobs that you're getting in your uh job scheduler. Each other's job says things like their start time, their end time, and then their resource requirements. You provide this simulator with a configuration, and that's essentially what you're going to search over. So you're going to kind of specify some configuration, then search a giant space of possible configurations. That includes things like how many servers do I have in my, you know, in the cluster that I'm considering, what are my server shapes, so how many CPUs, how much memory do I have in them, how many servers I'm attaching to each of those pools, and then how much memory am I putting in a pool versus in a server. And then finally, you can specify different scheduling policies. And then the last thing we have is what we call slowdown models. And these are representations of how much a given job is going to slow down because some of its memory is running in CXL instead of locally. You shove that all into the simulator, and then it spits out basically a trace, the same workload trace, but this time it's telling you when it actually started and finished rather than how long each job took. So it's giving you like metrics you can then look at how much scheduling delay was there, how long did every job take, et cetera. So I'll dive through each of these things. 

For workload traces, what we've decided to use is as many kinds of real-world traces that we can get our hands on. So, in particular, we're using traces from Microsoft Azure, published some traces in the late teens. So we're using those. And then Google Borg published some traces of their data centers in circa 2000. So, these are the kind of four different traces that we're looking at. We could look at more, and there's no reason you can't use your own if you've got them.

Okay, let's talk about how we actually sort of search this simulator space. So I think I kind of walked through, roughly speaking, what all the simulator parameters were. But to give some context, so we take your trace, and we look at what is the utilization over time? So on the right, all of our rights, you're seeing a graph that shows how the memory and CPUs are utilized within a particular trace over time. So the blue line here is memory, and the red line that you can barely make out is cores. We take the 100th percentile of requested cores. So we imagine a scenario in which you deployed your, your cluster with all of the cores you would ever need. So you don't want to have a CPU delay. But maybe you want to be savvy and try to save some money on your memory. We know memories are really high cost in these scenarios. So we're looking at things like, what if I allocated my memory at only the 50th percentile within this graph? What if I did it at the 75th percentile, 85th, and 95th? So those are the sort of four data points that we've considered when we've been searching the space. They're all memory constrained clusters. We're using as server shapes, we're using kind of large, something that mimics large cloud instances. So those are like two node 192 cores. We search the space of number of servers per pool from two to 32 by powers of two? And then we search pools from zero to 100% by percentages of 10. So zero, 10, 20, blah, blah, blah. And we included state of the art scheduling policies, as well as the other two things that we developed, which I'll talk about later.

Okay, the other kind of main thing we had to figure out how to do was how to deal with the slowdown that you perceive or that you experience from CXL. And the problem is, as we probably all know, it's not super easy to get your hands on these cards at this point. And it's not clear whether or not the ones you can get today are going to be anything like the ones that you could get performance-wise in, say, two, five years, whatever it might be. So we decided, can we come up with a methodology that allows us to pick some point in the sand today of how much we expect the slowdown to be, but then also vary that slowdown slightly so that we could say, well, what if they're actually faster than we think they're going to be? What if they're slower than we think they're going to be? How would this affect our end-to-end results? So, here's what we did. So we grabbed a dual socket NUMA server. We put all of the compute on one of the nodes. And we put varying amounts of memory on the other node. So, if you have 100% of the memory on the other node, that's kind of mimicking the idea of I have all of my memory on a CXL node. If I had 100% locally, then that's mimicking no CXL. Hopefully, you get the picture. We then run this with 65 different workloads. They're coming from open source, mostly analytics processing workloads. There's a little bit of transaction processing in there as well. So this graph on the right is representing for all of the local memory, each of the dots in the graph is how much some particular application slowed down when it was run at that particular local memory. So slowdown on the y-axis, so higher is actually worse. And then what we found was there were these kind of like four different clusters of how applications seem to behave following the blue line, the red line, the purple line, or the green line. And so you see there's some applications that have, like the blue line, for example, a lot of slowdown at when I have local memory, but very quickly I kind of level off. And so I have this working set where I get a lot of speed up by allocating just a little bit of memory locally. And then there's other jobs, right? The green jobs seem almost unaffected by local memory allocations. Okay, so we took all of these as our slowdown models, and we wanted to incorporate some way of saying, well, what if it's worse than we think? What if CXL is a lot worse than a dual NUMA node machine? What if it's somehow better, right? So we wanted to incorporate that, and we came up with this idea we call the NUMA factor, which essentially multiplies each of these NUMA nodes by some, sorry, this NUMA model by some constant factor. So if I said a scale factor of two, that means I'm imagining the world in which the CXL pool is twice as slow as what the NUMA simulator told us, right? So we'll vary that scaling factor in the results later. 

Okay, so that was the simulator. That's all I'm going to say about that. 

Let's go into some scheduling policies. So, we took existing state-of-the-art scheduling policies. So, there's some generic ones that you've probably heard about before. I teach them in undergrad classes, right? FIFO, shortest job first. And then there's been some state-of-the-art policies that have been proposed specifically for far memory. Most of them haven't been proposed specifically for job scheduling. They've been maybe proposed if I'm managing virtual machines, so they're maybe not the perfect fit, but they're the best thing we've got. So, in particular, those two things are what's called cluster far memory or CFM, and this work out of Microsoft called POND. Finally, we took these kind of alignment-based policies and we added far memory awareness to them. So, there's two alignment-based policies that we see used today. One is what's called, that's a typo, it should be EPVM, Extended Parallel Virtual Machines. And it essentially grabs jobs, taking them in order of FIFO, and then it uses it's called alignment in order to decide where to place that job. So, what alignment does is essentially calculates a dot product for each machine against what the resources of a job are, and that gives it a score of how well the job and the resources on the machine line up. If you do that across all your machines, you can pick the thing that has the best alignment with the resources you need. And the idea then is you're kind of fitting like-sized, like-shaped boxes into your problem space. So we did that for EPVM, we also did that for Tetris, and Tetris does the same alignment thing. The key difference is instead of using a FIFO-like policy, they're using a shortest job first-like policy. So Tetris tends to perform a little bit better, but maybe you get things like starvation, and so maybe you're not as happy to use that.

Okay, so to evaluate both the scheduler as well as the simulator, we answered three basic questions: How many servers should I attach to each of my pools? How should I split my memory between my servers and my pools? And then, how does my job performance vary when I try these different scheduling policies?

So I'm going to show you these, like, myriad of graphs over the next few slides, and each time I'll show you one example, and then I'll show you the kind of all the data again. So here's the first example where we're analyzing servers per pool. So I have the number of machines per pool, or servers per pool, varying on the x-axis, and then I have average completion time when I run a trace on the y-axis. I have plotted, in this particular case, I have three of our different workload traces. RC19 and RC17 are abbreviations for the Azure workloads. They're called resource central. That's RC. And then Google B is Google's B cluster from that data set. And we plot here, and we tend to show the knee point to try to give you a sense of where's the point at which I start getting diminishing returns as I add more servers per pool. In all cases, if you added infinite servers per pool, you would see better and better performance. There's always less fragmentation when I add more servers per pool. But there's other costs that come up, right? And we're not necessarily modeling those costs, so we're incentivized to pick the leftmost point in these graphs that still gives us a big benefit. So that's why we looked at the knee point. Okay, so this was one particular case. This was when I had my scale factor, remember that slowdown, of 0.5, which actually means I'm saying CXL is faster than NUMA. And this is when I imagined a memory cluster that is allocated for 50% of its memory requirements. So it's pretty resource constrained.

I then plot this kind of big grid where I vary the memory cluster amount from left to right. So from 0.5, 0.75, etc., I'll go up to 0.95. And I vary the scaling factor from 0.5 to 1 to 2, right? So that's faster, the same speed, and then slightly slower. And in general, our takeaway—I won't spend a ton of time on this slide, but our takeaway is that basically, if you put eight servers per pool, you achieve most of the advantages that you can get out of this system. And eight machines per pool seems like a pretty nice number. Most of the architects are telling us that that's a reasonable number that you can expect to attach to each of your pools. So we're pretty satisfied with that result.

Now, in a very similar fashion, we're going to kind of go through that same type of grid, but this time looking at a pool versus server memory splits. So this time, the x-axis on each of these graphs is the local memory percentage. So zero meaning all my memory is in CXL, 100 meaning all my memory is in local memory. And this time, we only used the RC traces, not for any particular reason; it just so happened that those were a little bit shorter. And we were in a time crunch when writing this particular graph.

Okay, so here we go. These, almost all of these, have this kind of U shape where we see a decrease in the average completion time into some number. Usually, it's around 80 or 90. And then we see a spike when we only have local memory. So, that spike at the very end is because there's fragmented memory. And the decrease is because most jobs run faster as I give them more local memory. So our conclusion here is, you probably want most of your memory to remain in local memory. It's probably only 80, probably as much as 80 to 90% for these workloads. So it's kind of rare that you should be shoving all of your memory into far memory. It's only those rare jobs that come in and would maybe experience a lot of scheduling delay that would actually want to take advantage of it. So I thought that seemed kind of interesting.

Another question that we wanted to answer and talk about was these scheduling policies. And so, in this particular case, we're showing CDFs. So these are a little bit different. So we have the completion time on the x-axis, and then it's a CDF, how many jobs finished before that particular completion time. And we're plotting a bunch of different scheduling policies. So we have CFM, that's existing state-of-the-art cluster for our memory. In the blue, we have a greedy algorithm that's just trying to use as much local memory as possible. That's kind of like a naive baseline. We have NOFAR, which is FIFO. And then we have POND, which is this Microsoft proposal for how they're going to schedule VMs. I want to call out, right, so I told you earlier that it was not designed for this particular workload. That very much shows up in these graphs. POND performs very poorly, and I don't think it's because POND's doing something wrong. I think it's because it's not designed for memory-constrained job scheduling in the way that we're using it. So it's an interesting baseline, but don't take too many conclusions about it. And then there's, again, a typo. Oh, no, it's correct this time. EPVM. FAR, that's one of our allocators. And then there's TFAR, that's another one of our allocators. So those are the alignment-based allocators. In this graph, the further to the left you are, the better. So you want to, if you're further to the left, that means that you're completing more things with a lower completion time. In almost all cases, we'll see there's some tail at which jobs do experience scheduling delay for everybody, and therefore we can't actually beat them. But what we're really looking for, is like, can we get most of the CDF done faster? So further to the left. So like TFAR, for example, here is a huge win. One other thing, all of the completion times are log scale, which you may not have noticed.

Okay, again, same grid where I'm showing the scale factors increasing and the memory as well. Oh, I'm sorry. I ran all of these at memory 85%, but what each of the columns is representing is a different trace. So these are my four, and I'm adjusting the scale factor 0.5, 1.0, and 2.0. So again, right, there's a lot of data here. The key high-level takeaway is that our TFAR is performing the best. So it performs better than our other FAR memory scheduling policy. And it outperforms noFAR, which was this baseline basically FIFO, by a massive amount, up to 33 times in certain cases. But it also outperforms cluster FAR memory by a massive amount in certain cases, up to 30x. It actually turns out when you look at the data, there's a lot of instances where not using FAR memory in these types of configurations actually beats the existing state of the art. So cluster FAR memory is actually performing worse than not using FAR memory in the first place, which seems kind of counterintuitive, but I think the problem is they're not doing a really great job at these memory-constrained clusters.

Okay, so let's take away a couple of conclusions. So our basic contributions, right, we built a simulator to explore these different configuration spaces: how much memory should I put in my pools, how many servers should I attach to each pool? And we found that, in general, that small pools work pretty well. We also proposed two novel scheduling algorithms that took existing some policies that people had proposed, Tetris and EPVM, and added far memory awareness to them. And we found that we can get massive improvements over the state of the art when we do that.

Okay, so at a high level, right, taking one step back for this talk, across both Heiner and my work, what are the contributions that we presented today? CXL is a promising technique to address memory cost. It's probably not a plug-in replacement, and there are going to be some deployment challenges that we have to address. We showed how you can model CXL performance and total cost of ownership, and get advantages out of CXL there. And we showed how automation can address the complexity challenges of CXL as well. Please reach out to us if you want to collaborate. So, there's our website, CRSS.US. I think Heiner and I will both be around most of the conference, so come bug us. Thanks. Yeah, go ahead.

Yeah. Okay. Yeah, so to repeat the question, so the question was about whether or not we considered processing, basically processing in memory or processing in a pool, in addition to other things that we modeled. So, not in this work. No, I think processing in memory is a very interesting problem, and there's certainly people at CRSS in general that have been looking into this space, but not anything we presented today. Yeah. Go ahead.

OK, so the question was: what were our assumptions with respect to CXL memory latency and bandwidth for the last work? OK, for the last work, so that was where we were trying to use the scaling factor to try not to have to make too many assumptions. So, we... Essentially, when that scaling factor is 1.0, we're assuming it behaves like a two-socket NUMA node. So, we're... node was like a Sapphire Rapids NUMA node, which is what people have kind of been using, I think, so that was what like Microsoft used as their simulator as well.

That's okay, so the question was: did we vary separately latency versus bandwidth? No, we tried not to get into the weeds; we just took like, how much is my application going to be slowed down by some percentage factor—were better or worse than what a NUMA would be. So, I agree, a more, I'm leading the question here, but I'm imagining what you're saying is: wouldn't it be better if you *could* vary those things independently? And I agree, I think the answer is, yeah.

It would be a clue for us providers to understand the impact of bandwidth versus frequency, which was more important, because in a lot of cases it's a trade-off, right?

Yeah, yeah.

It is a very interesting, by the way, I congratulate you on a lot of excellent results that provide us a lot of information, but the question that remains is how do you weed out what benefits certain applications or certain schedulers or certain, you know, there are so many variables? I know it's hard to grasp it all.

No absolutely, go ahead.

Can I tag on that? So, in the first part of the talk where we're using a performance monitoring unit approach, measuring bandwidth is relatively easy, right? You have PMU counters essentially for that. Latency sensitivity of applications is much harder, right? So, you could do basically two things. One is you build that performance model, you just try out different ratios and that gives you an idea of how it performs. Another thing that we did was, and which is a crucial parameter for latency sensitivity, is memory level parallelism exposed by the application. If you have a lot of independent memory accesses, which applies to certain data structures and algorithms, then you can hide the latency. You just emit additional memory requests and you're essentially bandwidth limited. If this is not the case, you're doing a pointer chase, for instance, where every memory access is sequential, then you are latency limited. And one good proxy for figuring out by how much an application is bandwidth respectively latency limited is to measure MLP, memory level parallelism. And we have some heuristics to do that by looking at how many outstanding memory accesses there are when you observe an LLC miss and things like that.

Yeah, so the question was whether or not we had considered rebalancing strategies. I guess the idea is, as I'm scheduling jobs, sometimes I find—so the problem was I find myself in this hole where I have nowhere to put my job. And the idea, I guess, was you could rebalance your jobs and then it would all work out? Yeah, potentially, right? So in this particular case, in the one that I showed, actually, there's no balancing of those jobs that fits. So it's like the fragmentation is sort of fundamental because I can't split my memory allocations. So there's no way for that case. Of course, that's not always true. Rebalancing is, of course, a technique that people have used. And the CFM paper, so that particular allocator uses rebalancing as how it approaches the problem. So we have modeled that problem, but not at the exam. It doesn't always work. And that's part of why CFM maybe doesn't work as well. So we are unfortunately out of time. But come bug us if you have more questions. Thanks.
