
Hi, everyone. My name is Arvind Jagannath and I'm a senior product line manager at VMware. I work on a lot of the infrastructure-related projects at VMware. And with me I have Sudhir Balasubramanian. Good morning. I'm sure you guys can hear me loud and clear. Senior staff solution architect. I specialize in Oracle technologies on VMware. So today we'll be presenting on capacity and TCO benefits. So one thing is, you know, we see all this buzz around CXL. So VMware is taking a unique step towards CXL in the sense that we are focusing mainly on the TCO value prop for customers. And this is not an artificial TCO. It's more about how you can actually substitute for DRAM. I mean, as you all probably know, VMware has been working on a lot of projects, similar projects in the past. And now we are taking this leap with CXL and we'll be doing a lot more with CXL in terms of cost savings by substituting DRAM.

So this is the agenda. So I'll be covering memory tiering and some of the challenges that our customers today face and covering some of the CXL use cases and deployment options. So Sudhir will be going through a real-world use case, which is for Oracle, and then we'll be presenting some key takeaways.

So really this is our message. So our big memory and disaggregation vision starts with CXL and memory tiering. So VMware, as you all probably know, it's a hypervisor. So it can abstract all the workloads. You can bring workloads from three, four years ago and run on VMware's infrastructure.

So what is the value of VMware's memory tiering over traditional approaches? So if you take a bare metal or even our competitive hypervisor solutions, so we are actually built into the kernel. So we have built the tiering technology into the kernel. So we provide a transparent single memory address space, which means that when you have a sum of different tiers, we can aggregate them together and present a single memory address space, which is when you look at the host operating system, you will see a combination of large memory, which includes all the tiers. So now the tiers, as shown in the diagram, it can be a CXL attached tier. It can be a CXL or over Ethernet. It can be NVMe. And one of the things we are working on right now is an accelerator-based solution, too, which we will announce soon. So the accelerator is also pretty unique in the sense that we are actually, for the first time, putting VMware's IP within the accelerator. And so what are some of the core value prop for VMware? So as you all probably know, VMware is the only operating system that actually follows the concept of virtualization. Plus, it has all the inbuilt features for clustering. And DRS, which is the distributed resource scheduler, is part of vSphere, which means that customers get this. And this is one of the core values why VMware customers purchase vSphere, is because we offer DRS and vMotion by default. And this can be used to mitigate risks. We also ensure fairness. So one of our philosophies is we want to make sure that there isn't a rogue VM that actually occupies the entire time and occupies DRAM. And we want to make sure that there's fairness. We also ensure minimum configuration changes and minimum performance degradation. So when you have slower tiers, the challenge is how do you actually quickly, proactively make sure that all the hot pages are moved to the fastest tier so that performance impact is minimal?

So with our customers, what we are seeing is really wastage of DRAM. So if you look at the dark green bars, they are really DRAM that are being overprovisioned, that are being wasted. But on the right, if you have a tier, that overprovisioned memory can come from a very low-cost, inexpensive tier. And the other key point here is, for a lot of our customers, active memory usage is really around 50%, which means that at any time, as long as you make sure that you move the active pages into the fastest tier, customers see almost negligible performance decline. So really our main goal is to make sure that we actively replace hot and cold pages between the tiers. And now, like I mentioned, we also have the accelerator, which can play a part in doing that page replacements.

So these are some of the key use cases that we see which are emerging in the industry. So VMware is not necessarily working on all of them at this point, but some of them for sure. And we have our own solution, too. So it's mainly-- our solution is built around lowering the TCO. So instead of focusing on increased capacity or scale, we are focusing directly on how can we substitute for DRAM for the customers, and how can we reduce the costs. And then we actually-- I have a slide which shows how this can lead to a future of disaggregation and composability, which is a bit different from the traditional way of thinking. But it will still give us a path and for our customers to move to disaggregation. So some of the concerns that I mentioned here on the last column here with respect to blast radius and security and memory orchestration, those are aimed to be built into this solution, which means that when we actually come out with this disaggregation solution, we will ensure that we handle all these challenges from the outset.

So in terms of our vision, on the left most, you see that we are working on CXL memory expansion and memory tiering. And then we'll move on to a type 2-based memory accelerator. So this is actually a device that can actually actively tier pages, and it will use type 2 features for CXL. And like you see here, there is a unique thing here, which is with the network link, which is unique to VMware, where we actually hope to do some of VMware's core value prop of vMotion and making sure that pages can be transferred over this link. And we can make sure that we can speed up applications like vMotion. On the right, on the right most, you also see a concept or a vision where we could potentially use an Ethernet to actually aggregate such devices. So what will happen with such a model is that we'll be able to have hosts sharing such devices, and then those devices being accessible and multiple devices being accessed by the same host. So we'll have like end-to-end or one-to-end kind of combinations.

So really, this slide is just to show that what are our core value props with CXL. So aiming to provide TCO improvements, we want to increase the host CPU core utilization, because now if you think about the device, it can actually offload processing into its cores. So we are also looking into improving OPEX. So if you look at the disaggregated architecture that I just showed, it's possible to actually reduce some of the OPEX costs for our customers. I mean, when you talk about vMotion or memory disaggregation. And then finally, you can look into future use cases on the right that is also possible if you provide a programmable model. So now I'll pass this on to Sudhir, who will cover some of the Oracle-based use cases.

Thank you. So essentially, if you look at Arvind's section, he basically spoke about the platform. He spoke about the framework. And he spoke about how we could use tier one DRAM, tier two memory. You kind of throw a blanket of abstraction over those disparate memory components. And that's what happens is at the end of the day, there's a reason as to why we are all assembled in the room and looking at this hypervisor proposition. It is to run workloads at the end of the day. Your workloads could be the Oracles, the SQLs, the Microsoft, the MongoDB, so on and so forth. So I come in from an application world. I come in from a completely different world than what you guys live in. I come in from an app world, where we start looking at the platform. We look at the architecture. And we see with this framework, where we have this memory tiering with CXL use cases, can real-world application workloads make use of it? So I'm going to just take Oracle as an example here. You guys can go back home and substitute SQL Server or MongoDB or whatever, and the use case would be almost exactly the same.

So just to recap a bit, up here, the advantages with the platform having the most awareness is VMware, vSphere is a platform that has the most awareness of the hot and the cold block. Remember when we had to go back to the days where even with NVMe tiering or even with, let's say, disk tiering, we used to say, well, you know what? These hot pages need to be on the hot side of the disk. The cold has to be on the cold side of the disk. Maybe you have tier one, tier two, tier three. How about if we just remove all of that and let the hypervisor of the framework do that work for you? That in a nutshell is the VMware proposition of a software memory tiering architecture or framework.

So we said, OK, is it all just theoretical? Or can we just put that to a use case here? And we just test it out. So what we did was we took two servers. And these two servers are set up exactly the same, ESXi version 8.2, which is the latest and the greatest out there, so 8.0 U2. And these servers each have four socket, 24 cores per socket, so four NUMA nodes. Server on the left-hand side, four NUMA nodes. Server on the right-hand side, four NUMA nodes. The only difference between these two servers is the server on the left-hand side of this illustration has an extra three terabyte tier two memory. That's the only difference here. Apart from that, security patches, OS patches, exactly set up the same.

So let me move on to the next slide. And that will make things a bit more clearer here. So when you look at this four NUMA nodes for server two-- and I'll call that the software memory tiering server, SMT. What happens is every NUMA node has 384 gigabytes of DRAM, 768 gigabytes of tier two memory. And the server on the right-hand side, just DRAM only, so 384 gigabytes. Apart from that, I've already mentioned this before, these servers are set up exactly the same.

What we did was we said, you know what? With 384 gigabytes of DRAM, let's try putting workloads. Let's start coming up with virtual machine. Let's start loading it up with Oracle databases. Let's see if we could run workload. And can we run a lot more aggregated workload? So the ask for today is to see, can we run a lot more aggregated workload with whatever we have? Can we push a lot more workload? So that's essentially what we did here. So the two virtual machines on the software memory tiering server, if you will, each virtual machine has 12 vCPUs. Remember, every NUMA node has 24 cores. If you have 24 cores, the best practices typically what we say is a core per vCPU. So you can probably fit two virtual machines there, right? So two such virtual machines on one NUMA node on the software memory tiering side, each having 256. Well, somebody would just stand up and ask a question. You just told me it's 384 gigabytes of DRAM. How can 256 times 2 fit in 384? Guess why? Because you have software memory tiering working for you. You have the tier 2 memory attached to the DRAM. And that's where the magic lies here, right? With the server on the right-hand side, all I had was 384 gigabytes of DRAM. So I could just use or I could just provision one such virtual machine. If you guys look at the bottom of the slide here, it says NUMA node affinity. There's a reason as to why we did that. We affixed that virtual machine to NUMA node 0 because this was done in a very controlled environment where we wanted to dispel all kinds of noise. So we said no NUMA interlatency, no such noise, and let the games begin here.

And that's more from an Oracle standpoint. So I'm just building this whole thing up. So you have the virtual machine. You have the NUMA nodes. Now you have the Oracle database here. And pretty much this is the latest and the greatest Oracle version, 21.5, OEL 8.5. Now it's 8.8, even 9 is out. But I don't think 21.3 is certified for 9. Nevertheless, we had three such virtual machines.

And what we did was we said, OK, let's start running workload generators, right? So the workload generator in question here that we chose was what's known as SLOB. Slob stands for-- and I don't know how many people are aware-- but slob stands for Silly Little Oracle Benchmark. Because that's exactly what it does. It's silly. All it does is just takes these blocks and starts pumping data into it and starts writing it. So what we ask here is how much can I run, right? And if you happen to look at the update PCT equals 0, it's a read-only test. So I wanted to stress the read aspect of the test here, right? So we set PCT equal to 0, read-only test. We ran it for 20 minutes. What we need to look at is the aggregated output, not individual output. So if you look at the aggregated output from the two virtual machines that ran on the software memory tiering server, what we saw was we got a lot more execute SQL per second. See what happens is when you start talking to database guys and you ask them, tell me, has the performance improved or has it degraded? Do you think you're doing a lot work today than what you did yesterday? The first thing the guy would say, he or she would say, well, you know, my executes per SQL increased. My transactions per SQL increased. I had a lot more logical IOs. I was able to push a lot more physical IOs. Those are metrics that are very critical for any database, be that MySQL, MongoDB, Oracle, SQL Server, right? So these metrics from a performance standpoint actually increased as compared to the standalone virtual machine, right? And when you look on the other test result from a logical read block perspective, again, the aggregate software memory tiering, if you were to put both the virtual machines together, that increased. I'm a no-brainer here, right? So all we're trying to show here is by using tier-two memory, especially with the memory expansion use case with the software memory tiering example, one is able to load a lot more virtual machines, aka workload, on that server, run a lot more workload than normally you would if you just had DRAM only. I mean, everybody knows DRAM is expensive. You have only two options here. Either use tier-two memory to put on your virtual machines or, well, you know, dip into your pocket, go to the market, buy expensive DRAM. Those are the only two options that you have here. So this is a great use case for workloads, especially hyperscalers. If you start looking at hyperscalers, they're like, wow, I can use tier-two memory. I can use -- I can put a lot more workload because to Arvind's point, the active memory typically is around 35 to 40 to 50 percent. It's not like 100 percent of the Oracle workload or the SQL workload. So you're able to pile a lot more workloads here.

The thing -- the bug doesn't just stop at the virtual machine or the database perspective. From a guest operating system perspective, if you look at the last line here, right, the last column, essentially the idle percentage, that increased when you had software memory tiering server perspective, right? Because what happens is now you have a lot more idle cycles that the guest operating system says, I have a lot more idle cycles, which means I can do a lot more job than I were able to if I was only on a DRAM-only server. So just a lot of metrics that we are throwing out to you, but you're looking at it from different perspective, right? You're looking at it from a database perspective. You're looking at it from a guest operating system perspective.

So I know we have only two minutes left here, but essentially to summarize, right, from a VMware performance results, right, software tiering memory, software memory tiering, that's like the next thing, next biggest thing for us. Arvind already spoke about the different use cases. Mission critical applications, typically it's the Oracles of the world, the SQLs of the world, right? The huge applications that are memory hungry, that are RAM hungry, that will do a lot more better if you were to give it a lot more logical I/Os than physical I/Os, because physical I/Os really kills these databases. Logical I/O helps them, right? They don't have to go down to the disk to read it. So those benefit a lot. And of course, CXL, pooling disaggregation, I think that's a great use case for real application workloads, but I think we are at this call to action paper, especially if you put a couple of links up here.

And pretty much anything and everything to do with Oracle and VMware, I know you guys are all here for the CXL aspect of it, but is there anybody who wants to read a lot more into, OK, how is Oracle working with VMware, the different performance enhancements, accelerators, so on and so forth? That's the slide to it. Sorry, that's the link to it. But.

You want to give the email address?

Yes. And in case you guys want to reach out to us, more so him, because he's a product manager for software memory tiering. Go ahead.

My email address is firstname.lastname@vmware.com. So please feel free to reach out. We are looking for expanding on our own ecosystem. Like you all know, VMware is an ecosystem place. So we would definitely solicit feedback, et cetera, from the ecosystem.

But if you guys have any questions 

So currently we are looking at NVMe, and like I presented in one of the slides, it can be CXL-based memory, inexpensive memory, or it can be a PCI-based NVMe memory. But right now we are working on multiple products. So it's -- currently we are -- you know, our roadmap has an NVMe-based tiering as well, plus we are looking into slower CXL-based memory also, which is inexpensive.

So the data that you show is not NVMe?

Both, actually. So it's -- you know, I showed a generic slide, essentially, you know, which showed that tiering can occur with NVMe or, you know, with CXL-based device.

Okay. Time's up.

How much bandwidth sensitivity do you -- does your application exhibit at this point, bandwidth versus latency sensitivity?

Yeah. So across our customer base, we don't see a lot of bandwidth sensitivity in the sense that the bandwidth that current, you know, DRAM offers or the DDR channel offers is sufficient for, you know, maybe 90 percent of our customers' workloads. So really, I mean, if you talk about sensitivity, I mean, we are not that sensitive at all, I would say.

So your expectation from a remote memory would be not to be equivalent to the local DDR bandwidth?

Yeah. So that should be okay. We can also take a slight hit in latency because, you know, if it's remote, the latency is higher too. But what we'll overcompensate with is by actively, you know, making sure we tier. And for a lot of our customers, performance may not be the single most important criteria. I mean, for a lot of the workloads that our customers run, I think having a slight performance degradation is considered okay.

Maybe we can collaborate on characterizing.

For example, if you look at data warehousing, right, you look at data -- that runs for, let's say, an hour, hour and a half, right? So a peak up here or an ebb down really doesn't make a difference. So for those customers or for those workloads, it's fine because over a period of time, it averages kind of out.

Right. But it's nice to understand the characterization of bandwidth versus latency impact to the overall performance.

Correct.
