
Thanks everyone for attending my presentation,  Proprietary Interconnects and CXL. My name is Larrie Carr,  I'm the VP of Engineering at Rambus,  as well as I'm President of the CXL Consortium. I've been asked to do this presentation  as we look at the marketplace right now,  there's a lot of discussion of proprietary networks,  as well as some questions around the CXL adoption rate. I figured that if we go through the history of how we got to this point,  it could give you some understanding of how we expect the adoption  of CXL and how architectures will change in the future. Hopefully, you'll find it interesting.

CXL, we've been working on it for a long time, approximately five years. It's evolved from a small direct connect solution trying to  expand attaching accelerators plus expanding memory attachment. It's now grown to a full-size fabric with switching and  composability between accelerators, memory,  and it's gotten quite significant in complexity. For the people who are new to load store architectures,  it's one of those things where a figure of CXL is the first and it's not. A lot of the problem statements that CXL are trying to address actually predates CXL. There were other interconnects,  other protocols that were involved to try and  solve what we're still trying to solve today  as we see these greater larger systems being created. I'm going to go back in history,  probably about 10 years-ish,  and we'll see how the world had gotten toward where it is.

We go to the beginning, PCI Express,  it's a great interface. It went through the great serialization event in the early 2000. The other parallel interfaces at the time also went through at the same time. Once we had a serial interface,  people looked at new ways of doing things,  which normally involves switching and fabrics and greater connectivity complexity. PCI Express is just a serialization of a protocol that dates back to 1991.  Where we created PCIe designed for peripherals. It's fundamentally a load store interface,  but it's not really designed for memory. It's designed for connecting peripherals. It doesn't mean it's slow,  because if you go look at a Gen 5 SSD now,  those things smoke pretty fast. But from its memory connectivity,  how it hooks into the processor,  it's just not designed for load store.  And it's a low-performance thing for when we just talk about load store. Of course, people wanted to improve on that. I would say the first company that did that was IBM,  where they created CAPI,  trying to bring in new accelerators to talk to their PowerPC  to accelerate certain workloads in each PC. They extended PCIe Gen 3 and Gen 4,  but it was a proprietary interface. They opened it up with the creation of the OpenCAPI Consortium in 2016 to support all ISAs. Some very neat things were built with OpenCAPI. We'll see how that evolved over time. CCIX also came in approximately the same time. Again, this is where they're trying to bring in cache coherency,  as well as memory expansion. They were leveraging Gen 4,  but they also wanted to go faster and non-standard rates. Again, a group of companies got together to create this protocol. It's been adopted by the military. I know it's in a few missiles. The connectivity acceleration and memory expansion to a processor,  I would argue, it's a problem that's at least 10 years old. People invested time and effort trying to do this,  both in proprietary as well as from a standard point of view.

Stranded resources, especially in memory,  2014, I would say is the first paper you see it published publicly. Again, NERSC was looking at  the workloads for their high-performance compute systems. This is where you had three high-performance computes. They are ever-increasing doubling of memory,  plus the inclusion of the HBM. The problem they were seeing is that while  certain workloads were achieving being able to expand and use all this memory,  most of the workloads didn't take advantage of it. The most damning event is if you look at Edison,  where 64 gigabyte DRAM per node,  71 percent of that would fit in the 16 gigabytes of HBM. That means there's a lot of wasted DRAM. When you're talking about tens of thousands of nodes,  this is a waste of resources. With HPC, it's particularly bad  because every node is running essentially the same program. There's no way like the hyper-scalers to pack different workloads in. Again, 10 years ago,  we were struggling with memory,  too much memory in one place and not enough in another.

In 2016, the Department of Energy launched the Exascale Compute Program. They're trying to accelerate the ever-increasing performance of high-performance compute.  specifically for the US nuclear stockpile. This is a strategic initiative because exascale compute or  exascale number of floating-point operations per second was  deemed key to maintaining scientific leadership in the world. Department of Energy owns a nuclear stockpile and they're responsible  to ensure it's understand how its life is progressing. Department of Defense tells you where to send it. But they were looking to acquire three high-performance systems  with exascale number of flops. At the time, we were serving 10 to 100 pentaflops. They were looking for a 10 to 50X improvement in performance. At that point, they were funding key initiatives. Pathforward was a key one to work on HPC connectivity. The leadership at the time was Slingshot owned by Cray,  very successful fabric. But they wanted to see an open standard to be  created that would allow new features and functionality and  performance supported by the industry. Gen Z was formed as part of the Pathforward efforts to create  open standard of connectivity that could be used not just for  high-performance compute but also for enterprise and  cloud to move memory and attach accelerators. It's very similar to CXL 3.1 from a feature set point of view. Some of the connectors they defined are actually used  within the industry today. And they got into things like optical and probably ahead of  its time in a lot of ways. Broad industry interest but the adoption rate on the processor  side definitely struggled.

So looking at the milestone, 2018, the first CPU-GPU hybrid  HPC system went online called Summit. Before that, it was all CPUs. So if we think about today where we're talking about,  "Oh, we've got CPUs on massive scales of GPUs."  That first was done in 2018, which isn't that long ago. That's six years ago. And we're talking about 9,000 processors and 30,000 NVIDIA  processors tied together with NVLink,  a proprietary network, and InfiniBand. And this was a leapfrogging performance. And this is where when we talk about the architecture today,  a lot of them look like, again, six years ago. The following generation of that used was severely--  they wanted to improve the DDR bandwidth. So instead of having parallel DDR interfaces on the processor,  they used actually 16 OpenCAPI serial interfaces. So they said and they found that they got 2x the memory  bandwidth to the core, to the processor,  versus a parallel interface. Because they could literally  put 16 times more serial links in the same physical space  to bring in the bandwidth at 25 gigabits per second. And with the memory controller, they  had less than a 5 nanosecond impact on their read latency. And because it was an OpenCAPI, the OpenCAPI memory standard  OMI became--  it's a public standard. They were the only ones that took advantage of it. But they produced some very massive memory bandwidth  processor with this interface. JEDEC standardized on a form factor called DDIMM,  an OMI DDIMM. Microchip did the processor. I was involved in the hardware. I was involved in the processor. I was involved in that. And in a way, it looks like, again, something  very similar to CXL. But there was an attempt to go open,  taking something that was proprietary.

So in sort of the 2016, 2018, we had  a number of new connectivity standards. I would say it's probably about 10. RISC-V was bringing-- a couple of RISC-V companies  were bringing stuff in. People were really trying to solve this problem  of memory expansion, memory movement,  and sharing, and accelerators, and memory bandwidth  to processors. And it had different adoption by various companies. The CXL founding members rallied around Intel,  who had their own internal specification,  because they were looking at, again,  expanding PCIe in a way to allow, again, memory sharing  and attachment to accelerators. And that was what eventually became CXL 1.0. Again, a group of companies are rallying around the standard  and saying, yes, this is the standard we're going to go on,  and we will expand it. I think the key aspect of CXL that has made it successful  is, by the CXL 1.1 time frame, which is 2019,  all major processor vendors pledged support  in their roadmaps. And that's a big difference from all the others  from open standard approaches and proprietary approaches  of the past. So what makes an open standard isn't just the fact  that the standard is available and allows everyone  to design to it. What makes it standard is adoption,  and especially on the processor side. That's what has made CXL to be as successful as from an adoption  point of view, because it has portability  between key processors.

Just to give you an exascale update,  we did achieve 1.6 high-precision exaflops in 2022. That was Frontier. Aurora recently just got--  I reached it as well. We did that with essentially 10,000 AMD processors  and 37,880 AMD Instinct GPUs tied together  with the proprietary network and the Infinity fabric,  as well as the HP's Slingshot. HP bought Cray, and therefore, when HP built the system,  they used a slingshot processor connectivity. So it's the most efficient HPC today, 63 gigaflops per watt. Still takes 20 million watts to power the system. And that becomes a physical restriction,  mainly because a lot of these systems  are replacing previous systems. So there's a set amount of power already in place. And 20 megawatts is pretty much the limit. To give you some context, three Vancouver skyrises  consume about 20 megawatts. So everything's electrified, cars, all works. Each tower takes 7 megawatts. We're talking a lot of power in a small little area.

And there's now more exascale initiatives  trying to do another 10x improvement. Which really  is focused on power consumption, because we really  can't push it with 20 megawatts. So then we're circling back to proprietary network. The key thing we have to remember  is that custom connectivity will always exist. The processor to processor communication,  it's a point of innovation. As they evolve their hardware architecture,  the communication between things also will evolve. There is, again, the two dominant proprietary fabrics,  NVIDIA with their NVLink, AMD with their Infinity fabric. They're not really proprietary in this traditional sense,  because they have opened up the fabrics. You're allowed to partner with them to get access to the base  IP. It allows you to connect your technology to those fabrics. But they're not open as in broadly adopted. But again, I think where you see is the industry  has kind of woken up to that fact. So in the past, it was sort of like, here's  something that's wonderful. We will win on technical brilliance,  and everyone will have to adopt it. What's happening now, and I look at Ultra Ethernet,  it's like, OK, we want to, as an industry,  we have to improve this. But not just one person or one company can do it all. So we're seeing a lot more common ground finding. So while we look at these accelerator fabrics,  I think as economies of scale come into play,  as they want to start to invest to accelerate,  their behavior may change. And I expect it to change. But how that will play out will be an interesting thing  to watch. They work really well, these fabrics,  in connecting GPU to memory--  GPU to process, or GPU to GPU. But from a memory point of view, they're really not designed for  it. CXL still plays a role in these architectures  as, again, from a memory expansion and memory coherency,  the needs of the processors are different  from the needs of the GPUs. So again, from a processor to processor, or GPU to GPU,  you probably will still see a certain amount  of company-specific dominant proprietary networks  in that area. But overall, that is not sustainable  as this technology becomes more expensive.

So final thoughts-- generative AI was disruptive. We had certain key things the industry was working on  that was, from an open point of view, very well aligned  to CXL--  green initiatives. Where it's like, oh, we're  going to go DDR4 reuse, or green initiatives,  stuff like that. And we're now in a crazy phase. Basically, we just can't get big enough. When we're talking about companies  buying nuclear power plants to power their systems,  that's crazy. So in the short term, yeah, everyone's  going to go, probably, again, innovate very independently. But given where AI fidelity is really a function of processing  power right now, the only way we're  going to make AI smarter, I guess, is to make it bigger. And you look at NVIDIA's Venado. They're at 93x of blocks, but at much lower precision. They're at 20 megawatts. And expanding beyond that is difficult. So the industry needs to get more power efficient, which,  again, is innovation and investment of research dollars. And it's, again, back to the economies of scale. So standardization, with JEDEC and SNIA and OCP and CXL,  it's great. It is the baseline that we need. But to get the adoption there, it's also, again,  one of those things aligned to the architectures  and some stability such that it's like, OK. We're  going to see commonality between these architectures to the point  where they can take advantage of these networks. So I always like the bird's-eye's dog  on the shoulder. But that's the problem we're going to solve, I guess. So hopefully, you found this interesting. It will take time. CXL adoption, it's relatively new. And it's mind-numbingly expensive to do silicon now. And without the standardization we have now,  I don't think we would be accelerating as fast as we are. So thank you. And please rate this session.
