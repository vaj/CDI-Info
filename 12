
I'm Andy Rudolph from Intel.And today, I'm really here as a member of the Compute Express Link or CXL Consortium.I'm here to talk about the CXL 3.0 spec and the features that have been added to the 3.0 spec.This is a fairly brief talk.I'm going to give you an overview of the features that went into 3.0.But I would like to point out that the Compute Express Link website, which is computeexpresslink.org, has a lot more materials on this, not just the CXL spec itself, which anybody can download and read, but there are a number of webinar videos that I find particularly useful.We're giving you a deep dive into all the features that I'm going to talk about today.So I just want to make sure that you're aware of that resource.And I'll go into my overview of those features.

So just to make sure that everybody starts ... this may be a whole new ... CXL ... is an interconnect between CPU, memory, and accelerators.It builds on PCIe and builds on that infrastructure.And I have an important point here.It leverages the PCIe physical and electrical interface.You're already starting to see machines emerge in the ecosystem that have slots that you can either plug PCIe cards into or CXL cards.And so that just shows you the power of leveraging that physical and electrical interface.CXL maintains memory coherency between CPU memory, the CPU memory space, and memory on CXL-attached devices.And I'm going to show you some pictures and some examples of what this really means, especially around segregation like pooling and sharing.CXL is an open industry standard.The standards are each new version is backward compatible with the previous versions.And again, anyone can go and download it and read it.On the right-hand side of this slide, I'm showing you the logos here for the board of directors.You can see what companies are involved as directors.But there are over 200 member companies in CXL.I have to say, I attend a number of the work groups.And there's quite a bit of activity from all of these companies.It's really great, actually, to see the level of enthusiasm around CXL.Across the bottom, I've put a timeline of the CXL specifications so you can get an idea of the history that's up to the most recent spec, which was released in August of 2022.That's the 3.0 spec that we're going to talk about today.

So another small bit of background here is that when we talk about CXL, we like to use these representative use cases here.We like to refer to them as type 1, type 2, and type 3 devices.CXL runs three different protocols over those electricals that I mentioned earlier.CXL.io is the protocol for identifying devices and managing them.And you can see every type of device supports CXL.io because you want to be able to enumerate these devices and manage them.Type 1 device has CXL.cache, which allows an accelerator to have its own cache that's part of the CPU's cache coherency domain.An example of this would be a SmartNIC, for example.And a type 2 device is another type of accelerator that does everything that the type 1 device does.It also has the CXL.memory or CXL.mem protocol that allows it to have memory that is accessible from the host.And so it's a more powerful accelerator, such as a GPU.And you can see both type 1 and type 2 devices have the word accelerator associated with them.The type 3 device, it's just the CXL.io protocol and the CXL.mem protocol is typically referred to as memory expansion or memory buffer device.And this allows you to add memory to the system.Using CXL, it becomes part of the system's main memory.So for example, this allows you to expand the memory bandwidth of the system or the memory capacity of the system or add new types of memory to the system, such as persistent memory.So keeping these types of devices in mind, I am going to speak a little bit about all three devices, but mostly about the memory buffer device because a lot of activity has been focused in that area because a lot of the use cases of interest are in that area.

But trends in the industry have really influenced what went into the CXL 3.0 specification.For example, these use cases that are driving a need for higher bandwidth, like high performance accelerators or system memory or smart NICs or this idea that there's this explosion of cores.Every CPU socket has a lot more cores these days, and those cores are memory hungry but with differing memory requirements.And so it's sufficient to just have a static amount of memory for each one of those cores.You end up with the so-called stranded memory problem where you're using the compute but not all the memory and you want to get that memory and use it elsewhere.And so I'm going to show you how CXL helps address that end point.Similarly, there's a trend towards more efficient resource sharing across multiple domains and between peers and addressing the bottlenecks between CPU and memory.So to address some of these trends, CXL introduces this idea of a CXL fabric and multi-headed and fabric-attached devices, management that goes along with fabrics, and the way to build these disaggregated infrastructures.You're going to hear me talk a lot about things like memory pooling, more complex switching topologies, new cache coherency capabilities.I'm actually particularly excited about that.You'll see why when we get to it and the things that software can do to leverage this.CXL 3.0 doubles the bandwidth that you could build with CXL 2.0, but zero additional latency has been added over 2.0.And like all CXL specs, it's backward compatible with the previous specs that were released.

So if I take these features and kind of lay them out on a table just so you can see across that same timeline I showed earlier, you can see how things evolved.On the top here, you can see how the bandwidth doubled, like I mentioned.And you can see there's this new 256-byte FLIT.So the FLIT is the little packet of information that travels across the CXL electricals.And you can see this FLIT enables a bunch of the features that are listed on these slides.They're the ones that have a little parenthetical remark there.So in CXL 2.0, we added some basic memory pooling.We added things like global persistent flush that allows us to support persistent memory.We added CXL IDE.That's the link integrity and data encryption security features in CXL.And we added a single level of switching.But then in 3.0, we expanded that to multi-levels of switching, features like being able to talk peer-to-peer.I'm going to show you what that means in a moment.This enhanced coherency, memory sharing, which is completely new in 3.0, and flexibility on the types of devices that you can have per report, and then the fabric capabilities.So I'll go through some of these in a little more detail.And again, you can find a lot more detail on the CXL website.

So just to remind you of where pooling was left in 2.0, it was based on this idea that you would connect all these devices to a switch, and all these hosts are connected to the same switch.And by programming that switch, you're able to take blocks of capacity and assign them to specific hosts.And it's a fairly heavyweight operation to change these assignments, roughly equivalent in software to what happens when there's a hot remove and a hot add.And so that's one of the things that you'll see in a moment that we've tried to address in 3.0.

Also in 2.0, there was just a single level of switch, like I mentioned on that table.And so this topology that I'm showing here was really the only switch-involved topology that's available in CXL 2.0, a single switch with a bunch of devices on it.

Now, contrast that with 3.0, which really expands what you can do with switches.You can see with a couple of the examples here, multiple levels of switches are allowed.On the left, you can see how devices don't have to be at the same level.Some of these devices are beneath a single switch.Some are beneath two levels of switches.You can see on the right, you're allowed to cascade these switches.And these are just a couple of examples.There's really a vast array of topologies here possible in 3.0.So that's been added as quite a bit of flexibility of what you could do with switches.

Now, when you start having all these levels of switches, it becomes desirable of these devices able to talk to each other when they're connected to the same switch, rather than having that communication go all the way up through the switch hierarchy to the host that they have in common.So an example of this might be, say, that this device on the right is a network interface.And it wants to receive a packet into the memory of this device on the left.And it wants to be able to transfer that directly without having to go through a much longer path.The peer-to-peer access allows that.And the new coherency mechanism allows that, because now this device can send these so-called back and validate commands up to the hosts that can see this device so that they don't then have stale data in their caches.So it's a very powerful mechanism for peer-to-peer operation, part of the 3.0 spec.

And that same coherency mechanism allows this coherent memory sharing that's new in 3.0.Here's just kind of a simple level cartoon to show you the concept.Here's a block of memory, S1.You can see it's shared by host 1 and host 2.And the coherency is quite flexible, allowing you to have all of it be hardware coherent, or maybe just a portion of it, or even none of it if you want software to take over the coherency entirely.So the spec supports a number of use cases there, including devices that can support both shared memory and pooled memory, like I show here.And I want to drill down a little bit into the memory pooling and sharing concept to talk about how it works, because there's been so much activity around this in 3.0 and post 3.0.

This is made possible by a concept that was added to the 3.0 spec, the dynamic capacity device.Now, you think about what the host sees when it sees a CXL device.Initially, the driver on the host will identify the device and issue a mailbox command called identify memory device.And up until now, devices return to what's shown in this gray area.That means part of their device physical address space here can contain some volatile capacity, maybe zero, but more persistent capacity, and maybe zero or more, and maybe some capacity that's either volatile or persistent and can be partitioned between the two.This gray box is all that was supported in the 2.0 spec.And that means at the time that the driver finds a device, it's told what the capacity of the device is.And that's a static value.It doesn't change over time.It stays that way until the device is removed or the system reboots or whatever.With memory pooling and sharing, we wanted that to be way more dynamic.And we wanted a lightweight mechanism for making those dynamic changes.So now with 3.0, the device can also report these green boxes over here on the right, which is some number of regions where a region is basically a range of addresses that have the same attributes.And these regions initially are not populated.In other words, the host will map them into the physical memory of the system.The host can't actually do loads and stores these addresses at the beginning because they're not populated.Any stores will just get thrown away.Any loads would return something like poison.So that just sets up the physical address space of the machine.And then the lightweight mechanism are these extents, which are shown as these yellowish boxes, yellow-orange boxes.And so these extents can come and go in a very lightweight manner.They can happen very frequently over time as something like an orchestrator decides to assign memory to a host.These extents pop up in its address space and now loads and stores happen to these extents.So this idea of a dynamic capacity device really enables the whole memory pooling idea.

So let me draw a picture of a simple memory pool.I took what is basically the simplest kind of memory pool that you can build in the CXL spec, which would be a multi-host single logical device, or an MHSLD.And so this blue box on the right, it's a memory pool that would contain a bunch of memory plugged into it.Conceptually, it might be a top-of-rack device that contains a bunch of memory and then has cables down to each host, just as an example of how this might look.You can see I have it hooked up to eight hosts here.And I drew a fabric manager, this thing that says FM.And an orchestrator that decides which hosts get what memory over time.

So at the beginning of time when these hosts boot up, there's a generic CXL type 3 memory device driver.And that's an important point because we want to encourage these hardware vendors to be able to build these memory devices or even memory pools without having to develop the software stacks to allow software to use them.We want some uniformity there so that software doesn't have to be rewritten for every vendor's device.So that generic driver at the beginning of time finds the device and maps it into the physical address space.As I said before, this is unpopulated.So this is the potential amount of capacity that memory pool might give to this device.Initially, there is actually no capacity there.It's the HDM decoders that handle the mapping to the host physical address space and interleaving and stuff like that.Those are all set up at the beginning of time.And they don't change after that.So all the heavyweight work is done.

And then the lighter weight part of a dynamic capacity device that I mentioned before takes over.The orchestrator here is able to assign memory.You can see indicated by these blue boxes.Able to assign memory to the hosts.And those are populated extents of memory that the host can then access.And they really are access to the memory and the memory pool, which I've shown as these dashed boxes over here.So each host has its own memory coming from the pool right now.OK, so that's memory pooling.And again, you can see how this might be used to solve the so-called stranded memory problem, where an orchestrator is putting workloads on each host.And it doesn't want to use up all the processing power of a host and leave a bunch of unused memory on that host.Well, now that the memory comes from a pool, or at least some of the memory comes from a pool, it's able to balance the memory as necessary between these hosts and use it more efficiently.OK, so that's pooling.

But what about memory sharing?Well, let's talk about what it would look like instead.Let's say that each one of these hosts now had memory assigned to them.Instead of each getting their own individual memory, they're actually all getting concurrent access to this same memory, this one dashed box over here in the pool.So you can see that's pretty powerful, just like hosts can share memory between threads.There can be shared memory on a single host.Now we can share that memory between hosts.And the spec allows for either software or hardware coherency.The software coherency basically just makes it software's problem on how to keep coherent access between these things.Hardware coherency, that's the exciting part, I think.That really gives you full CPU cache coherency between the memory shared by all these hosts.Now there are existing APIs, of course, for shared memory.Most of them were developed as part of high performance computing over the years.And those APIs can be made to work here.But I think we're going to see-- that's why I marked this as to be done-- I think we're going to see the emergence of some new software research in this area, some new development for advanced APIs for shared memory allocation and cross-host coordination with the load store sharing that the CXL memory sharing gives you.So that's kind of an exciting area to watch.And I showed this with just a fairly small number of hosts.

But an exciting part of CXL 3.0 is the Fabric mechanism, which allows us to scale up to thousands of nodes.And you can see in just this one example drawing here how I'm showing a bunch of switches with different kind of connectivity between them.And the endpoints here are their hosts, different types of devices, like a type 1, or a type 2, or a type 3 device.And there are a lot of different topologies possible here with quite a bit of scalability.So much to talk about here that I really want to encourage you, as part of this summit, go and listen to a talk by Vince Hachet, who is kind of a Fabrics expert.And I think he'll do a really great job telling you all about CXL 3.0 Fabric.So I just want to plug his talk here and say that's where you can get a lot more information about CXL Fabric.

OK, so in our little whirlwind overview, here's my summary.We talked about Fabric capabilities and that there's Fabric management added to the 3.0 spec.We talked about how the switching topology is greatly expanded over what you could do in 2.0.And we talked about this new back and validate coherence mechanism, which, again, I think enables all sorts of interesting use cases, including peer-to-peer resource sharing.CXL 3.0 doubled the bandwidth, added no new latency over 2.0, and is backward compatible with the previous versions of the spec.So I think we're going to see some pretty exciting new usage models here around memory sharing, around these multi-headed devices.There's more flexibility and support for type 1 and type 2 devices.And I haven't mentioned this acronym before, but it stands for Global Fabric Attached Memory.This is part of the fabrics that allows memory to really scale out to thousands of tones.So here's your call to action.Go download the CXL 3.0 spec yourself and take a look at it.It's got quite a bit of interesting content in there.And I would like to encourage you to support future development in CXL by joining the consortium and participating in the work groups.I actually find them really enjoyable.Like I say, there's quite a flurry of activity and a lot of really fun discussions in there.You can follow the CXL consortium on Twitter and LinkedIn for updates.And that's the whirlwind talk.I hope you enjoyed it.

Please take a moment to rate this session.We do read all the feedback and try and improve the sessions between the years of the summit.And thanks very much for listening.
