
So, welcome to the update from the Computational Memory Programming Workstream of the CMS group. This workstream is chaired by myself and Gaurav Agarwal, who is sitting here from Marwell. He will be giving a talk right after lunch. You also heard from one of our contributors, John Hermes, from ARM, earlier. We are reporting on behalf of an industry team and an academia team that is almost 51 members, with almost 20 members actively contributing, and a few of them have chosen to accompany me on the stage for the second half of this presentation.

So my goal is to give you a quick update in a few minutes about what we are about, and then hand off to the individuals on my right, one by one, to walk you through some specific deep-dive topics. So the team is working on enabling near-data processing for the same reasons that John outlined: that sometimes, as the distance between compute and memory increases because of CXL or other reasons, then some function wants to gravitate to data. The kind of functions that are best suited for gravitating to data are the ones that have lightweight compute against data. For example, searching, filtering, this kind of prefix reductions of various kinds. So our work stream's mission is to take the lead in enabling uniform and stable interfaces to near-memory compute for a variety of memory media. The ones that are most popular now are DRAM and Flash. So today we have separate interfaces for computational memory and computational storage. And this is problematic because if you are in a database, and you happen to cache some buffers into the memory tier, then your computational memory interface changes. So we have these kinds of goals to make it easy for developers in the future to work with smart, intelligent memories, and the way we are doing this work is generally useful for one set of accelerators talking to another. So near-data processing is not just about computational memory or computational storage, it could be one set of GPUs from one vendor talking to older GPUs. We are trying to make it as stable as possible to use any far accelerator on CXL that is sitting near lots of data.

This slide is contributed by Jay, who is a Senior Marketing Manager at MetisX, and it basically puts what I said into a few more words and puts some complexion on it. So, on the left side of the slide, you see we are talking about low computational intensity per memory access as one of the attributes of the workloads that are suited for this. And on the right, it is talking about operation diversity. So, depending on what kind of operation diversity you have, you might prefer a SIMD architecture or a high core count architecture. Or, in case of MetisX, their product, for example, is more suitable for this kind. He will talk about this. And he has also characterized some of the applications, like vector databases as a poster child for this, where you could offload distance function offloads. But you can imagine similar search-oriented applications, like DNA alignment or various data compression topics, like where you look for matching windows.

So, here is a quick summary of what we have been up to. Our work stream was created after an initial proposal that I presented on a broad-based acceleration programming interface. In that, in 2023, we had contemplated already the use of open formats like Arrow. But through the work that we have done in the last year, working together to identify what are the best practices and principles that are actually working in practice, such as you heard from John from ArmSight. You will hear from other speakers, like Garav in particular, later on. And Intel, in their OpenVINO work, also described this, that in addition to low-level matrix vector operations or vector-vector or columnar data operations, sometimes when you have very large workloads, like neural networks, it's best to describe that workload not individually by the behavior of a single dot product, but by behavior of entire layers. So Marvell and Intel, in the work stream, reported on work using open formats for neural networks like ONNX and TF. We'll talk about that a little bit. And then generally, what we have been doing is we have been putting into demos and best practice presentations how these actually can be implemented. So this year, we would have loved to report on a perfect API for doing this. We are not there yet at the third quarter end. But in the fourth quarter, based on the demos that you can see in the Innovation Village, where we have demonstrated some of the principles that we identified, we are moving forward towards an API. And to that end, I am welcoming to the group here, Shyam Iyer from Dell, who is representing SNIA, SNIA-SDXI effort. And in the end of the presentation, he will talk about what that effort is all about and how the work in OCP and SNIA might line up.

So it's a video. It's a very complex topic, because if you look at Arm's presentation today, it talks about the very top, where it is possible to have coherence because cross-ISA issues that John identified at the end of the presentation are not plaguing us. As you move farther and farther to looking memory and compute near that memory, for example, in the fabric tier, like pools and fabric-attached, or even in the storage tier, the situation gets more complicated, because you don't have a common ISA, for example. You have different characteristics of memory. The goal is that the computational memory interface should keep working. So we are looking for common abstractions that work across tiers and are not affected by last-moment decisions by the memory hierarchy to move data up or down the tier. So to that end, you know, some abstractions like memory objects, service meshesâ€”these are generic abstractions that are used already in hyperscale workloads. We are looking to embrace them in this work.

So I covered this. One of the two things I want to highlight on this slide is that the benefit is huge. One of the studies from Post Tech in Korea identified that, for example, for RAG, if you offload distance functions to the memory device, you will move 73 times less data compared to if you pull that data into the compute from far memory and then compute with it locally. We also found that some of the issues that John left open at the end of his presentation, like cross-ISA issues, one possible approach to ameliorating them is to use open formats for data and compute. An example of proprietary formats is Oracle's data format for tables in their own database. An example of an open format is Parquet that is used in data lakes, or Arrow that is used all over the place these days in machine learning and databases. And Arrow, interestingly, has now almost 3,000 pages of specification in Apache and has a far open format for data and compute. So some of the demonstrations that we'll be talking about are actually starting to go there, starting with MetisX and then my company, Elephance.

So let me pop it back up a level. So you have heard a lot of presentations that talked about the memory hierarchy. Most of the work, as you know, is done at the top of the memory hierarchy using technologies like HBM.

We talked about where memory is small, but compute is very rich. There is a lot of data at the bottom of the memory hierarchy, which is lightly used, as John identified, and as the speakers just before me identified. There, the compute is light. But if you do not exercise that compute, you will end up moving a lot of that data. And 90% to 90% of your power budget will be spent just moving that data. So, the goal is to contemplate this together. And some of the best practices are exactly like John identified, that it's not just enough to think about data placement. You should think about scheduling algorithms, for example, as John said, that can contemplate memory and data placement together.

So early on, coming down to the CXL topics here, CXL has an abstraction for this kind of thing. It's got CXL type 2 devices, which stands for accelerator with memory. This is a standard concept and abstraction in the CXL standard. Recent studies contributed by Intel and the University of Illinois that were presented in CMS identified that the extra overhead from going to type 2 devices and incorporating some of the functionality that a type 3 device would need to have to support things like dot cache protocol incur only about a 6% extra latency and really manageable area overheads. So after a year and a half, working on this, we are still very sure that the right abstraction for this kind of thing is CXL Type 2. There are very few CXL Type 2 devices in existence right now. So Intel have opened up Altera, and Intel have opened up their platform for those who want to try it out. And we encourage the industry for groups like us to supply more type 2 devices.

This goes a little bit into the detail, for which I don't have time.

Basically, you know, as you contemplate computational memory and CXL, new types of patterns come to the fore beyond what you can think about. So, normally, you know, when we are only talking about type 3 devices, we talk about promotion and demotion. But the promise of CXL is independent scaling of memory and compute. So, you could fire up a new compute to work on the data that you are holding in your memory. What kind of software questions will arise when you do that? That new compute will need to use that data quickly. So, either it will need to be coherent with the page tables of the computes that are already there, so it can use virtually addressed pointers and chase them across regions and devices, or it will need to use more sophisticated memory objects. We have seen both kinds of demonstrations. The ARM presentation, for example, talked about using their CMN IPs and coherence IPs. The presentation from Elephance, that is at the Innovation Village, talks about using memory objects. And then Jay will say a little bit more. But what I want to point out to you, being able to point to data rather than copy data across RPCs, being able to ship function to data, and then have it chase pointers across devices and regions.

These are the kind of things that you have to think about. So, our group, as a matter of principle, and communicating to you all quickly how we are thinking about this, we looked at past work from Intel. And we looked at how do you do portability and performance at the same time in this kind of environment, recognizing that accelerators could be multi-threaded or vector oriented. Those are the major accelerator types that we find in the industry. So, we found considerations such as: you should have abstractions that expose parallelism. You should clearly expose offload mechanisms. You should have memory management so you can tier, promote, demote, and point to, copy, share, move. These are things that Shyam worries about. And you should have clear communication abstractions because parallel computing requires hosts as well as devices to be able to compute. In fact, CXL equipped memory and storage devices have the superpower that is a little bit under exercised. They can do source shuffles that John talked about entirely amongst themselves without involving hosts or GPUs. So, these are things to think about.

So, very quickly now, how should this be consumed? So, it's designed to be consumed in framework libraries like CUDA or oneCCL or one API from Intel, so that application frameworks like PyTorch do not have to be impacted. And any applications that are built on PyTorch are not at all impacted by the work we are doing.

The other thing is that it should offer to providers a clear framework in which accelerators, DPUs, and other kinds of computational memory and storage devices can be connected and exposed as a single target for acceleration through one API.

So these are some of the things we are doing. I'm going to speak on behalf of John very quickly, summarize the key points that he made in his presentation. And basically, the key point he made was that joint scheduling of memory and compute and thinking about ways to overcome programming hurdles when you don't have the same ISA across host and accelerator.

He's their new CTO. He has joined us on the panel. And let me turn over to Durgesh. They're working on some instruction set architecture concepts to enable this kind of compute memory acceleration. So, Durgesh, why don't you talk for a minute?

Thank you, Pranav. Pankaj, sorry. I'll come this side. So, I joined MIPS six months ago. Before that, I worked at Intel and NVIDIA Grace. So, coming from x86 and ARM, when you come to the RISC-V ecosystem, MIPS is fully RISC-V compliant now. It was easier to change the instruction set, and that was a dream come true, which we are trying to do. And the second boundary condition was to keep the instruction set same between the application processor and the core processor, which is doing the near memory compute. So, what we did was, in the memory controller of the SOC and this FPGA prototype, we had a near-memory, a near-memory compute controller, which does a few key items, which I'll talk about. But we have already implemented it, and just some of the instructions, when we look at the traces for Redis, and we find there are hot spots, so we decided that it'd be good to look at it and see if we can customize the instructions and see the performance of Redis. So, our full system was not ready, so we couldn't demo it this time. The next time, definitely we'll bring it up. So, we are already seeing initial results, 30% improvements by pushing some of the usages and processing to the coprocessor in the memory controller, which can, of course, be moved to a CXL controller and other places. But some of the things were vector similarity search, which is being done in the near-memory compute, atomic vector update, then near-memory compute prefetch, and vector dimension, and reduction. So, that's four instructions which we have already implemented. And once we have proven it, we will, of course, put it in the RISC-V community for further evaluation. So, that gives an idea that near-memory compute is essential, and how it helps correlating the software and hardware ecosystem to make the performance much better than what it is.

Thank you, Durgesh. Let me turn it over to Jay Leon, who is the Senior Marketing Manager at MetisX. This is a company that's actually building what would be a future CXL type 2 accelerator with memories.

Thank you, Pankaj. I'm Jay from MetisX. MetisX is a public startup based in South Korea, and we have been contributing to this work stream since this summer; it's been a pleasure to share some insights with you guys here. So our product here... called CXL computational memory. We have more than thousands of custom RISC-V cores inside our chip to add computational capabilities near the CXL memories. And also, we provide a software framework. So from the low-level kernel drivers to APIs, we provide these to make it work seamlessly with the chips. And we aim to accelerate data processing in applications with large-scale data sets such as factory databases, gripe databases, and also DNA analysis. So I guess because of the time constraint, I only have one slide here. But if you have any questions, please come talk to me after this presentation. And also, visit our website at MetisX.com. Thanks.

Thank you, Jay. Jay actually has implemented a multi-level software stack with APIs that actually meet the principles that we outlined in the slide, meaning parallelism, offload, and embracing open formats. So I encourage you to learn more about MetisX and their product, which you guys said you are about to tape out.

Taping out soon, and then our chip will be ready by the first half of next year.

Thank you. So moving from MetisX, so I didn't introduce myself. I'm a professor in computer science at Ohio State University. And I have a second hat, which is my hobby. I founded this company together with my co-founder, Daniel Bittman, who has been demoing this at the Innovation Village. The company is called Elephance Memory. The product of the company is MemOS. And the connection to the topic of the day is that sometimes you just don't have coherence. And you still need to look at objects in shared memory and create them in one place and work on them from another while offloading near-memory compute operations. So Elephance does for memory what S3 did for storage. We create memory objects with the ability to do near-memory compute using open formats. And in particular, at the Innovation Village, our demonstration shows that if you have data in open formats like Arrow, and if you are offloading to vector compute operations in Arrow Compute abstractions, then you don't have any code shipping or cross-compilation issues. So please go and visit our booth. I will not stay too long.

And later, after lunch, there is going to be a detailed presentation by my co-chair of the workstream, Gaurav Agarwal, who is here. What Gaurav shared with the workstream is that sometimes, when you have intelligent devices such as DPUs and computational memories, it is possible to break off entire layers of neural networks, such as embeddings and other data-intensive layers, and execute them elsewhere other than the CPU or the GPU. So, for this, they have been using a graph-lowering compiler framework in which those layers are thenâ€”the abstraction is lowered down to a point where you can move from vector-oriented architecture on a GPU to a high core count or other architecture, very native memory architecture in the future, for example, for the vector caches and the vector engines. So, they have made this demonstration, and in this chart, he's showing that the Glow framework actually targets multiple possible back ends, including some contemplated Marvell devices, and I invite you to come back after lunch and attend a more detailed presentation from him. Moving forward, what this has meant for our group is that Arrow and ONNX, Open Neural Network Exchange, these have become the formats that we are choosing to embrace for describing what will be supported in the API work we are looking to do with SNIA.

So, talking about SNIA, that is the next topic, and Shyam Iyer, who is a fellow at Dell, is here today to tell us what SNIA, SDXI, Smart Data Accelerator Interface effort is all about. And as they move beyond 1.0, what are some ways in which they and us could work together?

Well, you gave me a couple of promotions, but I'll take it. So, I am a distinguished engineer at Dell, so I am representing SNIA, so thank you for the promotion. I think one of the good things about coming to events like this is the amount of innovation that's happening, right? And for a lot of the innovation that happens, you need something really boring. Sometimes standards fill that role of making something so boring that productization becomes the impact that you create out of that. So that's what we're trying to do in SNIA, which is to create a memory-to-memory data movement and acceleration standard that's basically independent of what kind of form factor that you implemented, whether it's on a CPU, it's on a GPU, an FPGA, an IO device, it doesn't matter. It's also instruction set ISA independent, so it kind of gives you the same stability that you have enjoyed for applications, but it also gives you at the level that you can have diverse accelerators implemented. And it's also meant to be such that it can use this memory data movement standards across different kinds of memory. And while you're doing that, you don't want to compromise on the address, the address spaces that you get the separation for. And while you're moving data, you also want to use the same kind of framework to do transformations for a lot of the memory operations that you do today. The ecosystem is developing, and we have been working on an OS-independent user space, a library called libsdxi, and we're enabling drivers across operating systems to implement this interface. So you can have one driver implement the initial initialization of it, and then, you know, you can leave it up to the user space applications to make use of this accelerator interface. So without getting too deep into this, I just want to mention that there is a 1.0 version of the spec available on SNIA that you can take a look at.

I want to connect you with the dots that, you know, the previous speakers talked about on how this could be useful with CXL. Now, certainly, there are two models you could use this. One of the simplest ways is the picture on the left, which is you have this accelerator. Let's assume this is implemented as a PCI device. It could be packaged within the CPU; it could be a discrete card. Doesn't matter. But CXL provides the opportunity to expand your memory address space, and now you need a data mover that can move your data to where the computation needs to happen, or at least move the data so that you can have, you know, a larger memory pool to work with. That's the role this data mover interface fills in so that now you can have different classes of accelerators be there, and you can enjoy the same API, the same driver interfaces that you can work with. The picture on the right is more involved and more interesting, I would say, and one of, potentially, the original goals for CXL, which was that it was meant to be for a Type 3 type of device. And so if you had a memory attached to the device, and you could also play with the cache coherency aspects of CXL, now you're not just moving bulk data with this data mover interface, but you're also doing more interesting operations because of the fact that you're now on the current link, right? And so this is kind of where we see more accelerators start playing a role, particularly on the CXL fabric.

And then this is kind of just painting the horizon on how this should look like, right? We have lots of memory. We have lots of stranded memory. We have lots of places where storage and memory needs convergence. So if you look at the picture on the right, we are looking at an architecture view where computational storage, NVMe, CXL, and SDXI all play a role in making sure you get data to where it matters the most and where the applications need it. So whether it is having a device with both an NVMe function and SDXI function, or using SDXI to move data from one device to the other device, to a memory pool, or just from one host to the other host. The picture on the top right is what I talked about earlier. The top left is all the different operations that you would do now that you have a memory data mover interface. All you need to point is a source memory and destination memory and define all the operations that you would do. For example, if you want to compress, if you want to encrypt, if you want to create a double copy, if you want to provide data integrity checks out of the data movement happening. All of that is going to be part of the standard, and this framework allows for it. And last but not the least, I cannot talk about anything without talking about AI. If you really talk about what AI memory operations are, they're allâ€”they're all tensor memory operations. You've got lots of data formats floating around for the different compute to consume. So the sky is the limit, but we have to create the right kind of building blocks. And that's kind of why this partnership that we have with the CMS group, I'm very positive and excited about. And I think I'm going to hand it off to Pankaj to kind of close it up.

Thank you, Shyam, for joining us on behalf of SNIA. And we are. We owe you a clear white paper outlining our ask. So, Manoj, our work group chair, actually has asked us to deliver such a white paper in Q4.

And that brings us to the last slide, which is a call to action. So you got a glimpse of what this work stream has been doing and who some of the contributors are. And we have a partnership here that's not just the big companies, but also startups, academia, SNIA, all working together. We are looking to adopt and refine open formats, such as Apache Arrow, so that we can build upon the work of the open source community and open formats community. We are looking to enable these core design use cases for the long term. So people like ASIC designers and high-level framework designersâ€”like Ray, CUDA, PyTorchâ€”they can have a stable interface to work with hardware designers at the bottom. And then these core design use cases, like vector databases, long context inference, some of the things that Sameer talked about. And in Q4 2024, new work on interfaces for devices to parallel compute alongside domain-specific accelerators. So, if there are half a million GPUs at Meta talking to 10,000 computational memory devices, what will that look like? Will each one of them try to talk to two or three devices, or will you be able to do collective communication operations of the kind John started to hint at? So we are going to actually start to tackle this one. Our URL is at the bottom. I'm a few minutes over, so let me stop here. Thank you for your patience, and I want to thank my guests as well. Thank you.
