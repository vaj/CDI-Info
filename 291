YouTube:https://www.youtube.com/watch?v=2sEG1NahufE
Text:
All right. Thanks for joining. My name is Michael Ocampo, Ecosystem Alliance Manager at Astera Labs.And my topic today is Accelerating AI and ML with CXL Attached Memory.Before I get started, I just want to say thank you to the Memory Fabric Forum for inviting me.It's always good to talk about CXL, all the innovations it brings and the benefits for AI inferencing and many other applications.

So as I was putting together this presentation, I was playing with some AI tools such as Llama3.And I put in this prompt, Improving GPU Utilization with CXL.And this image that you see on screen is literally the first image that was generated from the AI model.And it's kind of interesting because you see all these orange cables that are interconnected.And it reminds me of one of the products that we make at Astera Labs called our Aries Smart Cable Modules, which basically helps to extend the reach of PCIe and CXL over seven meters.So I thought it was a pretty interesting image to include into the presentation as we lead into the main topics here.Which include AI inferencing memory requirements.You know, the trends that we're seeing.Or AI inferencing servers and why it's important to improve the IO. efficiency for accelerating AI.And we did various testing with our own Leo CXL memory controller.And we were looking at all the data and trying to make sense of it.And we feel that we can really optimize AI inferencing with the help of CXL.And we can.Help to scale out LLM instances with CXL.So looking forward to sharing this data with you.

You know, most people are familiar with LLM training or AI training.But not a lot of people are really talking about LLM inferencing and the amount of memory that's required for it.So this chart on the left actually is from.Dan Rabinovits, who presented at the OCP Global Summit in 2023.And it effectively gives a good illustration of what the workload requirements look like for AI inferencing.You can see it's very memory capacity and bandwidth bound, as well as very latency sensitive.Which means if you try to scale out, you know, with some kind of standard protocol like Ethernet or InfiniBand.Um.It, it may not be the best way to optimize the performance because, um, the, the use cases require near real time or on demand to get the AI insights.Um, you can see it's basically trying to decode data as efficiently as possible.So some examples would be like GPT.Um, OPT, Llama, Mistral.Um.All of these are very memory intensive.And so the reason why it's so memory intensive is because there's a KV cache that's effectively storing the keys and values of all the previous tokens that were generated from, uh, let's say a prompt that you enter in something like a chat GPT.And what binds that is the context window.So that's setting the boundary for the KV cache itself.And this.Uh, workload, um, that I'm speaking of, it, it consumes a significant amount of memory in the order of about one megabyte per token.And, and this, this information is, um, gathered from a really good article written by a peer line art, uh, called LLM inference series, uh, focused on the KV caching.And so, um, you know, of course the KV cache size really depends on the precision of the model.So Llama 3.1 recently came out and that's, uh, it's original precision is at the 16 and the model itself, it requires about 810 gigabytes of memory and, um, beyond the model that the KV cache itself is huge, uh, depending on how many users or how, how long the prompt is and what the output is.So.So a fun example would be, let's say you want to, um, ask a question about 10 novels that, that the amount of information in those novels would equate to about a million tokens, which is about one terabyte of memory.So this is a pretty big problem that we're trying to solve with CXL.

Fundamentally, if you look at the system architecture.On the left, you have more of a traditional look of the basic computing elements, you know, you've got a CPU, you've got the local memory, you've got your storage, and, uh, for a lot of these AI, uh, workloads that rely on NVIDIA or AMD, the, the libraries and the drivers and accelerators are really centered on a GPU.And so when you, you're generating all this KV cache.You've got to store that somewhere and then be able to read from it very quickly and efficiently.And the, the system that we observed actually has the KV cache stored on cache by default.And so what ends up happening is that the CPU becomes the bottleneck.And so, uh, ideally you want to have, uh, a sizing of the KV cache or the response, and you can kind of limit that.On your, on a front end, if you're involved on the actual application side, but if you don't have those controls, I mean, you're, you're really at the mercy of how big the prompt is.So, um, the local memory may not be enough, and then you end up having to use the storage for, uh, caching, which is a lower performance, of course, compared to CXL.So when we did the tests, um, that I'm going to show later on, we effectively, um.Place the tensors on the CXL memory so that you can really drive the AI inferencing performance with this very high performance cache tier to supplement the local memory.

Um, so on the left-hand side is a system that we won an award for at flash memory, uh, summit, or now it's called the future of memory and storage.And, um.We won an award for the, the total solution actually in, uh, um, partnership with SuperMicro and MemVerge.So the system that we use is the AMD based system, AS-4125GS-TNRT.The local system.Uh, it supports 24 dims.And on the backside, it can support up to 8 GPUs.The case that we tested, we had two GPUs on one socket as a baseline.And then we had a couple drives, um, which I'll, I'll share more information on the configuration on the next slide.But what ended up happening is we had slower time to insights with the NVMe cache.Higher CPU utilization.And because of the higher utilization on the CPU side, that limited.Uh, ability to run concurrent LLM instances for servers.So with CXL, we can, we can boost the time to insights by 40%.We can lower CPU utilization per query by 40% and increase the LLM instances, um, by 2x.So here's, here's the data.

Um, so how to read this graph is you hear, you see here.Um, on the Y axis, you've got the GPU utilization and on the X axis, you've got time.So if you follow this orange line, which is basically the, um, it's, it's, it's a FlexGen engine that is, uh, driving the OPT-66B hugging face model.And a bunch of queries are, are coming in, uh, the run parameters.On the bar actually listed on the bottom right of this slide, you see PROMPT LENGTH is 512, GEN_LENGTH is 8 GPU_BATCH_SIZE equals 24 and the NUM_BATCHES equals 12.So that number 12 is, is key because when we change that number lower or higher, that would generate a lot of KV cash.Again, this is beyond the, this system, uh, requirement for them, for, for driving the model itself, which.Is already using the 122 gigabytes of memory.And then you have, uh, all these, um, these batches of prompts and outputs that, that were asking the system to, to process.And that query time without CXL is taking up to 700 seconds.Um, and then when you, we tested it with, with CXL, we're getting the full utilization.Of two GPUs.Okay.So then now that basically helps to, uh, improve the time, the insights by 40%.You can see the blue line drops down to 40%.So just looking at the, the hardware configurations really understand what's going on is again, we've got one CPU that is supporting two Nvidia L40S GPUs.So each of them have 48 gigabytes of GDDR6 or 96 gigabytes.So, so, it's not quite total, and then the local memory is 768 gigabytes.And then again, you've got the RAID0 of 2 PCIe gen5 SSDs.So what ended up happening is the KV cash exceeded the local memory as, as well as the GPU memory.And then it spilled out into disc essentially for, for the KV cache read and write operations.That's what was really pulling down the performance.And then so with CXL, we were helping to improve the IO efficiency.And so that's where the gains are coming from.

So on the flip side of this workload, we're looking at the CPU utilization here.So the y-axis, if you notice, has changed to CPU utilization.And so at peak, it was about 65%.A lot of people don't really realize that the IO operations from disk to the GPU is quite expensive from a system resource perspective.And if we could reduce that CPU utilization, we could try to run multiple instances.

So that's where we're coming from with this slide, which effectively gives you kind of a theoretical look of how many LLMs could you actually...actually support if we could reduce the CPU utilization.Well, let's go one by one here.So what we're showing here is the memory requirement for what we tested, right?So we have a KV cache and then the OPT-66B model.It required up to basically one terabyte of memory.It's approximate number just to keep them.That's simple.And you can see here in green was the GPU memory was filled.The local memory was filled.Again, there's 768 gigabytes just for local memory.And then without CXL, it basically spills out on the disk, as I mentioned earlier.And with that one instance without CXL, that is using up 65% of the CPU utilization.Now, we're going to go back to the CPU utilization.Now, with CXL, again, the CPU utilization is lower at 25%.The theoretical configuration here, what we're saying here is if we scaled up all the resources, let's say we're able to include up to eight GPUs and then max out the local memory and attach more CXL cards, you would essentially get to where...We're heading to on the very, very last column of this graph.But before we get there, without a CXL, you essentially would hit a CPU bottleneck, right?Because all those IOs and all those weights and the memory swaps with the disk and local memory, you're going to see a tremendous amount of pressure on the CPU.So by having two of these CXL instances, you really wouldn't be able to scale beyond more than two LLM instances, theoretically.Whereas with CXL, you'd be able to support up to four instances.So you can see here, we've scaled up all the memory requirements accordingly.You can see in purple, there's four OPT66B bars stacked onto each other.And then the orange bars represent the KV cache stacked up.And you can see that we should have headroom on the CPU to effectively support all of the LLM instances and possibly more, right?We'd have to test that configuration to know more.And just for completeness, you can see all the parameters that were used in the test.So it's Batch Size of 12.Prompt Length of 512.Generation Length of 8.And GPU_Batch_Size of 24.

So the key benefits are, again, faster time to insights by 40%.40% lower CPU utilization per query, which helps to improve the concurrency of LLM instances per server up to even 2x or more, just depending on the quickness.So that concludes my presentation.I'm happy to take any questions.
