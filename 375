
So it's almost time, and I hope you are having coffee and sweets, so you'll be all charged up. And I think I'll get some attention to my presentation. So today I'll talk; I am Mohinder Saluja. I work in KIOXIA. I lead the innovation on the computational storage and some of the memory expansions with Flash. So today I'll talk about RAID offload technology. This is the first time we'll be giving a complete overview. We have given this overview in different places, a part of it, but today I'll be covering almost end to end how it looks like and what the building blocks are and what are the different applications.

So this is my agenda, where I'll talk about the challenges of data redundancy and how the RAID offload can solve some of those problems, and the environment in which RAID offload can be adopted the way we are proposing, and the applications of RAID offload. RAID is the computational computation required for; for RAID has different applications. So I'll cover a couple of those applications, and then a summary of my talk, and then a call for action.

So what are the challenges of data redundancy? So, data redundancy is a critical application for aggregating the storage, and also at the same time, it protects the loss of any of the component on the storage. It depends on how many parities or how many erasure codes you have. Depending on that, the compute and the resource requirement changes. As the NVMe SSD performance is continuously going up, the redundancy applications like RAID 5, RAID 6, or erasure code, it continuously is challenged for performance and it needs more resources. We cannot change the nature of RAID or erasure code. They have to; each buffer has to be XORed or computed through some mathematical formula with each other. So that penalty has to be paid for redundancy applications. As the RAID complexity is increasing with the number of parities, we are talking there are implementations of 100 to 10,500 to 50 ratio of erasure codes. And how it is being addressed today, on the right hand side, on the top, I'm showing the CPUs are deployed, memory is required, there are solutions with GPU based parity computation and FPGA based parity computation. And all of them have their own challenges. If it is CPU or DRAM, then memory bandwidth is a big challenge, though it is sometimes catered through CPU cache, but it is there you see the CPU cache thrashing. And the lower graph shows how the memory bandwidth requirement changes just by increasing the number of parities. The first one is if you have single parity on 16 SSD configuration, each SSD, you if writing at 4GB per second, so that amounts to 64 GB of right bandwidth, the amount of RAM bandwidth required or maybe a high-speed memory SRAM or or it can come from anywhere, the bandwidth requirement will be uh, for full Stripe, Right, it will be around 280 290 gbps. If it is a partial stripe, just 50% stripe update, it is 450 Mbps. As the parity increases, number of parity, if it becomes two, for half stripe rate, it becomes 1.15 terabyte per second of bandwidth required. Or if it is full stripe, then it is like around 600 Gbps of bandwidth is required. That bandwidth has to be there to get this performance. And if it is for parity, many applications do erasure codes. Up to four parities, full stripe is 1.14 terabyte per second, and the half stripe is 1.8 terabytes per second of memory bandwidth is consumed for computing the parity. And so this memory is a big challenge there. There are hardware accelerators for compute. Compute is not a problem. Compute is available. Plenty of compute is available. There is not enough memory to feed to those. But if we are taking accelerators, if we are deploying accelerators, then they are the bottleneck. They cannot scale beyond that the design time, whatever performance was set for them. They are costly because they consume additional power, additional PCI slots. And the other solution is mirroring. So mirroring is easiest, is the fastest, but that consumes, that increases the cost of the storage to double. You have to have the same amount of storage. So these are some of the challenges of deploying data redundancy or RAID.

So what is our proposal? The proposal. So our goal was to address this challenge of increased complexity, as I mentioned in the previous slide. It has to be cost-effective, scalable, and standard-based. We don't want to build a solution in isolation. And existing applications should require minimal change. The biggest cost of any new innovation, it fails many times.. Existing applications cannot work with the new hardware, even if it is a very high-performing hardware. So our goal was to have minimum changes required to the existing applications. So our proposal is a host-orchestrated parity or erasure code compute and memory bandwidth offload to SSD. SSDs today has plenty of component. I would say 90%. It's already available, attached to every PCI device. The right bandwidth of SSDs for Gen 5, we are unable to meet the, unable to, I would say, saturate on the right path the PCI bandwidth of Gen 5. So there is spare bandwidth available on the PCI side. And every SSD has DRAM. And it is not fully used by SSD. So there is always a spare bandwidth. So we propose to use a memory-like controller-attached memory-like CMB for offload. And also we are proposing parity compute, parallel parity compute engine, as I'm showing in this bottom. So the parallel parity compute engine can take multiple buffers, as input, and their corresponding Galois coefficients or weights. And it can compute up to seven parities in one command. So we can give seven set or eight sets of buffer. But eight is not the limitation. It is just as an example. So we can give eight different sets of buffer and their corresponding weights. And the engine can compute up to seven parities in uh through one command. I'll show some of the benefits of this approach, how this approach can be leveraged for some of the, I would say, tricky writes in case of full stripes. Today, most of the time, it is full stripe write by log-structured file system. I'll show that case as well. But the parallel raid engine is very efficient in some kind of workloads, or some kinds of tricky writes on the full stripe. Then we are proposing a DMA engine for buffer-to-buffer copy. There is already DMA engines exist on device. There is one for data movement, one for command movement. And what we are proposing is buffer-to-buffer. Command DMA engine already do buffer-to-buffer, but they are not designed for performance. We are proposing that they should be designed for performance. So we are, essentially, we are adding only one parity compute engine to SSD, which is a minimal change. And then commands to control these functions. And the RAID applications can continue to use, can use these commands to offload, but can continue to manage the faults, as they are doing today. So there are many corner cases, write-hole problems. There's so many, these applications have evolved over a period of 20, 25 years. And they have hardened, those applications are hardened. And we don't want to play with them. We want to just supplement them with some additional support where the change required there is minimal, and they keep doing what they are doing at best today. So, sorry, I forgot to explain this diagram. So in this diagram, the, we, as I said, the controller attached memory, CMB. So CMB is a host mapped address space. It can be mapped to host address space, then it becomes like a local memory. And P engine is for computing the parity, as I explained, and DMAC is for moving the data buffers, not only from host, buffer to SSD, CMB, or memory like that. Or it can be also move data from one buffer on the SSD, on one CMB to other CMB, that is peer-to-peer DMA. If you have attended Eideticom's presentation that Andrew presented, a lot of questions are already answered, Andrew is here. And so a lot of these challenges are already answered. And this is part of the Linux standard kernel, so the building blocks are already there, and they are proven, already.

So we have developed the proof-of-concept using these commands. As I said, we have leveraged most of the standard things available on NVMe. We implemented a buffer-to-buffer copy command like peer-to-peer DMA, but it is a slightly different version. Which the buffer copy command copies the buffer from the host or any SSD or any CMB to another CMB. It can move data from one CMB to another CMB by using the DMA engine and a parity compute engine, which allows a number of parity blocks as input, their corresponding weight as input, and we can compute, multiply the buffers with those weights, and then XOR them. Or if it's a simple parity like RAID five, then we don't need to multiply the weight because the weight is one, and we simply XOR them together. The command activates the compute engine and based on the parameters, it will do the computation and place the output buffer to a designated location that the host has managed. So it is completely every single step is host controlled. There's uh SSD behaves like a dumb device that it is does today, uh for this purpose, it does many smart things. I'm just talking about in this context. So it is the same thing, it behaves like a dumb device, and all the orchestration is by application. And the... So each parity is, as I said, it goes to its destination buffer.

So quickly I'll go... So I have explained, but I'll go through the flow also, how the data flow happens. So right now, if you look at on the right-hand side, there is a new data which is coming for an NVMe device. It can work for NVMe or fabric environment as well. The grey is for command and the blue is for data, uh, these arrows. So we take a three-step approach. This is an example of read, modify, write on a stripe. So a new data has come, and we have to update a part of the stripe on one of the SSDs. So how we will leverage XOR, parallel XOR engine for this purpose, how we will leverage CMB for this purpose, I'll go through that flow. So we do three operations at the same time. Move the new data into CMB. Of the... Any SSD because we can do peer-to-peer data movement. But in this case, say I'm taking data on as a step one to one SSD, the new data goes there. The old data also is required because I want to minus that data from the parity. And also the old parity is required because I will update the parity with new data. So all three steps will go in parallel. Application is orchestrating these steps. The lower one, read, old data and old parity, they are existing NVMe commands. We don't need to implement anything new for that. Then we... Application issues a parity compute command. So parity is computed and placed in an appropriate buffer as you can see to the step two in the arrows. Once the... Parity is computed, we have used one command. Though we need to XOR these three buffers together, because of the parallel XOR engine, we just give one command with three buffers as input. In case of RAID 5, only one wait. So they will... It will compute the parity and then we issue a write command. Simple write command which exists today to move data from the destination buffer in CMB to... Any of the peer SSD for parity and for new data. In this case, parity is on the other device.

So we did a PoC with this. I'll go through more details. I just wanted to bring it upfront. So we did a PoC with this, with mdraid 5. mdraid 5 is not very efficient software. Everybody abuses it. We also took it because it is easy, open source, and we can basically do the development very easily and show the concept. We have in FMS, we presented with other very efficient RAID software, which is there also; we showed the same benefits as we are showing with mdraid. This was a five SSD configuration. The first column is without offload. The other column is with offload. It was a full stripe write of finder 12 bytes each, bits, bytes on each SSD. So, and we... Because it is a PoC platform, we had to restrict our IO rate to nearly 1 Gbps. So we could see... So the details are given down there, here. It is around 950 megabytes per second. We had to throttle it to that level. And the CPU utilization is 12% saving. This is a very inefficient implementation of mdraid. And this is a very small data rate. It is not very huge. One Gbps today is nothing for CPU to compute. So we'll not see much difference because the RAID state machines are heavy. They... They use a lot of locks and things like that. And mdraid do 4K based computation for every XOR compute. But the bandwidth penalty you can see very easily is 10X as I was showing in other slides. It is almost 10X with other prototype with very efficient software that I said... In FMS we presented. And here also we are seeing the same benefit. And because of this DRAM bandwidth challenges, many... We have come across customers or feedback where the application is not able to use the system for any other purpose than storage. Because it consumes that much of bandwidth. And it is difficult to change because that's the nature of the RAID offload or Erasure code.

So, with these results, I go forward and explain some other benefits of parallel parity computeengine and its use cases.

So, it's an example of multi-drive, partial stripe write request. Suppose an overlapping request comes, then we'll have to follow this step one to six. I'll just high level, I'll explain. So, this to the top buffers, D1, I have to read their corresponding parity, old parity, corresponding old data from two different disks. And then I'll have to XOR them together. I get another parity. Another data which will be intermediate. And then again I'll have to do, read the second overlapping stripes data into the host memory buffer. And then this intermediate buffer and this D2A or D2B, I have to again XOR them. And I have... It will be all serialized process steps.  How we can solve this with RAID parallel parity engine? So, what I can do is I can take all the... I can read all the corresponding buffers at the same time. I can place all of them at the same time in their corresponding CMB. And I can create either multiple XOR commands because I can take the benefits of multiple XOR engines available to me. Or... I can create one command where I can give all these buffers as input. And I get multiple parities as P, A, B, C. Basically, it's the part of the same parity, but it is split into three parts. So I can do three parallel calculations at the same time. Or I can give all these buffers into one XOR command because I can compute multiple parities at the same time. So... I can... go for either high throughput or low latency. So... And we avoid all the serialization steps for 5, 6, in this case. So, it eliminates the serialization and improves the overall performance of the system.

Second example is the multi-drive full stripe write. This is today. We do full stripe write is we... XOR each buffer D0 with D1, D1 with D2, D2 with D3, and then we come up with P, final parity. So it is all a serialized process. Whereas with the parallel XOR engine, I can do it two ways. One is if I want high, low latency, I want to do it very fast, then I can split vertically all the buffers and feed it to four different SSDs, and they can compute it in parallel, and I get partial part of the parity in all the SSDs, and I can just use one single write command where I use SGL or PRP to describe as a source of PAPB, PCPD, all these four buffers as source. And destination will be my destination SSD. So I can basically do compute very fast. But if I want to go for throughput, then I can issue one XOR command giving all buffers as input, and that XOR command will be able to compute the parity. But it will be a larger amount of buffer data, so it will... It will take more time. But then you get the power of multiple SSDs, so each SSD can give you throughput of whatever rate the specification of SSD will have. The other benefit of this is that I can also compute multiple parities at the same time in the same fashion. Basically, I can have, say, similar in latency fashion, if I want to compute very fast, then I can, to every XOR, I can give multi-parity command with their corresponding rates, or I can give multi-parity command to one XOR engine for throughput.

This is another example of parallel XOR where there is a four parity for erasure codes to be computed. There is an equation that needs to be used to calculate these parities. There are four parities, PX, PY, P0, and P1. The PX parity is all the X0 to X2 buffers. They are XORed together, so this is one parity for only this set of drives. This can be simple XOR. Similarly, for Y, if any drive within Y fails, then PY can help to recover. If any drive within X fails, then PX can recover. And then there is a parity of all the buffers of X0 to Y2. I can compute additional parities, P0 and P1, which is to address if any of the two drives fail, then I can recover from using this P0 and P1. There is an equation for this. There is a simple buffer. This is a simple weight. Galois coefficient is one, because it's simple XOR for PX and PY. But for P0, they have weights for every buffer. And I can just pass on these weights to my XOR command. And I can, and then corresponding buffers and the output buffer, and the operation is XOR, because the weight will be, when I pass the weight, it is automatically computed; the buffers are multiplied with their corresponding weights. Similarly for Y, P1, and then I can just send one command to SSD with all these four requests and generate four buffers, four parity outputs with this.

So, what is there's another benefit of parallel XOR? It's rebuild with minimum overhead. The rebuild is a big challenge; it is always resource-intensive, the whole system chokes when you're rebuilding, but with this, even the drive which is being rebuilt can participate because it also has a compute engine on it. And each parity command can build multiple stripes because we can create multiple stripes at the same time. So, I can create the same kind of parity, eight parities in this case, at the same time. And the system does not pay any compute or DDR penalty. There's no cost paid by the system; and it is all basically shared by all the devices on the system. We can build the destination drive at its best performance. Whatever the best performance that drive gives as a sequential write, we can build at that speed without consuming any system resources, without consuming any, without paying a very big penalty on the system's performance at that moment.

So, what are the different environments this can be adopted? So far, we have seen the, what is a concept, what are the different constructs, and we have seen the benefits of parallel XOR. We can rebuild faster; we can handle some complex cases by simple commands. So now I want to share some of the environments we think it can work, and we think it is flexible, and this solution is just offloading memory bandwidth and compute; rest everything is the same. So, it is a very versatile solution; it can be adopted in any environment.

So I'll take an example of with software-defined storage. In this case, on the left-hand side, this is a log-structured file system where there is a RAID 1 of three SSDs. This RAID 1, any new data comes; it is first logged in high-speed SSDs as a mirror, and at some specific intervals, and the idea is that any three, two drive failure is tolerated in this, in the log, as well as in the capacity tier SSDs. So when the new data comes, it is logged in this RAID 1, and later, at a later point, it is destaged from RAID 1 into capacity tier when it stabilizes, so all this data, full stripe data, is read into DRAM, system DRAM. You can imagine if it is connected on a network, it will come from the network; if it is connected or a PCI switch, it is consuming all the PCI bandwidth for just computing the parity on this log. This is a log structure which is already on SSD. And the data is, then, XORed by CPU or some accelerator. It does not matter where that computer is coming from, but the penalty is already paid for this one. And after it is, parity is computed; it is placed on the capacity tier, either on the network, through RDMA or RPC, or it depends on your RAID configuration. So we can eliminate it, all this penalty of DRAM and CPU compute or any accelerator because with the RAID offload. So with RAID offload, since data is already on SSD, we should not pay the penalty of moving data over PCIe. We should not pay the data of moving data over the network. We can simply read the data in local CMBs, and imagine it's all mirror. All three SSDs can build in parallel without affecting its performance because this is not IO path. This is a different path. This I mean to say is read IO will be used, but it is not in their mainstream path. The computation, computation is a separate controller. So we can read all the data into local CMBs, three SSDs, and they can compute in parallel and multi-parities. Each one of them can compute two parities. So basically at the same time, six parities can be built from this mirror devices. Or from this mirror devices, we can also use the local tiered SSDs also. They can also participate in rebuilding or participate in computing the parity. So it depends on the configuration. It is highly flexible. We can read data from RAID 1 into CMB of any of the peer device and compute the parity. So no penalty is paid by the system. It is only one time when data comes into SSD. So this is one environment. It's a log-structured environment. It can be easily leveraged.

So the next is also a case of XPU or DPUs, there are configurations these days where there is a, the XPUs or DPUs aspire to offload the complete storage stack and making the storage systems almost CPU-less. So there are storage services, which are being run on the DPUs, and they expose NVMe or fabric devices, the XPU will be challenged for the performance as the network bandwidth increases. We are talking about 200, 400 or 800 gigabits of network. They will again start seeing the same constraint that we are seeing at the system, multiply that memory requirement by 10x. So they will have to increase the memory bandwidth availability either by expensive components like HBM memory or something else. But they will either put a lot of memory or a lot of SRAM, so it will increase the cost of DPUs or XPUs. But with the offload, we can share the load with them, because the SSDs' advantages, they are many, whereas the DPUs, CPUs are few. So we can take the multipliers of the benefit of many SSDs, and we can move these services, the storage services can move to DPU and the compute can be offloaded to SSD. As you add more SSDs, it scales linearly. You don't need to, there's no need to add additional CPU or memory to expand the capacity of your storage server. It is only the application. We'll have to handle more commands and more maybe states. But it scales linearly with every SSD comes with enough compute. And we are not saying CMB has to be very high-performing like DRAM. That's not the requirement, because our goal is to meet the SSD's right bandwidth. Our goal is not to build a very high-performing DRAM on the SSD. So that's not the goal for us. It's, we need just enough compute and DRAM bandwidth.

So these are other kinds of RAID. There are many kinds of RAID that we came across, and the conventional software, even hardware RAID also can take benefits because it's the command transaction. ZFS RAID Z also can take benefit of it because just a memory buffer, variable stripes. It is up to the application, how it wants to manage it. Decluster RAID. RAID 5 with mirroring is possible. Even mirroring also is offloaded because CMB bandwidth will be consumed, not the system memory. vSAN, as I showed you, LFS log-structured file system, and or any other data protection scheme which use multi-parity EC, all that is possible to offload with this technology.

So, those were the RAID implementations, different implementations. So, now I will go through some of the applications, not some, maybe one application so far, quickly.

So, it's a data scrubbing. Data scrubbing is to ensure the data integrity at rest. Data is on SSD or on storage for a long time. And we want to ensure if the data is good, say after 10 days, 15 days, or at a regular interval, somewhere the frequency is very high where the data is critical. So, the scheme that is deployed is, in this case, it is a parity recompute method. Some places there is a hash or checksum, depending on, let's say, object storage, file storage. But for... For the underlying storage where RAID or erasure code is used, generally it is recomputed. And we... So, these are the steps that are taken. Move the data to the compute element, which is nothing but read into host memory, and CPU will compute, and it will do hash checksum or parity compute and compare against the expected result. So, whatever result we had stored at the time of... of creating that, or saving that data, that's a golden validation of that data. So, it is compared against that data, and then if it matches, then it is discarded. Otherwise, it is... Recovery process starts. And this is an unnecessary cycle. And generally, it is avoided, because it consumes, with the production time, workload, or it is done in the night, but that is best effort basis, whatever can be covered is covered. So, in this case, I'm just showing the P on the right-hand side. The parity and the data are XORed together. If the output is zero, then the data is good. Or, if it is a Q parity, means a weighted parity, then the corresponding weights are multiplied and XORed together. And if that is zero, then that parity is good. So, in this case, if I have 96 terabytes of data on the SSDs, then through PCI bus, through network, and through CPU, the data will be, all the data will be moved, and 192 terabytes of memory subsystem will be used for scrubbing, because first, data is read into DRAM, then from DRAM, CPU, reads the data for computation. So, it's a double bandwidth the DRAM has to pay.

So we are proposing a scheme to use RAID offload and parallel compute engine. So, what we are proposing is, there are multiple SSDs, each SSD can participate in, in data scrubbing. The first step is read the data vertically, means, these are stripes. Each one is a this horizontal is a stripe, but I can split my stripes vertically and create and read them into CMB, compute the vertical parity of those corresponding stripes, and I get an intermediate parity. So suppose they are 256 parity I am reading, then I will get intermediate parity only one output because suppose I have broken 16k into 4k each, then this is only 4k that's 16k are processed within CMB. This 4k I'll move across horizontally, I'll bring them to one same one SSD and XOR them together. Now I should get zero output if I get zero which means the data is good and I don't need to do anything. If data is not good then I'll have to find out which stripe is bad, and with this we already have a PoC running and we have with we could finish the the data scrubbing 91 seconds we consume significantly 10x less DRAM bandwidth, CPU utilization was low, L3 cache misses because now CPU is not doing this computation so cache misses were low, and PCI bandwidth is also 10x reduction in that workload.

So, it is a... so, I just summarized because we have 10 minutes. So, it is cost-effective, scalable, and sustainable because these engines are not very expensive. We are already paying the price of having this PCI device attached to the system. We don't need any additional accelerators. This is good enough; it can give you good enough bandwidth. It scales linearly with the number of drives. No need to add additional CPU or memory for any additional drive. And existing applications can offload, without with minimum change, rebuild at maximum performance, and data scrubbing saves significant system resources.

So we are open to collaborate; to we have a PoC platform available with a single XOR engine, and we are ready to collaborate. Please participate in NVMe standard discussions; if it will be great if we can give/get the feedback and standardize it as soon as possible, so that the benefits of this technology can be more democratized, and everybody can use it. And we are also exploring other offload functions on SSD. The idea is to have just enough compute and bandwidth on the SSD to offload—and it is a standard-based host control SSD will have an accelerator, and there is an offload brief that will be available. I apologize; I have not uploaded this presentation. I will upload it after the session because I was not in good shape to finish it in time. Okay, that's it from me. Sorry for the long session—oh, is there any questions, please?

There was a session on SDXI that does this new thing. I was wondering why you chose to add a new command instead of using existing technology to do the same thing. And then you used the word 'compute,' 'compute,' 'compute,' and we have this thing called computational storage, which also intertwines with all of this. Can you explain the thought process of how do we take all these industry standards that are out there and how do we make them all work together instead of reinventing the wheel over and over again?

Yeah, that's a good question. And we are engaged with the SDXI group as well, because this was being developed in parallel to when SDXI is evolving. So both are evolving, and we are in the standards committee right now. It is under review, and we will ensure that it is a standardized solution. We will leverage computational storage because it has been evolving over a period of time. So we have created this concept. Now we are taking it to the standards body to standardize it and engaging with all right partners. So it won't be something which will be only a KIOXIA only solution.

You said two different comments that are kind of related. You said that the DRAM was generally idle on SSDs, and you said that this did not impact the performance of the SSD very much. Could you speak to that a little bit more?

Yeah, so as of today, as we are seeing the capacity of SSDs increasing, the DRAM quantity on SSD requirement is high. But the bandwidth, there is a spare bandwidth on DRAM on SSD. That can be leveraged. But definitely as we evolve, maybe some additional DRAM bandwidth will be added for this function. But as of today, if we have, like KIOXIA CM7 has CMB already available on SSD. And it can give you four to six, four Gbps of DRAM bandwidth, which is freely available. So that much bandwidth is available. I don't need DRAM, high bandwidth performing DRAM on this, for this solution. As I said, we just need enough for to meet that compute requirement.

So, I like the idea of reducing the, let's call it, traffic on the PCI Express.

Yeah.

Aren't you creating different single points of failure now, with one drive being in the path of computing the parities? What happens if this drive dies?

It is a stateless compute. It is not stateful, right? You are just giving a buffer to SSD to compute. If this drive dies, data is somewhere in NVRAM or in the log, right? So the data is, you can always, that's the benefit. Actually, you can have many failures in terms of compute side. So that's not a problem. It's a stateless.

Do you, can you tell me more? If the DRAM needs to be in memory, does that add more cost, uh because of you know, now you need to have more data integrity assumptions data being in the CMB?

Yes, so it will be, if you want enterprise-grade, then enterprise-grade DRAM will, anyway, will be there, on those, on those devices.

So you're not adding you?

Yeah, we don't need to, yeah, yeah. It depends on the, the class of drive, and correspondingly, you will have to manage your risks.

So, in the very beginning of the concept of the sequence, how does it work? So, I'm not an expert on the, how to communicate the data, but how to communicate the data?

So these are data buffers, CMBs are data buffers, like any other local memory. It is mapped to host address space. If you look at this diagram, the all-CMBs map to host address space. It is; the source and destination description will contain the data source: a buffer which can be on CMB and the destination is a flash, or otherwise, from flash the destination has to be a memory buffer as long as it is in host address space, you can give just a read command and destination buffer as the CMB memory.

Yes, that's that's the idea. We don't want to manage that through SSD, yeah. Okay. Last question, I think we are out of time almost. Yes, okay. Oh, okay. I okay, yeah. So, if there's no question, I will okay. Thank you.
