
All right. It feels like last year when Samir overran and I had to basically skip all his remaining slides. But yeah, anyways, I'm Vikrant. I'm a hardware architect at Uber, and I'll be talking about a proposal for a benchmark for performance analysis of pooled memory when it is used for one of the use cases for containerization and virtualization, which Samir briefly touched on as well. Before that, a quick shout out to Reddy and Manoj, of course, for herding the cats over the past 12 months. I remember last year we were just talking about use cases. We had thought bubbles and ideas, but now we have come a long way, and we are talking about benchmarks and actual devices and use cases. So tremendous progress. I'm really proud to be part of this group and hope the traction continues.

So as a quick overview of the agenda, I'll talk about the approach at Uber that we took when we wanted to come up with a benchmark that will be representative of all the thousands of microservices that we have in our fleet and allow us to evaluate different hardware solutions that can give us a good signal of what our microservice performance will look like. And I'm going to then pivot and say how we can apply that same concept now that we have pooled memory appliances or native memory expansion alongside the main DDR main memory. I'll show some results and end with a call to action. And I have been told to also finish in ten minutes, so I'll try to skip some of the slides as needed.

So anyone who has operated container clusters at scale with thousands of unique services or thousands of VMs with different performance profiles always run into a challenge of figuring out, hey, what's the right benchmark to use? There are several approaches, and some of them could include using the most resource-intensive VM or the most resource-intensive container or using the top ten containers in your fleet. But again, these are not synthetic. They are hard to be -- they are not portable. They are not repeatable. They are not reproducible. So at Uber, we ran into the same challenge a few years ago when we had a lot of hardware being evaluated for our container clusters, and we wanted something that was synthetic and allowed us to also share it with our vendors and figure out the right signal for which hardware is actually meaningful to deploy in a stateless fleet. So again, there are a lot of tradeoffs with each of these approaches, but for us, the key was to have something that was synthetic and off the shelf.

So what we did was we decomposed the performance of containers and all the unique microservices in terms of its basic definitions, looked at things like IPC frequency, memory bandwidth, latency, using Intel's top-down telemetry collector, and then quantified the performance from the lens of the hardware. So when you have thousands of unique services, what we realized was over a period of time, as you collect data on different hosts over different times and different zones and different types of hardware, they're nicely converged into a pattern for the CPU stall cycle usage. And we then used the very popular SPEC CPU 2017 suite and tried to map it to the components within that that also exhibit a very high-fidelity CPU stall cycles. When I say stall cycles, when you look at the core pipeline, you can break it up into your instruction data and you can break it up into your dependency on memory. And what we found out that if you look at the SPEC CPU components, some of them are very less reliant on memory, such as x264, which essentially runs through the core caches and have very high IPC. And on the other side of the spectrum, you have SPEC CPU components like omnetapp or mcf or gcc, which have very low IPC and very high reliance on memory bandwidth. So when you start mixing and matching the SPEC CPU components, we ended up with a recipe that could mimic the core cycle usage that our stateless fleet exhibits.

So extending that same sort of topology and templates to the pooled memory, now instead of having your omnetapp copies or your x264 copies pinned to local DRAM, you could now do that for your pooled DRAM appliance or your native memory expansion device. You could get very creative and now start mixing and matching your x264, which are cache bound, along with omnetapp on the opposite cores or the adjacent cores, and do the same thing with local DRAM and pooled DRAM, and essentially start collecting all these metrics that will allow you to get a good signal of what your performance profile looks like and where you need to optimize and tweak to make sure that your containers and VMs run properly.

So we are fortunate enough to get a set up from one of the partners in this group, and it was based on the Intel Sapphire Rapids, or the official name is 4th Gen Xeon Scalable Processors. And it had six NUMA domains, out of which four are zero-core NUMA. And the first time I did this lscpu count, it was pretty cool to look at. Of course, two NUMA domains are tied to the two Sapphire Rapids sockets. And overall, it had 512 GB per NUMA domain and four CXL devices, two hanging off each socket.

So I added some more set up details, mostly caveats, because the numbers that I'm going to present have to be taken in the context. This particular system has eight DDR5 channels, so it's almost like 200 gigabytes of total bandwidth per socket, and then two CXL devices per socket, which is roughly around 40 gigabytes. And this can, of course, change based on the system vendors having more devices or smaller capacities per device. But just wanted to make sure that people understood that when I show the results, it's one-fifth the bandwidth for the CXL devices. And that was not our intention. That's the system we got. In terms of the idle latency, as you know, CXL devices are designed for around a NUMA hop, and that's essentially what we saw when we looked at the MLC idle latency. The other caveat is the CXL devices are early release. They are not production, which is why I'm not able to call out the vendor or give credit. And again, there are several optimizations within the firmware and the device itself that could improve the loaded latency. The benchmark itself we used out of the box. There are no special configurations that could benefit either of this. And the goal was to ensure that our recipe that we use at Uber for our current container microservices can run out of the box and allow us to play around with pool memory solutions and figure out what the right templates could look like.

So I'll skip this slide given we are very close to time. But the takeaway, the slight tweak that we had to do was when you're using local memory, we usually use membind as our flag. When you're using pool memory or CXL native attached memory, we just had to replace that with interleave with the NUMA domain number. So that was a slight change we had to do in running this particular benchmark.

So the results. I ran X264. As I said, it should not depend too much on memory. And that's exactly what we saw. And as you can see, the scores, irrespective of local DDR, CXL, using CXL plus local DDR, the scores all remain the same. And this is very important when you think about what Sameer and Manoj mentioned on TCO and looking at solutions that will really move the needle for the business case. CXL and DDR can be used transparently when your microservice does not depend heavily on the memory. So this was very good to see. And as you can see, the breakdown of the core cycle usage shows that you are essentially retiring the same amount of instructions.

On the other hand, when you have a memory-bound workload, as you can see with the local DDR, you get a score of 1.4. With local CXL, you get 0.5, which was expected. Again, this goes back to the bandwidth and/or latency impact, which could be tuned further. But what was interesting to me and what was actually the takeaway was when you start using local DDR and CXL transparently, we are within 42% of the local DDR memory. And when you have noncritical microservices that could tolerate some latency, this is really important. And something that can really start to get baked into your TCO solution, especially if you look at our fleet, we have certain services that are x264-like, which are cache-bound, some services that are memory-bound, like the omnetapp. And when you decompose your fleet and figure out the weightage, this starts to becoming real. And if you are within 20%, that's a TCO benefit. And certainly CXL plus local DDR can be used within our Kubernetes clusters. And again, this is just a starting point. I'm just a happy, glass half-full kind of person. So we are not 100% behind. We are within 40%. So this is very impressive.

So call to action. I'm looking for help. I'm going to upload all these benchmarks to the folder. And I'm hoping that vendors and other partners can sweep through similar templates. It's super easy to swap out omnetapp or mcf or gcc based on the use case that you have in your organization. And then collectively come up with a list of signals or metrics that all of us can use to compare different solutions and really identify what can be the right size TCO-- right size solution for the TCO benefit. And that is it on time. That's all I have. Thank you.
