
So hi everyone. This is Glenn Ward from CXL. Just a quick note, we are having a technical difficulty  that I think we will probably resolve in the next couple of minutes. So if people could just hang tight, we will start in about 2, 3 minutes. Thanks all.

Debendra: Hey guys, this is Debendra. I'm on the phone line.
 
Susan: Good morning Debendra. Thank you.

Glenn: Technical problem solved. Okay, are we ready to start?

Debendra: Let's do it.

Fantastic. Okay, so why don't I kick things off. I'll give a brief intro  and then pass over the reins to my able-bodied colleagues, Kurtis Bowman and Debendra Das Sharma. So my name is Glenn Ward. I'm one of the task force chairs for Compute Express Link. I'm really excited about today. Today is the first in a series of webinars  that we are producing to effectively educate the community on what CXL is,  the use cases we target, and how users can consume CXL, its specs, IP for CXL,  and effectively start working with CXL compliant devices. A few things on CXL in general, so what we are doing in the consortium  is that we are designing, developing, and constantly trying to advance  the state of the art in high-speed CPU to device, and CPU to memory interconnect technology. It really hinges on a few things like cache coherency, and a few target use cases  and scenarios that we will be talking about that we are really excited about  because it opens up a whole lot of opportunity for cloud, for HPC, for the industry at large. So far we've got at last count, it's tough to keep up with the count  because the count keeps changing. We have 96 member companies and growing  which is pretty phenomenal. CXL is the first open standard we know of for this architecture  in which every major CPU architecture provider, so Intel, AMD, ARM64,  and OpenPower have come to the table to join and promote a standards-based  within the node memory and accelerator interconnect. So that's absolutely something to celebrate. So I think that's kind of the basics for an intro. Why don't I pass the reins to Kurtis and Debendra, and you can take it from there.

Greetings. My name is Debendra Das Sarma, and I will be delving into the technical details of CXL.

For some reason I'm not able to see the slides, so I will call out the slide number  and we'll make that one work. So we are in slide number four. When we look at the industry landscapes today, we see some clear mega trends emerging. Cloud computing has become ubiquitous. Networking and edge computing are exploding. And they're increasingly using the cloud infrastructure. AI and analytics to process the data is driving a lot of innovations across the board. All of these are driving the demand for faster data processing. We see increasing demand for heterogeneous processing as people want to deploy  different types of compute for different applications. So whether it is general purpose CPUs, GPGPUs, custom ASICs, or FPGAs,  each is important in order to solve these classes of problems that we are seeing. There is enough volume in each of these segments to drive different and proprietary solutions  for each of these different segments. So what we are seeing is increasingly more and more people are deploying a combination  of these different types of compute in their platform. Now, in addition to the demand for heterogeneous computing,  we also see the need for increased memory capacity and bandwidth in our platform. And this is nothing new. People want more and more memory. They want more and more memory bandwidth. And it is getting to a point where we need to be able to satisfy that bandwidth demand  as well as the capacity demand. Now, at the same time, there have been significant technical innovations  in the storage class memory that have resulted in these memories  approaching DRAM-like latency and bandwidth, along with their non-volatile characteristics. So with that, we have a class of memory that is sitting between the DRAM,  traditional DRAM, as well as the storage or SSD. And we need to think of it as a separate memory tier,  offering some compelling value proposition due to the capacity and persistence. So, for example, now you are able to store an entire database in this new memory  and your search becomes a lot faster. So all of these mega trends that we are discussing here today,  they take advantage of these types of memory,  in addition to the evolution that continues in the traditional DRAM memory and storage space. So heterogeneous computing and memory, these are the two key elements  that we see driving the need of the compute landscape going forward. Compute Express Link is defined, ground up, to address these challenges  in this evolving landscape by making both heterogeneous computing,  as well as the memory efficient. And it's meant to sustain the demand of this computing landscape for years to come.

So we'll now go to slide number five. The question is, you know, why is there a need for a new class of interconnect? Could we have just simply leveraged all the existing interconnects? So let's step back and look into some of the existing interconnects. So CXL, I'll start off by saying, which is Compute Express Link,  is built on top of PCI Express infrastructure. And it fully leverages PCI Express. What it does is it overlays the caching and memory protocols  on top of the existing PCI Express protocols. So it runs on PCI Express PHY, it runs on PCI Express channels,  and we will see more of those as we go through this webinar. As we all know, PCI Express is the ubiquitous interconnect technology,  and it spans the entire compute spectrum, right? Everything from handheld to laptop to desktop, server, coms,  everything uses PCI Express. So what you see on the picture on the right, on the top right,  it shows the system level view with PCI Express. So in a typical system, you will have memory that is attached to the CPU. These are typically DRAM kind of memory. And they're normally mapped into the coherent memory space,  write-back memory. And there what happens is your data consistency is guaranteed by hardware. So the memory that is on the other hand,  if you attach memory to a PCI Express device,  PCI Express also is a memory mapped device,  so it's called memory mapped I/O. So that memory gets mapped into the system memory as memory mapped I/O,  but it cannot be cast in the system. PCI Express is a non-coherent interface. So you've got two types of memory. The memory that is attached to the processor can be cast in the system,  whereas the memory on the I/O side typically is not cast. So they have different semantics. PCI Express has its own ordering rules also that we need to be cognizant of. So for example, when a CPU wants to access the DRAM memory,  what it can do is simply cast the data, access the data from its cast,  keep accessing it as many times as it likes,  and later on do a write-back if it has updated the data. On the other hand, if the memory is attached to an I/O device,  what the CPU cannot do is cast the data present in that memory. So whenever the CPU has to access that memory,  it has to do an explicit load or store that goes across the link into the device,  and the device will do the read or the write,  and then the information gets into the memory or gets from the memory into the CPU. So that's the difference between the two sides. Now on the other hand, when an I/O device needs to access memory  that is in the DRAM, it also has to send explicit read and write transactions. These are known as direct memory access. So for example, when an I/O device wants to do a write,  it sends a write on a PCI Express link. It hits the CPU's root port. What it does is the root port inside the CPU, effectively it has a write cast. So it goes ahead and fetches that line, gets it into the write cast,  performs the write operation, and later on does a write back from the write cast. So effectively what the root port in that case is doing is it's doing a protocol conversion  so that it is following the producer-consumer ordering model of the I/O side,  and also at the same time it is satisfying the cache coherency model that exists in the CPU. So it's the same way when an I/O device wants to access memory in the other side,  it is doing through what is known as peer-to-peer. So what you see here in the picture on the top right, on the right side, is what I mentioned. Devices doing DMA, going into the memory, whether it is read or write doesn't matter. On the CPU side, since it is uncached memory, those are going out as memory-mapped I/O reads  and writes on the PCI Express link, and of course device-to-device goes through as PCI Express peer-to-peer. So the producer-consumer model that we just described here, it works really well for a wide range of I/O devices,  especially when you are performing bulk transfer, such as those involved with traditional storage,  or you are moving data in and out of the networking interface. It is very, very efficient. The bandwidth efficiency is very good. It works really, really well. And we definitely want to preserve that model for those kinds of usages. There is a huge amount of software that has been developed over the decades. Now, however, with the trends that we talked about in the computing, what we are seeing is a class of accelerators,  for example, are emerging that want to work on a fine-grained sharing of the data with the processor. So for those kinds of things, what we need to do is be able to augment the existing mechanism in PCI Express  and allow for that device, for those devices, to be able to cache, let's say, the system memory,  just like the CPU cores are able to do. So that's the part that you see in the picture as the write-back memory getting extended  and the memory load store in the right picture on the bottom one. And the devices that do not need to do this kind of DMA in and out of memory,  but want to do this collaborative processing, can use this model. So that's the reason why we need to be able to bring the coherence semantics into the I/O side. And the same way with the memory that is attached to the device. You want to be able to map that memory into the cacheable space so that any of the CPU cores  or any of the other I/O devices that have got this caching semantics should be able to take the memory  that belongs to the device and be able to cache it locally. And that way you're going to get a lot more efficient usage out of your memory. So that's the need for this new class of interconnect, which is what is present on the left-hand side. It allows for efficient resource sharing. We can share the memory pools across these different entities, computing entities,  and it results in enhanced movement of operands. You don't have to go through a DMA operation, transfer an entire data from one computing element  to the next computing element, pass a flag, do all of those. The fact that it is coherent memory allows you for the data to move seamlessly,  just like it does today between the different cores or the different CPU sockets. And this will result in significant latency reduction and enable us to have disaggregated memory in the system. So what the industry really needs is an interconnect that's open standards-based  and that can comprehensively address these challenges that we present. So the picture on the top is where we are today with the standard load store PCI Express I/O. We want to augment that and get to the picture in the bottom. And hopefully through the end of this webinar, we would go through and work how we are achieving that.

 Next slide, please. So I'll go through an overview of the Compute Express link, delve a little bit deeper.

Let's go to the next slide. So CXL is a new breakthrough, as we have talked about. It's a high-speed CPU-to-device interconnect. It enables for, you know, the speed is 32 giga transfers per second. It's built on PCI Express 5.0. So for example, if you have a x16 link, then you get 512 gigabits per second in each direction,  which is a terabit per second aggregate bandwidth. That's how much you are going to get for a device. So it's a very high-speed, high-bandwidth interconnect. And it's meant for all of these efficient usages across a class of workloads and also enables memory. And as we talked about, we maintain memory coherency between the CPU memory space,  as well as the memory that is attached on the devices. So this allows for more efficient resource sharing, and you can move your workload seamlessly back  and forth between the different computing entities without having to involve software  and things of that nature. CXL is an open industry standard. CXL 1.1 specification has been available for a while now. In the consortium, we are working through the next generation, which is CXL 2.0. And we will continue to innovate. The innovation does not stop with 1.1. We expect this to be a thriving and living interconnect that is going to keep evolving  to meet the needs of the next generation of our compute that we're going to see.

Next slide, please. So the picture that you see here shows the interconnect at different levels of the data center  looking outside in. So if you go from the top to the bottom, it goes from the outside in. A data center comprises of racks of servers. Each server in the rack is a single domain or a single node. The interconnects at that node level are CPU-to-CPU symmetric coherency links. And these are, you know, every vendor has their own cache coherency links. You build two sockets, four sockets, eight sockets system that work, that can work collaboratively  on a problem. So that is there in that level, the place where you see the processor interconnect. You also have memory over there. You also have PCI Express. And CXL is another one that is going to fit in that space. So as we said, it's an open industry standard, high bandwidth, low latency. We will talk more about the low latency. We of course are bringing cache coherency into the device side. So it's a coherent interface. Fully leverages PCI Express. We really believe in adding only things that needs to be added from an architectural perspective  and just leverage the rest. And you know, we are targeting high performance compute workloads to name a few, AI, machine  learning, HPC, coms infrastructure, things of that nature.
 
Next slide, please. So let me talk a little bit more about what is CXL, right? As we said, we use PCI Express 5.0 as the basis starting point, which is a 32 giga transfer  per second, 32 gigabit per second per lane interface. You can have up to 16 lanes. PCI Express 5.0 specification defines something called alternate protocol mechanism that you  can negotiate. So we leverage completely the PCI Express physical layer, everything from its link training,  the circuit, the channels, the retimers, the entire infrastructure gets reused. What you envision is a processor having some number of ports. In this example, we show just one port, a x16 connector. And you can plug in either a PCI Express x16 card, it will just work as PCI Express, or  you can plug in a CXL card and it will work as CXL. So this allows for us to build fungible platforms. Different customers may have different mixes of devices in a given platform. So being on that PCI Express port and being able to work in a plug and play manner helps  us, helps everybody, when I say us, it is the collective us, in order to basically keep  the platform costs low. You do not need to have a dedicated CXL slot, dedicated PCI Express slots, because when  you start going down that route, it makes the costs prohibitively expensive. These are expensive things. So imagine if I'm making a platform, I don't know how many PCI Express cards somebody will  need, how many CXL cards somebody will need. If my connectors are fungible, I can just have some number and they can put, let's say  there are 10 slots, they can put all 10 as PCI Express cards or they can put all 10 as  CXL cards. That's fine. Or any mix and match in between. But if I didn't know, then I have to give 10 of this type and 10 of the other type. So that becomes very expensive. As we said, first generation of the CXL aligns with PCI 5.0 at 32 gigabit per second per  lane, per direction. And the way it works is a CXL device starts negotiating at the PCI Express Gen 1 data  rate. It starts with 2.5 giga transfers per second as per the PCI spec. It's an alternate protocol on PCI Express, as we mentioned. It uses the 8B, 10B link training. And during the configuration, this is the early phase of negotiation. The link has still not been configured. During that configuration phase, it advertises that it is capable of doing CXL protocol,  which is again allowed. And the PCI Express spec allows you to advertise alternate protocols during that time. So it advertises CXL capability. If both sides support CXL, the link will switch over to CXL even before your link training  completes in Gen 1. If the other side is PCI Express, no harm. It will continue and it will work as a PCI Express port. And the usages that we mentioned in CXL really are expected, in my opinion, to be the key  driver for an aggressive timeline for the next generation. So PCI Express doubles every generation. I believe CXL will be one of the lead users of getting to the next data rate.
 
Next slide, please. So let's look into the CXL protocols and how they interact with one another. CXL comprises of three protocols, CXL.io, CXL.cache, and CXL.memory. Now CXL.io is the I/O part of the stack, very identical to PCI Express. We use it for device discovery, configuration, register access, interrupts, virtualization,  and most importantly, the traditional bulk DMA that we talked about with producer-consumer  semantics. CXL.io is a mandatory protocol in the CXL specification. CXL.cache protocol is optional for a device. It allows the device to cache the system memory in a coherent and low latency manner. CXL.memory protocol, that's also optional for a device. It allows the processor as well as CXL devices that have the CXL.cache semantics to access  the memory attached to the device coherently. So these are the three protocols. Now you will notice that two out of these three protocols are optional, and we will  discuss more about the permutations later when we talk about usage models. Now if you look at the diagram, the host processor has memory attached to it. The CXL device also may have memory attached to it, or it may not have any memory attached  to it. Both flavors are allowed. The CXL.io stack in both sides are completely leveraged from PCI stack. Now this allows for that interoperability that I talked about between a PCI Express  device or a CXL device on a PCI slot in a platform. CXL.cache and CXL.mem, they go through an independent stack. They are multiplexed at the logical PHY level, and we will go through the rationale a few  slides later. CXL specification defines FLITS as the basic unit of transfer, and CXL dynamically multiplexes  the IO, the cache, and the memory transactions in that FLIT format on a PCI Express PHY. So suppose we have a CXL.io transaction, which is again very similar, almost identical to  a PCI Express transaction layer packet. It has a payload of let's say 512 bytes, and you just started transmitting that particular  packet. And now you get a CXL.cache FLIT, which has got latency sensitivity that needs to be sent. What CXL allows you to do, it allows you to interrupt that CXL.io transaction at the FLIT  boundary, which is 528 bits, not bytes, 528 bits. You can interrupt it at the FLIT boundary, send the CXL.cache FLIT, which has got latency  sensitivity, and then resume to send the remainder of your CXL.io transaction as FLIT. So this helps with that finer-grain dynamic multiplexing of FLITs across the different  stacks. Now, a FLIT is a flow control unit. This is the atomic granularity of data transfer in a link. For CXL, the FLIT size is 528 bits. This includes 16 bits of CRC for the cache and the memory stack. And the physical layer adds another 16 bits to each FLIT to disambiguate between the different  stacks and to also do physical layer communication for link control mechanisms.
 
Next slide, please. We'll get into some of the CXL features and benefits.
 
Next slide, please. So now let's look into the CXL hardware stack, as we have shown in the picture. CXL follows-- so let's focus on the picture on the left side. CXL follows the standard of layering protocols with clear delineation of functionality between  each of the layers. So each layer has its own set of functions. Now, as we mentioned earlier, CXL.io shares the link layer and transaction layers that  are completely leveraged from PCI Express. And that's what you see in the picture on the left side. The reason there are some additional CXL boxes is we have certain enhancements in CXL, such  as additional virtual channels for enforcing quality of service, and also the ability to  break a CXL.io packet, which we talked about, on the FLIT boundary. But other than that, we follow identical PCI Express crediting mechanism. That's transaction layer packet, or TLP, ordering rules, data link layer packet, or DLLP, CRC  present in PCI Express, retry mechanism of PCI Express. All of those are followed identically in the CXL.io part of the stack. Now let's look into the bottom part of that picture, which is-- you'll see that there  are two boxes there, CXL/PCI/CXL/PHY, analog and logical part of that. So that's, again, almost identical to PCI Express. There are some enhancements. So as we mentioned, the data blocks, they carry only FLIT. So we have to make some changes to allow for that. We also have some additional optimizations for lower latencies, so things like drift  buffer if both sides have a common clock. We also have the ability to bypass the sync header if all components advertise their support  for it. All of these latency optimization features are negotiated upfront when we negotiated  the ability to do CXL protocol during the configuration state at 2.5 gigatransfers per  second. Now looking at CXL.cache and CXL.mem, the two green boxes on the left side, they share a common  link layer. You can see that they share a common link layer between them. They are inherently FLIT-based for improved efficiency. Credits are also used to manage the transaction flow across the link. CXL specification provides for an arbitration and multiplexing mechanism across the two  stacks. So you'll see that there is a dynamic MUX, CXL dynamic MUX that's also built into the  specification. This multiplexing happens at the physical layer level to keep the latency low. Now just for comparison purposes, we have shown a hypothetical protocol, or a hypothetical  approach that CXL could have taken on the right side where the multiplexing happened  at the transaction layer level. And you could have used separate PCI Express virtual channels for the CXL.cache and the CXL.mem protocol. Now in this hypothetical approach on the right-hand side, you would have added considerable latency  to the CXL.cache and the CXL.mem transactions, which are inherently latency sensitive. Now we expect that CXL.cache and CXL.mem transactions to have latency similar to symmetric cache coherency  links, which are significantly lower than PCI Express. Now the reason for that is PCI Express has variable payload. So your link layer, for example, your physical layer framing, all of them have a lot of pipeline  stages to be able to deal with this variable payload. And that adds to the latency. CXL.cache and CXL.mem have been designed, ground up to be fixed flit sizes in order to keep  the latency lower. So thus we can optimize the latency by having a separate link layer stack. That's the reasoning behind multiplexing at the PHY layer. There is also another reasoning. By multiplexing at the lower PHY layer ensures that your latency sensitive CXL.cache and CXL.mem traffic,  or memory traffic, are not stuck behind the large I/O transactions. So recall the 512 byte payload. If you are stuck behind that in order to send your, let's say, Snoop, which is very important  to be delivered, then that would add a lot more to your performance critical path.
 
So let's flip to the next slide. And let me, we kept talking about lower latency. So what kind of latency are we targeting? Now as I mentioned before, we are expecting latency in the range of the existing symmetric  cache coherency links for cache and memory traffic. Now the specification provides guidance in terms of the expected latency. So for example, the recommended latency for a Snoop miss, pin to pin, that is when you  send a Snoop to a device and when you expect the Snoop miss response to show up on the  pin for the cacheing traffic, is expected to be maximum 50 nanoseconds. So let me say that again. The maximum recommended latency for a Snoop miss, pin to pin, for CXL.cache traffic is 50  nanoseconds. Similarly, the maximum latency for a memory read to data return, so this is when you issue  a memory read to a device and you get that data return showing up on the pin, is 80 nanoseconds  if you have HBM or DDR type of memory. Now these are numbers that are close to any cache coherency, symmetric cache coherency protocol  links that you have. Now you can say that, hey, I have slower memory. My DRAM access latency is in the 30 to 40 nanoseconds range. I have memory that inherently is slower. If that is the case, that's fine. We do not disallow that. But there is a reporting mechanism, so that way you can report that you have slower memory. That way the system software can map the right applications to that memory so that the performance  is not impacted.
 
Next slide, please. CXL is fundamentally an asymmetric protocol. The asymmetric protocol example is given on the left side. CXL is the one on the right side. So protocol flows and message classes, because it is asymmetric, can be different between  the two directions, the host side or the device side. And it's a conscious decision in order to keep the protocol simple and the implementation  easy. We have experience with enabling the industry with symmetric cache coherence protocol. Invariably what happens is a vast majority of them shy away from taking it to the finish  line due to the complexity, huge design and validation effort. And quite frankly, this is true across different companies. The frequency with which symmetric cache coherence protocols change over time in a non-backwards  compatible way. So that's the reason why when we defined CXL, we were very cognizant of the fact that we  wanted to have something that is really simple for devices to implement, something that is  going to remain invariant and is going to be backwards compatible. Let's look into each of the components. Now if you look at the host processor, which is shown as a CPU, it has to have a mechanism  to orchestrate cache coherence between different caching agents. Everybody's host processor does that. You need to make sure that data is consistent across your cores. If you have the root port that is doing PCI Express, it has to have a right cache. You have to maintain cache coherency between CPU to CPU sockets. So everybody does cache coherency from the host processor side. So typically it involves taking requests, resolving conflicts, tracking cache line states  across the different caching agents, all of that. So that is what is the home agent functionality, that box that you see on the rightmost side. That's where a lot of that complexity lies. And it is tied to the individual microarchitectural choices. It tends to be different across different generations of CPUs, even from the same company,  let alone home agents from different companies. And personally I have not seen very many examples across different companies. Multiple generations of their CPUs work with each other. And there are very good technical reasons behind that. That we just briefly touched on. On the device side, for example, on the other hand, what you really need is something really  simple. You do need caching because of the types of usages we talked about. But also you need to work with multiple CPU architectures and you need to be able to,  for your device to work across multiple generations of CPU architectures. So you really need backwards compatibility. There is really no need for the device to get bogged down by the orchestrating of cache  coherency by having a home agent, which is very complex, as we said, and it changes. What we really want is simplicity in the devices. Basically, we want the device to get the benefits of cache coherency, but not really get bogged  down by orchestrating cache coherency, which is the home agent part. We just want to piggyback on the existing home agent functionality of the host processor,  anyway that host processor needs to have. So CXL as a specific case and does not get into defining the home agent. It assumes that CPUs have their home agent functionality. They know how to orchestrate cache coherency. What it defines is how the caching agent functionality works and how the host processor has to interact. So simple things like, you know, you are a device, you want to send a read to a cache line,  whether it is shared, exclusive, or you want a snapshot, you want to write back, you might  get a snoop in which case you have to respond. It's a very simple set of those transactions that CXL defines. Effectively the MESI protocol, right? So this allows us to advance CXL in a backwards compatible way. And another example I can give is that of a memory device on CXL. If I'm doing a memory device, I really don't really need to know how cache coherency works. I don't really need to orchestrate cache coherency. All that I'm doing is providing data storage for the data and, you know, getting, giving  the data back. And there might be some directory bit information, but other than that, I really, as a memory  device vendor, I don't really care about how cache coherency works in the system. And CXL defines it. So it just abstracts it to the right level. So that way it is very simple to implement and then move forward also in a backwards  compatible way.
 
Next slide, please. So we'll spend a little bit of time in the CXL's coherence bias. So the picture on the left side is the traditional model. The memory there is mapped as memory mapped IO space. So you cannot really cast that memory. So that's known as device bias in CXL speak. The picture on the right is where the memory is mapped into the system as write back memory  or as cacheable memory. So both the modes are allowed. And it is possible that a device can have some of its memory in device bias and some  of its memory in the host bias. Furthermore, CXL also allows for a memory range to move between the two. So if you're on the device bias, for example, and the CPU wants to access the line, the  particular memory, what it does is it just issues the uncached flow. It invokes the uncached flow. It does the read, the write, just like it does to the PCI Express. If you are the device on the right hand, if you're on the right hand side and you are  in the host bias, then that memory is mapped into the system's coherence space. So even when the accelerator wants to access a location that is to its local memory, it  still has to go to the CPU, just like it would for any cacheable access. It would then, the CPU would then invoke its home agent flows. In this example, it figures out that the data is present in the accelerator memory. So it will go through the memory semantics, get that data, and then through the cache  semantics, it's going to give the data back to the accelerator. So both the modes are allowed. And what is important is CXL defines a set of flows where if you move for a given location  between the two modes and software did not do the right set of optimization, there is  a bug in the software, it is hardware is still going to guarantee that your data is going  to be consistent.

Next slide, please. So let's go through some of the CXL usage cases.

Next slide. So there are three user cases. Left side is type one device, what we call. The middle one is type two device, and the rightmost one is type three device. In the leftmost one, type one device, we use only CXL.io and CXL.cache. And we have provided some example usages of a smart NIC that can benefit from caching. So if you have smart NIC, for example, implements a partitioned global address space, also known  as PGAS, then it needs to ensure that the ordering model of PGAS is preserved. Now note that PCI Express has producer consumer ordering model, and it mandates that writes  be able to bypass prior reads in order to avoid deadlocks in the PCI Express hierarchy. So PCI speak, it is known as posted transactions must be able to bypass non-posted transactions. Now, if you're doing PGAS, this can cause a problem because you do not expect, especially  for strongly ordered transactions, a subsequent write to be able to bypass a prior read. And there is a way to work around that. You can serialize the access from the device side, but that results in performance implications. But on the other hand, if you have the CXL.cache, you could just get the addresses, you could  prefetch them into your local cache, and you don't have to pay that performance penalty. All that you need to do is make sure that you retire the transactions in order in your  local cache. So that's where it helps. Another example that we have with type one is atomic. So increasingly what we see is applications are using advanced atomic semantics, such  as floating point operations and those kinds of things. And you can always say that I will take it and define that atomic throughout the whole  IO stack, but that can take years. And then different devices will want to do atomics with different flavors. So it's not really practical to do something that meets everybody's needs in the most efficient  manner. But if you have CXL.cache, you can just get ownership of the cache line. And once you have ownership of the cache line, you can implement whatever atomic semantics  you want. So you have this flexibility that you get from the caching. The middle system is a type two device. So typical usages are your GPGPU and FPGAs for dense computing. These devices have some local memory typically attached to them, which are used for their  own computation. And we expect these kinds of devices to implement all the three protocols, IO, cache, and memory. Now the caching and memory semantics could be used to populate and pass operands and  results back and forth between the different computing entities without having to involve  software or without having to physically transfer the data and passing flags and things of that  nature. You could just do that seamlessly with very low latency and high efficiency. And this is where you could also use the coherency bias flipping that we talked about while exchanging  data. The system on the right is a type three device. The usage would be memory bandwidth expansion, memory capacity expansion, storage class memory. So they only need to implement CXL.io and CXL.mem. So because you are a memory device, you don't really need to be worried about cache coherency  and CXL specification allows that. So that way it is fairly simple.

 Next slide, please. So let's revisit how CXL enables heterogeneous computing. We went through how CXL.cache works, how CXL.mem works. So you've got efficient population and update of the operand. And this is done through the movement of the data in CXL.cache and memory specification  without having to shift the entire data and flag across. Data is accessible to everyone depending on who is working on the problem. You can, same thing with the results, you can extract your results very efficiently. You can borrow memory resources temporarily. If I'm working on a problem that that memory is with somebody else. I could just flip the bias and that memory belongs to the system memory. User kernel data level data access and data movement is very simpler. And we talked about very low latency interconnect. CXL is designed, ground up to be low latency.

Next slide, please. So in summary, CXL has got the right set of features and architecture to enable a broad  open ecosystem for heterogeneous computing and server disaggregation. CXL supports cache coherency. CXL supports memory extension. It is low latency. We talked about how we go through a lot of details to make sure that the latency for CXL.cache  and the CXL.mem is low. We implement asymmetric protocols for reducing complexity. So it eases the burden of designing cache coherent interfaces as well as memory devices. And most importantly, we strongly believe in open industry standards with growing broad  industry support to innovate, right? That's where everybody's effort comes into picture and the industry just progresses tremendously.

Next slide. And I'll hand it over to Kurtis Bowman.

Thank you, Debendra. And thank you very much for walking us through the technical details of the 1.1 spec.

What I'm going to do is really talk about the consortium itself. So back in March, the nine companies got together and said all the stuff that Debendra just  kind of talked about is things the industry needs and is missing for all the use cases  that he brought up. And at that time, we formed an interest body and started to invite people to join in. Then in September, just a few months ago, we went ahead and moved from that interest body  to an actual incorporated SIG or special interest group and put together a board. And that board is made up of the 14 companies that you see here on the screen  where we've got a broad range of industry leaders helping to guide this spec forward. In addition, we added a large number of members. They've been with us through that journey starting in March and then have continued on  as we move into the incorporated section. So with more than 90 companies available today, it really has put together a set of companies  that are interested in taking the spec which is currently at 1.1 and moving it forward  into the industry as well as implementing that spec since we've got a really great ecosystem  to look forward to here. And there's a couple of levels of membership that we have. Debendra stated it really well. He said we want to be a very open consortium. And part of open is accessibility. So the first level of membership is free to any company that wishes to join. And what that means is you can get access to the specs, any of the release specs. Currently 1.1 is our latest, as well as you get the IP protection that's rolled out  in our membership agreement and our IPR policy. And that gives you the ability to manufacture and design systems based on the CXL spec. The next level of membership is the contribution or contributor member. And it's $10K a year with kind of a buy-in of an extra $10K or an ante of $10K at the beginning. And what that does is allow us to have some opportunity and working capital for the consortium. And that's really where you get to influence the spec. So you get to participate  in the CXL workgroups which I'll talk about in a minute. Clearly in those workgroups  that's where we decide what's in spec 2.0 that Debendra was talking about  and what's going to go into future specs as well. In addition to that you get access  to the intermediate specifications. So all the dot releases that are being worked on now  are available to contributor members.
 
With the 1.1 spec that we have available now, it has everything that Debendra talked about. So it defines what the host roles are. It defines what the target roles are  and how those should communicate. You heard him talk about there's recommendations for latency. There's the use case models that he talked through all available in that specification. And because of the way it's set up as I mentioned, anybody can implement or design  to that specification as long as they're a member. Now we want to be open and so another way to be open is to make sure that even nonmembers  can get access to the specification. And in order to do that it's quite simple. You can go out to our website, computeexpresslink.org, and you can do a click-through license  to get an evaluation copy. That way you're not feeling like, hey, is this what I need or not need  and join for no reason. Now we're very confident that once you read the spec, you'll look at it,  you'll understand why it's important to you, and you'll want to become a member  of one of the two levels. The future specifications, this is really where the contributor members play a huge role. And that is in the participation in these workgroups. There are five technical workgroups  that you see here. There's compliance, memory system, the physical layer, the protocol,  as well as software and systems. So those five workgroups are coming together  and they're really driving the technical specification as we move forward. In addition, there's an opportunity to promote the consortium and the specifications  as part of the marketing workgroup. And the marketing workgroup is responsible  for putting on this BrightTalk today. And then finally, the backwards compatibility. That's really important to us. Debendra talked about it, but I want to talk about it again as a key thing  that protects the membership's investment. It's something that we've seen as a best practice  in the industry. When you look at things like Ethernet, PCIe, USB, all of those  that are very ubiquitous have all driven a backwards compatibility.

So my encouragement for you today is join. Please join us. Come in, look at the spec,  go out to our website and join in. Become a contributor member. We'd love to have you. But also follow us. Watch what we're doing. You're going to find a lot of things going on  for the CXL group. And you're going to see it on Twitter as well as on LinkedIn. So I encourage you to jump in and join us today. And with that, I will move into our Q&A session.

Thank you Kurtis, Debendra, and Glenn for presenting today. And thank you to our audience for attending. We will now begin the Q&A portion  of the webinar as Kurtis mentioned. Please note that this recording  and the webinar slides will be available after the webinar  through the BrightTalk platform. They will also be made available for download  on the CXL Consortium website. So please feel free right now to send in your questions  and we will answer them as they come in. Our first question, Kurtis would you like to take this one? When do you expect to see CXL devices in the market?

So Debendra was talking about CXL comes in with the PCIe Gen 5. And so we are looking at those coming in the 2021-2022 time frame.

Okay, and the next question Debendra will take,  what is the security feature for this link since it is able to access all devices  and cache from one place?

Right, those are the things we are addressing in the CXL 2.0 specification. It is being actively worked on. We are going to leverage the security mechanism  that is being developed in PCIe SIG also.

Okay, Kurtis if you would like to take this one, the next question is,  how will these developments impact the industry after competing â€“  sorry, how will these developments impact the industry other competing standards?
 
So I look at the industry as something that evolves. And there are some similar specifications out in the marketplace. I'm not sure they are competing as they all have different areas that they focused on. CXL in and of itself is very much focused on riding on top of PCIe,  taking advantage of that infrastructure, and then giving you a memory centric, low latency,  high bandwidth interface for devices on the other end, be those accelerators  or memory type devices, the type 1, type 2, type 3 devices that Debendra talked about. So as we look forward, I don't think we have to worry about competing devices  as CXL will be able to win on its own merits, and it doesn't have to see a different specification  or standard go down because it's there. At the end I think we are going to have  the technical community choose which works for them, and they will implement those items.

Okay, and will CXL replace PCIe?

I was going to say, it's not something that PCIe is going to be replaced. There are a number of great I/O devices that PCIe has and will have going forward. Those are perfect for PCIe. What you saw in the presentation is memory centric devices,  the need to share memory, the need to talk to it in a way that the processor uses,  the read/writes, the put/gets, is really the key to using CXL versus PCIe. So I expect them to coexist for a very long time. Debendra, what are your thoughts?
 
Totally agree with you. I couldn't have said it as well as you did. Thank you.

 Okay, next question. Does CXL replace Gen-Z?
 
I'll take that one. We actually see CXL and Gen-Z being very complementary. CXL, Glenn put it at the very beginning, is looking kind of in the node type of a thing,  and that's what you saw in Debendra's slide. Gen Z is outside the node. It's at the rack level, at the row level. So these two working together  actually could help us realize a very fluid memory centric architecture for a data center  and even an edge deployment.
 
I might just echo what Kurtis said. In the aggregate, we see tremendous complementarity  and a lot of synergy in deployments overall where we would see both CXL and Gen-Z deployed. So certainly see that complementarity as a trend that's just going to grow over time.
 
Okay, it looks like we have time for two more questions. The next question is, does CXL support persistent memory?

 The plan is for CXL to support persistent memory. There is memory semantics already built  into CXL 1.0 and 1.1 specifications. Now there are a few additional things you need to do  for the persistence flow. Those are being added into the 2.0 specifications.
 
Okay, and our last question for today, again, thank you very much for joining. Is there a plan to extend CXL to have inter-device data coherency support?

I can take that one. So CXL, you already have that today in the sense that your home agent,  if you imagine that the home agent is in the host processor and then the devices  have their own caching agent, they can still work on the same memory and do,  just like a cache line can ping pong between different caching agents,  so they can ping pong between the different CXL devices. Now, if the question was, am I going to have devices connect to each other directly  through a CXL to CXL link in addition to a link up to the processor,  those are the types of things that we are looking for in the future. And I cannot talk too much about specifications that we are currently working on,  but those are the things that we are looking into. However, I wanted to point out that with 1.0 or 1.1, you already have that capability.

Okay, excellent. Thank you, Debendra, very much, and Kurtis and Glenn, as we discussed before,  we really appreciate your time today. Please feel free to get in touch if you have questions. And as I mentioned, the slides will be available for download via the CXL Consortium website  later, probably early next week. Thank you again.

 Thank you all very much.

 Thank you.

 Thank you.
