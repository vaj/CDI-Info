
Good morning everyone. Thank you all for joining us for the CXL Consortium Q1 webinar,  Compute Express Link 2.0 Specification Memory Pooling. Today's webinar will be led by Mahesh Wagh, Senior Principal Engineer of the Data Center Group at Intel,  and the CXL Consortium Protocol Working Group Co-Chair,  and Rick Sodke, Associate Technical Fellow at Microchip. You'll set aside approximately 15 minutes at the end of the presentation for Q&A,  so please enter any questions you have into the chat window at any time during the presentation. Now I'll go ahead and hand it off to Mahesh to share today's agenda.
 
Thank you, Kevin. Good morning, everybody. We'll start with the agenda. We will touch on Compute Express Link. We'll give a high-level introduction to what Compute Express Link is. Then we'll get into the details about CXL memory expansion,  and then follow that up with some benefits and use models for CXL-based memory pooling. And then Rick is going to walk us through CXL memory pooling allocation examples. Hope that this presentation is going to be useful for our audience today.

All right. From a Compute Express Link perspective,  Compute Express Link is the interconnect between a processor and a device. And we typically think about those devices as three types of devices. We categorize them as Type I, Type II, and Type III. Type I devices are caching devices or accelerators. An example of a caching device that participates in the cache coherency protocol would be an accelerator. A Type II device is an accelerator that is doing some amount of dense computation. And this type of device participates not only in the CXL.cache, which is the cache-coherent protocol,  but also CXL.mem, which is the CXL memory protocol. This particular device has both caching implemented on the device,  as well as that device memory is mapped into the system address space for the processor in the host physical address space. And that way, that's the device that we call Type II device. For today's topic, we're going to be focusing on Type III, which are memory buffers. It is an expansion capability for providing more memory bandwidth, more memory capacity,  and introducing different types of memory devices, high-performance memory devices, or storage-class memory devices. So when it's a Type III device, you have a processor with its local attached memory. And then you can expand the memory with the CXL Compute Express link going between the processor and the memory buffer. And through that, you can get the expansion capability. One of the key things that CXL runs on is the interconnect in between runs on PCI Express physical data. So from a bandwidth perspective, you're able to push the bandwidth that is supported by PCI Express. And with CXL, with these three protocols, we bring significant protocol-level advantages  and address some of the emerging use models for CXL. So we'll then focus on what are the memory expansion capabilities. We'll also talk about where does CXL 2.0 go from its scaling perspective compared to the first generation of CXL. And then we'll get into the details of the CXL-based memory pool. Next slide, please.

So if you look into the taxonomy of a data center, you have an SoC interconnect and then the processor interconnect. The processor interconnect is where you connect multiple processors together and the devices to that processor. CXL 1.1 was focused on single node connecting the processor and the devices, including memory expansion devices, to a given node. With CXL 2.0, we extend the reach of CXL at the rack level. And now you're able to have multiple nodes that are able to pool resources, in this case, memory between those resources. And you can do that with the introduction of CXL 2.0 switches and what is called CXL single logical devices or multiple logical devices  and get the scaling for these devices through the CXL 2.0 switch. Next slide.

All right, so we'll first touch upon the memory expansion with CXL 2.0 switch. In this case, in this example, we're showing the host on the left side. So that's the CPU with a CXL link coming out of that CPU connecting to a switch. The first level of expansion that you get with the CXL 2.0 switch is fan out. You can connect many CXL devices below a CXL 2.0 switch and get the ability to scale your memory capacity by adding more devices. So in this example, on the left side, we're showing a host with one CXL link coming out of it, a CXL 2.0 switch, and then four CXL devices connected behind that. These CXL devices could be high-performance memory. These could be storage class devices. A couple of additional things that CXL 2.0 brings along with that is the ability to support hot plugs, software-managed hot plugs of these devices. You're able to do online and offline of memory that is associated with each of those devices connected to that given host. Another key piece of CXL 2.0 switch is the ability to support memory interleaving. To get better memory performance across these devices, you're able to interleave your memory accesses across these devices. So in this case, you could interleave across all of those four devices, but that's not the only limitation. You can support memory interleaving between the DRAM connected to a host and the memory devices. You can interleave across these memory devices, and you can also interleave across the ports coming out of a given CPU. The right side of the picture is showing an additional level of scaling where you can, for every link that comes out of a host CPU,  you can attach a CXL 2.0 switch and provide the scaling that way. So in that example, we're showing you got the six devices, each switch supporting three devices coming out of that switch. In that example, you have the same capabilities to support managed hot plugs. You're also able to support interleaving across the two links coming out of the host and subsequent interleaving going across the devices. So that's this memory expansion with CXL 2.0 switching. In both of these examples, we're still within the same address space,  which means you're running a single operating system image or a VMM image on that given host. And in this topology, everything is in a single address domain. But there are several use models where you think about the data center,  you want to make effective use of the overall available memory, as an example, within a given rack,  and distribute that memory across multiple hosts. So there was a requirement coming into the consortium to support what is known as memory pooling. And we'll get into the concept of memory pooling in the next slide.

So what is CXL-based memory pooling? It's a combination of software, hardware, in this case, platforms, which is memory devices,  and some protocol enhancement to make efficient utilization of hardware resources. We do that by enabling dynamic management and allocation of resources, and in this case, primarily CXL-attached memory. So you have resources that are CXL-attached memory. You have different nodes. And one of the problems that we are running into data centers is the case of stranded memory resources within a given system. So every time a system is formed, you're trying to allocate a lot of memory that the guest operating systems of the applications that are going to run on that given system. However, in reality, what turns out is that a lot of that memory is not getting utilized effectively. You're over-provisioning and under-utilizing that particular memory. An effective way of utilizing that memory resources by providing pooling, which is on demand,  you would support the dynamic allocation and deallocation of memory resources. And the way that works is you take your pool, you take your memory, and partition that memory into pools,  and then allocate these pools to a given host, a given operating system, and do that in a dynamic way  so that you can meet the demands of the applications that are running on top of that. And some of the benefits that you get as a result of that are better total cost of ownership and those benefits. Moving to the next slide.

With CXL specification, if you look into the requirements that were brought into CXL specification,  memory pooling was one of the primary use cases that we were going to address in CXL 2.0 spec. Rick Sodke, who is co-presenting with me today, we were the two authors on the CXL 2.0 spec for memory pooling. And we looked at supporting different topologies for CXL memory pooling. And the spec addresses pooling using a single device, taking that device and allocating that device to a given operating system. Pooling with the multi-logical devices, which we'll get into the subsequent slides. This talks about taking a single device and then support and partitioning the resources within that device  and then allocating those resources to an operating system. And then for performance reasons, pooling without a switch. So supporting some schemes where you could do this pooling with a multi-ported device  and some amount of circuit switching and be able to support it. Another key piece of memory pooling with CXL is a fabric manager. And this is an important piece because you need some sort of software and a system element that's responsible for orchestrating,  which is providing configuration and control capabilities for supporting this pool of resources. We'll start with some examples in the next slide for what pooling looks like.

All right, so with CXL 2.0 pooling, so here's an example of the left side for memory or accelerator pooling with single logical devices. In this example, we're showing the CXL 2.0 switch with four different hosts, five different hosts connected to that CXL 2.0 switch. Each host is a CPU and an OS or a VM image that's running on that CPU. And we've color-coded them. So the brown one is H1, the blue one is H2, the magenta color is H3, so on and so forth. And you have a CXL 2.0 switch that is multi-host capable, in which case each upstream port of that CXL 2.0 switch is capable of connecting to a different operating system. And then connected below that switch are different devices. In this example, we're showing a device D1, D2, D3, D4, and D#. And the topology is set up such that device D2 and D3 are now allocated to host 1, and device D1 is allocated to host 2, so on and so forth. So if you follow the colors, then it describes how the association of each device is to a given host. Now in this pooling, this is device 1 is a device with all of its memory resources. And in this particular pooling example, the Fabric Manager would come in, would establish a connection with the host,  and each of those hosts will then inform the Fabric Manager about the requirements of memory capabilities and capacity that each of the hosts is requesting for. And the Fabric Manager then looks at its pool of resources and allocates those resources accordingly to each of those hosts. And in the next slide example, that entire device is assigned and allocated to a given host. So D1, for example, with all of its resources will be mapped to host 1. D2 and D3 with all of its memory resources will be allocated to host 1, so on and so forth. The example on the right side is showing a memory pooling with multi-logical devices. Now a multi-logical device is a device that has additional capabilities where it can partition its memory resources and allocate those resources to different hosts. D1, for example, all of its resources are allocated to the same host. In the example of D2, its resources are partitioned and a portion of its resources are assigned to host 1 and a portion of its resources are assigned to host 3. So that device is a special type of device, we call it multi-logical device, and it's capable of partitioning its resources and then allocating those resources to a given host. The Fabric Manager is aware of such a device and the Fabric Manager is also aware of the request made by each host for its memory. And the Fabric Manager comes in, configures these devices, and assigns and allocates these devices to each of those hosts. Now if you move to the next slide, we'll show you...

Sorry Mahesh, before we move on, there's a couple of relevant questions here you might want to cover. The first one is, should each CXL MLD have its own configuration space? And can each MLD itself contain multiple physical and virtual functions?

So if you hold on to that question, we'll dig deeper into that, but if you go back to one slide... Yes, each of these devices needs to have its own configuration space. As a multi-logical device, when we talk about the logical device aspect of it, it looks exactly like a single logical device,  in which case it has to have its entire configuration space, and it can also be a multi-function device. So there's no restriction of that within a single function. And I'll cover that because we're going to get into the details of single logical device and multi-logical device. Any other questions?

One more. Does memory pooling enforce all memory types to be the same? The memory devices could be of different characteristics, HBM, DDR, PM, etc. So how does pooling work in such a case? 

Right. That's a good question. So from a specification perspective, there is no restriction for how one system fabric manager would allocate those resources. However, when a device advertises its capability, the device for every HBM range... For example, a device has two types of HBM ranges that it supports. For each of its device capabilities, you can see that it advertises what its capabilities are. So what that means is what its sustained latency is going to be, the device characteristics type. And typically, we would expect that in a pooling to get a uniform performance. You're going to pool devices of the same memory type into a given pool and then allocate that given pool for a given HBM range and use that. Because from a system perspective, you're trying to get a balanced system out. And for that case, you would stay away from mixing and matching memory of different types. But that depends on how somebody wants to configure their system. Specification doesn't prohibit anybody from mixing those. It's more of the practical use cases that would limit certain types of pooling.

Can we advance to the next slide? This is showing an OS or a VMM view of CXL memory. When an operating system or a BIOS loads up, you can see that the OS or the VMM only sees the devices that are assigned to that OS or that VM. In this example, host 1 was assigned device B2 and B3. So when that particular operating system loads up, all it sees is a CXL 2.0 switch and the devices connected below it. And it can map its host physical address to these devices. The same thing for the host 1 connected to a multi-logical device. It only sees the devices that are assigned to it and only the portion of memory to those devices that it assigned to that given host.

Okay, can we move to the next one? So this is an example of CXL 2.0 memory pooling without a switch. The left side image is the one that's showing the switch. It's only for some comparison purposes that we've kept that particular figure. One key thing to also note related to CXL 2.0 switching is that you can connect the CXL 1.0 device also behind a CXL 2.0 switch. So one of the key characteristics of a CXL 2.0 switch is that for backwards compatibility, it is expected to support CXL 1.0 devices as well. So that was a subtle point shown in the left side diagram. On the right side is showing memory pooling with a multi-ported device through Direct Connect. In this example, we're showing you can implement pooling without having to switch, but some sort of a backplane circuit switching sort of capability. So in this example, we're showing each host has a CXL 2.0 link coming out of it, and it's showing 32 lanes coming out of that particular host. And if the backplane connectivity is four lanes each, then each host can connect to eight devices. Similarly, if the device has 32 lanes and it can support eight ports on that device, then it's an example of a multi-ported device. And each device can connect to eight ports. And that way you can get to pooling by supporting some sort of a backplane switching. CXL spec doesn't go into a lot of details about multi-ported device or the Direct Connect, but such a topology is supported through CXL 1.1 as well as 2.0 definition perspective. In other words, there isn't anything special that needs to be done for this sort of connectivity. But from a topology and application perspective, we wanted to make sure that we also cover multi-ported device usage case through this webinar. So we'll then start to look into what are the different device types, and then we'll go through the examples. You go to the next slide.

All right, so we'll first start with a CXL 2.0 single logical memory device. So this is a device that has one CXL link coming out of that device. And it can support one or more PCIe endpoint functions. And the key thing to note here is that it is going to be a PCI Express endpoint function and not a root complex integrated endpoint function as what is defined in CXL 1.1 spec. And there was the question whether a single logical device can be one or more functions. It can be one or more functions depending on how many functions the device vendor intends to use. All non-CXL function maps, so there is a function map that is used to advertise non-CXL functions. So within the functions that the device supports, there is a capability that is used to identify which of the functions are CXL functions and which of the functions are non-CXL. Another key capability is the type 3 device component register block now starts to implement support the HDM decoders. So with these decoders, then we are able to map the host physical address. So this is the address that the CPU or the host will send down to this device. And through these HDM decoders, then you map that host physical address into the device physical address. The HDM decoders then support the capability for interleaving, understanding which interleave offset the device belongs to. And that way it can do an effective mapping from host physical address to the device physical address. The key thing to note here is each of that, each CXL link that comes out of here, in this case, if it's a single logical device, it has only one CXL link. That CXL link is connected to a single virtual hierarchy, which means this device and all of its memory allocated to this device is going to be mapped to a single OS or is going to be mapped in a single address space. OK, we'll move then to a multiported device as an example on the next slide.

All right, so this is showing an example of a multiported device. What we mean by a multiported device is that it has multiple CXL links. So, for example, it could have four CXL links. Each CXL link could be a by four, by eight or by 16, depending on how much amount of bandwidth to each CXL link the device wants to support. Behind each CXL link represents a single logical device that's connected behind that CXL port. And then the definition for that single logical device is exactly the same as I was talking about in the previous slide. But the key difference here is that this device is capable of looking at all of its memory resources and then allocating its memory resources to the CXL ports that are connected behind each of those links. How a particular device does configuration of those resources are device vendor specific. In this example, we're showing that it's a four ported CXL memory device and equal allocation of pooled resources across those ports. And it's showing each color denoting that each of the CXL ports is mapped to a different OS or VM image, a different address space. And as you can see, when the host physical address comes in on each CXL link, it's coming for the address that's mapped into a given address domain and the device is doing HPA to device physical address translation. And that host physical address to device physical address translation is going to be specific to that device because it's managing its memory and allocating that memory across different operating systems.

We'll move to the next slide, which is an overview of a CXL 2.0 multi logical device. Now, the previous example was where you had resources and you divided those resources across each link, which means that the device had to have that many links, as many links that it can partition its resources into. What a multi logical device supports is that you have one CXL link coming in, but then you're able to partition your resources behind that one link and allocate those resources to multiple OS images that are connected through a switch to that particular device. So we call this type of a device a pool type 3 device. And architecturally, this device is capable of partitioning its resources into 17 partitions. 16 of those partitions can be used for assigning them to for memory as an example. So it can take all of its memory resources and partition up to 16 logical devices. And there is one special logical device that's known as the FM owned LD, FM owned fabric manager owned logical device. The fabric managed fabric owned logical device, it doesn't allocate any memory resources. It's only used for discovering this link, managing this link and configuring this device, configuring the device resources and partitioning it across these memory resources. Now, each logical device is accessed through what's known as a logical device ID. So every time a CXL.io transaction is made to the device, that particular transaction is designated with a logical device number. When the request comes in, the device looks at the logical device number. And then it through that it understands which logical device the access is coming in for and then services that request. Similarly, each memory request comes in with a logical device number so that the device then understands which of those partitions the access is for and then it maps it to that given logical device. As shown in the example on the right side, we're showing device partitioned into 16 logical device, each partition identified with logical device number zero through logical device number 15. The mapping of logical devices happens through the CXL 2.0 switch. And then the FM owned LD is the one that comes in and through an FM API informs the device about how many partitions it needs to do. And then each partition discovers the HDM decoders and the operating system running on each of those logical device identifies what capacity of memory is connected to that logical device, and then assigns the host physical address, any interleaving and any offset to each of those logical devices. So going back to the question about, so if I take this example, each of these logical devices, for example, for configuration request comes in, that configuration request will be followed with a logical device ID number. In this case, let's say logical device number zero dot config, and that config will try to access logical device number zero. This logical device could be a single function device, it could be a multi function device. There's no limitation on how many functions that particular device can support. And behind that device, it can support a single type of memory or it can support multiple ranges of memory. That depends on the device vendor. All right, so that was about the CXL 2.0 multi logical device. I'll hand this over to Rick from this point onwards, and Rick will walk us through some of the allocation examples for how all of this works along with the CXL 2.0 switch. All yours, Rick.

Sure. Just go back a slide, please. There was maybe a little clarification required. Each logical device appears to the outside world like an endpoint, in that it has its own configuration space. It could be multi function, as Mahesh said. And in the case of a type three pooled memory controller, it supports .io CXL.io traffic as well as CXL.mem traffic. 

One question on this specific slide was, what are these IO decoders? What does that mean?

 I think what I was trying to show here is IO decoders is your configuration and your memory map IO decoders. It's just showing that you have both the .io space, and I think you touched on it, as well as the .mem space, and all of the decoding is supported for that.

 Okay. And then one more question on MLDs. You mentioned partitioning resources between different hosts. And the question was, they're wondering about, is this just memory capacity, or are there other types of resources being partitioned?

In CXL 2.0, we limited to only memory resources being partitioned across the hosts. So CXL 2.0 only talks about memory resources partitioning.

Right. So from a resource perspective, it's only memory, but you can also do bandwidth allocation. And part of the MLD QoS, quality of service, allows allocation of different bandwidths and latency characteristics to different hosts, and that information is passed back as part of the specification.

One question, will CXL memory be shared between two nodes?

CXL 2.0 is limited to memory pooling. CXL dot sharing was looked into for when we were looking at the definition of CXL 2.0. And I say, at that point, it was pushed for CXL 3.0 as part of that consideration. So as of CXL 2.0, it's only limited to memory pooling, in which case, when you pool and partition, that partition is only allocated to a given host. So sharing is something that has been pushed out for CXL 3.0.

 Okay, the last question was, will CXL support load store semantics?

 Will CXL support? Yes, CXL supports load store semantics. CXL.mem is an example of load store. You're able to access that memory similar to how you access system attached memory.

So Mahesh has done a great job of presenting the whys, why do we need memory pooling, what's the value, as well as a good number of the whats. What does an MLD look like? What does a single logical device look like? And I'm going to go into a little more detail on the hows. How is it possible to implement pooling using the CXL 2.0 specification and the devices that are defined by it? And a critical component to it is really about the Fabric Manager connection. Fabric Manager is a control entity that manages the switch and the memory controllers. It can be an external BMC, an external CPU of some sort. It can be one of the hosts in the system, or it could be implemented with firmware internal to the switch. And we've got a number of questions on this topic. The most typical application is really that it's an external processor that's managing a lot more than just the CXL switch, and it has communication to the outside world. A Fabric Manager endpoint and the connection to the Fabric Manager is not required for many CXL systems, but in the case where you want to support MLD ports, as in a multi-logical device that has traffic associated with multiple hosts or virtual CXL switches, you must have a Fabric Manager. And if you want to do dynamic moving of resources, whether those are SLD components or memory within either a multi-port or an MLD component, you need a Fabric Manager to be able to do that. And the CXL 2.0 standard defines the standardized interface called the FMAPI that is used for the Fabric Manager to communicate with these devices. And the interface is actually quite simple and elegant. It's a very low-level command response protocol, but it's built on MCTP. And as a result, it doesn't matter what the physical interface is. It could be PCI Express, CXL.io, SMBus, Ethernet, USB, or in some instances where the Fabric Manager is internal, you just have a firmware protocol that passes MCTP packets between them. But in all cases, the commands are the same, the packaging within the MCTP packet is the same, and it's only the physical interface that changes. Another major concern that we had when developing the CXL 2.0 spec is to ensure that the Fabric Manager does not need to be performant. It's not responding to config cycles or memory cycles, so it doesn't need to be a high-performance CPU, and you don't need a high-performance link in order to make it work. So, it's a very flexible system that allows communication between the external controller and the system. And we'll go through some of the details on how that works. Next slide, please.

So, as I was saying, the Fabric Manager plays a critical role for supporting memory pooling, specifically around dynamic changes. Mahesh earlier commented that for initial configuration, often the Fabric Manager will get involved before the host boots, determine what devices are out there,  produce the bindings so that once the host boots and does enumeration, it can find the devices that are allocated to it, including logical devices within an MLD. But for the rest of this discussion, we're going to talk more about the dynamic changes that happen after boot. And some of the responsibilities of the Fabric Manager are that any time you have a resource in the system that is being shared,  so if we're talking an MLD port, the physical port is shared between multiple hosts, multiple virtual CXL switches,  one entity has to do the error handling, configuration, status monitoring, all of the things associated with it. And in CXL, that entity is the Fabric Manager. So, downstream ports connected to MLDs, including their PCI to PCI bridges, those are owned by the Fabric Manager. As we've seen on an MLD component, that one of the logical devices is used for configuration only. It does not pass CXL.memory traffic, and it is Fabric Manager owned so that it can configure the allocations between resources. And then unbinding and rebinding of logical devices within an MLD. We know that from the previous slides, an MLD can have up to 16 performance logical devices in it. Each of these can be bound to a single host. Well, it's possible that you could have more than 16 hosts in the system, and a logical device can be moved from one to another. And in the subsequent slides, I'm going to talk about a few examples. And these next three line items are really what I'm going to be covering. The first is reallocation of a single logical device memory controller,  a reallocation of memory within a multi-logical device,  a reallocation of memory within a multi-port single logical device. And one clarification there is that even though a multi-port SLD looks like an MLD,  it actually connects as a single logical device. And it behaves exactly like a single ported single logical device to individual hosts. It's just the management of resources behind it that gets handled slightly differently. So next slide.

So this is similar to the diagram we saw previously. I've just simplified it to have only two hosts in the system. We've got hosts at the top. We've got type three single logical device memory controllers at the bottom. So these are non-pooled devices. And we've got an external fabric manager connected to the switch. So in this example, we've got a host that's decided that it no longer needs some memory. And it's going to notify the fabric manager that that memory is no longer needed. That communication is not part of the CXL 2.0 standard. Communication between the host and the fabric manager,  there are many other standards that already provide these features. And there's almost always a connection between the host and the fabric manager external. So we didn't include that as part of the specification. So in this case, we've got the host notifying that it no longer needs the device number four's memory  and that it wants to be freed. So next slide.

So now we have the fabric manager. And it uses the fabric manager API to pass a command, which is an unbind. It's going to tell the switch to unbind device number four from the virtual CXL switch associated with host number two. In this case, the fabric manager doesn't need to tell the host anything. It's the switch's responsibility. And it's going to use that PCI Express managed hot remove mechanism,  which is well established and easily handled, to request that D4 be removed. Host number two will unload the drivers, quiesce all traffic. And once the handshake is complete, device number four is now completely unbound. At this point, the fabric manager could go into D4. It could do accesses to it and ensure that either the memory's been wiped,  in the case where I'm moving it from one host to another and I don't want to keep any of that additional memory. But it's also possible that host number two will have wiped that memory before it relinquished control. So at this stage, device number four is free and clear. Its memory has been wiped. It's ready to be moved to another host through another virtual CXL switch. Next slide, please.

So using a very similar command, only this time it's the bind command,  the fabric manager tells the switch to bind device number four to host number one's virtual CXL switch. And again, very similar to the previous slide, we use PCIe managed hot add. The switch notifies the host that there's a link state change and a presence change. The host comes out, enumerates the device, finds out what its capabilities are,  configures the HDM ranges in both the switch and the device, and then starts accessing it as if it's DRAM. And there was a number of questions earlier in Mahesh's presentation about latency  and characteristics of the device. As we said before, CXL uses PCIe style enumeration. Devices are found using PCIe discovery. The resources are allocated according to the HDM. And accesses are intended to be of DRAM-like speeds. So where a host would access its own DDR with a certain latency, if the devices are DDR,  the latency through the system needs to be comparable. So I'm not going to give any latency numbers for a switch or a device,  but the importance is that if the memory controller is DDR-based, latencies need to be low. If the memory device is storage-class memory, then the systems could be built that are a little more complex,  that have more latency to them, and you can still get viable performance. So next slide.

So this is the previous diagram as well, but instead of using SLD components, we're now using MLD components. And we've got a number of memory controllers,  and some of the memory has been allocated to the yellow host and some to the blue. Next slide.

And again, similar to the previous example, the host notifies Fabric Manager that some D2 memory is no longer needed. Next slide.

And whereas previously for the SLD, the Fabric Manager would tell the switch to do the reallocation,  in this case, the Fabric Manager sends an MCTP command that is tunneled through the switch to the device. It targets the FM-owned logical device in the MLD component,  and it tells it to do some deallocation of blue memory. This reallocation of blue memory causes the multi-logical device to update its coherent device attribute table, the CDAT. And that update results in an interrupt sent to the host, and the host now knows that my memory is now gone. It can no longer access that memory because the MLD component is responsible for enforcing security  and ensuring that a device can't access memory that it hasn't been allocated. And it will update its HDM ranges so that it no longer even attempts to. Next slide.

So very similar again to the previous example, the FM is going to send a tunneled MCTP command  to reallocate some of the memory to the yellow host. The CDAT is updated. The device notifies the host. The host comes out and finds out what's changed,  updates its HDM ranges, and then starts using the memory. So the important part of this is that no other traffic was stopped while these updates were in progress. So none of the traffic from host one to D1, but also none of the traffic from host one to its existing locations in D2  or from host two to its existing locations in D2. So traffic keeps moving while we're doing these updates. The only thing that changes is the allocation of this unused memory chunk that's moved from one host to another. Next slide.
 
And now I'm going to give an example of a system where we have memory pooling without a switch. And Mahesh had the giant crossbar links diagram. I've simplified this down to just two hosts. And here you can see the Fabric Manager has an out-of-band connection to the devices. In many cases, it's SMBus, but it could be whatever the devices are supporting. The physical interface is not mandated by the CXL spec. Similar to the previous examples, the host notifies the FM that some memory is no longer needed. Next slide.

The CDAT gets updated. The D2 notifies the host that something has changed. The memory is now free. Next slide.

And then the Fabric Manager sends a new command to the device indicating that host number one should now be allocated that memory. One thing to note in this system is that the commands from Fabric Manager to multi-port Type 3 memory controllers is not part of the specification. These would be vendor-defined or vendor-specific implementations in CXL 2.0. The other thing to note is that although this looks like an MLD, it's really an SLD that has multiple ports, and the memory can be allocated between them. Each port looks like an endpoint. It's own Type 3 memory controller has its own configuration space and its own performance parameters. Next slide.

There are a few other Fabric Manager API features beyond what we've seen already, bind, unbind, and set the logical device allocations,  that allow the Fabric Manager to do some of the things we need to do in order to make memory pooling possible. First is switch discovery. At boot time, we want the Fabric Manager to be able to come out, find out what the switch capabilities are,  how many VCSs it supports, and what the connected devices are when they boot, what they're advertising, what they linked up with,  all the things necessary to understand what's connected, and allow it to do the bindings at the initial stage. The switch is responsible for event notification, switch link events, advanced error reporting,  anything associated with controller status that relates to Fabric Manager-owned resources,  or resources that the Fabric Manager may not own, but it still needs to have information about. And then lastly, things like the Type 3 memory controller. We mentioned earlier the quality of service parameters,  the things that allow for bandwidth latency characteristics, and that result in the memory controller giving information back to each host  as to what those characteristics are. And the FM API allows all of these things to happen. So we've covered a fair amount of ground here on how memory pooling is supported using CXL 2.0. We've gone through how we can essentially perform disaggregation, have a giant pool of memory,  we can allocate it to individual hosts or individual root ports, and we can dynamically change it. It's extremely powerful, and it's a system that is being built today. It supports dynamic allocation and deallocation, and all of this amounts to a big savings in total cost of ownership. So that's the end of the presentation. My only other comment is that if you're interested in a lot more detail,  please join the CXL Consortium. We've got a lot of training slides that go into a lot more detail on how all of this works  and how these things look like, as well as getting access to the specifications. So I think we'll move now to questions.

Yeah, and the only other thing I would add to that, Rick, is also your questions. We'll try to address as many as we have in the next two or three minutes. And for the others, we would have some way of getting the answers back to all of your questions. I think one question, Rick, is are there concerns about a CXL 2.0 switch being a single point of failure for multiple hosts? Is that concern handled at the system architecture level?

Certainly, it's always a concern when you have multiple hosts going through a single component,  and that absolutely needs to be handled at the system architecture level,  whether that's with redundant switches going to a multi-port memory controller  or having more complex systems that will support certain types of failure modes. But it's definitely done at the system level.

There were a lot of questions related to security. There was one specific about does the CXL spec define isolation across logical devices for security purposes? And each LD logical device, can it be assigned to a different VM? So is the isolation devices responsibility?

So the security is an end-to-end problem, and it's not only the logical device's responsibility. Certainly, the memory accesses within it are ensured by design and by the specification  that one logical device cannot talk to the memory associated with another logical device. And in CXL 2.0, we don't support sharing, so there's no mode to even turn that on. But it also has to be maintained through the switch. The switch has to ensure that traffic from one virtual CXL switch can't transition into another CXL switch. So there's a security aspect in the switch as well,  and then within the host on ensuring that VMs can't access memory that they're not supposed to.

Thank you. There was a question about is there a single physical layer for all logical devices in an MLD? And also, there was sort of related to that question was can both CXL.io and CXL.mem traffic access the LD in an MLD? So the answer to that is if it's an MLD and you have a CXL link, then you're sharing the physical layer. You also have a common link and transaction layers. And the LD concept comes in where the transaction types, so your requests get tagged with a logical device number. So pretty much everything starts to be on the device side at the application layer side of it  where you have these logical devices. So every transaction, for example, CXL.io has a TLP prefix. Those who are familiar with PCI Express, every transaction is that TLP prefix will carry the logical device ID number. And similarly, every CXL.cache or mem request, in this case, .mem only, will carry the logical device ID number in its request.

Maybe we could take just one more question. Can a multiported device be an MLD type with up to 16 LDs per port?

Technically, yes, it could. We've only described a multiported device being SLDs, but as we've seen before, it could be multifunction. It could be multi-LD. From the outside looking in, all we see is a CXL port. And at link-up time is when the capabilities of the device are advertised. And the negotiation takes place to decide if both ends support an MLD, it'll come up as an MLD. If one of the sides only supports SLD, it will come up as an SLD. If one of the sides only supports CXL 1.0 or 1.1, then it will come up as CXL 1.1. And if both sides come up as PCI Express, we're going to be PCIe.

 Thank you, Rick. Looking at the time, we're at the end of the hour.

Yes. Thank you, everybody.

Thank you for your time. So we'll go ahead and wrap up today's webinar. And, Ash, thank you so much for your time and expertise. Once everyone knows the recording, it will be available on CXL Consortium's YouTube channel,  and we'll be uploading the slides to CXL Consortium website. That's Compute Express Link.org. And we'll also have a follow-up blog to answer some of the questions that we didn't have time to get to today. So thank you very much, everyone, for attending.
