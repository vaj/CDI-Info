YouTube:https://www.youtube.com/watch?v=tu0W55zkSsY
Text:
Hello everyone, I am Hassan, a PhD candidate from Symbiotic Lab at the University of Michigan. Today I am going to present our work on transparent memory management for CXL-enabled systems. This work is done in collaboration with Meta CEA.

Before I start, let's first talk about why CXL memory matters, what is its properties, and how existing page placement mechanisms work for CXL memory. Memory-intensive applications are widely being used in many cases, like in low latency services and data intensive analytics. These applications may have very good performance when the whole working set fully fits in the host machine's physical memory. However, if the host machine does not have enough memory, then these applications need to go through expensive disk operations. As a result, they face severe performance loss.

To illustrate this, we run TPCC workload on an in-memory database system, that is, VoltDB, and we measure its performance in terms of throughput per second. Here, higher throughput means better performance. As we can see, if we take away 25% memory from the host machine, VoltDB loses almost 80% of its performance. If the host machine can support only 50% of the memory demand, then VoltDB hardly performs, losing almost 95% of its throughput. Same thing happens for latency-sensitive applications like PowerGraph. We run PageRank over a tweeted dataset on PowerGraph and measure its end-to-end completion time. Here, lower completion times means better performance. Similar to VoltDB, PowerGraph loses almost 75% of its performance at 50% memory configuration. Now this is obvious that you lose your memory and you are going to lose your performance. But this performance loss is highly disproportionate to the memory loss. As you can see, here only 2 times memory loss causes 38 times performance loss for VoltDB, and 4 times performance loss for PowerGraph. To resolve this, when running memory-intensive applications, users usually allocate more memory than needed.

And that puts us in a situation between a rock and a hard place. On one hand, if we allocate less memory, we might face severe performance drop. And on the other hand, over-allocation causes under-utilization of memory in the cluster. For example, in most big clouds like Google, Alibaba, and Meta, on average, 30-40% of their memory remains underutilized. Most importantly, this underutilization is not uniform throughout the cluster. Some machines can be under severe memory pressure, while the others may have a significant amount of free memory.

Flexibility in memory subsystem design and fine-grained control over the memory bandwidth and capacity can solve this problem to a great extent. However, in today's server, CPU and memory are tightly coupled, and that restricts the flexibility of server design. In today, CPUs are usually connected to memories through a specific DDR channel. To increase the memory capacity of a server, we need to increase the total number of memory channels and populate DDR DIMMs at power of 2 granularity. This limits fine-grained memory capacity sizing. Not only that, all the memory nodes attached to the CPU are of homogeneous properties. We cannot mix and match different memory technologies with different cost per gigabytes and bandwidth versus latency profiles. Besides, when a new generation of CPU is available, we need to upgrade the DRAM accordingly. All the earlier generations of DRAM thus get obsolete and cannot be used anymore. This causes high total cost of ownership in data center management. For example, in meta, the total cost of memory is almost 40% of the total rack budget, and this is growing with every new hardware upgrade. The cost of power budget also increases with new memory generations.

CXL enables finer-grained server design by providing a flexible interconnect between CPU and memory. Through CXL interconnect, memories of different DDR generations can coexist together and enable different capacity to bandwidth ratio. We can also add cheaper and lower power memory technologies in parallel to the DDR ones. CXL also provides the opportunity to near-memory computation through attaching multiple accelerators to the CPU.

The prime benefit of CXL memory is it offers a byte-addressable memory in the same physical address space. Applications can transparently allocate memory on CXL-attached nodes using standard memory elevation APIs. CXL memory access is in cache-line granularity. The underlying hardware maintains coherency and consistency. Memory access bandwidth for a CXL node is similar to DDR channels. In some cases, the NUMA bandwidth can be even better than a dual-socket system. The access latency is also similar to NUMA access latency. It is expected that CXL will add another 70 to 80 ns over the NUMA access latency that is currently available in today's dual-socket systems.

However, this extra 100s of ns of latency can often be an issue for production applications if memory is managed inefficiently. To illustrate that, we run three applications in Meta's production server with large-capacity CXL memory. Here, 80% of the total memory capacity of the host server is basically CXL memory. Default Linux cannot automatically move pages between different NUMA nodes unless it's been instructed so. That's why when the CPU-attached memory gets failed, new pages get allocated to the CXL memory and get strapped there forever. This causes 14% of the performance drop for cache applications. Autonomous can promote hot pages back to the memory node attached to a CPU when the process is running. However, when the CPU-attached faster memory is smaller in size and always remains overloaded with new page allocation requests, hot page promotion from CXL memory gets postponed, and at that point, the benefit of Autonomous disappears. That's why, even with Autonomous, the performance loss for the cache application is 10% of the actual behavior. This observation holds for other applications like web applications and data analytics applications.

Thus, efficient management of CXL-enabled memory subsystems is a must to maintain the performance. However, doing so in a transparent manner is challenging for some reasons. The most important challenge is how we are going to manage the hot fraction of an application's working set. More hot pages served from a faster memory node means better performance. And to do so, we may need to put new memory allocation requests to the CPU-attached memory node. But this may not always be possible because of the capacity constraint of CPU-attached node. For this reason, we need to move less hot or warm and cold memory efficiently to a slower memory tier. And at the same time, if the pages moved to the slower memory tier becomes hot, we need to promote them back to the faster memory tier for better performance. All these hot, warm, cold temperature detections and page movement needs to be lightweight and of minimal overhead. Apart from that, some specific applications may have different sensitivity towards different page types. Like for some applications, page caches may get allocated early and hardly being used later. For such cases, allocating page caches to a slower tier node may allow more frequently used unknown pages being allocated to the faster memory tier nodes. The next challenge is how we are going to decide applications that are going to be run on the CXL-enabled systems. Not every kind of operations can be benefited from such systems. To find these appropriate applications, we need to characterize existing applications working set, what fractions of their memory remains hot, warm, or cold, how long a page survived, and how frequently they got accessed and so on. In this regard, we need a specialized tool that is transparent to both application and production environment. For brevity, in this talk, I would mostly focus on the first challenge, that is, how to efficiently manage hot pages transparently.

And in this regard, we designed TPP, an in-kernel transparent page placement mechanism for tiered memory systems. This employs a lightweight demotion mechanism for warm and cold pages, which is complemented by an efficient hot page detection and promotion mechanism. TPP decouples existing Linux kernels' page allocations and reclamation logic to reduce latency overhead. Besides, it introduces a new page allocation policy that can be useful for certain types of workloads. TPP is transparent to application. Although it's mostly designed considering CXL-enabled memory subsystem, TPP can work on even traditional Luma systems or any other tiered memory system. This work is done in collaboration with Meta CEA, and currently this is being used in Meta's production environment. The patches for TPP have been published to the Linux community, and they are actively being discussed for upstreaming consideration.

So before I start discussing TPP's components, let's first see how page placement happens in today's Linux. In default Linux, every memory node maintains some watermarks to determine its load. High watermark indicates the threshold of the total number of free pages in that node. If the available free pages go below this watermark, Linux kernel marks the node as full and stops allocating new pages to that node. It starts reclaiming old pages to get some free pages. Here, paging to disk is a difficult reclamation mechanism, and obviously, this is a terribly slow process. While reclamation is happening, new allocation happens to the next available node, and later, access to those pages has high latency overhead. However, when the reclaimer finds enough free pages to support the high watermark, new allocation can happen again on that local node. It appears both allocation and reclamation are dependent on the same watermark. As reclamation is a slower mechanism, if any application has an extremely high allocation request, allocation on the local node will halt every now and then, and in that case, more hot pages end up in the remote node.

In this regard, TPP initiates a faster demotion path. Instead of paging, TPP allows migrating to remote node during reclamation. This is order of magnitude faster than paging, and to do so, TPP introduces a demotion list which tracks the inactive and warmer pages of a node. We utilize the existing active inactive LRUs of Linux kernel for page age management. During reclamation, TPP initially scans the inactive file and unknown LRUs to find the necessary number of pages to be freed. If that's not enough, then it moves to active file first and then active unknown list. After finding the candidates, TPP initially tries to migrate them to the slow tier memory node. If the migration fails for any reason like no memory on that node or the node is unavailable, TPP falls back to the traditional swapping mechanism.

TPP also allows faster memory allocation through an optimized data path. It decouples the allocation and reclamation logic mentioned earlier. We introduce two separate watermarks that are specifically responsible for allocation and reclamations. Reclamations can be configured to trigger early when a node's free memory can go below the capacity by x percent. On the other hand, allocation can happen even a smaller amount of free memory is available. This extra headroom allows reclamation to catch up any bugs in the allocation request. This eventually helps TPP to have more pages in the faster local.

For promotion, we utilize the concepts of Autonomous but make it optimized to hide its overhead and pitfalls. To find the hot pages trapped in the slow tier nodes, we sample a subset of its allocated pages and reset their available bit. So when a CPU tries to access that page, it creates a minor page fault. And this is an indication of the page hotness and this is known as NUMA page fit. Inside that fault, we determine which CPU is trying to access that page. If it's coming from the remote one, we want to migrate the page to the corresponding CPU. However, a NUMA hint may come from infrequently accessed pages too. For example, one page can be moved to the slower memory tier and can be accessed once in every one hour. In such cases, moving the page back to the faster memory tier can be a waste if it eventually be demoted back. To reduce this wastage, we consider a page's activeness before promotion. If a page is in the active LRU during a NUMA hint, that means it has a high chance of being active later on. However, this active or inactive LRUs are maintained by the kernels to handle reclamations. So unless there is no memory pressure, Linux does not automatically update this LRU list. That's why upon receiving a NUMA hint, we first check whether the page is in the active LRU. If not, then we move it to the active LRU and wait for another hint. Upon the next hint, if the page is still on the active LRU, we set its active bit and migrate to the faster tier. This approach reduces the overall traffic across the NUMA nodes over almost 10 times. During this promotion, we check the elevation watermark of the target node. If the target node is low on the free space, we don't float.

We also introduce new memory elevation policies. Many applications read lots of file pages during their initiation and create a great chunk of file-backed page caches that are heavily being used later. But due to the default local node first elevation policy, these caches can take up almost all the effective spaces of the faster memory tier. Later, when NMS pages grow, they need to be ended up in the slow CXL memory tier. In such cases, applications suffer from performance loss. Simply allocating caches to remote nodes beforehand can help in many cases. We also augment existing interleaved memory elevation policies. One can specify the ratio of page allocation to specific memory tiers and get the utmost benefit from varied memory bus bandwidth of different memory technologies.

TPP appears to the system as a new Autonomous mode. One can use NUMA balancing memory nodes to enjoy today's vanilla Autonomous mechanisms, and this is for backward support. NUMA balancing memory tiering mode enables TPP for tiered memory subsystems. In a tiered memory subsystem, if there is a single memory node attached to the CPU, we automatically fall back to the TPP mode.

And now we evaluate TPP on Meta's production environment with live traffic with four different types of applications across three main service domains. We use two prominent caching applications, one social media application, and one data warehouse application for our evaluation.

In our evaluation, we find that TPP's optimized allocation and data movement policies can handle burst even more effectively. TPP allocates 1.6 times more pages to the first tier node, and always having a free headroom in the first memory tier can allow more promotions of the trapped-out pages. Promotion in TPP is 30 times faster than its counterparts.

Considering active LRU pages as a promotion candidate helps TPP add hysteresis to page promotion. Without this, when the query starts for cache applications, it tries to opportunistically promote pages, and as a result, you can see steep changes in CPU-attached local DRAM's memory access traffic. However, TPP's active LRU-based promotion avoids these unnecessary page promotions. Page promotion rate drops by 11 times here. As a result, the number of demoted pages that subsequently get promoted is also reduced by 50%. Reduced but successful promotions provide enough free spaces in local node, and that allows more new allocations on the local node. The time required to converge this traffic is also very minimal. It's almost similar to the same case without the active LRU promotions. It takes only an extra 5 minutes to converge the traffic between two nodes.

In terms of overall performance, TPP improves the performance of all the applications over default Linux and Autonomous. Based on an application's access pattern, TPP can improve the throughput by up to 17%.

In summary, TPP is a transparent page placement mechanism for tiered memory systems that is orders of magnitude faster during demotion, and it's 30 times faster and 1.5 times better hot page promotion mechanism. It's 1.6 times better page allocations and workload-over-page allocation policy helps applications maintain a better performance even with similar CPU-attached memory.

So CXL opens numerous possibilities in memory systems. CXL-enabled remote memory subsystems can be one of them, and in my PhD thesis, I'm exploring that opportunity. There are many open challenges for the practical adaptation of remote memory. For widespread usage, remote memory frameworks need to be capable of supporting both existing and newer generations of applications. And as remote memories are usually connected through the network, performance is one of the main concerns. Besides, enabling a cluster-wide remote memory means more failure domains, and to sustain that, the framework needs to be resilient. Next comes the heterogeneity issue. In today's server, multiple compute and memory resources can coexist together. A remote memory solution should properly manage and utilize all the benefits of the hardware heterogeneity. There is always room for efficiency. Like reducing unnecessary network overhead through front-end data management over the remote memory cluster can be very helpful for remote memory systems. Applications from multiple organizations can coexist in a cloud environment, and in such a case, we also need to care about isolations and the quality of service for each of the applications. In my PhD thesis, I try to address each of these aspects. LEAP addresses the first two challenges of performance and applicability. Hydra provides resiliency with scalability. And TPP manages the heterogeneous aspects of memory subsystems. In Kona, we reduce the network overhead through cache-length granular remote memory access. To ensure the QoS-aware memory serving during isolation, currently, I am working on Aqua. You can find this open-source project on my group's GitHub site.

And that's all from me today. Thank you for your time to be here and listening to my presentation. If you have any query, feel free to contact me to this address. 
