YouTube:https://youtu.be/CRCfRsX6IyU?si=XvBO3gYH9XQfd32e
Text:
Hi, my name is Kurtis Bowman. I'm joined by Anil Godbole. We are the CXL Marketing Workgroup Co-Chairs, and today we're going to talk to you about CXL and how it advances the coherent connectivity in servers and other devices.

As you look at our board, it's made up of a really strong group that represent the compute devices, the supply of other devices, OEMs, as well as the cloud service providers. That combination gives the full group the chance to look across the market, look through multiple markets, and make sure that CXL stays on track, meeting the needs of those markets as we go into the future. That helps to direct the 275-plus members that we have and make sure that the open standard is relevant as we move through the generations.

What you'll see is we first started the group back in March of '19, when we released spec 1.0. Then we incorporated and started bringing in members in September of the same year. We started to release a 1.1 spec, where the big addition to that spec was the compatibility area. Then we went forward and we looked at what was needed in the industry. 2.0 came out to actually broaden the scope. I'll get to that on the next slide. 3.0 came out and broadened the scope even more. And then 3.1 added some pieces to help fill in the gaps that we felt like were there for 3.0.

And as you look at our growth, the bottom left-hand side of this slide shows 1.1 really was about CXL inside the box, making it so that you could easily expand memory or add accelerators inside the box in a CXL interface. CXL takes advantage of the same slot that's in PCIe, so it made it very easy for OEMs to integrate CXL into their current systems. Then, when we went to 2.0, we started thinking about the rack scale. So, you could be inside the chassis with multiple nodes, or you could be at the rack with multiple servers and start to look at how do I expand beyond just inside the box. And this allowed for accelerators or memory to be put into a chassis by itself and then accessed by multiple nodes or a single node, depending on what you were looking for. And then finally, with 3.0 and 3.1, we blew out to a full fabric. And by doing so, we started to really allow for a disaggregated environment. And we also enabled sharing so that every device that was out on CXL could be shared by multiple nodes. We think this was really important in that it also gave things like accelerators the same rights as CPUs had had in the past, in 1.1, 2.0. And so, it really has allowed for memory sharing among multiple heterogeneous compute devices.

And as we look at the next slide, it's really kind of what it talks about. You can see on the left-hand side, I've got my CPU with its attached memory. And that's worked for us for decades. But what's happened is we've continued to add cores to our projects. We've continued to add cores to our projects. And when you start to look at bandwidth per core, memory bandwidth per core, memory capacity per core, we've seen that that's been on a slope that's kind of gone negative over time. CXL allows us to do things like add more memory. When you add memory, you also add more bandwidth for that memory. And you can also see that you start to add those heterogeneous devices. So you can add networking devices. So you can add networking devices with compute or without compute. You can add devices that would allow you to add storage. And what that does is really put your system in a place where it can grow to meet the needs of what you're doing in the future, rather than having to buy all of that up front and drive it as an initial cost.

The other thing we've seen is memory has really changed its environment. So, we started with the memory on 1.1. HBM has come into popularity, as has the CXL memory, both inside the server and outside the server. And, as you can see by the triangle on the right, as you look at things like cache, which we definitely need, you get into the HBM memory where it gives you lots of bandwidth. Then your main memory that's attached to the system. Then you get into the HBM memory where it gives you lots of bandwidth. Then your main memory that's attached to the system. Those are all hard to increase without spending a lot of money. CXL gives you a pay-as-you-grow model. And that's what allows you to spend what you need when you need it. And so, you can either add it directly to the server, or as the next box down shows, you can start to add it as a shared resource across the fabric-attached memory. So, with that, let me turn it over to Anil to take you through the rest of the presentation.

Thank you, Kurtis. Again, hello, everyone, from my side. Now, I want to talk about some of the other aspects of the CXL Consortium, basically talk about what really makes it a standard. So, let's start with this ecosystem slide. Kurt already pointed out how the consortium has now more than 275 member companies. And the members, the whole, if you go across, you've got all kinds of providers: the CPU, GPU makers; the accelerator makers; memory expanders; right, software; test equipment; so on and so forth.

So, one of the biggest things about CXL—next slide—is about, oh, okay, before we get to the compliance program, I was going to point out with this slide that the ecosystem is growing, as we saw. And we also work with one of the industry analysts. There are several. Someone called YOLE; they put out an initial estimate based on interviews with the various ecosystem players and some customers. And this is the takeaway here: the market is definitely growing. It was launched really two years ago; the first ES-level products came out. But as we grow, I think starting this year, the real version put out of products will be launched. And then we'll start some meaningful growth. So we expect, like, by 2026, if we include the memory expanders and the switches, and all these other devices and the memory itself, the market will continue to grow.

Okay. And then, okay, so now the compliance program, right? So this is the thing which really makes the CXL consortium like a real standard. As you can imagine, a company which puts out its product saying it's CXL compliant has to, you know, has to be compliant. It has to have some kind of guarantee that, hey, it is going to interoperate with other devices in the ecosystem, right? And that's where this compliance program comes together. And we started the first CXL testing, compliance testing, back in June of 2023. And then we have had it every quarter ever since. Just like we, you know, just like the PCI-SIG does their compliance events, CXL is doing the same.

And the big benefit, right? So, what this slide is showing is about the integrators' list, which we feature on the consortium's website. So, all kinds of device manufacturers, be they CPU makers or memory device makers or switch makers, right, once their device has passed the compliance test, they can be featured on this integrator's website. And yes, for any manufacturer, if they want to sell their products, they have to know that any customer looking for CXL, putting CXL into their systems, is first going to go to the integrator's website list, and look at, okay, who's already approved or who's, who's, who's already passed compliance tests. So, that for any manufacturer will be, you know, the key. And that's why they should do their best to always make sure that their products, every new product they introduce, is compliant to the CXL spec by participating in their testing.

So yeah, I mean that; that was really the aspect I was going to talk about. So, the call to action here is, you know, if your company is looking to do something in the memory expander or accelerator space, join the CXL consortium and particularly even join the marketing group. Myself and Kurtis are the co-chairs, but we welcome new members and highlight your use cases, so that we proliferate the CXL ecosystem going ahead. Right. And yes, please follow us on X and LinkedIn, the common social media networks out there. Thank you. Thanks for your participation, and thanks to MemVerge for inviting us to speak on behalf of Compute Express Link.
