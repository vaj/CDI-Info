
Welcome, everyone. My name's Kurtis Bowman, and I am  your moderator for this session. I'm very pleased to be joined by Sandeep Dattaprasad  from Astera Labs and Steve Scargall from MemVerge. We're going to talk to you today about how  we believe CXL can be used to improve performance as well  as functionality of AI and HPC applications. I've already done the introduction. Gentlemen, do you want to introduce yourself  a little bit more?

Sure, absolutely. Thank you, Curtis. Hello, everyone. Thanks for joining us today. My name is Sandeep, as Curtis mentioned. I'm part of the product management team at Astera Labs,  and I'm responsible for Astera Labs CXL memory control  or product line.

Yeah, thanks very much. So I'm Steve Scargall. I'm the product manager and software architect at MemVerge. So I spend a lot of my time, most all my time,  working on CXL line of products, so memory tiering, sharing,  and some of the other things that we'll get into today  as well.

All right, let's go to the next slide. So just a quick intro to CXL. It is an industry standard that's  been evolving since about 2019. And it is set up really to be something  that allows you to connect multiple types of devices  on an as-needed basis. And so some people call it pay as you grow. Other people give it different names. But it's that composable infrastructure  that allows you to bring in the types of devices  you need for your system and its performance. One of the key pieces is it's backwards compatible. Key there is it is something that allows  you to save your investment. So if you invest in an early version of a CXL device,  it continues to work as you upgrade  your network and your systems. And because it is composable, it tends to lower overall cost. You don't have to buy more and more and more servers  every time you want to expand a particular entity. You can buy just that piece. So if you need more memory, you can buy just more memory  and improve your performance because you  get more bandwidth and more capacity as you do that. Also, in order to be a good standard,  you have to have a compliance program. And so CXL has put together a compliance program. If you go out on the website, you'll  actually find what we call an integrators list of the people  that have passed that testing to make sure  that they can be compliant and interoperable  with the other devices. And finally, something to point out  is it's not just a spec anymore. CXL devices are out of the market by multiple companies. You can deploy them into your infrastructure today.

The next slide goes over how we came out  with the specification. So if you go all the way back to 2019  with our first specification, it was a point-to-point  interconnect. It had a set of devices we called type 1, type 2, type 3  devices. But it was really intended to be inside the box. With 2.0, we added switching capabilities, still, though,  local to the box. Those are the devices that are out in the market today. With 3.0 and 3.1, we actually started  to add a fabric piece to the specification. So it really allowed for multi-layer switching,  talking from multiple devices to other multiple devices,  brought in the sharing capabilities. But because now we were going outside the box,  we also needed to do things like bring in a fabric manager  to talk to you about what devices can  talk to what other devices. We also had to bring in security. So we brought in what we call the TSP,  or the Trusted Environment Security Protocol. So a lot of things came in to allow for a fabric  to be developed around CXL. And that's really the big changes  as you think through it. And you can imagine those 3.0, 3.1 devices taking a few years  after the spec is available. So imagine those out in the 25 time frame for, again,  putting into your infrastructure. So with that, that's just a brief overview. Now I'll turn it over to our experts.

And the next slide is a great slide for them  to really talk about what is CXL, how does it get used. And I'll hand it over, Sandeep. Why don't you tell us a little bit about how you see CXL  being used with AI and HPC?

Yeah, thank you, Curtis. Yeah, that is an interesting topic, isn't it? Because considering the level of traction  we've had in the industry, particularly related  to generative AI, such as ChatGPT,  we need to be paying close attention to what can be  developed today with the newer protocols such as CXL 3.1,  and how does it aid in generative AI and HPC. So we're talking about generative AI. The first part is the training part,  which requires significant bandwidth, which  is often met by GPUs to ensure large amounts of data  are processed. The second part is more interesting,  which is the inferencing portion, where  the trained language models, such as ChatGPT,  would rely on augmented generation. For LLMs, you need an AI framework  that enables use of additional data sets to improve accuracy. Because when you are typing in a query into ChatGPT,  you really need accurate results to be showing up. And for that, you need to store high dimensional data,  such as images and text, and so on and so forth,  so that the query will return the necessary result. And this is where CXL comes into play. And to move more to that point, here  we are showing memory expansion through single host, as well  as multi-host topology. In neither of those cases, what you're really enabling  is expansion of memory to generative AI types  of use cases. Multi-port memory expanders are a lot more  applicable for, obviously, when you have multiple hosts  and you want fabric managed capability. And this is where CXL protocol comes into play,  wherein it has added a host of fabric management capabilities,  FMAPIs, that are going to aid us in developing the entire fabric. Also, to add a little more on the TCO side,  today, GPUs generally use HBMs to increase the bandwidth  and lower the power consumption for AI applications. However, it's extremely expensive. So one alternative and a plausible solution  is you can use CXL, which would expand the GPU capacity  beyond the limits of HBM, and address  what's called as memory wall problem, where you don't have  sufficient memory to efficiently use processing capabilities  of attached processor. So essentially, you're expanding the capacity, as well as  the speed of data transfers, significantly. Because otherwise, you will not be  using the hardware compute capabilities efficiently. And it's effectively what today's solution provides. In fact, we have our new smart memory controller  through Astera Labs, which demonstrates  how you can overcome the memory wall problem in AI. We can talk a little more about HPC in a minute,  which is where the second block diagram with multi-level  switching comes into play. Maybe to that end, Steve, would you  like to expand a little more on the AI use cases  that we can target with CXL 3.1 capabilities?
 
Yeah, absolutely. So I guess before I get there, maybe just expand  on what Kurt said earlier. Now, this is more than a spec, right? So for quite a while now, the Linux kernels had CXL support. So as you were explaining in the previous slide,  we started with CXL 1.1 back in the 5.16 kernel. And now, at least at the time we're recording this,  the 6.9 kernel is almost ready to be released. And through that evolution, we've  gone through adding the CXL 1.0, 1.1, 2.0,  and we've also got 3.0 and part of 3.1 spec  being integrated as well.So as far as operating system support goes,  at least from the Linux side, we can  cover most of these topologies and use cases. Which  is good news for us as software guys.  Because that allows us to begin working  on the next generation of software  so that when the hardware lands, we're all ready and prepared  and we're not trying to catch up. And the way we do that is, yes, the kernel has support for CXL,  but unless you actually have access to hardware,  it's very challenging. So the other piece of the puzzle here  is that QEMU, the virtualization engine,  also has some of the key features of CXL. So we have memory expansion. We have DCD, the dynamic capacity device. We've just been pushing up some patches  for multi-headed devices. We can do memory sharing and others. So whilst it's not hardware performant,  it is at least functional for the software developers  out there. So again, that's been a great advancement for us. And then as we go to the right picture there,  fabric management, how do we do that? Again, QEMU has been helping us develop fabric management  software. So there's some software out there  from Liqid and Jackrabbits that's  open-sourced their solution. And the vendors will probably have  proprietary APIs for their switches and endpoint devices  and this, that, and the other. So the ecosystem is coming together and giving us  all the tools that we need to build  on top of our final solutions. And that includes the AI side. So I like what you were saying earlier, Sandeep. The data throughput is key. I mean, we don't show GPUs in this picture,  but you can definitely imagine GPUs  either connected into the hosts or be in a separate GPU  cluster. And then how do we get data into those devices? I mean, that's the key here is traditionally  data has to come in over traditional storage protocols,  which are inherently slow, probably over the network. Now, as we move over to a fabric network,  we significantly reduce the latency  and vastly improve the bandwidth and capabilities of that. So if my data from my AI model, whether it  be training or inference, is sitting out there in a memory  fabric, now I have the opportunity  of reconfiguring that fabric and moving my data  to where it needs to be. So if I need to train first, then I  can provision my topology such that my data  is pointing at my GPU clusters, and I can go train my model. Once it's trained, I can then move the results,  the trained model, and move that over to some other environment  where I can then start to do inferencing on that. So now we can start moving data closer to compute  and where it's needed without having  to do massive data copies of how we do things today. And we demonstrated some of this at the NVIDIA GTC conference,  just a local data with a local GPU,  but showing the clear benefit of CXL  and being able to feed that beast, the GPU, with data  faster than you can do traditionally  if your data set spills over from the relatively low memory  capacity of GPUs today. So yeah, we were able to show that data was available in DRAM,  and then the data set was larger than DRAM,  so it then spilled over to CXL. But we were able to achieve a 90%, 95% utilization of GPUs,  which is the most expensive resource in this type  of environment, right? So making sure they're fully utilized or better utilized  than what you can do today gives you also better time to result  or better time to insight. So not only are you getting better TCO on the hardware  that you've purchased, but you can also either do more uses,  if that's data scientists, researchers, et cetera,  on the hardware that you have available  and getting the results faster.

Yeah. Yeah, thank you, Steve. That's a very good point. Fabric management capabilities, having the software  infrastructure to enable that was extremely important,  and I'm really happy to hear that with CXL 3.0  and its capabilities of 3.1 to be more specific,  we are able to get there. But I do want to expand a little more on HPC side of it  in the sense that when it comes to HPC,  it's basically a collection of separate servers  called nodes, which are connected  via fast interconnect. Obviously, there are different types of nodes  depending on the task that you want to accomplish. But essentially, what it comes down to is for HPC workloads,  you need a mixture of accelerators, GPUs, TPUs, FPGAs,  so on and so forth. And I bring that up because what you mentioned earlier  about composability, which is enabled by 3.1,  is extremely important to enable HPC use cases. If you want to accomplish having systems  with disaggregated resources, then you  need to have protocols like CXL that enables it. And for clusters and data centers,  where modular expansion is essential as we  grow the infrastructure, CXL 3.1 becomes imperative. So yeah, it's very interesting to see the ecosystem develop  along with the protocol.

I agree. So great thoughts there, Hiz. Now, do you see a big difference in the usage models  for either hardware or software as we look at AI versus HPC?

So in my opinion, there is a little bit of overlap  and a little bit of difference. The overlap is really on the hardware side of things. In other words, the kind of hardware infrastructure  or resources that you target would be somewhat common  between AI and HPC. In other words, whether you are trying  to accomplish composability via various hardware components  being part of the system or having a homogenous system,  your objective would still be expansion of resources, memory,  bandwidth, so on and so forth. So that's where the overlap or commonality is. The differences between AI and HPC use cases  would be the application, upper level application that  would use these hardware resources. Maybe to that end, Steve, you can add a little more  on how it differs on that level.

Yeah, I think I would go back to the where's my data  type of thing. I mean, HPC is typically an MPI type of interface  where I'm trying to send a lot of messages around my network  across the hundreds or thousands of nodes. And I think in the 3.0, 3.1 time frame  when I've got memory appliances and my data  is available to all nodes at all times using byte addressability,  I would probably see a future where  we would have the applications access the data in place.  Versus having to copy it locally, process it, and then  send some result back out to some other node. So yeah, I think that's the way that I would go  with this type of solution here. And how we access that, I mean, clearly the applications  need to be aware that they're running in this type  of environment, right? So go away from the traditional local resources  and more into a fabric-centric view.  again, Where my data is accessible,  byte addressable versus block addressable,  or sending some large packets over the wire, right? And again, there's some great research and solutions  in this space. I mean, there's a couple of file systems  that are currently being developed  that are fabric-aware. So rather than using something like NFS or Gluster  or anything like that, then a CXL fabric file system  has significant advantages here. So allowing you to keep your application either unmodified  or make some slight modifications to it,  but still being able to access that data. And I think for the AI side, again, it's more of,  am I doing the inference? Am I doing the training? Which piece of the puzzle am I doing? And what resources do I need? So going back to the composability thing  and from an infrastructure person,  how do we get the best utilization out  of this environment for us? So yeah, I think software has got a long way to go,  but it's definitely getting there pretty quickly. And I think it'll definitely be ready by the time  we get hardware to market here in the next year or so.

Excellent. So what I'm hearing is not only will you  see a performance increase because you're not  moving that data around, but you're  going to see a power reduction because you're not  moving all that data everywhere and you're spending more time  in your compute cycles.
 
Yeah, no, exactly. I mean, power and data center cooling and everything else  is a big factor for sustainability reasons, right? And yeah, I definitely see the topology more  on the right-hand side, right, as we  get into the big fabrics being a great beneficial solution  to that end.

All right. Any other comments on this slide,  or should we go into the security piece? We can move into the security piece. All right. So go ahead, Sandeep.

Oh, sorry. I was just going to say security is an important aspect of it,  isn't it? As we move towards expansion of our AI infrastructure,  it behooves us to ensure we have all the industry standard  security protocols to be implemented.  Particularly related to cloud service providers,  as well as enterprise customers for that matter. So yeah, there are a lot of things in CXL 3.1  that are defined related to security  that we can talk about here.
 
And Steve-- oh, go ahead. I was going to ask Steve. On the software side of the house,  how do you see the TSP or trusted execution environments  meshing with the efforts in the OSs?

Yeah. I mean, obviously, the effort to enable  this type of feature in the hardware and software  is key, right? And we definitely don't want data leaks, data breaches,  or one VM accessing data. And so this is going to be absolutely critical  for the future environments, particularly  as you do the composability. And let's say you've got some Kubernetes environment out  there where you don't really know what pod is going to be  run where. You definitely don't want a pod to exit either cleanly  and then somebody come along and access the same memory  and gain access to data they don't have access to. So we're going to be looking at that. We're going to be looking at how do we  enable this type of feature to be  able to run on the same memory and gain access to data  they shouldn't do. And the same is true for VMs. So from a kernel perspective, all that stuff's  being put in place here. The same for the virtualization technology, again, like QEMU. All of that stuff's either there or being put in place  very shortly to allow us to get that isolation that we need  in these big environments.

So I think that's a really good point, Steve,  because even with CXL 2.0, we do have a certain level  of security because it provides integrity and data encryption,  also known as IDE, which is prevalent in the industry. And with that, traffic across root complex switches  and devices at link layer would be protected. And that is the center part of this block diagram. The question now is, how do you ensure end-to-end protection? And this is where the enhancement,  if you have one, comes into play. And if you think about it, from the data center perspective,  what you need is you need protection of data at rest,  data in flight, and data in use. And you talked about virtual environment. That is also extremely key because as you  move towards more and more cloud service provider  kind of applications, virtual environments  are going to become extremely important to secure. And also, you need to confirm the identity of the device  itself. And this is where the attestation piece of it  comes into play. So what CXL 3.1 did was it implemented the Trusted Security  Protocol, also known as TSP, for handling security  at a platform level instead of just at a device level. And what I mean by that is CSPs would have multi-tenant VMs,  sharing devices that are connected via CXL. And they will need that end-to-end security  that I was referring to, including virtual environments. And as a result, things like confidential computing,  which is becoming extremely important  and is a hot topic in today's cloud VMs,  need to extend past the confines of the server  to all the devices that are attached to the fabric. With 3.1 TSP, we can enable confidential computing,  particularly targeting workloads and virtualization-based  trusted execution environment. And this is exactly what this block diagram shows. You have the TEE virtual manager,  which is TVM in this block diagram, which  is separate from the VMs that are traditionally  part of CSPs infrastructure. And all the things that I mentioned earlier  about protection of data at rest, data in flight,  data in use, can be accomplished if you  look at this block diagram. And if I were to describe a little more about that,  you can enable encryption of the sensitive data  at rest in both host and device memory. Which  is the two ends of the pieces that you  see in the block diagram. Obviously, IDE would be the centerpiece,  as enabled by 2.0, and that carries over to 3.1. And now you can control and define the security features,  enable them as required, lock the configuration,  because all this is enabled in 3.1. And last but not least, confirming the device  identity. Are you talking to the right device? And that you can accomplish even before your VMs  and the operating system kicks in. And that is what you accomplish through attestation  as well as authentication measurements,  such as secure boot. This is where all the capabilities that 3.1--  

I've got to cut us off. We're at time. But I do want to thank you and Steve both for your insight. And as we look at the future of compute,  CXL looks like it's got a real strong place in AI and HPC  environments. So thank you again.

 Thank you.
