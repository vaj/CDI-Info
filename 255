
So how many of y'all know anything or some about CXL? So, good bit, all right, cool. So when I was asked to do this,  I noticed a lot of people, especially in SNIA,  did not have any real background  in computational storage. In NVMe, try three, in CXL. So what I decided is I was gonna do kind of a CXL overview  and kind of some of its evolution  and then kind of talk about a couple use cases  for CXL that kind of come. So it's a pretty high level  and obviously because of time constraints,  it's going to not focus deeply on a lot of the mechanics.

So about me, I'm a technologist in DE at Dell. I've been there about 25 years. I've done standards for over 20 years. That's how I know probably about half the room  from doing T10, SATA. I was kind of on the original ones in NVMe with Tom Pratt. He was there too a long time ago. CXL, and then I've done a couple of SNIA twigs like CDMI  and there was a couple other I did. I'm currently one of the co-chairs  of the protocol working group at Dell. I currently focus on GenAI solutions now since GenAI,  if you're not working on it, then something's wrong. So, all right, so I do a lot of that  and then I'm also do CXL standards. And then lastly, last year or this year,  this is my first year being an empty nester. So that's kind of been an interesting transition for me.

All right, so let's start with CXL 1.1, right? So it's based on Gen5 electricals. It uses a thing called the alternate protocol negotiation  that allows it to switch between doing  at least the root port between acting as a PCIe route  or doing a CXL route. And that's done during kind of the link training. It communicates using FLITs  which contain these things called slots. What is a FLIT? Well, it's not actually ever defined in CXL. I assume it's like flow unit or flow control unit  is what it means, but they don't actually define it. And a FLIT is kind of what's used  as the fundamental communication,  I guess call it packet, that goes between on a link. And those contain slots. And a slot is kind of like a fixed format size. They have two kind of slots. They have header slots and they have generic slots. I think one's like ends up being like 12 bytes  and the other ones end up being 16 bytes. And that is kind of what the actual content of CXL goes into  as I'll talk about the different protocols. They actually go into the slots. Now, FLITs are not kind of an end-to-end routed thing. So even when we get to CXL2 and CXL3,  when we're talking about FLITs,  FLITs actually only operate from the link,  so point-to-point on the link. And like I said, these slots carry,  they carry the requests, the responses, the data,  and then some control. And these slots are kind of fixed structured. And what I mean by that is you have generic slots  and you have kind of generic slots defined G0 through,  I think it's about G7 or G8. And within that slot, there's actually a fixed structure  of what it contains. So it's not like, it's not,  it's like preset commands or preset responses or things. So it's like, it could be, let me kind of give you an example. Like an H or a G1 slot may contain  two .mem requests plus two .cache responses. So they're really fixed structured. And what slot gets used is actually done  really at the hardware, depending on what it's trying  to send, what it's trying to request. They have these kind of things  that control how they pack the slots. And those are actually the things that actually choose  what slot gets picked or what slot format gets picked  in order to do this. So that's all done kind of, all done at the hardware level. That all make sense? So CXL is comprised of essentially three protocols  and these end up being muxed on the link. And although the way it reads,  it says they're all muxed on the link. The reality is because of these,  we talked about these slots and stuff. It's actually PCI gets muxed on the link within cache mem. 'Cause they don't, there's no separation  from .cache and .mem from a perspective  of where the muxing occurs at the Rmux. So it's either a TLP slot or something oriented  with PCIe or it's CXL. That's kind of the way it works. So then we have these kind of three protocols,  those being .mem and that is for reading and writing  from, sorry, from device exposed memory. And then there's .cache and .cache is kind  of the other direction. And that allows the device to participate  with the system's coherency. So it can now own, I don't wanna use the word own,  but it can interact with memory, it can cache,  it can cache memory address and stuff like that. And then lastly, there's PCI, which is the .io. And for CXL 1.1, it was direct attached only.

So then this is the chart that everyone includes. And there's a lot of stuff on it. But in CXL, there's kind of three conceptual device types,  type one, type two, and type three. And the idea is these are really like caching devices  or accelerators, these are accelerators with memory,  and these are memory buffers. So this is kind of the, what you're seeing a lot today  of all the initial products are really around memory  buffers, memory expansion, allowing you to add additional  memory to systems that's not on the DDR bus. Instead, it's on this bus. But it still kind of conceptually does the load store,  the load store action, right? So as you'll see in these different device types,  they require different types of CXL protocols,  those being CXL.io, CXL.cache, CXL.mem. CXL.io, you'll see is across all of them. That's because that is required and is used to do  kind of the configuration of the device,  the discovery device, 'cause it follows a lot of the same  PCIe kind of constructs. It's actually, it actually is using CXL.io,  so it is using PCIe for discovery,  a lot of the air handling and stuff like that. Then we have type one includes .cache,  because the whole thing is it really wants to interact  with the processor cache and be able to kind of  temporarily own cache lines or have cache lines  that are being modified by it, and so it has .cache. And then this actually came later,  which is having the capability to do .mem  to go out to talk to something else. But that was never in CXL1. It's actually like a 3.0 thing. And then we have the type two device. So the type two device is like this,  but it also has memory attached. Now the one thing I wanna say is memory that's exposed  through this kind of HDM range is actually,  when you expose memory through CXL, it's through an HDM,  or host device memory range decoder. And that memory ends up being owned by the host. So even though it's on the accelerator,  technically the host owns this memory. That gets exposed. You can have private memory and do whatever you want. And essentially it has to use cache protocol  in order to interact with that own memory  that's really its, 'cause it is not the true owner  of the memory from the perspective of the host. So that's really what .cache is used for in this model,  is to allow me, and think in the future  of computational storage device,  to write to my own memory that I've exposed. And this has changed a little bit  as we've added this thing called HDMDB,  which I'll talk about in a little while. And then lastly, we have memory buffers. And this is kind of the simplest case. This is kind of what most of CX,  if you see CXL1 or 1.1 devices,  they're gonna be these devices. And they again started out as this HDMH memory,  which means, and these got added actually later,  the -h, the -db, and the -d,  which was really talking about,  and it kind of goes to the generations of CXL,  why they got added. So this is like host memory. This d means kind of device member, and db,  and I don't actually know what db means. I assume it has to do with back invalidate,  but I don't actually know what the acronym is, to be honest. So those are kind of the three. And you'll see things kind of changed over time.

So this is kind of the history. The only thing I really wanted to point out here  is they had a pretty fast cadence. But you'll see from 3.0 to 3.1, there's a pretty big gap. And a lot of this has to do with the stuff they added. And a lot of it has to do with, I don't think CXL,  3.0 was ready. We added things kind of on the hardware side,  but we didn't have the management structures  to actually use it. So we added, and there was some value to that,  in that vendors could start building the hardware  and put the channels, 'cause so CXL uses a lot of concepts  of channels, directional channels,  for its .mem, .cache flows and response flows. So it was good that we put it there  and the hardware was baked,  but a lot of the management side of it,  of how you management, how you use it, was not in there. And most of those management things end up being software. So that's why it took a while to get CXL 3.1 in.
 
So in 2.0, so remember 1.1 was direct-attached only. So in 2.0, we added kind of the single-layer switches. So now you can think, I have a single-layer switch,  I can do concepts like memory fan out. So I can have my root, a switch,  and a bunch of memory drives. Think, you do this today with NVMe sometimes,  if you don't attach to direct,  you have a root, a switch, and a bunch of NVMe drives. Same kind of concept. It also added the concept of virtual hierarchies  and what I'll call zoning. So I could have a host and kind of create a zone  between these guys, so this host owns these two,  these guys have zones. You have that in PCIe. A lot of vendors support this through,  you know, vendor-specific features. The one interesting thing was is that  because we had this standardized fabric manager,  we actually created a kind of a standardized API  that allows you to control this. Now one of the downsides of this was  that we can only have one T1, T2 device  within a single hierarchy. So these two devices in host one  could never be both .cache devices. The next thing we added was really the ability  to take this device and break it up  and allow it to expose different parts to different hosts. So that's what you're kind of seeing here. Right, yeah, this guy owns all of this,  device two owns this part,  device or host three owns this part. Now the memory here is not shared,  so you're really, I mean,  from a perspective of the backside memory  or if you're, you know, thinking in NVMe drive, right,  the media behind it only gets exposed,  a portion of the media only gets exposed  to a single device at a time. So that's why they called it pooling. We added global persistent flash  or global persistent flush,  and this was really a two-stage mechanism  that allowed you to flush all the things. 'Cause think about an environment  where you potentially have what's shown here,  where these devices are attached to this device,  plus you have a little bit here,  you have some off here that may be CXL  on a different root port. Depending on what type of device it is,  so let's say this is a caching device  and it's caching data from here  or it's caching data from up here,  'cause they can cache this data too, right? Now it's owned by the host,  but it can technically cache that data. So you needed the global persistent flush thing  to basically tell everyone to flush everything, right? Because this guy, just 'cause this guy's done flushing  doesn't mean the rest of the system's done flushing. So that's kinda why they added the two-stage mechanism. Lastly, we added IDE,  so basically link encryption on here. So CXL today does not support flow-based  or stream-based encryption that IDE  on the PCIe side does. And that's actually because of the FLIT,  I mean, initially because of the FLIT concept  and the concept that content in the FLIT as it transfers  does not necessarily go to the same place. So when a FLIT comes in with a bunch of messages and stuff,  those messages may not go to the same destinations. So if I did, let's say I did two memory reads coming in,  one may go here, one may go there, right? So you couldn't kinda encrypt on the link  or do selective encryption at the moment,  it probably will come, right? Because when that FLIT comes in here,  and even in more granular terms, inside the slot today,  the slot gets broken up, right? So as this FLIT comes in, it has four slots. And the four slots come in, inside those four slots,  you may actually have memory commands  that are gonna be destined for two different things. So the switch is actually very complicated in CXL  in that it has to take it in, it unpacks it,  figures out where it's gonna go,  throws it onto different outbound queues,  repackages it up and sends it. So because of that, the initial uses of encryption  were really at the link layer, right? Think kinda MACsec, I just encrypt everything.

All right, so then 3.0 came along,  and you see we're getting more and more complicated  and trying to address more and more use cases. We bumped the speed up to Gen 6. We also changed kind of the FLIT structure  from being a 68-byte FLIT to now we went  to a more bigger 256-byte FLIT. And that added a lot of ability to do other functions. Now the only downside is a lot of the new functions  that get added are not backwards compatible  with the older 68-byte FLIT, just because the field  doesn't exist in the old structure  that's in the new structure. But what it did give us is the ability to do,  oh, I also should point out that in 2.0,  well, because there's only one switch,  it doesn't really matter, but in 2.0,  addressing or let me say routing was based on address, right? So when I had, so inside like a .mem read,  there was basically an address of what I was trying to read,  and the switch did the routing to know what device  that went to based on that address  in what virtual hierarchy it existed in. So everything was basically address routed. So there's no kind of concept that most fabrics  don't use kind of addresses. They use more ports or MACs or something else  to actually to do the switching. So this is actually, but this is pretty,  this is kind of how PCIe works. So it kind of followed that model. So in 3.0, right, we added the ability to do,  kind of have what I'll call an ISL  or have the ability to do multi switches. And we added the ability to, you know,  to have more caching devices inside of virtual hierarchy. So now a virtual hierarchy can have more than one. And then we also added kind of a new  kind of routing protocol called port-based routing. And so now instead of routing based on address,  we're gonna route based on ports. So this would be more traditional of a fabric, right? Or something like that,  where you're actually routing from the end,  you're routing based on these end-to-end things,  kind of like, you know, fiber channel works or ethernet,  somewhat ethernet works and those kinds of things. And then on the other side,  if you look at the MLD concept,  which was we were pooling,  we added the capability of doing sharing. And that really came from this concept of back invalidate,  which is now previously, right,  the type three memory device was basically dumb, right? It was owned by the host. If you did, I mean, if you did,  if you had the capability to do peer writes to it,  that would be bad, right? 'Cause you don't know if the host is caching  that memory and everything. Luckily, you couldn't actually do that. If you tried to do it just based on PCIe and .mem,  the traffic would actually go up to the host,  resolve coherency and come back down. I mean, that would happen automatically. But what back invalidate allowed us to do  is now the type three device becomes more intelligent  and is now going to somewhat participate in the coherency  in terms of if a write comes in,  he's gonna have to do a snoop request to the host  or whoever and find out what the state of that cache line is  and if it's dirty, the host has to write it down  before that write comes in. So what this then enabled  was kind of the ability to do peer to peer. So it enabled peer to peer  without having to go up to the root, sorry,  up to the root port and back down, right? Now, that only really applies for CXL devices. If you were doing like a PCIe wanted to write to .mem,  it still would have to go up to the host as a PCIe TLP  and then comes down as a write to as a .mem, right? 'Cause it's a different protocol, right? You don't read and write from type three memories as PCIe,  right, it has its own protocol, it's .mem. I'm kind of gonna skip that one in the interest of time,  but essentially we added protocols in reverse. The fabric capabilities really came along  with the Port base routing  and a lot of the new structures we're adding  to allow you to do multipathing and all the other things. So really, what I want you to think is like 3.0 and 3.1,  although they're added, this which is really key,  the back invalidate stuff,  they're also driving down the fabric path. So we're putting in things that can now allow fabrics,  right, so you can do multipathing and those kinds of things. And then also they added this global GFAM. I'm not gonna talk about it that much,  but I think it has its histories from Gen Z, right? 'Cause it's kind of more like a Gen Z device  where it's a device out on the fabric  and it's address based zero to N  instead of based on the host addresses. Like most devices, even though this device  is inside what I'll call the fabric,  its HDM range for this device is in his host map. This S2 is in his host map. This S3 is in his host map. And this guy has to track what address,  who's coming to talk to me and from what host,  and therefore what physical address  I have to have to respond to for these different things,  even though they're talking to the same backside media. That make sense? 'Cause they're all in different host address spaces.

 I'm gonna skip that one based on the time.

So lastly, 3.1, we started to build the fabric. So now with PBR, you're going to be able,  I'll say that clearly, you're going to be able,  assuming there's silicon and there's software  and everything, that allow you to do really  these kind of more mesh oriented things. And that's what 3.1 was really about,  was the fabric manager. We added trusted execution 'cause apparently  that's important now, so we added that. But it's only direct attach for now. And a lot of that has to do with actually the IDE stuff  of not having selective flow IDE. So, and then additionally, the addition of the HDMDB,  which is kind of the back invalidate,  the smart type three memory adds a lot of complexities  to trusted compute if you can have peer writes coming in  and that changes the dynamics  of the trusted compute environment. We have GIM, which was Global Integrated Memory,  which was a concept of now I can use,  actually, I skipped the UIO slide, didn't I? You can use kind of UIO memory or UIO,  which is called unordered IO. It was a thing that PCIe added and it allows,  and it's a separate virtual channel and allows you  to kind of not follow the standard PCIe ordering rules  and to get into like a device. And CXL can use that for peer to peer now. And then we added a bunch of RAS stuff, which I can cover.

So this is kind of the GIM concept  and it's essentially allowing the concept of using UIO. You can kind of access this guy's memory. It is kind of the concept. UIO and PBR, you can do this kind of access. It's really kind of what it's about.

So yeah, that just shows you.
 
All right, so now let's talk about kind of the CXLness. And how much time have I got? But I don't wanna take too much. So these are kind of the storage use cases. So when I started looking at kind of CXL and NVMe,  even internally, people were like,  yeah, we're gonna put NVMe on CXL. I was doing kind of CXL committee  and I would sit in and go, why? What's the value of putting an NVMe device  with a CXL interface? You have a register interface, you're block,  you put your submission queue, you ring the doorbell  and it fetches the data. What is the value of CXL? I say, yeah, you can do it. And if you ever build one, I'd be interested,  I actually would be interested to test it  just to see if it's any better. But reality to me is I expect it to be slower  because of the overhead of doing the FLITs  and everything else. So then I started thinking, well, CXL or CXL,  NVMe has these other concepts like CMB,  controller memory buffer  or persistent memory region and things. And those today are accessed via MMIO  and what can go in there? Well, the spec is somewhat, gives some kind of ideas. You can put data in there,  assuming that the bits are set to allow data. You can put queues in there. And so would putting that on a .mem range make sense? For this, yeah, maybe. I think so. But it really comes down to, a lot of people will say,  oh yeah, it's cache coherent now. Well, not necessarily. It's cache coherent for one host, right? The one host who owns that drive,  yeah, he can read drive to it. But wouldn't you wanna have the ability to do peer? Which, okay, yes, you can,  but now you need like an HDMDB device. So you need that smarter device that knows to issue  the back end validate. So yeah, okay, that's a possible. I don't know if it really would provide you that. What I struggle with is I don't know if it'd really  provide you that much performance gain  versus sourcing the data from somewhere else  or writing it there. I mean, he talked a lot about, he's doing NVMe peer,  'cause he has a computation storage device. He can fetch via, so long as the PRP  or the scatter-gather sources from there, it just does it. And you could do this too. You can do it even without NVMe, right? Because it will go up to the host, to the root port  and come back down and achieve the same thing. But I don't know if there's really gonna be  a performance gain in it. That's something I'd like to test  if anyone actually builds these devices. Putting the queues there,  I don't know if there's that much value. And the reason I say that is 'cause you still have  to synchronize the head pointer, the tail pointer. You still have to ring the doorbell,  all those other things that still need  kind of some of these posted writes. So you're still doing them. So is there that much value of having them,  the ability to either fetch them from the host  or write them, so do the post or the .mem write  versus actually during the ring the doorbell, go fetch. I don't know the answer, or ring the head pointer,  I should say go fetch. Lastly, you could map them in as part of a namespace. This gets interesting if you have coherent access  versus HDMDB. And you could probably map,  and I think a lot of this will end up,  you'll end up starting to see in Jason's presentation,  I hope, or on computational storage,  'cause a lot of it I think maps to it. Oh yeah, here, right here, and you'll see it. One of the interesting things I think may be useful  is adding, if you're not gonna do any kind of CXLness,  adding UIO support to the NVMe drive  I think will be an interesting, right? 'Cause that allows sourcing and destinations  of CXL.memory going through PCIe  instead of having to up, or going through,  and I guess it can be PCI,  instead of having to go up to the root and back down. So this is, I think, I think this would be  one of the interesting ones if we're not  directly talking about computational storage use cases. And I don't know, maybe applicable to SDXI too, right? Yeah. Now, the issue with UIO and with CXL  is everything has to support UIO. So the T3 device has to support UIO,  the switches have to support UIO. That's kind of the downside of UIO.
 
And then we have a external storage. Two minutes? Okay, cool. So this is really, and this is more very conceptual. There's a lot of work that needs to be done on this. But today we have, there are some big storage systems  that kind of have this concept of front controllers  and then they have a backside fabric  and then they have kind of the media devices  and these guys can all access any drive anywhere. I mean, there's a couple of them. Obviously, Dell sells one too. And generally this is InfiniBand based. And so what this slide is really about is  could we replace InfiniBand or that other backside fabric  with a CXL fabric? And the answer's kind of yes,  but the answer's kind of it needs a lot of,  it needs a lot of help in terms of,  I need UIO support, GIM support, PBR support. And there's a lot of aspects of HA and error handling  that in my opinion are still kind of missing in CXL. And a lot of those have to do with  what happens if the memory disappears? How do you do real multipathing? Do I map both of the, do I map the memory up to the twice? Who is gonna like kind of hide? Or does the application see both memory and say,  I only use this one, I only use this one. In case of a failure, then I do the system. All those things need to be solved. So this one I think is very interesting,  but it has a long way to go.
 
And then lastly, I'll close with this slide,  is composability, right? The one nice thing I think that with PBR,  so now I can build these fancy, fancy meshes,  and with this fabric manager,  I now kind of have a standardized way  of kind of starting to build this,  where I can now zone storage, networks, storage,  maybe a VF from this NIC and a NIC. I mean, you have endless kind of possibilities  of how you can merge all this stuff together. And I think the enabler is really around the API. And there's work going on, I believe, in,  DMTF owns Redfish, right? Yeah, in DMTF to enable this API at a composability. Now this too, I think there's still more,  some work to do, and I think like CXL4 or 3. whatever coming in, really is gonna start focusing  on quality of service, multipathing,  and kind of air handling kind of within the fabric. And I think that's it.

 All right, fantastic, let's thank Kevin.
