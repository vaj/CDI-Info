
All right, so my name is Jason Molgaard, and I am a Storage Solutions Architect with Solidigm. I have a number of different roles within SNIA as well, co-chair of the Technical Council and co-chair of the Computational Storage Technical Workgroup. I'm here with William Martin, who is also co-chair of the Technical Council and co-chair of the Computational Storage Technical Workgroup. His day job is with Samsung and working on SSD IO standards. So together, we will give this presentation.

All right. So briefly, what we'll cover is an overview of where we are with the SNIA Computational Storage standardization. We'll take a look at the SNIA Computational Storage architecture for anybody who's not familiar with it, just to level set, we're all on the same page. We'll look at the SNIA Computational Storage API, again, just to level set, and in both these subjects, we'll also talk about what are the new things that we're working on, what's new that's coming soon. We'll make a comparison between SNIA and NVMe Computational Storage, as there are some differences in the implementation. So we'll cover that, and then talk about something new that we're working on, Computational Storage and SDXI.

All right. So where are we? Currently, the Computational Storage TWG has developed two different documents. We've got an architecture and programming model document. A version 1.0 was released in August of 2022 and received a most innovative memory technology award at FMS in 2022. We're currently working on a version 1.0.4 that is available for public review. You can go download that. It's from the SNIA website and take a look at what's changed, what's different, and provide your feedback. If you've got any comments, you can submit those comments and make it available to everyone within SNIA, so we can consider those. There are two main changes that we have worked on in that architecture and programming model. The first one is sequencing of commands, and the second is security enhancements for multiple tenants, and I'll get into both of those here in a subsequent slide. From the API perspective, there's a version 1.0 that was released almost a year ago in October of 2023, and it received the most innovative memory technology award at FMS last year as well. We're working on a version 1.1, and again, we'll cover the details of what that looks like in an upcoming slide.

All right. So let's take a look at the architecture and make sure that we're all on the same page. So SNIA has defined three different architectures for computational storage. On the left-hand side, we have the computational storage processor, and this is a device that has computational storage resources, but does not have any device storage. So it sits on the fabric, it can perform offload, it can perform computations, but it doesn't have the ability to store any data, it doesn't have any persistent media. So computational storage processor is abbreviated CSP. The computational storage drive in the middle are CSD. This is more of a traditional drive that you're familiar with, that has computational storage resources. So it has a storage controller, it has the storage media, so it can store data and it can process data as well. Then over on the right is a computational storage array. This is a traditional array that you, you may be familiar with that has multiple drives, but it also has some computational storage resources. And with those resources, of course, you can do computations on all the data stored in those drives. The architecture also allows you to have computational storage drives as part of that array. So you could theoretically have multiple levels of offload, although I'm not aware of anybody who's working on that currently. So just a bit of nomenclature at the bottom, you'll notice here we've got CSx defined. This is a computational storage device, and it's any one of a computational storage processor, computational storage drive, or computational storage array. I've definitely heard CSD misused in, in the world out there where people call that a computational storage device. A CSD is a computational storage drive and a CSx is a computational storage device.

All right. So the one thing that was common across all three of those architectural pictures is that teal colored block that says, "computational storage resources." So let's take a little deeper look at that block because that's kind of the key in the architecture for each of those. All right. So we have the computational storage resources, which are the resources that are available and necessary for a CSx to store and execute a CSF, or computational storage function. Okay. So what's a computational storage function? So the computational storage function, my screen was flickering a little bit. I want to make sure that the screen over there wasn't flickering. Computational storage function is a set of specific operations that may be configured and executed by a CSE in a CSEE. So this is the function that we want to execute. It's the data manipulation or the change we want to do to the data within our device. All right. So what's a CSE and what's a CSEE? So a CSE is a computational storage engine. So this is the resource that can be programmed; an example, a couple of examples of a computational storage engine. One would be a general-purpose CPU. Certainly, we can program a general-purpose CPU to do whatever we want or an FPGA is another example that could be programmed. We can program an FPGA to take on some function and do something for us. A computational storage engine environment, so this is an operating space for the computational storage engine. So if you're thinking of a general-purpose CPU as your computational storage engine, then the operating system to boot that CPU would be the computational storage engine environment. So we need to boot that CPU, bring it up, get it operating, and then we can actually execute a program or a function now that the CPU is operational. Similarly, if your computational storage engine is an FPGA, then the bit file needed to configure that FPGA, so it takes on the identity of some type of function or capability, is what is the computational storage engine environment. Regardless of what type of engine or engine environment your FPGA or your system has, your computational storage, then you're going to need some memory. And the architecture has defined function data memory, or FDM. And so this memory can be used for temporary variables as the data is manipulated, transformations, whatever the case may be that's needed as part of that function as it's executing. You may have a large block of function data memory, and you want to partition it or allocate it to a specific function. That way you can have this block divided up across different functions. And so we've defined allocated function data memory. And then lastly, the resource repository is the final box that's on that diagram. The resource repository is additional resources that are available in your device, but have not been activated. So they could be activated. They could be used at a future time. But until they're activated, they are just sitting there and available.

All right, well, now that we've kind of level set on where we have been with the 1.0, let's kind of look at the new features that we have been working on for the 1.1 or the 1.0.4 that is available for public review. So the first one is sequencing of commands, and sequencing is uh, the we want to enable a sequence of CSFs to execute in succession. So in other words, if we want to, we want to read the data, we want to perform a manipulation, and then we want to write it back out, that could be a sequence, and we want to give that whole package to the drive as one operation, say please go do this and then report back when we're done, uh, and so uh, and I've got a couple of examples on the next slide uh to talk about a little bit more about sequencing. So sequences typically execute in order. That's the way we've defined it in the architecture. And we want minimal host involvement, right? We want to kick this off and have the sequence just execute and only report back when it's finished. In order to support that, we've defined an aggregator CSF. So this is a CSF that basically manages the execution of the sequence. So it's going to track the completion status of each CSF. It may be preloaded or downloaded or preinstalled. And it supports both fixed sequence and variable sequences as defined by parameters that are passed in from the host. I have a couple of examples about that coming up here on the next slide. So error handling is actually the area where we had to spend the most amount of our time in defining sequences. So it's certainly complicated, and I encourage you to take a look at the spec. But suffice it to say that errors may be handled by the host or by the aggregator CSF based upon ability or desire, whatever you want the system to be able to support.

All right, so let's look at a couple of examples. So on the upper left, I've got an example of a fixed sequence. We've got our read CSF, an encryption CSF, a search.slash sort CSF, the encryption CSF again, and then a write CSF. And basically what we want to do is we want to read data, decrypt it, search and sort through that decrypted data, and generate an output list, encrypt the output list, and write it back to the media. Just an example, right? You may want to do something similar. Okay, that's perfectly fine. That's definitely a fixed sequence and is going to happen in that order. And, you know, theoretically would run to completion. A variable sequence that's shown over on the lower right-hand side, I took a very similar example and adapted it just slightly. And instead of, you know, simply having the search and sort take the output list and encrypt it, I added in a new CSF that analyzes the results of that search. And if the result is more than 20 items, then let's search again. Let's refine the search. Let's get a smaller list. So send it back into search again and potentially keep repeating that until we have reduced down to something that's a little bit more manageable in size, and then encrypt it and write it back out. So this is definitely a variable sequence because it could be, it may go through one time, it may go through it 10 times. We don't know based upon the data that we're reading. It's important to keep in mind that variable sequence aggregators support, support recursive calls to themselves. So you can, you know, have recursion where the aggregator calls itself, you know, again and again as necessary to execute a sequence. So a couple of considerations that we had to spend a lot of time thinking about in the computational storage TWG. So what if the data, you know, thinking of our fixed sequence in the upper left, what if that data that's read by the first CSF, the read CSF, is already decrypted and no decryption is required? Can, is it an error or is it valid for it to continue on to, to the search algorithm? Well, I would suggest that it probably is perfectly valid for it to continue. There's no need to decrypt it. It's not really an error. Maybe you want to report that to the host, say that this data that you gave me was already decrypted and I didn't need to do anything, but there's nothing that prevents it from continuing execution. So, it may as well. But what if the decryption, what if we have in a different situation, the file is decrypted and it, and the decryption fails? Well, now we have a different problem. We can't really search through that, that encrypted data because it wasn't successfully decrypted. That becomes an error situation that needs to be addressed by somebody. Is it addressed by the aggregator? Is addressed by the host? And certainly that's implementation specific, but you're not able to, the sequence is not able to continue. All right. So that's sequencing.

So now let's take a look at the other new feature, which is security. So in version 1.0, that is out in the wild, we made some assumptions in order to simplify the security problem. And it was—it was a deliberate effort because we needed to try to simplify the problem and get this back out. So the assumptions are that the environment consists of a single physical host or virtual host with one or more CSxes. The host is responsible for the security of the ecosystem that the CSxes operate within. And the CSx security requirements are comparable to the security requirements of common SSDs and HDDs. So, if I can distill that down, basically what that means is, you know, we're going to have one host. The host is responsible for the security, and, you know, whatever security implementation we have in the drive needs to be consistent with what a normal SSD or HDD would support today. Clearly, we made some simplifications. We did define privileged access and the notion of elevated privileges necessary for certain operations. But, you know, the simplifications definitely aren't representative of, you know, actual systems.

So fast forward, we've, we went back and we revisited all the security, working very closely with the security TWG in SNIA to try to figure out what should a more robust security solution look like. And we came up with some different assumptions and a more comprehensive list of security. All right. So the assumptions are reduced down to that the environment consists of multiple physical hosts or multiple virtual hosts with one or more CSxes. So no longer is it a single host. There's multiple hosts. Multi-tenancy. And the security requirements are comparable to the security requirements common to SSDs and HDDs in a multi-tenant environment. So, you know, this is definitely more representative of, you know, systems we see today. And so in order to support that, we're going to have to have trust relationships. And those trust relationships include, you know, identification. You know, what's your username? And then your, and then authentication. What's your password, right? Right. We're all very familiar with that. So we have to provide that at some level, you know, to say, this is who I am and I can prove it. And so maybe that's at the CSx level, you know, at the highest level. And then of course, to support that, we have to have authorization. Are you authorized to actually do something in this drive now that you have told me who you are and proven it? And, you know, maybe you can do something at a, you know, at a CSF level. Or maybe you have complete control over it. You know, whatever the case may be. And then in order to manage that, there has to be some amount of access controls. Again, just, these are consistent with any security type and situation where you either are, will allow or will not allow a particular authenticated user to do something. So those elements of those trust relationships may be at different levels. So we may have the user authenticate themselves at the CSx level. But then authorization potentially could be at the CSEE level. And then the access controls could be at the CSF level. Are you allowed to execute the CSF or not? And, but there's nothing in the standard that, or in the specification that says you can or cannot, that has to be done this exact way. This is just an example. So it could be that you want to have, you know, your authorization at a different level. And then your authentication or access controls. But it's entirely up to you and your implementation. All right, that concludes security. With that, I'm going to turn it over to Bill, who's going to talk about API.

Thank you, Jason. So for the API, I'm going to start, as Jason did, with kind of an overview of what we presented last year, what we presented before, just to bring people up to speed for those who may not have been through these presentations. This is our kind of overview of what we have in the 1.0 API and what the goal of the API in general is. So the API is one set of APIs or functions for all CSx types, whether you're a computational storage processor, a computational storage drive, or a computational storage array. So it's one set of functions that anyone could use. They are intended to hide the device details of the hardware or the connectivity and provide you a library where you have a library function that maps to your specific device, your hardware, and your connectivity connections for that device. So it provides abstract device details. One of the things that Jason talked about was the fact that you have privileged access characteristics. There are some of the functions, for example, discovery, that may in certain cases be privileged accesses and in other cases are not, but that privilege is determined by your library as to what privilege is required for that access. You have access that requires some level of security. You have device management. That would definitely be something that you want at the privileged access level. Memory management to allocate free and initialized memory. Again, probably at the privileged access level. However, the privileged access may then hand out memory that the non-privileged user can take use of. You have the ability to download and execute CSFs. Again, depending on what you're downloading and how you're downloading, could be privileged, may not be, but the basic concept of the API is to be something that abstracts the computational storage device such that you can use any device out there as long as you have the library that maps from the API to that device.

So what are we doing now? As we move forward and we are working towards version 1.1, there are two major things that we're looking at. One is to clarify the batch operation. So we had a batch operation; we've clarified it. I'll go a little bit more into details on the next slide of showing how that actually works. But we changed the functions that create and modify the batch requests. In particular, instead of having three different functions, we have now obsoleted a couple of functions. And we've created a new function that we've called "configure batch entry." And this "configure batch entry" allows you to add, delete, reconfigure, join, or split your batch request. And we've updated the flow diagrams for doing this.  One of the things I want to point out related to the batch request. Jason talked a lot about the fact that we added the aggregator functionality to the computational storage architecture. This is another mechanism to do something similar; it's done at the library level, which allows you to either create an aggregated function on the device or to have the library actually create that or do that for you by creating it at the library level. So it's two ways of doing it. Where the aggregator function is all down on the physical device, the batch function is actually up in the host. It's managed by the host through the library. The other thing that we've been doing has taken a fair amount of time is we are clarifying the error return values.  The biggest thing about this is we had some fairly general statements of you return success, otherwise you return one of this list of errors. But there was nothing that said when you returned a specific error. We've been clarifying the spec to say what conditions cause each of these errors. So if you call your function and you get an error back, you know exactly why you got that error back. So this is important for interoperability that we have a fairly well defined set of error conditions and why you get them as opposed to just a general statement. Well, you get one of these errors without any general statement of why. So again, this is going towards better interoperability. We're also doing some additional editorial cleanup from version 1.0. We got some late-breaking comments as we were publishing 1.0 that we have addressed and been modifying to do editorial cleanups. All of this is going into 1.1.  One of the things that Jason didn't mention is what's our timeline? Our timeline right now is to try to release both a 1.1 of the architecture document and a 1.1 of the API document by the end of this year. That's becoming a little bit more of an aggressive goal just because of the amount of effort that is going into defining what causes the error conditions. That has turned into a fairly big work item. There are, I believe, 33 to 39 functions defined in the API. And for each and every one of those, we have to go through and determine what errors are actually generated. And we found a lot of places where we were very loose in our initial generation of those.

So I want to jump into an example of a batch process and how you create it. So you'll see that this batch process has a... It has a couple of items that do storage requests. Two different streams do storage requests, compute requests on that storage. Then another compute request on that storage. The output of both of those feed into yet another compute request. And then finally you copy data from the device back into host memory. Well, how do you build this thing? This is just an example, but it's one example of how you would build it. So the first thing you do is up there you'll see N1 at the bottom left corner. You're kind of building it from the back up in this case. And you have in the previous slide, and I'll go back to this for a minute, we had the add, delete, reconfigure, join, and split options to this configure batch entry. So we are doing add functions here. Every single one of these is an add function. So we have parameters that say it's before a certain item in the list, it's after a certain item in the list, and it returns what the current item number is in the list. So the first add entry had before a zero, after a zero. It returns N1 saying this is number one in your list. And it's a new entry and it's not associated with any others. Then we do a new add item. We're adding item N2 that's up there. And it is before item N1. It's after nothing. And it returns that it is N2. So this is a new entry before N1. I now add the new entry before N2. And this is entry N3. Or sorry, no, I add N3, which is a new entry. It's not associated with anything yet. And I build N3. From N3 I say, next I want to build N4, and it's after N3. The next thing I do is I build N5. Well, N5 now is after N4. And it is before N2. And that makes the connection between the top row here. So it builds the connection between N3, N4, N5 to the N2, N1 row. I then do the same thing where I create N6. It's not associated with anything. I then add N7 that is after N6. And I then connect the N2 and N7 to N8, creating my complete batch request. Now, the other characteristics that are defined is that, in addition to add, I can delete. And the spec talks about what delete does, how it breaks things apart, what happens if you delete, say I were to delete N8, that N7 would be not connected to anything, and N2 would only be connected to N5. And we also have join, where you can join things together. So all of those are defined as functions for creating and modifying batch entries.

With that, I'd like to jump into a different topic.

The topic is, how do the SNIA architecture and the NVMe computational programs command set, how are they related? So, NVMe computational storage ratified the NVM, sorry, the computational programs command set in January of this year. The computational storage in NVMe implements the SNIA computational storage model. There's a very high cross-pollination between the two groups of people that are involved in both. So, we've done a fairly good job. While the names may change to protect the guilty, the concepts are the same. In addition to that, the SNIA API supports the NVMe computational storage or computational programs command set.

So how does that interact? Jason talked to you a lot about the SNIA, the NVMe, sorry, he talked about the SNIA computational storage terms. We have similar terms in NVMe. So you have a compute namespace that has compute engines and programs in it. Programs operate on data that is in sets, and then you have subsystem local memory. So the subsystem local memory is allocated as memory range sets. It includes program input and output data, as well as a place that you can use for scratchpad memory. When you call and execute a program, you give it a particular set of memory range sets, or a memory range set that has ranges in one or more computation lines. And then you have additional subsystem local memory spaces. So NVMe namespaces are persistent storage of data. They could be NVMe command set data, ZNS, the zone namespace data, or KV, which is the key value storage. Data is transferred between the NVMe namespaces and SLM using the memory copy command.

So how do they match? There's a SNIA term, computational storage engine. This is the equivalent of the compute engine and the compute namespace in NVMe. In SNIA, the computational storage engine environment, right now that is something virtual that's not currently defined in NVMe. So it's a layer that NVMe does not have. You have the resource repository where you can download CSS and CSEEs. That's the downloaded programs. You have preloaded CSS and CSEEs. That's the device-defined programs. Now, the CSEE part of that doesn't actually apply because CSEE is not defined in the NVMe side of things. Both sides have the term activation, and they mean the same thing. In SNIA, we have function data memory. In NVMe, we have subsystem local memory. In SNIA, we have allocated function data memory. In NVMe, we have memory range sets. And finally, we have device storage in NVMe. Those are NVMe namespaces in NVMe. So that's kind of a wrap-up of how the two are associated with each other. And with that, I'd like to turn it back to Jason to talk about something he mentioned earlier of how we're moving forward with this.

All right, thank you, Bill. So one of the areas that we're focusing on, between the computational storage TWG and the SDXI TWG, is a collaboration between the two, and how could they work together.

And so, if you're not familiar with what is SDXI, it's the Smart Data Accelerator Interface. Again, this is another SNIA technology that's being developed within SNIA. And basically, it's a standard for memory-to-memory data movement and acceleration. It's a glorified, you can think of it as a DMA engine that's standardized, that everybody can access. So it is extensible, forward compatible. It is independent of IO interconnect technology, although I know in their spec they have an example based on PCIe. But that's just an example. And it provides data transformation features. And it's this last bullet here. Here in particular, that makes it kind of interesting for computational storage. And I'll get into that here in a moment on the next slide. There is a 1.0 that was published in November of 2022. If you download the slides, there's a link there if you want to go get it. The slides that are online right now, I'll tell you, haven't been updated yet with some of the material here, but they will be.

So one of the things that we've kind of been thinking about is, well, what would it combine? What would the combined computational storage and SDXI architecture look like? And essentially, you know, these diagrams probably look familiar from earlier in the presentation. But it's been expanded a little bit to include, you know, shared memory that's shown over on the right-hand side. There's red arrows and green arrows all over the place, you'll notice. And basically, what they're showing is: a red arrow is where the CSE or the CSF could be the producer of an SDXI type data movement where you potentially want to move data from, you know, one computational storage drive to another computational storage drive. The green arrows are where the host is the producer. So the host would have the SDXI instance and would be potentially initiating a data movement operation.

And so we've, you know, made a couple of diagrams that represent the vision there. Where you'd have, over on the left-hand side, we have SDXI that's external from our drive. You can see it on the, you know, external of the drive right here. And then on the right-hand side, we have SDXI actually internal to the drive. And it's specifically sitting next to the computational storage. And so why is this interesting is because the SDXI has these data transformations where it can do some type of a transformation of the data as it's moving it. That could be the compute of our computational storage or part of it. And so if we want to get data from one drive and bring it into a computational storage drive, in the process of bringing it in, we could do some sort of transformation on it that the SDXI supports that's already done now. And now we just have to do the compute operation or subsequent computer operations now that it's in that drive. And so it's still....

Offloads the host, which is one of the tenants of computational storage. We're getting data to the drive where... you know, compute that makes the most sense, which happens to be within a computational storage drive, and then still performing the operation as an offload to the host in the computational storage drive.

So this is a very, very brief whirlwind tour of what we're talking...what we're working on in this subgroup. There's more material in the conference. If you wish to learn more about it. So there's a session coming up, actually in the next hour, SDXI and computational storage overview and panel discussion. Last year we gave a presentation, and we didn't even get halfway through the presentation before we started getting peppered with questions. And so this year we said, "Great, we'll just give half a presentation and open it up to just questions right away." And people can just start, you know, asking questions and trying to, you know, get their questions clarified. So I believe that's in the Cypress room, but double check the monitors. That's definitely coming up in the next hour. And if you want to learn more about SDXI in general, there's another session later on today. I'm not sure the exact time and location, but smart data accelerator interface, use cases, proof points, version 1.1 and beyond. And Sham is the chair for the SDXI technical work group. So he'll give that presentation later on. So check the monitor on exactly where that is.

So, if any of this is interesting, then we definitely encourage you to come join us and help contribute. We're always open to new ideas. So, again, in the slides, there are a couple of links to either join SNIA and/or join the Computational Storage Technical Work Group.

And speaking of new ideas, we have a little quiz for you as we're trying to get some new ideas on where to go next. So if you scan this QR code, it will take you to a short four-question survey. And we would love to get your input. So the first question is, what is computational storage? So this is multiple choice. You can pick the definition that you think most closely matches what that is. Question two is, how would you use computational storage? So this is a fill in the blank. So you can imagine you've got carte blanche. You can get whatever computational storage drive you want with whatever functions you want. You can do whatever you want. How would you use it? Just tell us what you're thinking. Question three, what is the future and evolution of computational storage? So this is a multiple choice question or multiple guess, as it were. Your crystal ball is probably as good as mine. And this is definitely the area where we would really value your input as we are trying to decide where do we go next? Once we finish these 1.1s, how do we move forward? What are the next things that we should be considering and working on? And then lastly, the fourth question is, do you have any other thoughts or ideas on computational storage? So again, free form question. Put in whatever makes sense that comes to your mind. All right. So with that, on behalf of William Martin, I'm Jason Mulgaard. Definitely, thank you for joining us today. I think we've got a few minutes for questions if anyone has questions. Yeah, it looks like we've got maybe 10 minutes for questions. If anybody has anything that they'd like to ask.

Yes. All right. Let me see if I can try to repeat that. So do we envision having CMB for computational storage and SDXI that can do data movement and transformations into and out of that memory? Or will they be separate pools? Is that pretty good? So that's a great question, and thank you for asking that. So there's another session at 4:25 this afternoon, I believe. Or 4:35. I definitely encourage you to go to that one; it's it's about TP 4184 where we're going to essentially add to the subsystem local memory the ability to have it be host addressable, and so now if your host addressable, it you know it... So there are actually two methods, and I don't want to steal all the thunder from that presentation, but one of the methods that's supported is a PCIe BAR very similar to the way CMB is managed, and so now that it's host addressable you can have an SDXI instance in your host because it can it now has host addresses for a source location and a destination location, and it could theoretically move data from you know one drive to a different drive and do a transformation in the process of doing of that data movement.

So it would be host addressable, so the question was: is it host addressable from a in a virtual address or host addressable from a, you know, another way? So SDXI supports, for you know, virtual addressing without question, and it's expected to go through the MMU for translation. How exactly we're gonna handle that at the device level, we, we have some development to do still, so I'm going to wave my hands around a little bit and say some magic will happen. But we have to crawl first as we're trying to come forward with that. Bill, maybe has a couple of thoughts.

So one of the things you did see on Jason's presentation about SDXI is the fact that your SDXI engine could also live on the device. So the SDXI transformation engine could be either in the host or on the device. But in terms of translation, that translation is automatically done by the system of virtual addressing to physical addressing by whatever the memory management is for the system. And depending on where your SDXI engine resides, either the translation occurs before the request gets to the SDXI engine if it's on the device, or the SDXI engine may actually do that if it lives on the host.

All right. Let me just repeat that for the recording. So the suggestion is to read the Twizzler paper (T-W-I-Z-Z-L-E-R) from UC Santa Cruz, as they've done some work on address mapping and translation. Okay. Good suggestion. Thank you. Yes?

After the fact, there's been a lot of special stuff. Such as filtered filtrations, compliance, bureaucracy; I think there's a critical infrastructure before that comes out. And I can foresee that we can bolt on an existing entity, replace an existing entity with a, let's call it, a competition entity. We can do some filtering, some kind of searches, those kinds of things.

So let me try to repeat that. The comment is that there's a lot of embedded systems out there with storage in them that don't have computational SSDs today, which could really benefit from having that additional compute, given that the embedded device can't change. But you could add capability by adding the compute in the device. I agree. You're absolutely right. I think there's still a lot of applications out there. I think computational storage is still evolving; it's not reached its full deployment yet. I think there's still a lot of future for it. All right. If there are no other questions, then I think we can head to break. So, thank you, everyone.
