
Let me start my presentation. I'm from Samsung. Today's topic I'm going to present is SMDK-inspired changes for CXL, especially CXL DRAM.

Prior to start my presentation, on behalf of the SMDK team, we would like to thank you for inviting and giving us the discussion opportunities. And we also sincerely appreciate all the experts here for the advices and comments and interest on this topic. So today I have two agendas.

And firstly, prior to explain the CXL requirement to kernel and the SMDK proposal, so let me briefly explain the background of the CXL.

Let me say the background of SMDK. As people here know, CXL is a promising technology that leads to fundamental changes in computing architecture. As a CXL DRAM provider, Samsung has developed CXL DRAM hardware and software over the last couple of years. And we have seen the rapid evolution and widespread of CXL DRAM in the left. So we have been developing our CXL software development kit, SMDK, since 2021 March, working with some industries and academic partners. So meantime, we gained some kernel requirements from the works and customized the SMDK kernel. And we have been working with some industry efforts. So as we know, as a result of the work, the CXL adoption stage is, I believe, gradually moving forward from the basic enablement to real-world memory-tiering use cases. So around the stages, we would like to discuss the CXL requirements to the kernel and introduce some of the SMDK's requirements. But first, don't get us wrong. We want to explain our thoughts and approaches, but never force the approach. So personally, I majored in operating systems and experienced kernel development since version 2.4 around 2004. So I respect kernel experts very much and strongly believe it should be changed for origin religion and for public use.

So as another background, so as we know, when a system with a CXL DRAM would consider a memory-tiering solution, but in terms of a memory-tiering solution, it is typical that when I say the very high-level abstraction of memory-tiering solution, the solution is to locate the hot data on near memory and cold data on far memory as accurately as possible. And the hot and coldness of data is determined by the memory consumer of the tiering solution, while the near-far memory is determined by -- I'm sorry -- determined by the memory provider. So in this operation, the memory consumer needs an identifier to determine near or far memory. So Samsung, as a memory vendor, so SMDK put more weight on near-far memory determinism rather than hot-cold determinism. So we put our effort on this way to offer the various memory-tiering systems for memory consumer layers rather than hot-cold data determinism. So the following five requirements and the two proposals are the ordinates from the backgrounds.

So please tell me if you have any questions or inquiries about my presentation. So the first requirement is -- so this is about the CXL DRAM identifier. It could be the API or ABI. So the first issue is addressed to a user or kernel context has to use a node ID of a CXL memory node to access the CXL DRAM. So what is -- node ID is not a stateful information because it can be changed during logical memory on or offline or the physical hot add/removal operations. Also, the node ID does not present a near-far memory attribute of the node. So the user space and kernel space memory-tiering solution need an API or ABI to identify near-far memory node. And the second requirement is -- so it needs to prevent the unintended CXL page migration. So the issue was happened to the while Zswap operations. So in order to store the swapped-out pages on the far memory, in this case, the original content was saved in the CXL pages, but as you know the Zswap operation, it used the DDR pages. So we thought it is kind of a promotion because the CXL page is changed into DDR page. So what is -- so on the swap operation, the context that was employed, the far memory, the CXL page, should not be unintentionally permitted to use the near memory, DDR memory. So we thought this is kind of the unintended promotion from CXL to DDR.

So I don't think I agree with either of those requirements. I mean, we already have the concepts of different remote nodes. We already have ways to do migration. User space doesn't generally need to know the kernel handles migration to different nodes behind the back of the application or the application can ask for it using APIs that already exist. So I'm really struggling to see what is missing from our current APIs that prevents you from using the NUMA nodes like we currently do.

I'm sorry.

I'm sorry. Could you please (repeat that?)

Yeah, i guess what Matthew is trying to say is that you can Control your memory placement, right? But i guess what the concern in his -- this is that you use The APIs to explicitly put some memory on a remote node, but Then you have a memory pressure on that node. You get that memory swapped out, but it goes to the front swap, Which from that point of view is a closer memory. So essentially you are moving memory from a distant node to a Closer node while it's not being used. So essentially kind of inversion of the hotness with respect to Close memory. Is that correct?

Yeah.

Is there any reason why we can't store the pages in Zswap on the same node? Why does it have to be DDR?

So actually it happens in the Zswap operations. So as you know, the PFRA, the Zswap implements the front swap. So prior to doing the disk swap, it tried to locate the DDR page, and when it succeeded, it stores the swapped-out pages, compressed, and the DDR page. So why it happens is Zswap has -- internally Zswap has three types of allocators. So the original content was stored in a CXL page, and while it's in the Zswap operations, it is a front swap, the Zswap allocator finds out the prepage in kernel spaces from DDR pages. So it is located. So as a result, the CXL page is stored in the DDR page.

So that's what happened in Zswap operation. So the allocator that Zswap Is using, for example, zmalloc or zbot or whatever it is, has a page in DDR is what you're saying. So perhaps what you're saying is we should enlighten these allocators to use pages on the same source node.

Right. So this is correct. So this result kind of unintended promotion. So what is sort of this case, probably we could use a CXL Page as well. So otherwise, let me say in The case where you produce a DDR and CXL page, it is normal Because it is a demotion.

Yeah, right, but it's not very much different from a regular NUMA case where you are reclaiming something from what tends to be a remote node to a Zswap. So it sounds like that we need to extend Zswap to preserve locality of any memory that's swapped out.

So basically, yeah, this is kind of what happens in operation, but as you said, we need some more information that we can access. We can split the CXL or DDR memory.

One follow-up question. I mean, when we swap something to Zswap, we expect that it's cold because otherwise we wouldn't be swapping it out. So wouldn't it even make more sense to prefer slow nodes over fast nodes in that case? like, would you want to preserve like the node or would you actually want to go to a slower Node because you're swapping something out so you don't expect anyone to use that in the near future?

Right. So actually, which one is slow or which one is faster is kind of a -- it is kind of a different problem area because even the same CXL memory or the multiple CXL memory can be the slower, faster node, but here in this requirement, what the address is not to solve the problem you addressed, but to solve -- it needs to wait to protect explicit way to avoid the unintended promotion.

Okay. And the other thing is in your thought, in case number one, you said that there is no way to provide the near far attribute of a node. At least in sysfs, I know there is a distance attribute for each node, which tells you, I think, from HMAT or whatsoever, like, If it's fast or slow, i would have assumed to some degree. What else is missing there?

Right. Actually, yeah, it's true. HMAT or SLIT or SRAT or some CDAT or some information. It is geared to provide the near far information. So actually, this could be the way. But here, what you want to say is -- so it is needed, kind of -- the data information is further needed. So here, what it addressed, what it solved, why this problem.

Okay. So I guess I have heard somebody. Is somebody remote having questions or comments?

Yeah. I was trying to ask -- so the two bullet points that We kind of brought out, right, the first part, i think there is libmemkind, which is already kind of providing a user space Api to kind of help applications allocate based on, like, you Know, criteria. I want to allocate memory with This attributes kind of thing. So the fact that the NUMA ID can Change across memory hot plugs is kind of abstracted by Libraries like libmemkind. And the second part, like, you Know, whether the CXL page should be demoted, i think we Should look at this from the point of view of hierarchy. What does it mean to have a hierarchy with the CXL device And a front swap, right? Where do i put Zswap? Should Zswap be a lower memory than a CXL tier? I think that clearly controls where the demotion happens and How the demotion happens. Clearly not sure why CXL page Is getting demoted to Zswap with the DPR kind of thing. The only reason could be that the compression overhead is Higher than the latency access, right? So those are the two things i think i wanted to bring up.

I'm sorry. I don't catch the point of your Question. 

I was not sure whether others are able to hear. The first part of the point, Can libmemkind solve that problem?

You mean the memkind? Right. Yeah. memkind is kind of the high level use case that needs this one. So we also use the jemalloc extension, you know, the base library of memkind. So, yeah, the hip extension, the third party hip extension Library is one of the use cases that need the identifier. So in case we could say -- yeah.

Why was libmemkind not able to use the attributes to make that decision? why would you want a stable attribute? isn't it smart enough to make that decision?

Here in this requirement, What I want to address is, yeah, node ID is more or less managed to be used to identify the near memory or far memory. But ID itself is just the integer number. So it doesn't present the near or far attribute, and the node ID can be changed. So we thought another way better than the node ID. So let me present how our presentation, our implementation to solve the requirements. And probably it will be helpful to help you understand.

So for the -- these two requirements, so we designed some new APIs to allow the explicitly allocate the CXL memory or DDR memory. Specifically, we extended three System calls so far: mmap, mbind, and set mempolicy. And in kernel space, we expanded the alloc_page. So here specifically, we added the MAP_NORMAL or MAP_EXMEM. So map, for example, MAP_NORMAL explicitly access the DDR memory and MAP_EXMEM explicitly access the CXL memory. And inside the kernel, it is mapped with the GFP flags that is a precious resource. So -- I'm sorry, we use that. So we also use the GFP_NORMAL and GFP_EXMEM. So we also experience the similar problem with them like. So, yeah, here, what we want to address is -- so we allowed implicit and explicit CXL memory access in user space. So what it means is -- so when user space calls mmap or mbind, set_mempolicy using these two specific flags that it can access DDR memory or CXL memory explicitly. Otherwise, the allocation will face. But on user space, we also allow the implicit call. What implicit call is just vanilla use. So when the NUMA or zone failback happens, then the CXL memory can be allocated implicitly. Why we allow this is for compatibility use. And as you mentioned, the use case of this is user space memory-tiering solution. So specifically, it could be heap allocator like libc or memkind or jmalloc or libnuma. And here, what we want to say is -- but inside kernel space, we only allow the explicit allocation request to CXL memory. Why we do this is about the third requirement. This is to avoid the unplugable condition by chance. Because when CXL -- when the kernel allocates CXL memory implicitly, it could make -- when the data is the metadata of the kernel, it could make the CXL memory unplugable. So we allow the kernel space only able to access CXL memory when it explicitly requests CXL memory. So we have ten minutes. So let me move on to the -- another requirement.

So this is the requirement three. It is about the CXL pluggability that we discussed a lot. The issue has happened. Random unmovable allocation made the CXL DRAM device unpluggable. So it happened out of kernel space. So kernel space allocation is specifically pinning for kernel space metadata, which is not movable. Such as the struct task_struct or struct page or zone. It mostly happens on kernel space allocation, but it even rarely happens on user space. For example, pinning for dma buffer.

If user space allocates From zone_movable and you try and dma pin, it reallocates the Page from zone_normal. It moves it out of zone_normal. So that can't happen. If user space allocates a page from zone_movable and then It tries to pin it for dma, we reallocate the page from zone_normal. So what you're saying there Can't happen.

Okay. What I say is, yeah, it will be allowed on zone_normal. When user space is pinned for DMA buffer...

If you don't use zone_movable, you get what you Ordered. If you say give it to zone_normal, i want any kind of kernel locations to end up here. Actually, a kernel location ends up there, then it's your fault. You should have configured memory hot block to use zone_movable, zone_preferred_normal, and not zone_normal.

This is kind of arguable that it could be an issue or not. But here, what I want to address was when you use zone_normal, so this was happening, especially in kernel spaces. So actually, we don't experience the user space cases. But in our analysis, we found out even the user space can happen. But the real issue was happening in the kernel space location. As we discussed a lot, I think using zone_movable or zone_movable_preferred can resolve the requirement. But here, this slide, what I want to address is just the issues that we experienced and the requirement.

Just to stress again, if you're using zone_normal, you're Telling the kernel use it for whatever you want. Use it for unmovable locations, movable locations. So if you want some guarantee that you can unplug something Again or evacuate it, then use zone_movable. I think with CXL, if the CXL nodes are managed in a way that The kernel can decide to assign them all to zone_movable Somehow, for example, as we had with the DAX framework where you Can then online the memory, you can tell it to do that.

Yeah, not to mention that you really do not want to have Your kernel metadata in something that has unbound Latency. So you don't want to use zone_normal for CXL nodes whatsoever.

Yes. So I think in this requirement, I think here we all agree that probably we all agree that. So zone_normal is not enough to handle the CXL memory. But we addressed the new zone, but probably zone_normal and zone_preferred_normal is enough to handle the probability Issue. But there's some other Requirements we have. So we came to address the new Zone. So, yeah. So regarding this, our -- so the CXL DRAM, probably this is a Bit different from the people here, but our -- we thought that The CXL DRAM should be able to be used in a selectable manner. The pluggable or unplugable. We thought that the kernel context Should be able to determine it. I mean, the zone level should Not confine it. That is what we thought. So -- but I apologize for confusion while discussion. So please don't get this wrong. So -- but pluggable and Unpluggable is mutually exclusive, so it cannot happen At the same time on a single CXL DRAM channel.

And let me move on to the two more requirements, and let me Talk about how we addressed it, how we solved it. So the first requirement is too many CXL nodes can be Appearing in user end. So issue was CXL, the server Vendor has addressed it. The many CXL memory node Would be appeared to the user end along with development of CXL capable server, switch and fabric topology. Right now, industries may -- CXL capable server system is Being made more than ten CXL memory channels. Then what could happen is currently a user end need to be Aware and manage the node using third party software such as numactl and libnuma, for example, to lead to the Aggregated bandwidth among the CXL nodes. So what is thought was kernel Would provide -- also would provide an -- sorry. An abstraction layer to deal with the node seamlessly. Traditionally, a node implies multiple memory channels from The same cpu distance. So we thought that a multiple CXL DRAM can be appeared as a single node as well as a Separate node. So what is -- at that time, By the way, node is the largest memory unit in mm subsystem as you all know. So node, zone, and page. So also historically, a new Zone has been added to properly deal with the new different Hardware and software algorithm. So we thought what if the Management for a single CXL memory channel would be smaller Than node. Or if a single CXL memory Channel would always be a separate node, then to handle The multiple CXL channels, then what it means, as i Mentioned, the user space need to aware the multiple memory Nodes and user space need to aware and control it. So the management responsibilities moves on to The user level. So the kernel space need to make a bigger management unit than node. For example, kind of a super node. So that was our thought.

Yeah, so I'm sorry. I will have to cut you short because we are overflowing to the next slot, but you can talk to people. I guess you have outlined what the problem is.

