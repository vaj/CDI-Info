
So good morning, everyone. Thank you for joining my keynote today. So this is actually the third time. I have a different topic every year. Probably I don't have any more next year, but let's see. So I'm Yang Seok Ki, I'm vice president of Samsung Memory Solutions Lab at San Jose. I'm glad to be here and have the opportunity to discuss the new opportunities related to CXL-based SSD.

So what this figure is about. So many people think that judging between stupidity and brilliance is simple. Oops, what happened? So this side. Because they know what is good, what is bad. But let's see whether actually that's true or not. But such judgment can change over time. So for example, when these two wheelers were first introduced in 1900, observer of this saw them as nothing more than passing crazy. But this is actually the starting of the bicycle. But at that time, people didn't think this is a good idea. Similarly, when the first movie with audio track, this is the one, The Jazz Singer was played in 1927, many people thought that synchronized audio was just gimmick that won't last. But now all movies is with audio.

So today I'm going to talk about a controversial topic in the memory and storage community: CXL, the SSD with CXL interface. In other words, what if a storage device has a memory interface? What do you think about it? Is it a good idea or a stupid idea?

So before passing judgment, let me first discuss the needs of modern system, particularly in the context of memory hierarchy.

So as you already know, the basic idea of the memory hierarchy is to keep the frequently accessed data close to the CPU to reduce data access latency. Memory closer CPUs has a lower latency and capacity, but have higher bandwidth, and so as a result, it is more expensive. However, this existing architecture are not well suited for the handling modern workload like big data and machine learning models because the working set is simply too big to fit in the main memory. So it's not fitting the memory.

So storage class memory, which is between DRAM and SSD, provides comparable performance to the main memory and the persistence of the storage. CPU and BIOS recognize such devices as persistent memory. Storage class memory has been used to provide low latency operation for large data through byte-grain relative access and to guarantee the fast and lightweight persistence. For instance, this is the simplified version of Oracle Exadata system. So the Oracle Exadata uses storage class memory since it's M7 series to provide large buffer cache and fast redo logging. On the other hand, when the storage class memory is used as a device, a log device, a few instructions that flush CPU cache and the memory controller queue can replace the hundreds instructions across multiple software stacks to flush buffers in the host side and the device side. And in different context here, Intel DAOS was designed for high-performance scalable storage with byte-granular storage access. It's also heavily reliant on the Optane. However, as you know, the Optane, the Intel announced discontinuation of Optane and many technology in the storage class memory area actually deseed.

 And the second topic is about the secondary memory. Historically, general storage devices have served as secondary memory, effectively extending the physical memory capacity through software implementation either at the OS level or the user level. This approach aims to enhance TCO considerations. In this scenario, the CPU and BIOS typically recognize this device as a storage, not memory. This type of secondary memory is application agnostic, creating the illusion for the application that system possesses significantly more memory than its physical constraint would suggest. For instance, the VDI, the virtual desktop infrastructure, each virtual machine often requires quite a lot of memory, several gigabytes of physical memory. To extend the capacity without incurring the cost of additional cost of DRAM, the VDI infrastructure typically uses the swap space. But this strategy becomes economically favorable, especially considering that not all users are active concurrently. In a different context here, the Redis enterprise introduced auto-tiering feature at user level. Auto-tiering extends the database beyond the limitation imposed by the available DRAM by employing the SSD. So Redis enterprise actually used SSD as extended memory. Auto-tiering proved particularly advantageous when there is a need for high-speed data processing over a substantial data volume. However, it is important to note that software-based swapping and/or tiering come with inherent cost. Its throughput is notably lower than the native storage performance, and excessive swapping and tiering can lead to the phenomenon of so-called threshing, so severely degrading the overall system performance.

And the third kind of need is about the fast small I/O storage. So high-performance small I/O can help process metadata, or temporary file, a small file, or fine-grained file access. Especially, we actually noticed, too, the recent kind of trend in the machine learning. Fine-grained file access is common in the machine learning model, such as recommendation system and large language model. Different from the secondary memory, which conduct the tiering between the primary memory and the secondary memory at either OS level or system software level, the application for fast small memory are typically aware of such device and optimize for high-performance small I/O. For instance, this is the recommendation system Meta announced a few years back, and they are heavily using this model right now. The recommendation model, like DLRM, the leverage embedding tables to reduce the computational complexity, associate the sparse items, meaning that if you go to the Amazon, there are a lot of product and items. They actually have to represent each of them using some representation. So that is the sparse item, and they store that information in the embedding table. Typically, the entries within embedding tables are very small in size, ranging from tens to hundreds of bytes, so which is quite different from the unit that usually we provide for storage. It's 4K, right? So then how we can actually improve the access to such small kind of data in the storage. You may argue that why you want to store that there. You need to use the DRAM, but as you can see, this embedding table size is growing very fast, and right now it's more than terabyte, and the two years back this year, it was two terabyte, next year it's going to be four terabyte. So it keeps increasing quite rapidly, so it's very challenging to store such big embedding table in DRAM. So there are several papers from Meta and NVIDIA whether they can use SSD to store such embedding tables. It is important to note that most storage devices conventionally employ 4K page size as a default unit of data transfer, but as I mentioned, this use case actually is different from the traditional one, so consequently the majority of product available in the market do not deliver the higher IOPS performance for small IO. So if you measure the performance 4K IO on the 1K IO, 512 IO, whether its performance getting better, mostly not, it's almost same, or actually goes down. So then why do they want to do this? Because it's bandwidth limited basically. If you can have more IOPS, you can do more processing, but if you do the 4K IO, then what are you going to do if you need only 128 bytes out of it? Then majority of bandit basically wasted. How can solve this problem? One way is to provide more fine grain kind of access to the device, then you can say bandit and improve performance. That's the wish, but actual device does not provide better performance with small IO, and even worse, so we have sort of a dilemma right now.

So the needs for the storage class memory, secondary memory, and fast small IO are not something new I address in this talk. Everybody knows this for a long time, so then why I'm talking about here? So I argue that the hybrid of DRAM and NAND can form a family of product concept to address the needs in the industry in a consistent manner. I call the family of concept as CXL-based SSD, but I use CXL-based SSD as general concept, which encompass a variety of use cases built on top of SSD technology. Please note that the CXL-based SSD is not a product name or a trademark from Samsung, because CXL, we have to use the term very carefully, so again, it has nothing to do with Samsung directly, but I use CXL to explain what kind of device concept I try to explain in this talk.

So since I believe most of you already know CXL, let me briefly touch base on the CXL to have a kind of context. According to CXL Consortium, CXL is cache coherent interconnect for processor, memory expansion, and accelerator. The CXL consists of three protocols, cxl.mem, .io, and .cache. So first, the cxl.mem provide a host processor. This core is a host processor with access to device-attached memory using load store semantics. On the other hand, CXL.cache allow peripheral device to currently access the host memory with the low-latency request and response kind of interface. Cxl.io is functional equivalent to CXL protocol with some enhancement and useful initialization, the link up, and device discovery kind of thing. So it's mostly for management.

And the CXL Consortium also kind of classify the CXL device into three types, one, two, three, and even though there are some confusion about the device type, in essence, the device type are determined by the protocol the device actually support, not functions that device provide. So sometimes people say accelerator is type two, but actually it is not. It's based on the what protocol the device support. Among the protocols, CXL.io is common for all types because the device management, as I mentioned, and configuration is done by-- done through the CXL.io protocol. But if you add CXL.cache to the device, then we call such device type one device. If you add .mem to the device, we call such device type three device. If you support both of them, meaning the .mem and .cache, then we call that such device type two device. So if there's no actual additional protocol except the CXL.io, such a device is a traditional PCI device, and in that case, we may want to use CXL.io as a data path as well, not just for the management.

So CXL-based SSD consists of DRAM and SSD, but its functionality can be very diverse. So in this talk, I categorize the CXL-based SSD into four types based on various device features and characteristics. First, here. So a CXL-based SSD can be used as storage class memory, say the personal memory, if DRAM space is advertised as a device memory space via CXL.mem, and then it's used as a backing store in case of system shutdown, reboot, or power failure. Second, a CXL-based SSD can be used as secondary memory if the NAND space is advertised as the main device memory via CXL.mem, and the DRAM space is used as a cache for the NAND space. And third, CXL-based SSD can be used as storage, as it is if NAND space advertises device storage space via CXL.io, and the DRAM space is used as a cache for the page of storage. Last but not least, CXL-based SSD can be used as a combo device of memory and storage, and if DRAM space is advertised memory space and NAND space is advertised as a storage space, meaning that it's just you combine memory and storage together into one device. And why do you want to do that? You can actually manage the memory more efficiently for storage, and the data transfer between the memory and storage can be optimized. So all the CXL-based SSDs except the storage use case are actually CXL type 3 device in the context of CXL type. So even though we say CXL type 3, there are many different type of devices, so it's very hard to differentiate between these. So because of that, I'll talk about the type, the CXL-based SSD type, even in the CXL type 3 device. And in addition, this device architecture may remind you the NVDIMM technology in mid-2010, right? So there are many different type of NVDIMM, take the NVDIMM N, NVDIMM F, NVDIMM X, NVDIMM P. So there are a lot of words related to the concept and they are mingled together, so I'd like to clarify what I'm trying to say through the CXL-based SSD types.

So let me talk about the CXL-based SSD as a persistent memory. I assume that you are familiar with NVDIMM technology. It's okay if you don't know. I'm going to rephrase what each device actually means in NVDIMM technology, so you don't need to worry about that. But pretty much many people here already know what is NVDIMM technology. According to the spec, both NVDIMM N and NVDIMM P combines the feature of DRAM and flash and the computer have a direct access to the memory module supporting both block addressing and the byte addressing. The CXL SSD, CXL-based SSD as a persistent memory similar to NVDIMM N in the sense that the DRAM space is exposed. Here, what is exposed to the system is actually DRAM space even though we have NAND, but NAND is used for backing store when the reboot or system power failure happens. However, the capacity, the latency, and durability requirement are close to actually those of NVDIMM P. So at first, it looks like NVDIMM N, but the detail requirement is not quite NVDIMM N requirement. It's close to NVDIMM P requirement as you can see in the table. NVDIMM N targets small capacity and low latency at the level of tens of nanoseconds. So 10, 20, that range of nanoseconds while NVDIMM P targets much larger capacity and the medium latency at the level of hundreds of nanoseconds. Then we have to know what is CXL latency, right? Typical CXL controller show latency over 100 nanoseconds without actual media access. And the device can support tens or hundreds of gigabytes space and it also supports the GPF, Global Persistent Flush feature, to make sure that the persistency is guaranteed. So I already introduced several types and from NVDIMM perspective and CXL perspective, it is mingled right now. So the device I'm talking about here, CXL-based SSD as a persistent memory, have different characteristics compared to existing technology. So this is the requirement or actually performance we actually see in the real product.

So this is the simulation, the emulation, the kind of animation, what happens. When power failure happens, how device is going to respond. So in general, in a normal situation, when you write data from a host to the device and that data is written to the DRAM in the device. So usually we may have the battery to support power failure. You don't need to write back the data to the NAND in the normal situation. But if power failure happens, then the system may want to maintain the host level power using the backup battery or the backup power to maintain the PCI slot live. And CPU will start to flush, CPU cache and memory controller buffer to make sure that all the data in the cache and the memory controller is written to the device. And at that point, the device uses its own battery and makes sure that all the entire content in the DRAM cache is written back to the NAND back-end store. And when power comes back, the device loads data from NAND into the DRAM and provides data from the DRAM to the system. That is the typical scenario with NVDIMM N, NVDIMM P, and CXL with CXL 2.0 GPF functionality.

And we have, as a prototype, Samsung has a kind of prototype to implement this functionality and compare the performance to the existing solution. This slide shows the persistent memory performance varying the read/write ratio. The CXL-based SSD shows consistent performance for read and write. You may be wondering why, because it is mostly limited by the CXL bandwidth. So usually the internal bandwidth in the device is much higher than the external bandwidth, but from the host's perspective, it is not observable. So it's primarily limited by the CXL interface. And as you see, the read percentage of the increased CXL device performance gets closer to the DDR5. And it also shows that CXL SSDs show much better performance than the leading persistent memory technology, especially when the concurrence is very high.

So this is the video clip. I skipped this. I don't know how to play the video here. So basically we actually developed the system, and this video is basically capturing how the system can shut down and reboot. So the prototype we developed is working well in this kind of scenario. So in this demo, we used Memcached and with the CXL-based SSD, and populated 1 million items, and the system is rebooted. And whether the Memcached is restored at the state, it actually shut down.

Okay, so the second device type I talk about is secondary memory. And Memcached provides secondary memory different from the traditional ones. The traditional secondary memory relies on the host software to give an illusion to the application as if they have more physical memory. That is, the device for secondary memory itself is not a memory device. Actually, it is a storage device from the CPU perspective. In contrast, CPU recognizes the CXL-based SSD as a memory device. And then the space is posed as a memory capacity to the host, like NVDIMM F or NVDIMM X, or even NVDIMM P devices. The CXL device type supports both block addressing and byte addressing, similar to NVDIMM P. But NVDIMM X targets the medium latency at the level of tens of microseconds, while the NVDIMM F targets medium capacity at the level of sub-terabyte. So it is not exactly one type from the existing NVDIMM technology perspective.

So CXL-based SSDs consist of DRAM and NAND. Depending on how DRAM and NAND is used, three options are possible. Simply put, you can expose the NAND space as main memory, and you use host DRAM and the device DRAM as a cache, a different level of cache. Or you can use host memory and the device as a memory extension. So the DRAM in the device is not visible to the system. It's at play as a cache. And the last option is you actually can expose all of them to the host and let the host manage based on their needs. So host DRAM, device DRAM, and NAND space is mapped to the HDM host, the host manages the device memory, and the host software manages all the memory region is exposed to them. So even single device, there are many options to manage the devices.

It's hard to see here. So depending on the option, you may have different choices, but this one is option two. Basically, you use the device as a kind of extended memory and measure the performance, what is the performance implication. But this graph basically said if you have a good cache kind of hit ratio, device performance is close to the interface bandwidth. But you can see that it's very critical how to manage cache efficiently because here is 90%, this is 100%. If you cannot keep the cache hit ratio very high, overall performance is not going to be good. So how to manage cache efficiently is going to be very important.

So CXL and the next one is CXL-based SSD as small IO. And again, this one does not correspond to a specific NVDIMM type. It is a mixture of them. And the basic idea is that, as I mentioned in the previous slide about the DLRM example, we want to access smaller data than the 4K. And how to implement it architecture-wise is going to be similar. So you read the data from the NAND and into the cache in the device, and you provide finer-grained kind of data to the system. By doing that, you can actually reduce the amount of data which is wasted in terms of bandwidth. So...

And from a host perspective, how you can handle this. So it is a combination of the CXL.mem and CXL.io. So from a host perspective, and the user's perspective, you can think it's a kind of hardware-based mmap. So you have a storage device, and you have LBA. That LBA is exposed to the host, but you have a way to access the LBA space using memory protocol as well. So from the host perspective, there are two options. You can have a 4K.io or small.io until system support, meaning that up to 512, you can actually do the block.io, and this device is going to cache page into the DRAM and serve to the device. But if you want to go further and smaller than 512, then you can actually use .mem protocol to access fine-grained data like 64-byte. So conceptually, as I mentioned, it's mmap, so you have LBA, and you expose that LBA space through the memory space from zero to a certain capacity, and that memory space map to the physical memory through the HDM, and the application actually map that physical address to this virtual address. This kind of mapping is going to happen in the system. The difference compared to existing mmap, it bypassed page cache.

And the performance when you use this device for the Meta DLRM case, as you can see, this is actually an example. You can use NVMe device and do all the operation, which store the embedding table in the NVMe device and do the operation using IO stack, or you can use host memory as a cache and do the IO using NVMe device, or you can use CXL device, the CXL-based SSD, as a kind of high-performance small IO device and measure the performance. The basic idea here is that even though you can do the host-based cache by reducing the amount of the data transfer device, you can actually improve performance.

So I talk about several kind of the use cases how CXL-based SSD can be used. But the thing is, as I mentioned, this is different from CXL types 1, 2, 3. It's different from the NVMe protocol. So we may need a kind of standard in the ecosystem to drive this technology.

And another concern is that latency tolerance. So this graph is another kind of representation of the memory hierarchy. And the device that I mentioned here is correspond to 1, 2, 3 there. And the question actually, if we have this kind of device, can CPU support this? And the CPU typically can handle only hundreds of microseconds, several hundred nanoseconds latency. And if you take longer than that, it has an implication, because CPU need to do the kind of out-of-work execution using the real buffer. But most of CPU right now cannot handle this properly. But it is not just the CXL-based SSD issue. If you're using the CXL 2.0, 3.0, you have a switch. And the switch has multiple hops, which add around 100 one way. So you just go to the around 500 nanoseconds latency. So how can we handle this one? I expect the CPU vendor are going to address this problem properly and handle the issue, because it is not just CXL-based SSD issue. It is fabric issue as well.

And the cache management performance is very critical, because the device, how you configure device is up to the vendor. But managing the DRAM inside device is very critical for overall performance and functionality.

So I'm a little bit rushed, but let me wrap it up. So CXL SSD with CXL interface can have various use cases. And I talk about personal memory and secondary memory and high-performance small I/O. And also can be a near data processing platform. But the issue is that actually there is no clear definition about what it is and how we're going to handle this and what is the benefit to the system. So Samsung is actually driving this effort and want to work with the community to build a standard and ecosystem together. And also the CPU vendor needs to actually address the CXL technology properly from the latency tolerance perspective. GPU cache may be okay, because you have many cores, so you can hide the latency issue properly. But CPU side, we need to dig deeper how it can be handled properly. Okay? Thank you for listening. And since it's almost time's up, so if you have any questions, you can do a few or you can do later. Thank you.
