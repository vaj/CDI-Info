
So what that means is we're here to help advance the state-of-the-art computing solutions  we're not just going to develop a  Photonic technology for the sake of technology. We're here to help improve computing solutions. That's our mission as a company  My name is Ron Swartzentruber and I'm director of engineering at Lightelligence and I manage the  CXL over optics products  Used in the data center to provide an optical based fabric, so we see  CXL over optics as one of those groundbreaking improvements  And so I'll talk a little bit about that.  

So we are a member of the OCP community we're one of the startups  and  in fact Cliff mentioned us yesterday. And  We're also an OCP solutions provider.  So you can go on to the OCP website and look at our  Solutions we'll talk about those  at the end.  And  You can see  AI and optics so we'll talk about how those how the intersection and where we see that in the future.  So we'll talk a little bit about the data center the memory centric shift in the data center  the growth of AI and the LLMs.  The popularity of CXL so we'll talk a little bit about that if you're new to CXL not sure what it is. And  Then we did a case study my company  Because we weren't sure was does this CXL over optics make any difference? So we did a case study on an LLM and we actually have it here at the show on display.  You can go take a look  Showing the improvements in decode throughput for the LLM.  

Okay, so first let's let's start with the data center. What we're seeing is is a shift  Towards  You  Memory and access to memory memory bandwidth. You know as drew showed these models continue to grow so that means more memory  I could connect  on the same machine  right.  But now what you're seeing is that VMs connect across physical machines? so  You know we used to have  Software defined networks now what we're seeing is software defined data centers where the applications. Now actually can decide  Which machine they run on? This is something extremely new and exciting.

The models continue to grow, and what we're seeing is disaggregated architectures are required to help them expand. Basically, you can't fit your model in a single server. Actually, and what we're finding out is that there's multiple racks of servers required to run these models. So in this world, it's all about latency and memory bandwidth. Okay, so CXL is, as you can see on the chart on the right, CXL memory is a single NUMA hop away from the processor. So it's a low-latency interconnect. You don't want to use SSD. SSD memories way down a few tens of microseconds. So now your application has to do fancy work to hide that latency using multithreading, et cetera. But what you really want is a load store architecture. So if you've got CXL memory, then what we need is optical CXL to basically allow you to go across the data center and through multiple racks.

So today, your traditional Ethernet is going to be RDMA-based architectures. So you're looking at a variable amount of latency, many tens to hundred microseconds, specifically when you add in the FEC that's attached to a lot of these copper interconnects. But with CXL, again, it's a much lower latency. The latency is actually fixed per the standard of PCI Express. It's consistent. It's hundreds of nanoseconds. 

Okay, the other reason we're sort of betting on CXL is that it's an open standard. Okay, the CXL consortium, as you can see, is participated by many of these companies here. It's highly backed by AMD, Intel, and you can just look at all of the names here, Meta, Google, all a part of the CXL consortium. And what CXL does is it adds memory and cache coherency functions onto PCI Express, which is the dominant load store interconnect used in the data center. So CXL gives us what we need in terms of a low latency memory interconnect. And with the advent of CXL 2.0 and 3.0, you can now have hierarchical switching. So now I can actually build a fabric, a real compute fabric, compute to memory fabric, using multiple switches, multiple racks, you know, thousands of endpoints.

Okay, so let's talk a little bit about optics. Why do I need optics at this point? Well, first off, you know, yesterday there was many talks on sustainability. So you look at the first chart. Your signal loss over distance is many tens of dB with copper. And in fact, the best PCI copper cables are maybe two meters at Gen 5 speeds. At Gen 6 speeds and Gen 7, it's going to be much worse. Versus optics, extremely low loss over, you know, meters, hundreds of meters and even further. Also the cross-sectional area, if you've ever opened up a server and you look at how these MCIO cables are connecting to all of the different locations for your PCI connectivity, they're fairly bulky. Versus optics, you can have, you know, multiple fibers in a single, you know, 3 millimeter cable. Add on to that future technologies with multi-wavelengths. So you're really talking about a dramatic shift in cross-sectional area for your cables.

So simply put, what our product is going to do is it allows you to break through the rack and extend your CXL fabric over multiple racks and through the data center. And that's what Optical CXL gives us. The product we've launched is called PhotoWave. So it's a low latency, high bandwidth, data center reach type of product. And it's the first of its kind. 

Okay, so let's talk a little bit about our case study. Again, we wanted to prove that this was going to be useful. So what we did was we put a simple use case together where on the left-hand side is a 2U super micro server with an AMD Genoa CXL 1.1 processor in it. So this processor supporting CXL are starting to come online. This is a Genoa that's the Turin supports 2.0 and Intel Sapphire Rapids are also coming out with 2.0 capable CPUs. Along with that is an NVIDIA A10 GPU that's doing the large horsepower for running the LLM. And what you can see is a PhotoWave card that's a low profile PCI form factor CEM slot card. So your ubiquitous PCI card connected to on the right is our memory expansion box. So CXL memory expansion boxes don't exist yet. Okay, we believe they will be coming on the market where you can actually buy a CXL memory appliance to store your model. But we built a purpose built box with two CXL memory expanders. So Micron and Samsung are both making them. They're 128 up to 512 gigabytes of memory. And what we did was we stored the large language model on the memory expansion box and compared that to storing the model in local SSD memory.

Okay, the model we chose was OPT66B because it could fit in a single memory expander. So again, it was a small case study.

And then here's the results. So the CXL memory achieved 2.4x better decode throughput than disk. So the important thing is this model was running faster. And so you can see the different and even the CXL memory approaches the performance of system memory. Obviously, if you could put all of your models in local DRAM connected to this CPU, that that's the best. But CXL memory was maybe 70 percent of that performance. So not bad. But 2.4x better than the SSD.

So this is what the model did, its news text summarization. To summarize, it takes a page of news, summarizes it into a few sentences. It actually takes about six minutes. So this is a rapid display here. But, you know, to have it run two and a half times faster is pretty important. Right. Six minutes down to just under three. 

Here's some more of the results that I won't go into,  but basically shows you over time how the CXL memory performed compared to the NVMe.

And. Let me just show. So these are the form factors I mentioned, the PCI card, the first one, the OCP 3.0,  SFF card is in the middle. So that's our that's our OCP compliant card. And on the right is active optical cables. So we're actively working on several form factors. So CDFP is the first one. We're also working on QSFP DD and OSFP XD. So those are the form factors that were very active in the PCI SIG. Those are the form factors that the PCI SIG is also sponsoring for for active optical cables form factors. There will be an ECN to the PCI 6.0 spec coming out this year that includes those. So a few of the features. So jitter reduction. The cards have electrical re-timers on them. The AOCs do not. The AOCs are pure linear drive. So less than a nanosecond of latency across that connector. So you're really dominated by the time of flight five, five nanoseconds per meter of cable. And one of the things we've pioneered is sideband signals over optics. So with PCI Express, you have your SMbus reset reference clock, all of these signals that are required by the interface. We can transmit them across the fiber. So with that. Thank you. 
