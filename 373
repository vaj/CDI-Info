
So, hello, my name is Donpaul Stephens. I'm the founder and CEO of AirMettle, and I'm here today to talk about a computational storage service. It's an analytical data platform that has the ability to do intelligent analytics offloads.

So, to start, let's take a look at computational storage.

Look at the devices. And you've probably heard a lot about computational storage. The concept has been around for quite literally decades, and brings back pretty fond memories of graduate school back in the 90s. So, like Eric Rydell, I just realized it was quite a shock he passed away this last year. So, I remember him giving lectures about the work he was working on in his PhD about active disks. And he graduated in 99. But I really want to step back and say, well, this concept of computational storage, why hasn't it really made a significant impact? And what can we do to really put it to work and make a really dramatic improvement in how we build and manage storage systems and deliver analytics to clients?

So, let's take a look back through the years. Computational storage. So, when I was first introduced, the concept, again, in the late 90s, it was research at Carnegie Mellon and IBM. And, like, it was research. So, you weren't really expecting it to be in the product.

And, of course, thud. It's a wall. And then you say, like, well, then there was a concept of processing memory. Which Micron and Samsung, like, really, really came up with the concept. But the problem was putting a processor into memory devices. It was kind of prohibitively expensive and really didn't get any commercial interest.

So, thud. And then, go forward. And then there was some concept of putting FPGAs into the SSDs. And so, a number of companies like ScaleFlux and Samsung, with their Smart SSD and NGD, experimented with, like, ways of, like, “What if we put an FPGA there?” And, like, “What do we see, like, what people might do with it?” It was really complex. It was expensive. And, like, it was… It was kind of like, “Are we going to put a computer in a drive?” And what is it going to do much more in a standards-compliant way besides just internally compressing something?

So, if you got to NVMe... So, like, that made it a little bit easier to consume. But it turned out that it seemed that the people that made  those  were able to keep these commercially successful were ones that were effectively customizing a pseudo, like, embedded system with some of their customers. Particularly in the case of ScaleFlux. From what we see from some of the public statements, but, again, I've only seen some stuff externally.

And now, we're looking at the concept of using software-defined systems. And there's a... There'll be another talk later today from Kioxia about some stuff that they're working on with... for doing some RAID protection inside the system. And we've been talking to PHISON for a while. So, it's really refocusing and looking at the use cases and seeing if you could support what the software is doing on a particular host.

So, we really think of it... It's kind of like looking from a device-centric approach of computational storage. To now, it's somewhat of a more host-centric approach. But there's still limits of what you're able to achieve if it's device-level or host-centric. Whenever the storage in these modern systems are really stored across a very vast service, that spans literally hundreds to thousands of devices.

So, if we look at the host-level approach, it has applications. But, like, clearly, there's limitations. And so, what might these be?

Well, if you look at how a host... how storage protects data. Like, way back, concept of RAID, erasure coding. Like, it's basically moving lots of data. And you have a very simple comparison function. So, it's trying to protect your data. And these are tried and true methods. It's been used for decades. And these are really brought up in the point of time where, like, gates were horrifically expensive. It's not that the wires were free. I mean, like, you literally had people wire-wrapping things. It's that the gates were so incredibly expensive to manufacture. That it was dirt cheap to have people wire-wrapping things. Like, you literally used to do this. So, now, if you're going to protect data. So, it's like, okay, basics have an XOR code.

But, like, why don't we take a look at it? If you look at a set of data, and you put them into cut, nice, fixed, uniform boundaries. Ka-chunk, ka-chunk, ka-chunk, ka-chunk. Well, it seems like this is a straightforward process. We've, like, done this for decades. Like, works for, like, hard drives and SSDs. Kioxia, as I said, will be giving a talk about this on their computational storage hook later in the day. But, so, what's the problem? So, you can protect your data, in case anything has lost. Well, like, let's take a look at this table. And if you take that data in the table, and put a fixed set of bytes down into the drives.

Ooh, yuck. What happened? Well, we basically, like, shredded the data. Like, some of the metadata is in one place, some of the data is in others. You're cutting key records and entries off. You can't actually analyze anything anywhere. But, it's nice, uniform, fixed eyes. But, like, that's not the problem today. The problem is, like, how do we actually analyze and get insights out of the data? It's not just, you store it, you retrieve it, send it somewhere. Like, the networking itself, within the storage layer. Like, the networking is increasingly being the bottleneck. You have insane amounts of compute in almost everything you pick up these days. There's more. More compute power on my wrist, doing just about nothing but, like... Then the entire planet had to put a man on the moon. I mean, think about that. Like, processing, if you can tap it, is effectively free. It's moving stuff around that's getting so expensive.

And that's probably why big data just seems to be so slow. So, how do we make big data not be slow? How do we get insights more quickly? So, we look at this and said, well, what if we take a different approach?

What if we can look at this and say, "What's a systems-level approach to looking at this problem?" How do we enable much faster and more efficient distributed analytical processing? Because your data, whether you like it or not, or really recognize it or not, is actually stored in a massively distributed manner across a vast fleet.

So to do this, we took out Occam's razor to this Gordian's knot of analytics and said, "how do we make an actual computational storage service across a vast number of systems?" So, if you think about that issue where we showed a very trivial table and it doesn't actually partition,

Well, what we do... In our system, is that whenever you ingest data, there's a lot of common data formats that are incredibly pervasive. And you would suspect that because you can have different applications open data files that are the same data type, we're actually taking advantage of that within our system, that whenever data has been written into our system or put, then we will actually look at the boundaries and effectively bring out a super precise exacto knife, if you can think of it. So we effectively, precisely cut the data and effectively wrap it in a very lightweight envelope so that we're able to keep processable components of records together whenever they land on back end storage. And so, like this concept is like, we keep the original object metadata, but we will store just a tiny bit of internal metadata on each one of the cuts. This is very much not to scale. I mean, the typical overhead that we add to enable fully distributed parallel in place analytics, is like typically like 0.1%. It is completely dwarfed by the overhead of data protection. Now, but you probably noticed that, well, if you're, the issue I showed earlier was that when they were fixed, nice sizes, they didn't align perfectly. So yes, the data elements that are stored on the back end storage devices are not actually fixed size.

But we think that's not really, really, really a huge problem because whenever we take the data elements that are now potentially variable size, there's some variance, but we simply correct that in the erasure code. The erasure code algorithm doesn't become that much more complex; it's a little bit trickier, but it's trivial on the grand scheme of things. So, if we look at this and say, well, what are you actually able to do if you're able to divide data and do processing on those subcomponents? In parallel.

So we look at a number of applications of classic tabular data, whether it's CSV, JSON, or Parquet. So these are widely used in analyzing historical data, looking at current events. So security information, event management type applications. You have a lot of data, which is collected in JSON, like could be things like the reports that Dan was just talking about, or it could be like, just, just about anything. It's amazing how much stuff gets collected in JSON today. And you're looking at statistics, you're monitoring things, you're trying to analyze the content. So how do you, like, what can you do with this?

Well, you have a basic select API. So Amazon had introduced a select API on directly off their object store. It wasn't actually that fast. Our system is quite literally a hundred times faster. The reason it's a hundred times faster is we divide the data. And the components that it's stored on can actually be processed and filtered while it's stored in a massively parallel manner. And a lot of these basic SIEM type use cases, like you need to parse and go through all the data, but the amount of content that you actually need to return is typically three to four or more orders of magnitude downscaled. Like literally, there's orders of magnitude of, uh, more content like that is in the storage than is actually needed to return very basic answers. You might need the statistic, you might need like the average, you might want to know how many elements are between this range and that range meeting a certain criteria. Uh, and the bottleneck again is the networking to moving all the data from your storage layer to your computer. If we can actually process the data and return the answer, uh, it's going to get, uh, get the answer to the client faster than you could even fetch the content now. Okay. So this is a hundred times faster. So I know with Silicon Valley, everybody likes talking about factors of 10 and there's a factor of a hundred. Well, like is, is that okay? Well, there's a hundred X, but of course the question is, it's a hundred X of what? So like how much of the more complex problem is this hundred X? So if you look at something like Star Schema benchmark, then the simple, the simple scan filter aggregate, uh, will many times turn out to be something between two thirds, 80% of the runtime. And if you look at your classic, um, those law, if you take the big problem, you say like the 80% problem and you squish it by a factor of a hundred, you're still left with the remaining 20%.

So unsurprisingly, if you look at this, we run this, uh, on a more complex problem and using say Apache Spark, uh, that's going to be the interface to our system since we're able to do the scan, filter, aggregate within the storage layer. What you get is a net acceleration of a more complex problem by a factor of five. Uh, it basically varies by how much of this portion of the problem, uh, we're able to do like in the storage layer, but this is effectively just by swapping out the storage and having the right API to basically say, "I don't want to retrieve the whole object. I just want to process the object, which way it can give me the answer." So that simple change alone can get you a factor of five. Uh, and the higher-level tools, they're the same. So what kind of analysis can we do, uh, with this?

So we looked at this and there's a, uh, like, if you look at the classic seam type use cases, uh, they have to do a number of things like collecting samples, identifying certain flags, grouping things, potentially by a minute or by some other metric, and then you're returning a number of statistics. So you're basically like a more complex set of processing. Something potentially like that. So this is the nature of the kind of SQL that we can support in our system in a massively parallel manner. So it's more complex than S3 select, but it's still fairly basic. But these are the common set of reporting queries that you would see in a typical SIEM type use case. And of course, we return the data not in human-readable text like the S3 select API does, but we can also return it in Arrow. So that it's directly processable by the tools whenever it lands. So there's adding basic data organization, data characterization, like basic things like group by, standard deviation. This gives you powerful tools, you can actually do quite a bit of basic processing. And one of the things you probably picked up on is that a vast majority of big data you collect today, you have to collect the content to find out what's in it. And so you collect a huge amount of content, and you're trying to say like, what is normal? What is abnormal? And then you gotta find like what is the range of the piece of the abnormal bits that you're actually interested in drilling into. So you're going to have to collect a lot of data. You're probably gonna keep it stored around for a long time, but you wanna be able to get the relevant insights and then compare that data to how it changes over time. Now, so we've been working on this for a while, and then obviously one of the key people who's been pushing the industry for like enabling like computational storage, because it can solve their problem, is a team at Los Alamos. And they asked us, well, if you can enable distributed processing, then can you go back to this problem of can you actually put something in the device? And what's the right layer to push something into a device so you can get another scale up? Because we solve this on a scale out, but there's been a lot of industry focus, again, for like 25 years, on like how do you scale up in the device?

And so, what's the right way, in our view, of doing device-level processing to accelerate this?

So, in effect, if you're looking at the SSE, how do you go from this concept that's been around for 25 years, and make it something that is able to really look and apply, attack your problem head-on, and give you actionable answers much more quickly in a hardware-accelerated manner?

So what kind of use cases do we want to look at? And for whom? What are the real drivers for this? So a lot of mass amounts of data, say, SIEM-type data, security information, event management, network operations data. So there's, like, these are representative. These aren't our current customers, but these are like, kind of companies, illustrative, that have these big problems at scale. Weather, climate data. So like, well, who's interested in weather data? It's not just weather forecasting, but weather forecasting is actually very important. For doing things like power pricing and natural gas pricing, because temperature really drives power demand for HVAC. And if you find things like cloud cover, then this is going to determine the kind of solar output you can have. Wind provides, like, what you can get out of the turbines. So there's a lot of large financial markets that want to take a lot of this data and optimize it and analyze it so they can trade on it, they can get insurance products priced from it. Semiconductor process. So there's a lot of companies who make chips, then make sure that things are produced uniformly or like find variations that shouldn't be there. Real-time analytics. So there's a wide variety of use cases that people are collecting these mass amounts of effectively array-based data, generally keep them in blobs, and in some cases, these are like, write once, read maybe. Or if you find that it's something of interest, then you read it a massive amount of time. So what kind of operations are really going to be applicable to the kind of analytics that we've been focused on?

We really see them as like a select or a multidimensional select type query. But then you say, like, well, like, don't talk in the abstract, like—to push something in the device, specifically what kind of data type, data formats, are you looking at?

Like, ah, well, like, common ones that people see are like Parquet and Iceberg. So these are like column or array-based data formats. Or if it's weather data, it's NetCDF, HDF5. Like, you have arrays of data to give you addressing and to where the other arrays are. So these are starting to look rather complex. Like, yeah, they're public data formats, but it gets really hard if you're going to say like, well, like, you have some open source code to run it. It's like, these can get very complex. If you put too much complexity into the drive, you're effectively turning it into another computer with all the various overheads that are involved.

So if you ask a question of "do you want to or can you cram a modern processor and operating system into what effectively amounts to a hard real-time embedded controller?" Like branches, branch prediction units, memory management, security issues on those—those can get to be hard problems. These complex data types, which might change, that gets a bit even worse, and trying to navigate all this starts to look like it's almost seemingly unbounded complexity.

And we stopped and said, "Well, Henry Youngman's classic, it's like the patient says, 'Doctor, it hurts when I do that.' And of course the doctor always says, 'Don't, don't, don't, don't do that.' That's like not a good idea. So what do we think is an alternative way of handling this?"

So, we looked at this and identified that you could have a different paradigm. So we're looking at having a technique where you can effectively look at the data and say, like, if you have the right to read the data, can you do a very basic operation on the content? And the kind of basic operation we see is the notion of being able to check something and then like use that as well to fetch. So a very simplistic check-fetch API. I understand people come up with very complex approaches for what you might want to delegate to storage for a long time. But when we're looking at this, we think it's better to pick something which is simple, basic, and can be proven to work at scale. We can, there's plenty of time to add complexity. What we need to do is be able to say, how can we find a base common layer that can be applied efficiently at scale? And ideally, have the ability that can actually run at line rate. Because, like, cost is important. And I think some of the problems of the historical approaches for computational storage is they've turned out to be too complex to use, too complex to manage, very expensive to manufacture, separate supply chains. When you compound this, it basically ends up that it's fascinating for printing PhDs, but it turns out not to be very effective for making a big market impact. And we want to have something that makes a big market impact. So if you say, like, well, check-fetch, this seems relatively simple, so what kind of scale could you apply for this?

Well, if it's broken down to the individual elements, it's obnoxiously paralyzable. So, if you take something like, say, 20 gigabytes per second, an internal bandwidth of like a modern SSD, but your base LBA size you're accessing off the flash is four kilobytes. Well, how fast do you need something to run if it's processing four kilobytes at a time? Your internal controller could run this at five megahertz. Megahertz. Yes, you can have something that can run incredibly slowly and keeps the power down and avoids the need to move the data even from the drive to the host. So you get a dramatic reduction in the operating frequency required and the amount of content that needs to move. And we're looking at things where you literally are taking a bit. Because if you check it, the answer is T or nil, yes or no. Do we match it or not? So how could you apply this?

So we look at, say, some parquet data, and you have a very simple common query. So it's like, take a table and you want to select customer order, parquet, revenue, like there's a lot of fields. But you're looking for things in particular where there's a selection that you want something between the quantity between, let's say, 11 to 20, and the order date is less than a certain date. It's within a range. So how can you do this on parquet data, like stored remote system? So if you have the drive, you say, well, this is easy. It's parquet. Here's this library. You simply have to run it in a Linux environment and... No. That's too complex. Well, if you know where the data is and what the boundaries of the content within the objects are, then the host can maintain control of where you go in the object. What you have to do. So the host, you want... Our view is that the host needs to maintain control over what is done on the content. We can tell you where, and there's a simple operation. The only requirement is that it's array-based processing. You want to do continuous processing over an array.

So with a check-fetch approach, when we're pushing the basic in, we had to read the footer; we had to find out where the boundaries are, where's the starting position of that array, but then since all the elements in the array are one after another, it's a completely sequential operation. Check every individual field, then you can check each one of these cases as basically a delegated check: check this field from this point to that point, give me a bit field in return.

and then you check one, then you check the other field. Now, if you have like a CMB, controller memory buffer, then you could actually take these intermediate bit fields and even just write them into a local memory on the device itself, therefore avoiding the need to even move content for the intermediate processing back to the host. And you can take, compare on one and compare on the other, and then you have simple operations, quite literally. Like in this select field, it’s an and, you can do simple arithmetic operations between bit fields and do those on the drive as well, and that enables you to take what looks like fairly complex SQL,

And you can actually do those operations on the drive itself, and then whenever you get a resulting bit field, then that field you can then use it to fetch data off of other separate arrays. And it's just, since it's a bit field, the size might change on which fields you're returning—is it a one-byte character? Is it four bytes? Eight bytes? It doesn't matter. You don't actually care what the elements mean. The only thing you need to know is that if they're fixed-size elements, you can use the same basic handshake to then retrieve data elements. And then what you get back is nice and packed, like in memory. So quite literally, you're taking that and then instead of getting this whole table with a few operations...

You're able to get it to a very tiny sub piece, you can literally cut the data coming to the host from a drive which is connected to that host by two orders of magnitude, and you might wonder, like, 'Well, like, this seems like really complex, and so, like, can you support this in any device?' Yes.

So, PHISON, we've talked to some others, so it's, it's completely practical that you can actually implement these sort of check/fetch APIs for enabling delegated analytical sub-processing to the drives themselves. This can be done in firmware or hardware on existing commercial shipping drives. So, we're looking at collaborating with PHISON and/or others, potentially bringing some of these capabilities to market for systems like ours going forward.

So, I should give some acknowledgments. That's because our work has been supported, with some government support, under grants from both the National Science Foundation for basic core scaled analytics on the National Oceanic and Atmospheric Administration—the work that we're enabling for processing like climate data models in that CDF—and the US Department of Energy for some work we're enabling for doing parallel AI inference on like multimedia data sets. So, see, that's a quick overview of our computational storage service on our analytical data platform. So, anybody have any questions? Little slides, hopefully it was entertaining, any questions? Sure.

Okay, so you're asking if we've thought about, besides CMB, if there were other uh, like hooks uh, that are standardized on NVMe? So we're we're talking a couple of uh, different uh, device vendors about this, so we are uh, we're we're a software defined storage vendor, so we don't pH, we don't actually make hardware. I'm kind of uh, I try make I've been there, done that, lost the T-shirt. I, I like, we want to work with other companies that are enabling that, so we're talking to some partners that, like, how would it best be to implement our architecture with their systems? So we're, like, CMB's an obvious one. The other thing is that the check-fetch nature of it is both that we're providing some data to the drive for what needs to be done, and we're retrieving data, whether it's coming back to us directly, or even if it's being written into, like, an internal memory such as a CMB. So, like, there's a fuse operator that makes it more straightforward in NVMe to be able to have two commands that kind of go back to back. We've been told that some, like, operating systems aren't really friendly to fuse. So, like, we're looking at the practical ways that, like, this can effectively be implemented and used, like, through a standard Linux stack. Did that answer your question?

Yeah.

Okay. Yeah.

So you're using the equivalent of the SQL queries on the host, but pushing the, effectively, that query to the drive?

Oh, no. Go ahead.

So my question is, so in order to build that mapping...

Yes.

...you're also providing the interface to your customers to say, 'I need to convert this query for use on the, like, the computational storage.'

Oh, this is entirely hidden from the client. So, like, he was asking, like, do we provide a mapping so that the client is seeing how we've, like, unrolled this? So we're a software-defined storage system, so it's connecting to us, and it might issue a, like, an S3 select API or an Arrow, like, a select API through Arrow, like an ADBC connector, but, like, we have the data stored in a massively parallel manner across a large number of storage nodes. So we are the software that's running on the host that's connected to potentially, like, the drive.  The handshake of, like, what the host, what our host sees, our host is one of many components in the fleet that implements the service. So our node will see that, like, we have data that happens to be in Parquet, and so when we see that there's Parquet data that we have to implement to select, like, we can potentially retrieve the data, the data from the footer, and the host will process that to figure out where are the actual boundaries where each one of these array components begin and end, and the only thing that's pushed to the drive, we want to push the drive, is the range checks or the fetch operation. So what the drive sees is effectively, be a check this, say that we assert that this is a, like, an int32 or it's a float64, and we're doing, like, a range check or something, so we have a very basic, this is a check operator that we want to run.  It starts here, runs for this long, and we're getting a bit field back. Whether the bit field is coming back in a CMB, like, or the bit field comes directly back to the host, like, we're looking at the most practical way to implement that, but, like, we're, like, our view is you want a simple handshake, but all the drive needs to do is run these very basic check or fetch operations. It doesn't know that it's doing SQL, and if you're doing something like you're, you want to get, like, the average precipitation or whatever, like, on a range of data in, like, in that CDF4 file, you can still break that into a number of simple components, which amounts to range checks to get values. So we want to keep anything which is dealing with the more complex nature of how the data structures are built, how do you interpret them. Those are done in the host. We want to delegate effectively the data path portion to run on the drive, because that's something that is very simple to delegate cleanly. That answer your question?

Okay, thank you.

Yes, sir?

How are you using, like, a standard entity approach to do the communication back and forth with the CMB, then, like, to actually do the analysis, push it into the CMB, and then have it be accessed and so on?

So are we doing a standard NVMe approach? We're, again, we're looking, we're talking to some partners about, like, the right way to, like, implement this, and whether it's on, like, standard NVMe commands or if it's potentially vendor-specific commands, so that, like, that's still in the to-be-determined camp. Like, we would like this to be as standardizable as possible. The real objective is that this should be able to be implemented, like, completely in firmware. So, like, you want, we aspire for this to effectively be, like, it's just a change in firmware on the drive, and the drive can just delegate and just do this in place. Because it is, it's actually alarming if you, if you go into the numbers, like, how, you actually have more scan bandwidth, like, in the SSDs themselves than you do, like, if it's fully populated than you do in a server's DRAM bandwidth, which seems nuts. But if you look at, like, the, an SSD has, like, 14 to 20 gigabytes per second, like, read from each device within Gen 5, and that, like, and you can have servers that have, like, 48 drives. Like, your DRAM bandwidth is, like, it's a bus. It's unidirectional. It's, like, 300-something after you take a raid, like, bus turnaround overheads. You don't need many SSDs to be able to literally have the ability to scan, scan, in the drives faster than you can in DRAM. It's mind-boggling. But, like, that's the path we want to go to. Okay?

Yeah, I guess a follow-up to that, related to the data transfer approach. I'm coming from a biased point of view, that's my talk later, but are you using, like, PPGMA to do the communication then, or how are you accessing, like, the drive, getting the data to write from the drive onto the CMB internally?

Oh, like, so how are we managing writing the data? So, again, we're not a device vendor. So, like, we've solved this as a scale-out on, like, the software layer, and we've been looking at how can we push that capability into the drive. We're talking to a couple of SSD manufacturers about, like, what is the right way of doing the handshake. But, like, that's... Like, we're going to be driven more by, like, what are the particular trade-offs that the partners we're talking to could actually implement. And I don't want to be too fussy about, like, it must be done particularly this way. Yeah, we just... Like, we want to get these basic operators, but the operators that I showed you, like, they're very simple, but if you can chain them by the host control, it's very powerful. Okay? Anybody else? So... No? Then... I guess you could just, like... Thanks, everybody. And, like, please take a moment to rate this session. So, hopefully it was useful and entertaining. Thanks, everybody.
