
All right, thank you, Frank. Thanks, everyone, for attending the session. Great to be back at OCP at a pretty packed house this week. My talk is called 'Bridging the Network and Memory Worlds with the Accelerated Compute Fabric Switch.' So a bit of a disclaimer-- some of the things that you're going to hear in this presentation might sound slightly contrarian relative to, I think, what the predominant thinking around CXL and CXL Fabrics that I think are completely valid from a perspective, but have been mostly the mainstream conversations through today and at the previous CXL forums that we presented at and the like. But I ask that you look at what we're presenting with an open perspective with respect to how infrastructure gets built.

So let's start with some common ground. It's interesting because I think this is the slide, the first time in a conference I've seen the same graph shown in 15 different presentations in different ways. So this is our version of that same graph, which is the memory wall in accelerated compute. And I don't have to go through the numbers here, but I think the clear takeaway is that flops-- if the memory wall had already existed with respect to conventional CPU compute, with respect to accelerated compute, it's slamming straight into that wall already. And the flops are racing away architecturally with advancements and what vendors in that space are doing. But you can see on this log scale graph that not only is there a memory wall, there's an I/O wall. And no matter how many CXL ports we can put on a device, we can't just fill that gap. It's not going to be possible to just say, you can close the memory wall gap by adding a fabric or adding a CXL port here or there. In fact, that's much more cost to be able to actually go scale to the types of bandwidths we're talking about. Memory wall certainly does refer to bandwidth, the amount of bandwidth that's available through memory relative to the amount of compute horsepower that's there. So the flops are racing away. But the challenge that we have in terms of cost of scaling is both with respect to scaling the memory bandwidth and the capacity needed. Because as models are continuing to increase and the services continue to increase for workloads such as AI, it's the memory bandwidth in aggregate and it's the memory capacity in aggregate that can be used to support the workloads across an accelerated compute fleet that poses the challenge. So if we look at how that memory gap can be solved, there's certainly solutions that exist in terms of advances in HBM, advances in what we call the near memory layers. But the capacity problem actually extends not just to the near layers, it's across the entire rack or the entire cluster. And this is an important point to note because there's actually a lot of bandwidth in data center clusters today. It's all ethernet bandwidth or network bandwidth. But there's many, many-- there's an entire established infrastructure of ethernet networking where 800 gigabit or 400 gigabyte pipes today, 800 gigabyte pipes tomorrow are interconnecting many, many nodes, connecting up hundreds of terabytes of memory capacity. It's important to note here that 800 gig is actually equivalent to the throughput of two channels of DDR. So can we use this? Can we use this install base of ethernet networking to at least alleviate some of the memory wall problem that we're talking about from both a bandwidth and a capacity perspective? That's the question that we think is interesting.

Another very commonly shown slide-- I think this is maybe second place of the most shown chart in the conference, which is what does the memory hierarchy look like? The modern hierarchy obviously extends because read access time matters. It matters differently for how that memory movement is actually consumed. So you can see in the top layers, lower latency going to higher read access latency, ways of talking between processors and memory. And you'll note that the bandwidth also has some differentiated scaling when it comes down to what you can get through HBM, DDR, and then through DDR, through CXL-attached memory. And then I'm going to point you to this layer down here with respect to far memory, which today is a memory access over technologies like RDMA. It's a copy, not a coherent load store access. On the rightmost column, the programmers view, this is actually what we believe is most important, because it's about how the application or how the user space application is actually making use of that memory or that memory movement. In order to be fully compliant with how systems are accessing memory today, this is pointer dereferencing and load store access that applies to these top layers, like on-chip memory, HBM, DDR, even across a NUMA interface, and potentially over some amount of CXL radius as well. But there's also a significant amount of memory movement that happens today in accelerated computing workloads that is based on high-performance copy, copying data from one location to another, from CPU-DDR, for example, to GPU-HBM, or between GPU memories, or between remote store and local store. That's actually something that's commonly supported in collective stacks in AI today. And a lot of that is actually moving today over network. So the important thing to note here is that not every problem drills down to a coherent load store access within one node or within a set of nodes. We can actually pool and share and solve the memory scaling cost problem by looking at it as a cluster scale problem.

So let's go into a little bit of detail on what's inside these systems today. If you take an accelerated compute fabric-- or accelerated compute server today, there's effectively two sides, right? The network scaling side, you can look at the left-hand side as being your scale-out through an RDMA NIC or DPU, and on the right-hand side, which is populated with PCI switches and PCI device trees. Now, historically, those have been divorced, even though you could obviously put the PCI switch on the other side and have the RDMA NIC or DPU be connected through that switch. In fact, that's actually how it's done in many cases. But we have thought about memory fabrics separately from network fabrics, two different sort of access paradigms.

In 2024, we will see the first accelerated server install base, which will actually feature CXL memory. Now, these will be directly attached to the server CPUs or XPUs themselves. And this is what effectively those machines would look like, right? Memory attached, or CXL memory attached to CPUs, with those same NICs for scale-out. And the promise of CXL fabrics is to extend that so that deeper and deeper trees of CXL devices, primarily memory, but could be other devices as well, will build another fabric on this side, potentially separate from the network fabric. And as a networking person, and somebody who's lived through how difficult it is to stand up infrastructure and networks from scratch-- and many of you in the audience I know have lived through that as well-- there are challenges to get that deployed at scale, right? The right-hand side is already implemented and scaling today. So the challenge for the ecosystem is to figure out how to make the right-hand side here, which is of tremendous value in terms of bringing CXL semantics and capabilities over a wider radius of the data center. How do we best do that?

So Infabrica has put forward the concept of an accelerated compute fabric. And specifically here, if you notice the difference between this diagram and the previous, we have what's called an accelerated compute fabric switch in the same spot that that RDMA NIC or DPU was. Now, the purpose of this device is effectively a convergence of the right-hand side and the left-hand side from the previous diagram. CPUs and other processors are able to leverage scale-up connectivity to peer devices that are CXL or PCIe through an accelerated compute fabric switch. But through that same device, memory movement can be enabled to go scale out over a very large radius of the network. We're not talking about just a few racks. We're talking about tens of thousands, hundreds of thousands, millions of nodes. And there are many efforts that you've heard of this week in OCP, like the Ultra Ethernet Consortium. And what Google sourced on Falcon, which is intended to allow accelerated compute fabrics to scale to extremely large sizes at very high bandwidth. Memory is simply another service that can be enabled on such a fabric.

So an accelerated compute fabric architecture, this is an exemplary one in Fabrica. But it is a concept that can be built by anyone, which combines the attributes of a multi-terabit network fabric with a multi-terabit compute fabric. And you'll notice that these are switched interfaces. So they can switch on the network side. They can switch on the PCIe CXL side. And they can switch in between each other, transmit and receive packets as an existing NIC or DPU does. The accelerated compute fabric switch speaks two languages-- memory semantic, page movements or cache line transfers or flits, and it speaks the language of packets, ethernet in this case.

The power of this architecture is that memory movement can now scale in a cohesive and tiered way from local resources all the way to very large scale clusters. So in this diagram, this is an example of a flat, large AI cluster, where tier one is the memory access that is extremely tight, low latency, high bandwidth. And but very limited in capacity, because it's super expensive. Especially in the AI world, if you take into account COOs and the cost impact of that being packaged into processors, that dollar per gigabyte is an order of easily a couple of orders magnitude higher than, or one order of magnitude plus higher than the cost of DDR. So solving the problem by adding more and more HBMs is actually going away from the solution of actually trying to-- what most people here are doing, which is trying to solve the memory wall by sharing and pooling memory. Tier two in this diagram is what we call accelerated compute fabric local memory. This is CXL memory that is effectively peer to these. And this is on the order of tens of terabytes of capacity that can be enabled with admittedly lower bandwidth than HBM or what's local within a CPU subsystem. But it's still bounded on latency, order of a few hundred nanoseconds of latency. And it's much cheaper. Tier three is actually a very interesting tier because this is actually unlocks hundreds of terabytes of memory that exist across every server. And as we know, since the main concept here is memory is underutilized across the entire fleet, how do we make use of it anywhere in the cluster? And the way to do that is to enable it to be a pipeline feeding any other node in the network. The penalty here is latency. When you go across a network, you're talking about the order of microsecond latency, but not order of 100 microseconds of latency, and not the order of 50 microseconds of latency, or even 30 microseconds of latency. We're talking about the order of a few microseconds of latency, which fit into the window of many operations that can be done to pipeline an AI workload. Because as you know, AI workloads are highly choreographed in terms of the pipelining of communication and computational phases in order to feed data to when it's absolutely required by the GPU or the processor.

So this is the schema by which we can look at memory movement across the AI clusters that are seeking to be built. Taking that a step further, the idea of network-attached memory can be applied as a system concept. In this diagram, we're showing an ACF switch, which connects to a large pool, tens of terabytes, of CXL memory that has the attributes of being pooled and accessible from a large number of GPU, AI, or high-performance computing servers. So this acts as a multi-node memory server appliance with many hundreds of gigabytes per second of aggregate read bandwidth with that bounded few microseconds of latency. Why would we make use of a system like this? It uses the existing pathways and footprint of high-bandwidth ethernet I/O that are already being laid down in the data center. It allows the movement of data across those tiers, and that capacity tier can be shared with path diversity, i.e. minimizing queuing delay, across an extremely wide radius.

So Infabrica is actually making this product available. In 2024, we will have nodes that will be able to support accelerator networking at multi-800 gig and with PCIe and CXL cabling, as well as a network memory appliance. At multiple terabits per second of throughput, they can connect any collection of CPUs, GPUs, CXL memory, SSD storage, any PCIe accelerator, or PCIe device, or CXL device, for that matter, with the attributes of having programmable network transport over ethernet. That includes RoCE, TCP, and we expect to be able to support UEC as well, based on what's been released to date. That integrates the functionality of NICs, PCIe CXL switches, and TORs in terms of functionality in a composable and modular form factor. Because the idea here is to be able to deploy resources at scale with the most efficient utilization possible, whether that's memory, storage, GPU, or CPU cores alike.

So inclusion, if the goal here is to accelerate the deployment of these ideas in the technology, our belief is that we need to leverage as much as we can of what is existing and robust in terms of data center system infrastructure, which means that there will be trade-offs. There will be trade-offs in terms of making systems design meet a realistic objective versus the objectives of every single workgroup that seeks to optimize or meet all the requirements in one category, for example, in the category of latency, or the category of security, or something else. Our asks from an industry roadmap perspective are that we should have higher and more efficient CXL bandwidth going forward. In terms of lane speed, there are higher bandwidth SerDes interfaces out there, and skinnier controller IP from the industry, because it has to be more efficient. Wider CXL controller silicon with more memory density. And one thing that I think is important is, what is the real performance in systems? We'd like to see tail latency characterization through fabrics to understand the relative strengths and weaknesses of CXL technologies as applied at a particular scale. And the last but not least is reliable and low-cost interconnect and cabling, because that is a less developed ecosystem relative to other interconnect technologies. And we'd like to see them aligned to more data center deployed today cabling technologies. And with that, thank you for your attention.

You want to take some questions? Any questions?

So you must be terminating protocols to change fabrics. What is your termination architecture, and how scalable is it?

We terminate-- and the concept of it is similar to how you would treat that in a NIC as well. We terminate PCIe. We terminate CXL. We can also be a transit for them. We don't have to terminate. But when you're moving between domains, you are terminating.

And how well are you doing that?

I think we are pretty capably doing that.

So line rate is what you're saying?

Oh, for a performance per second? Absolutely. There's no-- anything subline rate is not even an option.

OK. Fair. OK.

Yeah.

I guess Pankaj asked some of the questions. First thing is, it looks like you're a switch in NIC, and I think CXL switch as well as Ethernet switch. Mine write about that. I'm trying to also understand the benefits of what looks like traditionally what we see is a backend architecture and a frontend architecture, and it looks like combining those. I can see the cost reductions, but I'm trying to more understand other benefits as well as your software architecture. Thank you.

Thanks for the question. The benefits are really in three different categories that matter the most to infrastructure. Bandwidth, we can scale to more bandwidth. But we're using this because basically we have everything that is pointed in the scale up direction through CXL also be able to be converted into Ethernet. And for that to be multi-railable, so we can create leaf spine classes up to any radix, meaning that we can build server architectures that have tens of terabytes, the terabits per second of throughput exiting it. The second piece is collapsing latency. Again, there's a trade-off here because we're not shooting for the lowest latency possible to match the DDR or NUMA latency. That's not our goal. When you look on the other side of the network, there's a significant amount of latency when you go through a PCIe switch, a NIC or a DPU, and then a top rack switch to get to the first hop switch of a network to then do the same thing on the other side to hit a GPU that's, let's say, on the other end of the row. That's a many microseconds latency today. So we're able to collapse that. And the third is fundamentally on TCO. These move forward based on price performance. So we're basically scaling by having a more efficient device that addresses scale up and scale out. And the TCO benefit varies based on customer and workload, but we estimate it to between 30% and 40%.

OK, Rochan, thank you.

Thanks.
