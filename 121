
Hi, my name is Jonathan Zhang. I'm fortunate to have this opportunity to represent the engineering team from Astera Labs to discuss with industry colleagues about what's on our mind when we are in the journey of engineering the CXL memory controller for deployment and facing those opportunities and challenges, what kind of innovations and ideas we have to tackle them.

So in this morning, our product manager, Priya, gave the presentation about Astera Labs memory controller products. We know that during the value proposition phase, we need to get performances there, live within the power budget and the latency budget. Now when it really comes to deployment, then what we are looking at, I always think about a spider web, that you have performance, stability, interoperability, and new features, right? The old things you cannot trade off. You have to gradually fulfill all of them and expand all of them at the same time, like a spider does things. But in terms of performance and stability, the problem statement is straightforward. So there's a lot of work that needs to be done there, but the problem statement is straightforward, so I'm not going to talk about that here. So I focus on interoperability and the features, features like manageability, debugging, security, and RAS.

I take those as examples. Now with interoperability, that as a CXL, the industry expectation is whatever DIMM that's supported by the processor, we can just plug into the CXL card and expect it to work. Now the thing is DDR5 technology itself is not fully mature. So we need to get it to on par with the processors, what's out there. And the root port on the processors, there are CXL 1.0, 1.1 CPUs that are already on the market, but there are more coming, and 2.0 are still in development phase. So we need to interop with all the major vendors. On the compliance part, we need to work with all the analyzers, exercisers, pass protocol testing, electrical testing. So those are all the things, those are all the challenges facing our teams.

Now how to get there? I think one of the important factors is to establish very close relationships and work together with our partners, our partners including silicon vendor partners and the memory vendor partners together. Internally, test is the most important thing. We need to run sufficiently scaled testing. Our customers say you need to run 3,000 cards for 30 days. So those are the things that we need to make sure internally we have sufficiently scaled testing. So those testing, enough testing also provides solid support for the engineering. So that includes telemetry data get together. And for the software developer, if they make a change, they would be able to get a full picture very quickly. Now, what are the obstacles? So obstacles could be how would you be able to test the memory controller without dependencies on other components? Because other components, one is that they may not be mature. Secondly is you may not have enough volume. We don't get enough of the customer support, customer reference board from the silicon vendors. How do we do that? So we have some innovations going on there and try to solve those problems and how we can test things quickly without waiting for a server to boot. So I'm not going to give details there, but we have ideas to do that.

Switching topic about the manageability. So how do you have the one firmware source code code base and the one firmware image to support the various configurations that both support product usage and support debugging, support the silicon validation? So we need to be able to configure for many different things. For example, card. We have our own card. Some customers use our card for product, but other customers choose to develop their own card. So just the card design will be different. For example, GPIO is different. I2C topology are different. How are you able to support all those variations? How are you going to be able to support the CXL link configuration, either it is a x16 link, one x16 link, or two link, each eight lengths? How do you set up the RAS policies, throttling parameters? Those are all the things that need to be solved before you can get a firmware solution to production.

In terms of in-band manageability, the CXL spec defined-- kudos to the CXL forum-- the spec defined a lot of things that's very useful. An open source community developed CXL driver, cxl-cli, Rasdaemon. Our thinking is that we will utilize that as much as possible. But there are gaps. So how to deal with these gaps? So we deal with gaps through two things. One is that we go through, as much as possible, go through the cxl-cli command, vendor-defined command, as much as possible. Second is we provide a C-based software development kit to fill in the gaps. That kit can run directly, either at the system firmware during Linux boot phase, or in the host Linux, or in BMC, BMC Linux. And the functionality is also provided to run in UEFI shell.

Out of band-- so when it comes to out of band, it's a little bit more muddy, comparing to the in-band, where the JEDEC, the CXL forum, and the DMTF organizations are working very hard, and they crank out a lot of specs. But the reality is that the implementations in BMC are not quite there. And also the BMC communities are a little  . But the one thing that is in common is that all, most of them all, use Linux kernel. So in the monthly Linux open source community meeting, a startup presented a proposal to the Linux community that our proposal is use the same Linux kernel driver code base for both the BMC kernel and for the host kernel. So this is a diagram-based presentation about that proposal.

And here's a table-based. So the Linux, the OS kernel CXL driver and the BMC kernel CXL driver, they could share the same code base. Why not? And they share the same CXL component command interface code, and the same CXL event management code. But what's the difference? You use a different kconfig to build it. And the lower level is different, that in case of the in-band, it's all through the PCIe MMIO. And for the out-band, it would be MCTP over I2C or I3C. And for the in-band, they produce CCFS. And the out-band, they may not need to. And the event interrupt source is different. The memory controller device would interrupt the in-band CXL driver through PCIe VDM, but the VDM method. But then for the out-band, it's through MCTP. And they use different mailboxes. So that's the proposal we made to the kernel community and the BMC community. But for us, we have to go without this grand picture. So we have CXL directory over I2C driver to solve that problem.

Switching topic about debuggability. So when the CXL solution is deployed in the data center, it's a let's off situation. So people just tell you, hey, there's a problem. What tool you have for me to get some data for you? And then it's up to you. So we need to think about all the angles of figuring out how to do that. And also, we need to think about, over the time, in a bare metal instance, you do not have access to in-band. You only have access to out-band. How to solve those problems?

Security. So when it comes to security, there's different angles. Like, for example, for all the device firmwares, you talk about the secure boot. Now, specifically to PCIe device, for memory device, you think about the IDE. How to encrypt the data flowing through the wire. And memory encryption. How do you make sure the data on the DIMM itself is encrypted? So that the tenant does not have to trust the CSP. They choose not to trust. But here, I focus on the interface protection. How do you protect the CXL device itself? So when it comes to the CXL firmware interface, there are two types of interfaces. One is a spec-defined interface. That includes the MMIO spec-defined register, MMIO registers, and the CXL mailboxes. Another is a memory controller supplemental interfaces. So for us, we define all the interfaces carefully through what we call API-based interfaces. And so that we can encapsulate all the register access details, device register access details, in the firmware. Another complication is that even though the CXL spec defines so many mailbox commands, in reality, let's say for example, in the tenant use case, you do not want the host to be able to issue many of the CXL mailbox commands. That would change the state of the CXL device. In that case, you would apply a log list. And you'd log all the violations, access violations. For example, from the inbound, you're not allowed to change. Because the inbound is not trusted. So you're not allowed to issue the, let's say, set alert config, or change the notification policy. And for all the register accesses, our firmware moderated.

OK. So this is the last page. Call to action. This morning, we had Priya from our company give a presentation. So that would be a good resource to look at as well to know the context of my presentation. Now, for us, we work for the memory expansion solution. We think everything is there. And we will see it gets deployed in the data center, see that it gets sold as products, the services. Now, can it be improved? Yes, it does. For example, we can tailor the workloads to fully take advantage of CXL memory expansion. Now with heterogenius interleaving, you don't need to do any change. It's just one piece of memory combining both the processor side and the CXL side. And the latency actually gets improved when the bandwidth crosses a certain threshold. But that being said, if the workload is tailored and to fully take advantage of the capacity and the bandwidth increase, we will see more benefits. I just talked about the single CXL Linux CXL driver for both host and the BMC. Now, the CXL DMTF stack for BMC and host is defined in the spec, but this gets into the code. Now, the first wave of the CXL, the step one is memory expansion. The next step would be memory sharing and memory pooling. So for memory sharing, without any code change, we can already support active/passive memory sharing because synchronization and the memory barrier is done implicitly. Now, when it comes to active/active one, there will be need software support for it. How do you set up synchronization, make it easy for developers to take advantage of it so that you avoid memory copies? That's expensive and it costs energy. Another call to action is for the memory pooling. I'm very grateful for the CMS project for the lead center for the volunteers that let's follow the hardware software co-design strategy. Let's all work together. If you guys see anything that needs to be done from the device side, let us know. Another suggestion, feedback for the CMS project is that to build the pooling-related compliance and interop guidelines in the suites. PCIe spec, PCIe is so successful. One of the key reasons is it has very great compliance test suites and all the vendors from different segments, they all work towards compliance testing. And then when it comes together, it just works. So CXL's memory pooling is the same story. A lot of companies are involved, parties are involved. Let's just work together to make the end user feel effortless. Thank you. That's my presentation. Any questions?

Thanks, Jonathan. My question is about this combined stack for -- the combined stack for BMC and host site. How realistic is that?

Okay. Thank you, Prakash, for that question. So I presented that idea to the CXL kernel community two, three months ago. Very good feedback, but I'm not sure who is working on it. And we are also approaching the BMC community as well and get a lot of good feedback. I think it's just a matter of time.

Okay. Yeah. I think that would be something that reduces duplication of work.

Yeah. And make quality even better. Thank you. For us, we make that proposal. In the meantime, we move forward with our own solution.

