
Hello, everybody. Today we're going to be having a brief discussion over PCI errors,  correctable and uncorrectable errors, as they apply to our platform. As you know, our platforms  today are heavily leaning into the PCI bus for a lot of our endpoint devices, and some  of these errors can have performance impact on our applications in terms of bandwidth,  application hangs, performance, etc. So, and the way, currently, these errors are being handled by  the platform, heavily leaned into what's referred to today by the industry as Firmware First,  which basically uses SMI interrupts as the main method of reporting errors to the platform  and having the platform handle these errors. 

So, stepping back a bit and giving everybody  here a 10,000-foot view of. What we have today inside our training clusters, we have many,  many systems that have hardware acceleration in them, and these systems are all interconnected  over our backend network. Now, our backend network is basically the backbone of all of  the data that's being consumed by our accelerated training racks, and these backend networks  are basically what's consuming all of the data from each rack and keeping our training  models moving forward and progressing as we train. Now, it's critically important that  these systems remain stable, error-free, and whenever they do take errors, that the platform  is able to process these errors without affecting the performance of the application or, for  example, generating an application crash, because that would be catastrophic. 

Now, looking  at a rack itself, for example, this is one of our recent racks that we announced at OCP. This is the Grand Teton rack. It's basically made up of two Grand Teton systems with 16  accelerators, right? The system itself has a backend switch, and this is the main rack  that we're using to train our models today, like Llama 3 that was also announced.

 So taking one of these Grand Teton systems and looking at the topology itself, we can  see that the general breakdown is we have two CPUs. These CPUs have multiple PCIe root  ports, and these root ports are communicating with a Gen 5 PCIe switch that allows us to  break out the PCIe bus into many ports so we can talk to either Ethernet devices or  drives or accelerators, right? And the main takeaway here is that these systems have tons  of PCIe receivers, and each one of these receivers has AER capability. The AER capability allows  the reporting of errors back to the root port, and if all of these devices are generating  errors and the system has to handle all of these errors, there can definitely be performance  implications. 

Now, looking at a slice of one of these CPUs,  like I mentioned before, you can see the root ports, the NICs, the SSDs, et cetera, and  all of these devices are transferring tons of data at Gen 5 speed, and we have to ensure  that the reliability and serviceability of the system remains stable in the event that  we see errors. 

Now, Anil is going to go through and give us a quick presentation on the proposed  solution to overcome the limitations of the SMI error reporting.

Thank you, Carlos. So, first of all, bon dia. Anyone attended last year's OCP Regional  Summit in Prague? Very good. So we actually shared Grand Teton RAS or reliability solution,  some part of it last year in the Regional Summit and also in the Global Summit. At that  time, our focus was how did we handle the uncorrected errors. So all the various tasks  which are like downstream port containment, the ECRC kind of features. Today, we're going  to talk about correctable errors. So how do we design the system to manage the correctable  errors? Now, generally, correctable errors is correctable errors. Hardware corrects it.

 So what's the problem? Well, there are problems. There are two problems, actually, that we  experienced. One is whenever there's a correction, there's a retry. There's a link retry, retraining,  a retry and retraining. Now, if there are excessive retries happening, let's say because  of a bust of errors, then that could cause a latency issue to the applications. Now,  based on our experience, what we observed since we started deploying, we don't see that  kind of performance impact because the way we designed the system, PCI links have enough  bandwidth margins, so therefore, even if there's a retry that's happening, we don't see the  application impact. The second issue is whenever there's an error, as Carlos was mentioning,  the hardware is going to correct it, but they also will do an interrupt. So it creates an  interrupt or interruption to the application because that signal, the SMI, system measurement  interrupt, goes in all the way to the root port, to the CPU, and all the cores have to  now handle that error. Now, that actually did cause a problem to us, and that's what  we observed. 

So what was really happening as the--so if you look at the flowchart here,  the blue box and purple box, this is all the--and then the gray box is the hardware, right? And any time there's a correctable error happens, there will be SMI interrupt. That interrupt  will be handled through the firmware, the blue color box. And the firmware then will  notify to the OS through this system control interrupt again. So there is a hierarchy of  interrupts have been generated to handle every correctable error. And think about it that  if there are a burst of errors, the application will get interrupted quite often, okay? Now,  that's where the challenge we faced. Now, the reason we use SMI is because the way that  this PCI spec is defined in the implementation, either all the errors, correctable and uncorrectable  errors, have to be from your first or through SMI or through OS first. Now, for uncorrectable  errors, we have to use SMI. And therefore, we were forced into using SMI for correctable  errors also. And that's really become a problem for us. So we had to come up with a solution  where correctable errors do not trigger SMI, okay? Now, there was no simple solution. We  talked to our vendors. There was no support in some sense. 

So we came up with our own,  we can call hack or alternate solution where what we did was for uncorrectable errors,  we still kept the same SMI flow. But for correctable errors, we changed the flow. And instead of  using SMI, we did MSI, which is the managed--what's called media system interrupt, which actually  goes to the OS directly. So no firmware interruption of--to the firmware, there was no interruption  to the application, okay? Because once it is reported to kernel, now it is really--there  are all the cores are available and only one core will get interrupted. And now there are  so many cores. So therefore, application interruption is minimized significantly. Now, and then,  once you do that, next challenge you face was that we still have to log the error into  this--what the orange color box, the BMC cell log. Because that's a persistent space. And  then for us, when we deploy the systems, we have to make sure that all--any kind of inter--any  kind of error happens, it is saved--the log is saved in a persistent space. And that's  the strategy we had to use the BMC cell log. Now, the question is, previously, the firmware,  the SMM, was doing the cell logging, which was easy, platform-based solution. Once we  do it to the OS, what do we do now? How does OS can write to the BMC? There's no direct  path. So we decided to use Rasdaemon, the dark green color, Rasdaemon, okay? Rasdaemon is,  again, an open source software, which--we like open source software, right? So we picked  the Rasdaemon. Now, we have to still modify that, which we did. We upstream the change. And so that's the solution we came up with. So OS will get the interrupt, create the--read  the errors information, and notify to Rasdaemon. And Rasdaemon will then write to the BMC cell,  okay? Because that's a user space daemon running. And then the rest of the tooling, which is  the--on the top, the blue box, that's our own tooling where we pull these cell logs  and look for any errors, any new errors. And then as the errors are being recorded, we  compute the error rate because our solution--correctable errors, as I was saying earlier, that hardware  is correcting anyway, right? So it's not like we have to do something intentionally, like  take the service down, or machine down. But we need to monitor and make sure that the  error rate doesn't exceed certain limit. If it does, that means something can go wrong. So it's a preemptive action, okay? So to do the preemptive action, we have to compute  the error rate and then monitor those error rate thresholds. So that's why the blue box,  our own tooling logic would do that. So the beauty of this solution that we came up with  was that we didn't have to change our tooling at all because that tooling remains same whether  with the errors are reported through SMI or now this new scheme. So we had kind of took  the baby step. First, we changed the platform, modified the platform, and then kept the tooling  the same. But this is still a hack. This is not a complete solution architected by spec,  okay? So there are some additional work need to be done, the next step. Our work is not  done. 

So what we have to do, and this is also a call to action here, we have to work with  the PCIe forum, SIG, and especially the firmware group, and propose some changes. ECR is the  engineering change request, right? So we have to make some changes in the spec so that the  firmware and OS interface can be designed where uncorrected errors still report through  SMI and correctable errors report through MSI, through the kernel, right? So that is  change we are actually working on right now and the ECR, we'll submit it and then hopefully  if it is accepted then that's the first step. The second, next step is implementing this. So we have to implement it, there will be some changes required in the platform firmware  as well as the kernel AER handler to really implement this revised change. As you know,  this AER handler, advanced error reporting, it's a legacy for decades. So any time you  have to change any legacy software, it takes some time. So we understand it will take some  time, but this is the direction we are going. The third thing is we obviously want to leverage  open OCP, so we want to contribute the solution in the future through the OCP hardware fault  management subproject. This is the subproject that we have been running for multiple years  now. And call to action is really to, again this is all collaboration efforts, so anyone  who is interested and willing to help, reach out to us, myself and Carlos, or just join  our OCP hardware fault management, there's a hyperlink there, subproject, and help us  implement this faster than what we can do on our own. And there are additional things,  the links are here, like PCI firmware spec, in case if you are interested in looking at  more what the firmware spec really is and where the changes will happen, reach out to  us, we can explain that, what we are proposing. And there are additional contributions already  being done in the PCI space, not really related to errors alone, but just for the PCI, how  do we handle, how do we monitor the PCI information, so PCI crawler is one application that we  contributed in the past. There's a lane margining tool, so again what happens is that whenever  the lane margin changes, it can result into receiver errors, which is also correctable  errors. So it's important to monitor those per lane error rate. So lane margining tool  allows us to do that. And this is again contributed, already available, GitHub, you can click the  link and then download and play with it. With that, we'll stop, and any questions?
