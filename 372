
So, this is a presentation on Host Addressable SLM, how NVMe and CXL are collaborating. It's given by myself and Jason Molgaard. We are both co-chairs of the SNIA Technical Council, co-chairs of the SNIA Computational Storage Technical Work Group, as well as working together in NVMe and OCP, and I don't know—where else I seem to work with Jason. It's a case of two competitors working together for the good of the industry. Jason, additionally, is with 25 years of experience, works for Solidigm, and I, with, sorry, 25 years of experience in storage, work for Solidigm. I have more years than that. I have experience in storage and work for Samsung.

With that, with the introductions, we'll get into the presentation. So, we're going to be talking a lot about NVMe TP4184, which is Host Addressable SLM. This is a technical proposal that is still in the architectural phase, but we'll talk about what we are allowed by NVMe to talk about. We'll talk about computational storage and use for Host Addressable SLM. We'll talk about combining NVMe and CXL technology, and we'll talk about use cases.

But with that, let's get into what this is all about. So why do we want to provide Host Addressable SLM? So NVMe devices provide Host Accessible Memory in the form of SLM, Subsystem Local Memory. This is a new command set that was done by NVMe. It was done as part of the computational storage; it's a storage story, which included this command set as well as the computational programs command set. Accessing this memory by a memory protocol will, one, be more efficient. Two, it allows for cache coherency, and we'll talk more about that later. Three, it allows peer-to-peer communication using a memory model. That's something you've heard about, I've heard it, in three different presentations already today. So we've heard about how important peer-to-peer communication is. This is the NVMe standardization effort to try to put together a standard that allows for that. And it also eliminates context switching. What do we mean by that? What it means is, if you have an application that is storing or loading or storing from memory, you don't have to do a context switch to an NVMe driver to do an NVMe command to communicate that data between the application and your NVMe device, you can simply load or store it to a memory address and it automatically gets to the NVMe device. What's the computational storage use? So computational storage drives have host accessible memory than a traditional storage device. So that's a place where we may want to use this. The benefits from peer-to-peer communication, we've heard a lot about it already. We'll talk even more about it in this. So if you're tired of hearing about peer-to-peer, well, you can leave. Not really. You may want to hear what we have to say about it. Why not CMB PMR? The biggest thing about CMB and PMR is there's currently no mechanism to specify a location in CMB PMR to the device. There have been proposals for how to do that, but it does require something new in order to address that in terms of an address to the device.

So what does host-addressable SLM provide? So it is in the architecture definition phase. It means we haven't started the nuts and bolts of it yet. We are still trying to determine some of the architectural aspects of it. Some things like, do we utilize the CMB PMR space? Or do we use something new? We've come to some of those conclusions in the NVMe technical work group, but we're not going to go too far down that path. We're staying at a higher level of what we're trying to do. So SLM is addressed at a host physical address. There's two possible host physical addresses addressing mechanisms that can be used. One is the PCIe BAR accessing mechanism. The other is CXL-HDM. For those who don't know, what that three-letter acronym stands for? I've had to struggle with it. It is host-managed device memory. And it always confuses me because I think the D is in the wrong place. I think that it's HMD, host managed device memory. But anyway, for those of you who need to know, that's what it means. So SLM memory can have a virtual mapping for host applications. So a host application does not have to switch contexts for SLM memory accesses. Again, my point earlier of how you avoid those context switches in the host. SLM memory is accessible by both the host and the device, meaning the device has a way to access it through the SLM command set addressing mechanism. SLM can be read or written with either host load store commands or CXL.mem commands. So it's giving some flexibility of how you can read or write from that memory. Compute, when we look at the computational program's characteristics for using this, utilizes the host-addressable SLM as a mechanism for an application to directly store data into that memory or load data from the memory. And utilize it for its computation. It also allows, the thing we've heard a lot about, peer-to-peer data movement based upon an HPA. Whether that HPA is a BAR access method or an HDM method. SLM is still accessible using the SLM command set memory read, memory write commands, and also the memory copy commands. So we're not taking away from what's in the SLM command set. We are expanding on it.

So how do we use this? One of the things that we've spent a lot of time is trying to show the ecosystem for how you can utilize this new functionality. So, I'm not going to walk step through step through the flowchart on the right-hand side of your screen. But I'm going to walk through the textual description of it for you, so you can kind of understand how you would use this. This is the flow for the CXL version of this, and we'll go through another flow like this for the CMB BAR version of this. So an NVMe device with CXL SLM is similar to a CXL type 2 device, for those who are in-depth familiar with CXL. The plan for CXL type 2 devices is for the OS to do standard PCI configuration, for example, allocating BAR space. The BAR space has to do with how you define other information about the CXL space. And then it loads driver CXL configuration using the device's PCI identifier. The NVMe device with CXL SLM will use an enhanced NVMe driver. So that driver will be enhanced for CXL utilization. The device will use the existing NVMe class code and may use a new programming interface identifier to explore, to expose CXL capabilities. What does that mean? What that means is, we're not asking you to do a dual function device. You don't have a device that is a CXL function on the device and an NVMe function. You have an NVMe function. Now, how does that work? So the device driver discovers the device's capabilities. And once it's discovered them, it calls the OS CXL core services for CXL memory setup. So it hasn't come up as a CXL device. It came up like an NVMe device. And then it goes and it calls a special thing that is an OS CXL core services. Through that, the core services gives you kernel interfaces for the driver to set up the CXL capabilities, such as the HDM decoders. And return the necessary information. For example, the host physical address range for that device. So the NVMe driver calls the OS services. The OS services sets up the device. And then to that driver, it returns the pertinent information that the driver needs to use in order to access that space. The device CXL memory allocation is controlled by the NVMe driver. So once it's called the OS services. It's gotten the address information. It then is in control of managing that memory space for the device. One of the things that's not really on this slide is the fact that that memory is a restricted memory. That memory is only available to the NVMe driver. It's not a generally available CXL chunk of memory. And then finally, there is currently not Linux support for CXL type 2 devices yet. But that is planned. And the driver owns the runtime management of the device CXL memory. So if you are doing virtualization, it's up to the NVMe driver to split up that memory between the virtual systems in coordination with the virtualization machine. But that CXL memory management is not done by the host. It's done by the NVMe driver.

So what does this look like figuratively? So when you have the SLM namespaces. The SLM namespace, each one of these would map into host physical address, base address, and a size. The at the point in time that you have called the CXL OS services, you are allocated a contiguous block of HPA space for all of your SLM. You could, like this diagram shows, put your SLM contiguous in there. You put could put a space in there, but that space is still allocated to SLM for the device even if you don't happen to use it. So all of your SLM falls within a contiguous chunk of host physical address space. So your namespaces may not be necessarily contiguous, but your space is—you cannot have a space that's allocated to you for some of your SLM, a space that's allocated to something else, and then another space that's allocated for more SLM. The application has some sort of virtual addressing potentially for its use of a piece of your SLM. The mapping of that is all handled in the host. When the device ends up requesting access to that space, the mapping gets done. It gets mapped to a specific host physical address space. Which then, in turn, as it goes through the HDM decoders on the device, gets mapped to a specific area within one of your SLM namespaces.

So going on from there. Moving to the PCIe BAR mapping. Again, I will go through the text and not through the diagram over there. But for those of you who have downloaded the file or if you download it later, you've got a flowchart. It kind of shows what the words say. But so, the device statically assigns one or more SLM namespaces for PCIe BAR access. So, the device decides how much BAR space do I need for all of my SLM. The device advertises that need or that requirement, which includes SLM usage as well as any other BAR usage. So, if the, if the device is using BAR for PMR and CMB and SLM, it adds all of that up and figures out how much BAR space do I need and requests all of that from the host. It gets allocated some amount of BAR space from the host. The host PCIe driver allocates the space as normal, part of the discovery process. So, the NVMe driver manages the available BAR space for SLM access. By determining a namespace ID, the SLM namespace offset, the PCIe BAR offset, and a length. It may be, and actually these slides didn't get updated since we decided to not require that this be associated with CMB. It is a contiguous range, just like in the previous slide. It is a contiguous space in your BAR space. Okay, again, and on the next slide you'll see it a little bit more. We're not requiring that the SLM namespaces be contiguous with each other, but that there is only one space allocated for all of your SLM namespaces. So, the device maintains a mapping to translate PCIe addresses to SLM. Again, we'll see that a little bit more on the next slide. The application uses the BAR range it is provided by the driver to access the SLM directly using PCIe read and write operations.

So how does this look? It's very similar to the last slide, except now instead of using the HDM, we're using the BAR. You will see on this particular slide, however, that we've shown that you may have multiple namespaces. The namespaces are all in one contiguous BAR window or BAR address space on the device, but there may, depending on the sizes of your SLM, be spaces in between individual SLM namespaces. So all of this looks pretty much the same as the other. We've kind of been through how it's configured, but this all maps together again from the application side. The application gets a potentially a virtual address and it maps that through, and that in turn turns into a host physical address that addresses the particular address range within your SLM namespace.

So, how is this used for host address? How is computational storage going to use? Or is it able to use? How is it addressable SLM?

So earlier today, if you were in a session that Jason and I did, we kind of walked through the computational storage architecture. I'm going to kind of gloss over this slide of the fact that we presented this so many times; I think that most of you have probably seen it and are tired of it.

But what I will jump to is the fact that when you get into computational storage, you have the concept of function data memory. Function data memory in NVMe is called subsystem local memory. You also have the concept of allocated function data memory.

And allocated function data memory, on our next slide, we'll see that that is associated with your SLM range sets. So how do we use this? I'm going to back up to this slide of the fact that the allocated function data memory, which is that little dark blue chunk that you saw on the previous slides, is the memory space on the device that is used as potentially input to a function, output to a function, scratchpad for a function. So now, if you have the ability from the host, from the application, to use memory read and write operations or CXL.mem operations to that memory space, you're able to read or write that memory space, perform your computation, and then perform your appropriate functions on that memory space. For example, you could load something into, or sorry, store something into that memory, tell your device to do a computation on it, and then that output, which is into your allocated function data memory or your SLM memory range set, could be the source of a write command that simply does a write from that memory address that's on your SSD into the NVM space, the SSD space of the device. Yes?

First question, if you have this function data memory, what's the point of having CMB?

We're totally divorcing ourselves from CMB. In the fact that we're saying, you could put this in CMB. Then you have a conflict of whether or not CMB—you have traditional uses for CMB, and you have computational programs' uses for CMB or SLM uses for CMB. So the real question is, how are you using CMB? Do you want to use it in the way it was initially developed as a command memory buffer, or are you using it now as SLM? So we're talking about this really being used as part of SLM, and it is a different BAR space, and that was a direction that we got from the NVMe technical work group.

Can you overlay this?

No, there's no real reason to overlay them. You can put them contiguous, and that's a device choice as to how it allocates that memory as far as its PCIe BAR space. But there's no, we could find no reason why you would overlay them where you would want to access something as CMB and at the same time want to access it as this host addressable SLM. And if there are reasons for that, we're still, we are in the architecture phase, and that can be changed. So if you have reasons for that, please help us understand those reasons within the NVMe community.

So we've kind of talked about how we use it for computation, and at this point, I will turn it over to Jason to talk about combining NVMe and CXL.

All right, thank you, Bill. So now that we kind of understand TP4184 and what we're doing there, I thought we'd take a look at why we would even want to do this, and what are the benefits that we gain from doing that.

So in other words, what does CXL bring to the table that benefits NVMe? So kind of four things came to mind. First is coherent, that CXL would provide a coherent memory between a host and one or more devices using that subsystem local memory. CXL provides low latency, fine granularity path to SLM, taking advantage of the smaller flit sizes for those transfers, especially if you have a small amount of data to transfer. CXL.mem provides direct load/store access to SLM. And then the other thing is you can have support for larger memory capacities than if you were just doing PCIe BAR access. There is kind of a limit to how much PCIe BAR space you can allocate in your system, whereas with CXL you can certainly have much more memory in terms of a virtual addressing space. All right, so how is this different from the PCIe BAR access? I think that this is kind of along the lines of the questions that were asked earlier. So CXL allows both coherency with host memory and MMIO space, while the PCIe BAR access only allows load/store access over PCIe using uncached MMIO space. So I think the key summary there is coherency. You're not going to have coherency with CMB over MMIO, but you do have coherency with CXL. The second item is related. CXL provides coherency for device access to host memory. So we have the coherency both ways. We can have the memory in the device and memory in the host being coherent with one another bidirectionally. So we kind of touched on this one already. CXL protocol is more efficient than the PCIe memory access protocol. So we can have lower latency, higher throughput. And get the right order there. And then CXL protocol has less strict ordering rules than PCIe memory access protocol. And then here, as Bill mentioned earlier, peer-to-peer is coming up again. So we can do peer-to-peer using CXL.mem instead of with MMIO.

So I know we've talked about coherency a lot, and I think that many of us are probably familiar with that. But I... we put together this slide to just level set on what is coherency? So that way we're all on the same page. What are the benefits that it provides? And this is true not just of CXL, but anything that's coherent. So all devices perceive the same view of memory. So the memory between them is consistent. All devices perceive the same view of shared data. So the data is up to date. Devices and hosts can push data to each other and pull data from each other. So if one has a...version that it needs, if a host has a version that the device needs or vice versa, we can send communication to one another and say, "I need that" or "give that to me" or make that request automatically. And this includes device-to-device communication or peer-to-peer. And then I think this last point here is certainly provides the best summary: avoids or reduces copies that can grow stale. We don't want a copy. We want one source. That we all reference and that the hardware can maintain that so that there isn't anything, anyone getting a stale copy.

All right, so with that, let's move into the use cases and try to talk through, you know, how would this be used? What would it look like?

So we've got four use cases. And they all use an example of using the figure over on the right-hand side; they're all very similar to that. So let me kind of explain what the figure, what we're looking at with that figure and how it, you know, what's happening there so that way the rest of the diagrams all make sense. So up here at the top, we have the host. So that's our, you know, our host system, our host CPU. It's got CPU cores. It's got a CPU cache. And of course, it's got our CXL.io and our CXL.mem interfaces. In the middle, we've got this oval labeled CXL fabric. So this is our interconnect fabric where, you know, we're going to connect everything together. And underneath that, of course, down here in the bottom, we have what we've labeled as a computational storage drive. So you know, Bill showed the architectures for the computational storage drive earlier. And some of the talks we did earlier today, we went into great detail. This little figure is changed a little bit. We've kind of abstracted away some of the blocks that are less relevant and expanded on one key area. And that is the subsystem local memory namespace has the SLM media itself kind of expanded out so that we can describe more readily what's happening within the memory of that SLM as it's worked through the namespace. So we'll get into that on a use case by use case basis. Over here on the left side, we've got a diagram that shows our HPA space. Here's the whole memory space. And part of the memory is, you know, mapped to CXL subsystem local memory or host addressable SLM. And then there's an input data buffer, an output data buffer depending on the example. And the application is doing some kind of a load or a store to that SLM, that host addressable SLM. And then lastly, kind of as a legend, three different types of arrows. So green, solid green is CXL.mem. The dashed brown color is kind of a CXL.io or NVMe. And then lastly, the solid blue is CSD local. So it's internal to the computational storage drive. Okay. So with that background, then let's walk through this first use case. This is a data post-processing use case. So the data is going to come into the drive. We're going to process the data in the drive before writing it out to the storage media. So what is the value proposition of this use case? So the value proposition is that we're not going to DMA data, kind of using in the traditional sense like we've been doing. We're going to take advantage of lower latency CXL. You know, it's actually true if we have small data transfer. Our configuration in this example, we've got an input data buffer that's in the host addressable SLM. So that's, you know, shown here on our host physical address space. And then it's also shown over here in our expanded SLM media. So we've got our input data buffer that's in the host addressable SLM. And then we've got an output data buffer that's just in traditional SLM. So that isn't necessarily host. It is not host addressable. Could it be? Yes. It doesn't need to be for this example. All right. So then each of the numbered steps over here on the example corresponds with a number on the arrows on our picture on the right. So step one is the application is going to write to the input data buffer using CXL.mem. So that's our green line here. So a number of writes are going to happen. Essentially, the host is setting up the buffer. It's setting up all the data that it wants to be stored. So normally this would happen in system memory, right? In host memory. And we're just doing it straight to the drive, to the drive's SLM instead of to the host memory. Some or all that data may still be in the host cache when the application finishes those writes. And that's okay. Coherency will help resolve that. All right. So when the host is done, then it issues an execute program command, an NVMe execute program command to the compute namespace in the drive. So that's this arrow two coming into our compute namespace. So because the intent here is that the compute namespace or the function or the program is going to operate on the input data buffer and write it to the output data buffer, since it's a CXL memory, it recognizes, hey, I need to make sure I've got all my data from the cache. So it's going to use a snoop back invalidate to tell the cache, hey, you need to flush all your contents to me. Invalidate whatever you have. That way I've got the latest copy, I've got the latest data, and I can operate on it. So all the data gets out of the cache and into the device then with that operation. So step three, we perform the operation. We do our post-processing. We do the manipulation or whatever function we want to do and write it to our output data buffer. And when that's finished, we send a completion back, which is the number four going back to the host. Now the host is going to send a NVMe copy command to copy the data from the output data buffer to the storage media. So this command is designed to take the contents of our output data buffer, our subsystem local memory and copy it over to the NVM namespace, which then writes it out to the storage media. So that's step six. And so the data gets copied in that fashion. And then lastly, step seven, completion is sent to the NVM namespace to indicate completion or sent is posted for the NVM namespace indicating completion of that command.

Okay, let's move on to another use case. So use case two, we've called this pure data processing. And for anybody who was in the previous session that was talking about RAID offload, this was Bill and I's attempt to... do something similar to what was described in that talk using SLM. I can't guarantee it's 100% correct, but it should work this way, whether that's how they envisioned it to work, I don't know. All right, so the value proposition with this use case is that we're going to bypass data movement through host memory. So we're going to do peer to peer, in other words. Our configuration is a little bit different than the first case. The data initially resides in a peer device. In this case, it's going to be in computational storage drive B. And we want to ultimately get it into the input data buffer in computational storage drive A, so that we can perform a computation on it, such as computing parity. And so you may say to yourself, well, why is this a computational storage? It doesn't need to be. It doesn't need to be. Yeah, you're absolutely correct. I drew the picture intentionally this way for two reasons. First, I wanted to show an example where we did have two computational storage drives with different subsystem local memory namespaces that were each allocated a separate space in the host physical address space. So yes, this use case doesn't actually take advantage of the memory for that, for SLM namespace 2. But if you had an actual system with multiple drives that supported host addressable SLM, then your memory would be allocated in this way over in the host. And that was the goal. And for that same reason, as we walk through this example, you'll see that the second half of the example is very similar to example number one. We would not have had to have the output data buffer in host addressable SLM. Or this use case. But again, just wanted to be able to show that. All right. So step one. We're going to, the host is going to issue an NVMe read command to drive B. So here we are sending our number one command down to the NVM namespace of drive B. With the destination is the input data buffer on drive A. And since it's host addressable, it knows where that is. That is a valid host address. So we want this to be our destination over here for that read operation. So. In step two, the data is going to be transferred peer to peer from drive B to drive A. So it's going to come out of the NVM namespace, follow path two, and get over to the input data buffer in our computational storage drive A. And then at that point, a completion queue entry is posted for that read command. And then this is where the back half of this example is very similar to our first example. We're now in step four. The host is going to issue a. Next. An execute program command to the compute namespace. So here's our four coming into the compute namespace. We're going to take the data and we're going to write it all from the or process it and reading the input data buffer and write it to the output data buffer. And so that's step five. And then step six, a completion queue entry is posted for the compute namespace. In step seven, the host issues an NVMe copy command to copy the data. From the output data buffer to the storage media. So here's our copy command coming in. And it comes into our device. We copy it over from the output data buffers in step eight to our NVM namespace. And then a completion queue entry is posted. One second. So you'll notice there's no CXL in this at all. So we are taking advantage of host addressable SLM in this example. It doesn't need to be the CXL version of it. It could be the PCI BAR style access. And that would be a perfectly valid use of the TP4184 and not actually use CXL. Question?

In step seven, couldn't you just do a simple NVMe write command?

But the data is already in our drive.

Yes, you could.

You could.

What's the difference?

None.

Okay.

Well, that, you know, maybe. If the device recognizes the address, it can do an internal transfer. If it doesn't, it's going to have to go down to the root port to get translated through the IOMMU. And you get sent back to the same device. It has the potential to be identical.

Yeah. The device should be smart enough to know that.

It sure should.

Okay.

So for the recording, I'll see if I can try to summarize that. The comment was, "Couldn't you just use an NVMe write command?" And the answer is, "You could, and it may be the same, but it may be different." The difference is that it may end up having to go back to the root port, which would be traversing across the PCI interface and create unnecessary traffic.

Okay. Let's go to use case number three. So this is a data post-processing with a standard SSD example. So in this case, we actually do have a traditional SSD over here on the left. We have a computational storage processor shown over here on the right, so this is not a drive. It doesn't actually have any storage, although it probably could. You could probably do it anyway. The value proposition is very similar to use case two in that we're going to bypass data movement through the host. We're going to, you know, transfer the data. You know, transfer data peer-to-peer once again. So in this case, our input data buffer and our output data buffer are both in host addressable SLM space. So step one, we're going to use write, the application is going to do writes or stores to the input data buffer using CXL.mem, very similar to use case one. And similar to use case one, we're going to still have potentially data in the cache. That, you know, is done, that's still residing there after those writes are complete. And that's okay. We know that's going to resolve itself. When the application is done doing those writes, the host issues an execute program command to the compute namespace. So that's step two. The compute namespace operates on the input data buffer and writes to the output data buffer. This is still similar to example one, you know, and again, because of the coherency, we have the, you know, do a snoop back and validate, make sure that all of the data was transferred to the device from the host, or sorry, from the host to the device. So we've got all of our data there and we can then, you know, operate on it. So we perform the operation in step three. And then we, step four, we send a completion back. Now here's where the example gets different. In step five, the host is going to generate an IO write to this traditional SSD. And it's going to have the data pointer point to this output data buffer over here, which it can address. It's a valid system address. The SSD is going to use PCIe UIO for a peer-to-peer transfer and we're going to transfer it out of the computational storage processor and into the NVM namespace of our traditional SSD. And we'll post a completion in step seven for that.

All right. Use case four is similar to use case one, but it's going the opposite direction. In this case, we're doing a data pre-processing before sending to the host. So basically, we're going to bring in some data from the storage media, process on it, and then the host can read it. So let me move through this example rather quickly since some of it's very similar, highlighting, you know, some of the things that are quite kind of different. So in this use case, our input data buffer is in SLM, so just traditional SLM that's not host addressable. And our output data buffer is in host addressable SLM. All right. So step one, the host is using NVMe memory copy command to the SLM namespace, and we're going to read the data from the NVM namespace and into our input data buffer. So step one is the command comes in. Step two. The data is transferred from the media to the input data buffer, and a completion is posted for that. So now we've got the data in our input data buffer. We have an execute program command that's going to be sent in step four to the compute namespace. Coherency is going to ensure that if there's any data in the host that, you know, it's invalidated because presumably we're going to, you know, generate a bunch of new data. We're going to process on it, put it in the output data buffer. So that's step five. And a completion gets posted in step six. At this point, this is where this example kind of differs from some of the earlier ones. Now the host can simply come and read the data out of that memory, that SLM, using CXL.mem at its convenience. There's no longer any transfer, no longer any DMA to the host memory. It's just sitting there in memory that the host can access whenever it is ready.

All right. So let me kind of summarize where we're at, and then I have a quiz for you. All right. So we think that CXL and NVMe technologies can be used simultaneously. I think that, you know, we think that there is going to be a convergence there. The coherent memory between the host and one or more devices with SLM is going to be kind of key to some of these computational storage devices moving forward, and the CXL.mem provides direct load store access to SLM, so you no longer have the DMA. You can just access it directly using load and store. TP4184 that Bill talked about is in development, and it enables that addressability of the SLM. We're going to have two methods in that TP4184, so we're going to have the CXL method and a PCIe BAR method. This is still in architecture, as Bill said. So... It's still early in the process. It will take some time. If you've got input or requirements or whatever that we need to consider, then by all means, please make sure it gets to us. And looking ahead, you know, these two technologies are going to intersect, and having host addressable SLM is kind of the first step to enable that.

All right. So as Bill mentioned at the beginning, we're both co-chairs of the Computational Storage Technical Workgroup. And we're very interested in you taking this quiz. If you scan the QR code over there on the right-hand side, maybe "survey" is a better word. So we're wanting to understand, after we work on 4184, what other directions do we need to go? What other things, you know, related to computational storage do we need to think about? So there's only four simple questions. The first question is, what is computational storage to you? What does that... What do those words mean to you? This is a multiple choice. So pick the definition. Pick the definition that you think makes the most sense. Question number two, how would you use computational storage? This is a fill in the blank. You can write whatever words make sense to you, but kind of think in terms of, you know, you've got carte blanche. You can pick whatever function you want for computational storage, whatever implementation you think. How would you use it? Question three, what is the future and evolution of computational storage? This is multiple choice or multiple guess, because your crystal ball is probably as good as mine. You know, we definitely want to know where it is going. Where do you think it's going to go? What other technologies need to be incorporated in with it? And then lastly, question four, any other thoughts or ideas on computational storage that you'd like to share? And we definitely will take a look at this, and we'll kind of drive what direction things go in the future. All right. So with that, I think we've got a few minutes for questions, if there are some additional questions.

Question on use case three.

Okay.

So we need step six, the SSD needs to use PCI-UIO because the source buffer is in CXL-HDM. Would it be possible to define another SOM, post-addressable namespace, that was not coherent, and be able to use a normal PCIe BAR at the end?

Yeah. So let me try to repeat the question. The question is, can we define another SOM namespace that we could use so we wouldn't have to use UIO? Is that a good summary of the question? So we could certainly define another SOM namespace, but I'm not sure how we would address it then, if it's not host addressable, because...

Sorry. It would be host addressable. Just a separate namespace that's not coherent.

Not coherent.

Jonathan, I think that the issue here is the fact that the standard SSD does not have CXL capability.

Right.

Therefore, that SSD cannot use the CXL.mem protocol. So that's where this picture came from; it was saying if we have a traditional SSD, it's going to end up using CXL.io.

I understand that. And I was just wondering, how do we tag the host addressable SOM namespace as CXL-coherent, and is there a different way to tag it so that it's not, and therefore, accessible by the BAR? Because if the host isn't going to view the result, and only the destination, or only the traditional SSD is, could it be in a specific namespace that's non-coherent?

It could be.

I'm just wondering if the standard is being developed in such a way that that could exist.

I believe that the way the standard is being developed, that yes, that would be allowed to exist. But I think that what you're looking at is that step number six is the particular step you're looking at. And yes, that could be; the real thing is, in step number six, you are giving a write command to your standard SSD.

Right.

That standard SSD doesn't even understand CXL necessarily. It simply goes out and says, "I need the data from this address." It has no concept of coherency.

The reason you can't do it through CMB is because step three has to go to SLM. Step three can't talk to CMB.

Because this computational program's command set doesn't have any concept of reading or writing from CMB.

Yep.

It doesn't have addresses. It only has offsets in an SLM namespace. That's the only address that exists in an execute command.

I think you said that backwards.

In an execute command, step three is an execute command. It can only address offsets from the base of an SLM namespace. It has no memory addresses; it only has SLM offsets.

But if your SLM and CMB are all under a BAR in a contiguous space.

SLM's not under CMB.

SLM itself has nothing to do with CMB. It's never had anything to do with CMB.

SLM is a namespace with a link.

Yeah, but I thought you just put up there that you get a BAR and you get a continuous chunk of memory space, and under that you map your SLM, your CMB, and all this stuff.

So that's the proposal that we're...

You have separate offsets into the BAR for the beginning.

And we've been directed by the technical work group that we should use a different BAR.

Right. We need to support a different BAR.

A different index into the BAR. But we are not using the CMB index. So again, that's an architectural thing. We'd love to hear your thoughts on it, and we'd really like to consider them. I think there was another...

Somebody had another question over here.

I don't know if I'm ready, but... well, it's a hypothetical question. If... if the host server does not have CXL capability, can this system operate without CXL?

I mean, it could. It could do everything with NVMe commands, but you wouldn't have any of the coherency. You couldn't do a CXL.mem to write to the input data buffer; it would be back to using the existing memory read and memory write commands.

Well... I guess I'd qualify that a little bit more. You could set this up to solely run on PCIe BAR space. Which means you don't have to have any CXL for that. You are simply using PCIe BAR as your address space. And you're using MMIO for your data transfer, right?

All right. It looks like we're at the end of our time. So, thank you, everyone, for coming.
