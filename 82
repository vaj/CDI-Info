
So, my name's Rick Kutcipal. I'm a product planner within Broadcom's Data Center Solutions Group. I own the CXL ecosystem. And with me today, I have Sreeni Bagalkote. He is a peer of mine in the planning organization, and he owns the PCIe and CXL ecosystems. And what we wanted to do today, and it was a great follow-on to the Fabric discussion, is talk about some of our recent changes and our recent updates to our product plans in order to enable, you know, an open AI ecosystem. And so, with that, Sreeni's going to start.

Thanks, Rick. Yeah, good afternoon, everybody. My name is Sreeni Bagalkote. And so, there's a lot of excitement, discussion, and, you know, about AI, NICs, SSDs, CXL memory modules, and we just heard, you know, CXL, not only CXL memory modules, but the use cases behind that. And so, what we want to tell you today is how we are enabling that PCIe and CXL ecosystem. Apart from all these various components like accelerators, NICs, and we need a fabric. And we just heard PCIe and CXL fabric and how important it is. And we want to tell you what we are doing from our side on enabling that ecosystem and not just enabling, but accelerating the ecosystem.

So, at a 100,000-foot level, this is our PCIe and CXL switch roadmap. It's very simple. The way you read this is we name all our switches Atlas. Gen 4 is Atlas 1. Gen 5 is Atlas 2, and so on. And as you can see, we have been doing this for a while. And, you know, one thing that you can count on us to do well is to execute at a very high-quality silicon, deliver high-quality silicon at a predictable schedule. And we've been doing it generation after generation. So, Atlas 1 is our Gen 4. Atlas 2 is our Gen 5 switch. And it's been shipping for a couple of years, and it's been doing very well. It's our single most successful switch ever. And in and through this, again, as you heard, the PCIe switch itself matured. Previously, what was just a simple fan-out connectivity to expand the switch, you know, the CPU PCIe lanes. Now, it's matured and grown up to be an open standards-based fabric within a system, especially AI servers. So, you have all these various components, all these various use cases like memory pooling, memory sharing, and then expansion. You need some fabric. And PCIe and CXL is the right fabric, internal fabric. And that's how the switches, we have been evolving. We have been doing our part to, you know, in that maturing of PCIe into an internal fabric. And that means adding the right lanes, right-sizing the, you know, radix, adding the right silicon features, adding the right software features, and also a lot of ecosystem enablement type of thing like telemetry, diagnostics. And we've been kind of pushing our PCIe switch roadmap in that direction. And like I said, Atlas 2 has been shipping for a while. And we are looking forward to repeat the same success with Atlas 3.

Atlas 3 is our PCIe Gen 6 and CXL 3.1. We will be fully 3.1 compliant. That includes everything that you just heard, you know, in the previous session, Vincent. I mean, it will support CXL.cache, CXL.mem, all the PBR, the fabric features, fabric manager support. So this is going to be our 144 lane in the 5 nanometer. It will have not only the required features, obviously, the Gen 6. In fact, we will support PCIe Gen 6.1, CXL 3.1. It will also support several optional features like FLIT performance measurements, FLIT error injection, things that are really needed to have a very successful and stable AI systems. And apart from that, we'll also have some features that are not even part of the spec, like, you know, peer-to-peer across virtual hierarchies, not only in the CXL domain, but also in the PCIe domain. We'll support the whole complement of CXL device, .mem, .cache, like I said, in future GFAM that we just talked about. And underpinning all this is our best-in-class service. This is not going to be an easy transition. Gen 5 to Gen 6, like Rick will talk about in a second how difficult it is. And so our best-in-class service underpins our Atlas 3 product line. And apart from that, it will also have one cool feature called embedded PCIe analyzer. So like I said, one of the things that we are doing is we are doing our part to evolve PCIe as the best choice for interconnect in the open AI systems. So part of that is being able to diagnose not just the PCI switch issues, but the endpoints and the host connected to the PCIe. And so every x16 port will have an embedded PCIe analyzer. So externally in-band, out-of-band, you can connect to it and then do majority of your debugging that you would otherwise require an external PCIe analyzer for. And we are taping this out in July next year, and then we will have the samples available, customer samples available in December next year. And this has already been the pre-Silicon environment has been running very successfully and stably. And now Rick will talk about what we want to do to enable others to join us now and kind of take advantage of where, you know, at this platform.

All right. Thanks, Sreeni. You know, so there's a lot of excitement, a lot going on. But at the same time as owner of the ecosystem, right, I have to keep an eye on a lot of the complexities, and one of the largest complexities that we're going to face as we move to Atlas 3 is the transition to PCI Gen 6, right? We've been through a number -- we've been through a lot of technology transitions. I mean, everybody in the room has PCI Gen 2, Gen 3, Gen 4. Those have largely been speed bumps using the same general SERDES infrastructure. But now as we move to -- well, I should say with the exception of Gen 2 to Gen 3, there was an encoding change. But -- and that caused -- that did cause some delays, you know, as the ecosystem moved to Gen 3. But now as we go to Gen 5, right, we're taking on a whole new SERDES architecture with PAM 4. Not that PAM 4 is, you know, real new or all that hard. Broadcom has a number of product lines that are based on PAM 4, and so it's a very well understood technology. But from an ecosystem perspective, everybody connecting all of their stuff together, PAM 4 will be very new. And remember, then everything has to be backwards compatible, right? So it is not only PAM 4, but then it has to be backwards compatible to traditional architectures. And, you know, it doesn't sound that hard, but it's going to take a -- it's going to be a learning curve, and there's going to be a lot of work to make it work seamlessly across the ecosystem. And there are other things that we're adding to it, right? So in some of the previous conversations, you've heard about flip mode, right? This is to enable forward error correction. And things like this that are all new and have to be done correctly to be able to make this all work. Another complication is the differences in flip modes between PCI and some of the CXL specifications, as pointed out earlier. And so in order to help with this, and also the learning curve of CXL itself, I mean, you saw in the previous presentation of all the translations that are required by the switches, right?

So there's a lot of learning that comes along with that. So what we're doing is -- we're doing a couple things. One is right now we have an FPGA implementation of Atlas 3. Atlas 3 is the Gen 6 device that Sreeni was just talking about. We have that in the lab today. It's up and running. We are testing it with a number of different CXL components on both sides. And we want to open -- you know, and we're seeing a lot of advantage of doing that. And what we want to do is we want to open this up to the ecosystem, you know, at a larger scale. So what we want to do is work with the different vendors, the CXL vendors, to be able to test with our platform. And this -- and so it should be pointed out, this is really a CXL-focused thing. So this is not PCI Gen 6, right? This is more about the CXL protocol. Could be 3.1 -- or 1.1 components, 2.0, 3.0, 3.1. You know, we'll take anything to test. So I encourage you to reach out. My email address is on that slide. And see if this is the right fit for your particular device, because it will benefit, you know, not only us, but you and the ecosystem at whole.

Then, like Sreeni pointed out, we're planning to tape out Atlas 3 in July. That gives us samples toward the end of the year. And we plan on putting those on two different board-level platforms to enable this level of testing. The first one is our RDK, our Rapid Development Kit. And this is really the superset. This is our full validation system. This is probably overkill for interoperability. This is -- this has not only CEM connectors on it. It has MCIO connectors, SMA connectors. It has a wide variety of things, along with all the debug ports and things like that. So it's probably a little overkill for standard interoperability. But what I don't have is a picture of our other vehicle. And that's what we call it, our host interface board, or HIB. And it's a CEM form factor board x16 to the host. And it has our 144-lane switch on it. And then on the top, there are two female CEM connectors. And there are also four, I believe, four to six MCIO connectors for different types of connectivity. And so with these two platforms, what I encourage the ecosystem to do is to, you know, work with us, you have my email address, so that we can start planning to be able to trade equipment to do interoperability testing together. Because again, right, without that kind of interoperability and that, you know, that ecosystem work, a lot of this is going to take much longer time than, you know, than we think. So with that, Sreeni.

Thanks, Rick. So we talked about PCIe and CXL being the most important open standards-based internal fabric. And that trend is firmly set in. And we told you what we are doing on our side to accelerate it. Rick, we also told you about what we are doing to enable the ecosystem. Now, looking forward to it. So the, I mean, we talked about open internal AI fabric. What it means is being able to connect CPUs on the north, CPU complex on the north, accelerator complex on the south, various components, and various use cases. So that, I think, is pretty well understood. But we are not stopping there. Our customers and our partners want to go and take PCIe and CXL fabric much further than an internal AI fabric. They're asking us to create a high-performance, low-latency, standards-based, coherent interconnect for the GPGPU complex. Again, let me repeat this. A high-performance, low-latency, cache-coherent, south-side connectivity interconnect between four accelerators. And that's what we are doing.

So today, we are publicly announcing our Atlas 4. Atlas 4 is our PCIe Gen 7. It's a PCIe Gen 7 and CXL 3.X switch. And we are not only accelerating Atlas 3, as you saw, we are significantly accelerating our PCIe Gen 7. We are highly confident that we'll be very successful in executing this Atlas 4 because Gen 5 to Gen 6 is a complicated transition. Gen 6 to Gen 7 is fairly standard. We have the right technology. It's based on our Broadcom internal 128-gig service. It'll be based on 3 nanometer. And the best of all, it'll be available just 12 months after we have Gen 6 samples will be available. So 12 months after Gen 6 samples, we will have Atlas 4 Gen 7 samples will be available. Yes, sir?

PCIe SIG will not have it. 

That's true.
PCIe, we will be, we are running ahead of PCIe SIG. And hopefully, we will take SIG also along with us. So we are talking to SIG to accelerate. And we also expect, I mean, that's a, I mean, the fair question. We also expect there will be some things that the PCIe SIG will do. And we may have to reconcile those and adjust in our B0. But A0, we will hit A0 by December 2025.


Yeah, fair point. 128 gig Gen 7. And also, it's important to note that we are not doing this in a vacuum. There is a healthy partner ecosystem that's doing this along with us. If no questions, thanks for the opportunity for us to present a roadmap and say, and show that we're significantly accelerating our Gen 6 and Gen 7. Thank you.
