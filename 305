
Good morning, everyone. I'm Lisa with the CXL Consortium. Thank you for joining the "Exploring Coherent Memory and Innovative Use Cases" webinar. I would like to introduce Kurt Lender, Marketing Working Group Co-Chair for the CXL Consortium and Senior Ecosystem Enabling Manager for the Data Center Group at Intel.

Good morning, everyone—or evening, wherever you are in the world. I guess, first and foremost, in this day and age, I hope you're all healthy. I was going to joke that this was always going to be a virtual event, but that's probably not in good taste these days. With that said, though, this is a continuing series of webinars that the Marketing Work Group of CXL will be hosting. So, expect to see these on a regular basis, potentially even on a more regular basis given the world we live in right now. Some quick logistics: you won't be able to ask questions directly. You can enter those in the window. At the end, if we have time, we will deal with them. If we don't, we do blogs, social media that we will update answers through blogs. So, again, we'll see how time permits in that sense.

So, I just want to really quickly go over what we're here to talk about: CXL. There was a December high-level update on this introduction to Compute Express Link that Debendra Das Sharma, who is one of the Co-Chairs of the Technical Task Force, gave. The link is here; it's on YouTube. This actual presentation or webinar will also be posted. So, there will be basically an archive of all the webinars. So, real quickly, CXL is a new processor interconnect, a high-speed CPU to device interconnect. It is being delivered as an open standard. That is the CXL Consortium. I'll cover that at the end with all the different work groups. It is high bandwidth and low latency. And one of the other main features is that it leverages PCI Express, the PCI Express 5 electricals, and PHYs. Two things that we're going to cover today and go into more detail are basically the coherent interface. It is a coherent interface on the I/O side. And then, some of the target applications and usage models.

So, with that, I will introduce the two presenters. Siamak Tavallaei is also a Co-Chair of the Technical Task Force of the CXL Consortium with Debendra. He also is a Principal Architect at Microsoft Azure, in the Azure group. Rob Blankenship is on the CXL Consortium Memory Work Group, and he is a Processor Architect and Principal Engineer at Intel, and one of the authors of the spec.

So, with that, I would like to present Siamak, who will cover the use cases, and then he'll present Rob later for the coherency update.

Good morning. Thank you very much, Kurt. This is Siamak Tavallaei again. It's good to be here with you guys, going over a number of interesting things that can be done with CXL. In general, a number of use cases for CXL include interconnect for expanding memory beyond what the locally attached memory provides to the processor. It is an interconnect between the processor and CPUs and accelerators. CXL will enable new concepts in modular systems and disaggregated computing as well. We will explore some of those today.

As you know, a number of emerging architectures have come about, and they are based on different network topologies.

When we look at this thing, we can think of two different, distinct layers. Some of these architectures are optimized around fabric components; some of them around compute components. So then, we can look at the problem as being homogeneous versus heterogeneous, or these interconnects enabling a scale-up natively or a scale-out using network systems. In the homogeneous versus heterogeneous aspect, if the devices can benefit from peer-to-peer transfers such as accelerators, or if the accelerators or devices can help the CPU to offload some workload, we come up with the concept of host-to-device interfaces. So, a host to an accelerator, to a storage element, or to a memory element.

So, from the interconnect point of view, then, a number of concepts have emerged. Some of them optimize around the processor itself. In some other environments, to accelerate the capabilities of GPUs and FPGAs, and other silicon-based accelerators, some proprietary networks have been developed. And there have been other consortiums in the industry that focus around memory semantics and expand through memory concepts. In all of these, data movement is very important. Coherent load-store semantics for control planes, or optimization around data throughput for data planes, using DMA and RDMA in block mode. Another major aspect of system design these days is the interconnecting for local elements, either being done at a rack level or even a data center interconnect.

So, the industry is converging around the interconnect that's suitable for things outside of the CPU domain. The focus is on Compute Express Link.

Then, CXL creates new opportunities for us, particularly for accelerators. It is creating an environment for special purpose compute, such as artificial intelligence or HPC. It is good for computational storage for databases, or for network accelerators. What's in common between these is just the offloading that they do. The offloading that these devices do requires data movement. CXL can also benefit memory expansion because it provides an efficient load-store, high-bandwidth, low-latency interconnect. Once we develop the CXL concept, it offers flexibility to bridge outside of the CXL world through gateways or bridge components to broader fabrics, eventually allowing compute disaggregation and optimizing storage class memory within a rack or data center. As we develop CXL concepts, then mechanical aspects become interesting as well, and we will have new innovations in form factors as well.

So, let's take a little deeper look at what CXL is.

It is fundamentally based on PCIe interconnect, but it provides new protocols. At a physical layer, devices auto-negotiate; CPU and the device auto-negotiate the capabilities of CXL or PCIe. The bit rate of the target now is 32 gigabits per second per lane of PCIe Gen 5. CXL will create a good foundation for use cases that might be included within PCIe Gen 6 in the future.

So, the new protocols that are available with CXL include CXL.io, which is very similar to PCIe. It is useful for discovery, configuration, register set, and RAS features. Additionally, CXL.cache and CXL.memory are suitable for bridging the world between the accelerators, the smart NICs, and the CPUs to create a converged memory pool for them to work on. These three protocols, in FLIT format, run on the normal PCIe physical layer.

So, these protocols eventually provide an environment for flexible programming models for heterogeneous environments and disaggregated computing. They allow efficient data movement using the load/store semantics in the cache coherence model between the host and device, and the device and host. They allow user-level and kernel-level data movement to access data, produce results, and store them efficiently. And, eventually, CXL allows borrowing pools of memory, for example, either from the accelerators, from the CPU, or from memory devices themselves. In contrast, PCIe has been optimized for block moves. Data between the CPU and PCIe devices are not cached, and most of the data movement is done through DMA.

So, a number of use cases for CXL include a caching device, an accelerator that can cache data, and provide its memory to be cached. In that model, the protocols used will be CXL.io and CXL.cache.

A different type of device might be just a memory buffer; CXL to memory translation for a particular memory device. In that model, the protocol of choice will be CXL.memory, but all devices will still have CXL.io to program, configure, and enumerate.

A device that might have an accelerator component, and basically a caching element and memory element, is possible as well. In that model, all three protocols will be used: CXL.io, CXL.cache, and CXL.mem.

So, putting this all together, three different types of devices are possible: caching devices, accelerators that combine with caching and memory, and memory buffers.

Now we can go a little bit deeper and explore the coherency models. Rob, the processor architect and one of the authors of the CXL-Spec, will lead us through that discussion. Rob?

Thank you, Siamak. I'm going to walk through CXL protocols again, just touch on some similar aspects to what Siamak mentioned. Then, I will get into cache coherency introduction, kind of the hows and whys on why we do cache coherence. Look at how CXL fits into a CPU cache hierarchy. Then, we'll do an introduction to the CXL coherence protocol itself and some of the flows around that.

So, this is, again, just looking at the three protocols that we have on CXL: We have the I/O protocol, we have the memory protocol, and we have the cache protocol. I/O, as Siamak said, is very similar to PCI Express. The memory protocol is a protocol from the host CPU to the device for memory access, but mostly simple reads and writes. And then the cache protocol, which we're going to focus on today, allows coherent access from the device to the host CPU.

So, just talking about what caching is: Caching is the ability to bring data closer to the consumer. So, in the picture on the right, we see an accelerator at the bottom. He can access data from his cache at low latency and high dedicated bandwidth for that accelerator. If the data isn't in the cache, then he has to go to host memory, and that host memory access latency is much longer. And there's high bandwidth in host memory, but it's a shared bandwidth across all sources that are accessing that memory. So, if the accelerator can bring the data into his cache, he gets lower access latency and more dedicated bandwidth, improving its performance. The performance improvement can come in two ways: either by the device doing prefetching of data into its cache or from locality. And just talking about the definitions of those terms, prefetching is basically the device loading data into the cache before it's required, so that when he needs the data, it's there and ready to consume. Locality comes in two forms. Spatial locality basically says that if a device accesses a given address, memory address X, he's likely to access address X plus N at a point in the near future. So, if you bring in data, you're likely to see a hit in the cache for data within the same region that you brought in data earlier. So, bringing in data in chunks larger than being requested is often valuable here to see spatial locality improvements. Temporal locality says that locality and time, so that if you bring data in once, you're likely to use it again within a time window. So, if you bring it into the cache, you leave it there for some amount of time, and you're likely to see an access again and get improvement in performance.

Okay, so looking at a CPU cache, this is just an example CPU cache hierarchy. How does CXL fit into this example CPU cache hierarchy? So, the picture on the left is a CPU socket in the big gray box, and we can see that there are CPUs, level one caches, level two caches, level three caches, a home agent, and the memory. The main memory that's connected behind that home agent is either directly connected memory or memory connected on CXL.mem. Talking about the caches that are involved in the picture, we have modern CPUs that tend to have at least two levels of cache. In this picture, we have three levels of cache for the CPUs. The lower levels of cache are smaller in capacity. They have the lowest latency and the highest bandwidth per source, but their capacity is limited to hit those targets. The higher levels of the cache have less bandwidth per source but much higher capacity, and they're also shared between multiple sources. So, when we look at the CXL device, the picture is at the bottom where it says "device," we see that there's a cache here, and I'm showing it as about a one-megabyte cache. And this is a CXL cache device connected to the CPU. And the CXL cache device, from a CPU point of view, sits below the CPU's L3 or its last level cache in this example. So, this device has a dedicated cache of roughly one megabyte and has access to the last level cache or the L3 of the CPU, which is a larger cache and shared with the CPU. And you can see in this example there are two devices shown. Each device is a peer to the other, and largely, we'll talk about the details of the protocol coming up. And the picture also shows an example of where there's a CPU coherent—I should say, coherent CPU to CPU symmetric links. This is the link that connects multiple CPU sockets. This is proprietary for most CPU vendors. And this link still exists in this system, and it's above the level of where CXL connects. So, this link is largely hidden from the CXL interface, and the complexity of that link is close to CXL. And we'll talk about the details of the protocol coming up.

Rob, as you mentioned, the home agent is responsible for its memory. But, do these devices require home agents, and where does the home agent live?

Yeah. The devices... We keep the home agent functionality on the CPU host. Devices don't have to know about the home agent complexity, and I'll show some examples of that in a minute. But largely, the home agent complexity is relegated to the host CPU socket, and the devices can implement what we think is a simple coherence protocol where it doesn't have to be aware of the home agent, or it doesn't have to know the details of the functionality there.

 Very good. Thanks.

So, before we get to that part, let's just mention what cache consistency is. Cache consistency is basically a concept of how we keep caches consistent when there are many caches in the system. We need to make sure that all caches have a common view of memory. So, how do we do that? We make sure that before we update a given memory location, we have to invalidate all peer caches. And we can manage this, and we call this coherency. And we can manage this coherency either through software or through hardware. Different protocols and different products may implement it differently. But, CXL expects that we use hardware cache coherence. So, that's the discussion we'll be having today: how does hardware manage the coherence of the caches? And, one thing we also use in this definition of hardware cache coherence is we define a point of global observation. We'll denote that as "GO" in some of the flows coming up. And we define this point of global observation when new data is visible from writes that happen into a cache. We also define a granularity for tracking coherence. And this granularity is called a cache line of data. In CXL, the size of a cache line is 64 bytes.

So, getting into the details of what the cache coherence protocol is: Modern CPUs are generally built around four cache states, M, E, S, and I. This will often be abbreviated or pronounced as the "messy" protocol. Each of the four bits has a distinct meaning and requirement. "M" stands for modified, and in the modified state, data can only be in one cache in the system. It can be read or written from the device that has it in its cache. However, this data is not up to date with respect to the address that's in memory. So, this is the one case where data is not up to date with memory. In the exclusive state, this data can also only be in one cache, similar to the modified state. It can be read or written, but the data is up to date with respect to memory. So, that's the difference between the exclusive state and the modified state. In the shared state, the data is allowed to be in many caches at the same time. However, because of that, it can only be read, and the data is also up to date with respect to memory. Then, the final state is the invalid state, where it's not in the cache. These states, the MESI states, are tracked for each cache line address in the cache, and they're tracked on this cache line address granularity. In CXL, because it's a 64-byte cache line, we support address bits 51 down to 6 for the cache line memory location. Those lower bits that are excluded, 5 down to 0, are bits within the cache line, and we don't track states for those lower bits, just the upper ones. So, I should note that each level of the CPU cache hierarchy follows the MESI states, and the layers above have to be consistent with this. I'm not going to get into details of how it works between layers of the protocol; I'm just going to talk about within a single layer in this presentation. Going between layers is a more advanced topic. And then, I should also note that there are possible extensions to the four states mentioned, and many CPU vendors use extensions on top of this. But those aren't covered or part of the CXL definition.

So, this is getting to Siamak's question earlier: How are peer caches managed? Peer caches are managed through this home agent logic, which is in the host CPU. And the home agent is within each cache level. And the home agent uses a snoop to basically check the state of caches, what it needs to cause a cache state transition in the peer devices. So, a snoop would be sent from the home agent, or in our picture, the case of the host CPU. And we have example snoops I'm showing here. There are three snoops that exist in the CXL definition. We have a snoop invalidate, and that's abbreviated in CXL as snoop Invalidate "SnpInv". And it causes the cache to degrade to I-state. So, if the home sends this message to a cache, the cache will transition to I-state, and it must return any modified data. And I'll show some more details on how this cache state transitions in the next slide. But, just summarizing here, the next snoop type is a snoop data. And its abbreviation is shown. Snoop data causes the cache to degrade to S-state if it has E or M state, and it must also return any modified data. And then, the final snoop type is a snoop current. Snoop current is saying the home wants to know if there's any data, any modified data in the cache, but it doesn't require the cache to change its state.

So, bringing up the state transition diagrams, I'll focus on the picture on the right to start with, since we just talked about snoops. The snoop invalidate case is shown in this diagram, where if a snoop invalidate comes into a device cache, and looking at the M state, if the cache was in an M state to start with, the cache will transition to I-state. And because it was an M state, it's going to return this response that says, "response I and forwarding M." And it's also going to send the M state data to the host. So, this is showing the messages that the cache would return to the home agent in this transition arc for the M state case. If the cache state started in E when it received the snoop invalidate, the state transition would also transition to I. But in the E case, the arc is showing that it's going to send a response message saying, "response I," meaning the cache is going to I-state, and a hit indication saying it hit the cache in S or E state. But no data is returned because the data wasn't modified. So, the cache didn't need to return data to the host. And then the final case is a shared state, and similarly, it's going to transition to I-state for the snoop invalidate, and it actually sends the same response type as the E state transition, saying that it went to I-state, and the hit indication is combined for S and E. The other case shown here on the left is if the device is doing a request. And I'll get into a detailed flow in a minute. But if a device sends a request to the host CPU, what cache state transitions are supported? And in the diagram, I'm showing a number of different request types that can be issued from a device and the cache state that results. So, starting in the I-state, in the right of this diagram, we have a read shared. Read shared causes from an I to an S transition, as indicated by the name. The device is looking to get a shared copy of the line, so it issues a read shared. The host would return the data to the device, and the cache would be put into the S state. Going from S state back to I requires the device to issue a clean eviction message. In this case, we're showing an S to I transition with a clean evict sent from the device to the host. That's how the cache would transition back to I-state. When the cache is in I-state or S-state and it wants to go to the exclusive state, it would issue a read own command to the host. And this is shown in the arcs going from I or S-state to E. The read own command tells the host that the device wants exclusive ownership of the line and would cause the cache to install an E-state. Similarly, when degrading from E back to I from the cache point of view, it should issue this clean evict message saying that the cache is transitioning to I and letting the host know that it no longer needs to track this particular cache line in exclusive and that the device has given up ownership in the cache. Now in an E-state, the device can also silently upgrade to M-state if it updates the data. So, I'm showing a transition there saying silent write going from E to M-state. This is a device updating the data in its cache, and it's now updating the data such that it's no longer up to date with respect to memory. So, that definition says, the MSI definition says that that state is M-state. And once we're in M-state, if the cache wants to evict the M-state, it can send a dirty evict to the host. This dirty evict is sent with data indication to the host. The host gets the modified data, and at that point, the device cache can transition from M to I-state. And I'll show a detailed flow for some of these coming up in a later slide.
 
And then, Rob, the advantage of silently moving states is to reduce bandwidth requirement, right?

Yeah, the device can have low latency directly in its cache for doing a write to data, and it can write this modified data over and over within its cache once it transitions from E to M-state. It wrote that data. It can reread the data and do it all locally without even informing the host. So, it can happen at high bandwidth, low latency, without sending messages on the link once the data's made it into the device cache.

 Very good.
 
Okay, so I'm going to walk through a read flow example. We often use these diagrams to show the message flow over time, and the diagram on the right, sometimes referred to as the Feynman diagram, has agents on the X-axis. So, I'm going to show a CXL device that has a cache. This is the red CXL device. Its cache state is going to start out in I. There's an I indication in the diagram below the CXL device. I'm showing a peer cache, and its cache state is going to start out in E-state. I'm showing the home agent, and in CXL, this home agent is in the CPU host. And then, behind the home agent, we have a memory controller, and this memory controller, again, could be native DDR memory in the CPU host, or it could be CXL mem, and I'll show more detail on that in a minute. And to have a legend in the corner, just kind of highlighting the cache states we mentioned, and we'll see that we use these little X indications, green and yellow, to indicate tracker allocation from the source of messages. And it's important to note that time in these examples goes from top to bottom. We have time on the Y-axis going, starting from time zero at the top, going to increasing time below.

So, showing the flow, the CXL device that started out in I-state issues a read shared to the home agent, CPU host. The home agent receives this read shared request, and it allocates a tracking structure. It will send a read to the memory controller to start reading the data, and in parallel with that, it's going to resolve coherence between all the peer CPU caches, or all peer caches in general, which could be a CPU or a peer CXL device. And to do that, the home agent is aware of all the caching for a given address, so it knows, in this case, that the peer cache has a cache copy of the line, so it issues a snoop data to that peer cache. The snoop data, as we talked about earlier, is received by the peer cache and causes the peer cache to transition from E-state in its cache to S-state. It returns this response indication to the home, telling the home that it went to S-state, its response S, and that it was in the cache in E or S-state. Once the home receives that snoop response and receives the data from the memory controller, at that point, it sends the "GO" indication to the CXL device, in this case, "GO-S," saying that the device has global observation for shared state in the device. That allows the CXL device to transition its cache state to shared, and it also sends the data in a separate message to the device. And that completes the read/shared command from the CXL device point of view. And now, he has shared state in his cache. He can read that data going forward.

Rob, as you were talking about the responsibility of the home agent in this regard, when the allocator sees the probes, where does it know, and how does it find the appropriate memory? Is it always local, or could the memory be in the external device?

Yes, so there's one home agent for every address in the system. So there's only one place to go to resolve this peer cache coherence. And I'll show examples of where the home agent might be in the next slide. But the home agent in this case is known by the CPU host. The CPU host routes this read/shared command to that home agent. The home agent has visibility and tracking of all caches that may have a copy of the address that was requested. So, in this case, the home knows exactly which peer caches may have a copy. And it also is responsible to know where the memory controller is. In this case, the memory controller could be, and I'll show in the next slide, where the memory controller could be. So, let me just transition to that slide to show an example.

So, mapping that flow back to the CPU cache hierarchy picture that we visited earlier, we have the device at the bottom that was in the diagram earlier. This is a CXL caching device, in red, at the bottom of the diagram. So, where are the home agents? Where are the memory controllers? Where are all the peer devices? Let me show an example of that.

So, the peer caches could be all over the place. They could be CPU peer caches. They could be peer caches in a peer CXL device. Or, they could be peer caches in the other CPU socket. So, peer cache in the flow example was just showing one, but that peer cache could actually be any number of peer caches. And, it's up to the home agent logic to track all these peer caches and know their state and know how to snoop them. And, this is complexity that is, again, hidden from the CXL caching device. It's up to the CPU host home agent to know all this information.

And when we look next at the home agent itself, the home agent could be on the local CPU socket; it could be in the remote CPU socket. Again, it's not—the CXL device doesn't need to know exactly where it is, but there's one home agent that's responsible for resolving all the coherence. And behind each home agent, there could be memory controllers that are either native DDR memory controllers or memory controllers that are behind CXL.mem protocol. So logically, I'm showing CXL.mem at the top of this diagram. Physically, this is all on a shared link, but the memory that's being accessed could be CXL.mem in the local CPU. It could be CXL.mem connected in a remote CPU. The device doesn't need to know where it is; it's up to the CPU host to figure out where that memory is and also where the home agent that's responsible for that memory is. Does that answer your question, Siamak?

Yes, so it makes it very flexible and makes the design of the CXL device easier when it doesn't have to worry about the home agent, right?

 Yes, I agree.

Okay, so I have one final example for a flow. I'm going to show how writes work within a cache on CXL. Again, showing the Feynman diagram style. In this example, we have a CXL device that's going to be issuing the write. He starts out in the I state. The peer cache has a shared state. And again, we have the same home agent memory controller in this example. And in this example, we're going to be doing a write within the CXL device cache. There are three phases for this write: the ownership phase, the silent write phase, and then the cache eviction phase.

So, we start out with the CXL device issuing the RdOwn to get the data into its cache. And so, the device is going to issue a RdOwn to the home. The home again resolves coherence in the same way it did in the prior example. Except, in this case, because it's a RdOwn command, the home agent has to send a snoop inval(SnpInv) message to the peer caches. In this case, there's a peer cache with a shared state, so we send a snoop inval(SnpInv) to that cache. It causes his cache to transition from shared state to I state. And once that transition has happened, we give the response: I, hit S, E indication. This tells the home that the device's cache is in the I state now. The home was also reading memory in parallel, just like the prior example. And once it receives snoop responses and the data, it again sends the GO indication to the CXL device. In this case, it's GO_E. And so, indicating the CXL device gets E state. And the data is also sent to that CXL device. Now, that finishes the ownership phase. So, the CXL cache device now has exclusive ownership.

And next, it's going to move to doing the actual write. So, during the write phase, like we talked about earlier, the write can happen silently without informing the home agent. And we transition from E state to M state in the CXL cache device silently. This can happen at high bandwidth. There could be multiple writes that happen during this phase. It's up to the device on how it wants to implement these writes. But at the end of these writes, it's sitting in the cache in M. And it can stay there as long as the device needs it or until it's snooped from the host. In this example, I'm going to show, after the write phase, the device is done with the data and wants to evict it from its cache. So that's the cache eviction phase.

And what happens during that phase is that the CXL device will initially issue the dirty eviction message to the home agent, saying it wants to evict the data from its cache. This eviction message does not include the data itself; it just includes the address of the data that it wants to evict. The home agent receives that dirty eviction message, and when it's ready to accept the data, it'll send this GO indication with the write pull. "WritePull" means that it's ready to receive the data; it's pulling the data into the home. When the device receives this message, it transitions its cache from M to I state and sends the data to the home agent. When the home agent receives the data, it then updates memory. In this case, the memory controller could be in any number of locations, but it updates the memory controller with the mem write(MemWr). Once the home receives the completion(Cmp) from the memory controller, at that point, the eviction is done from the home agent's point of view, and the write overall is updated now in memory.

Very good, Rob. Now, a comment perhaps about the ordering rules: When in the ownership phase, the allocator tracker sends the GO_E and data. Does the order matter? Will the CXL device see them in order or not?

From an ordering point of view, if a CXL device is trying to implement, say, traditional I/O ordering, where it wants to make writes visible in a certain order, imagine a producer-consumer model where a device is writing a payload and then writes a flag. It can issue the RdOwns out of order, get all the data into the cache that it wants to modify, and then basically ensure the order is visible to the system by doing the write within the cache in the order that it needs. So, the ownership phase can basically happen out of order in that example, and we ensure that the ordering happens within the cache itself and is done in a way that's consistent with the model that the device requires.

Very good! So, this flexibility on the ordering is one of the reasons CXL is more efficient than PCIe, right?

For sure! It gives an allowance to do things out of order in terms of access to the host, and the in-orderness can be handled within the cache.

 Thanks.

So that's it for my main part of the presentation; I'll hand back to Siamak, and we can talk through the summary.

So, as we covered the details of coherency, there's a lot of questions about why it matters and how it helps. So, let's just go over the ways that people can take advantage of cache coherency between a device and a host, and specifically in contrast with block moves that are possible with a producer-consumer model that PCIe follows. So, these are some of the very good examples of how small packet data can move back and forth to maintain coherency and flexible ordering rules. So, Rob, what are the good examples, for example, for this breaking out of the ordering rules? How does it help?

Yeah, so the ordering model that PCI Express has is somewhat constraining, and by using CXL cache, we can prefetch data into the cache before we need it. This is basically from a producer-consumer model point of view. It allows you to prefetch the data portion of a payload before you've checked the flag, which saves latency on data consumption because you can do both in parallel. When you find the flags, you know the data is valid because it's already in your cache in a coherent state. From a streaming write point of view, you know, in PCI Express, we have posting semantics for writes. In CXL, we have streaming write accesses, which you can use outside of a cache, so you can use streaming writes to the host that don't require you put modified state in the cache, and these writes get completions. So, this is another example where you can use CXL cache to stream writes with completions, and this example use model for this has been PGAS, where the PGAS model needs to know when writes are committed in the destination. Because CXL has an explicit write for the streaming access, we can ensure that the data is committed. And we can also do writes, as we mentioned and showed in the prior flow, we can do writes directly in the local cache and order them appropriately within the cache.

Right. So, this is a good example of data reuse in that, between the device and the CPU, either the user mode, kernel mode, or the device driver can spend on a particular semaphore locally, without having to send the data back and forth in the form of a flag, right?

 Yes, I agree.

Right. And then, other examples that, again, people are interested to know why .cache or .mem are more efficient than block moves might include complex atomics, right? So, you can combine commands and data, send the entire packet, and expect the accelerator to execute on that, right?

Yes. And within the cache, we can do, once we have exclusive ownership, atomic operations such as a floating point add, right? Or maybe a more complicated atomic that doesn't exist on PCI Express. We can do whatever atomic is necessary, and the device can hide it from the interface. So, it can do increment, decrement, whatever atomics it needs, directly in the cache.

Okay. And, it is quite all right for a device to have only cache and not memory—an external device.

For sure, yeah. 

Okay. So, normally, following all the ordering rules is hard, much harder than just not worrying about it. And caching is a more complex concept than following just producer-consumer models. But, there are these elements that are benefiting lower latency and higher bandwidth models. What are the risks that we have in implementing cache coherence using external devices?

Yeah. So, with cache coherence, a badly behaving device may affect system performance. An example here is if we have a cache line that's trying to be accessed from CPUs or many devices at the same time; they're all trying to get ownership of the line. This can create serializing events in the host, where the home agent has to go invalidate a cache, pan the data off to another cache. So, if you have many caches all trying to access the same address, you can get into bad performance scenarios. So, the recommendation is to be careful about performance, and devices should adopt best-known practices for CPU coherence flows. Examples of that may be to use a ticket lock approach rather than a spin lock for better scaling with multiple agents. It should also have LRU-type caching principles in the device.

Very good. Thank you very much, Rob. This was a good touch on what the coherency is within a CXL. But there is much more to talk about. And the overview that we gave today is, of course, not sufficient for a full ASIC design and CXL device design. But it is giving, hopefully, people's idea of where to go, what to do, and which specification to dig a little bit deeper. Let's give the floor back to Kurt. And Kurt, would you please take us through?

Yeah, sure. I want to do a quick wrap-up and just tell you a little bit more about the consortium. First of all, this presentation will be available on both the BrightTalk channel and the CXL website. Ultimately, in the next day or two, it will be out on YouTube, as the first webinar was.

So, I just want to highlight that the CXL consortium is an open industry. And right now, we're at, in the high 90s actually, 95, 96 companies. There are three levels: adopters, contributors, and promoters. Adopters actually are free. And there's actually an evaluation copy of the specification. We are at 1.1 today; 2.0 is being developed. It's being developed by the contributors and promoters who are the members of the consortium that can join the workgroups listed here. There are five technical workgroups: protocol, PHY, system and software, industry and compliance, and the one marketing workgroup that Glenn Ward and I chair. And then, you can see the board of directors. If you missed it back in the latter part of the year, the nine original promoters and board expanded to 14. We do have the major CPU companies involved here, all the major CPU companies. And someone asked about products in the market. Right now, you could say the CXL is very much in the development stage. The consortium has said first products probably in the '21 timeframe. However, I will note that there have been announcements so far of some of the tool vendors and IP vendors, some of the core infrastructure that's needed for CXL. So, those announcements are happening this year even. So, stay tuned. And as those happen, we'll be going to either doing webinars. I was going to say going to shows. We'll see how that pans out with the environment that we're dealing with.

So, the other thing: if you need to join or want to join CXL, go to the website. There is a Q&A there on how to do that. And, like I said, you can download the spec for, I think, it's about 30 or 60 days. It's listed in there. And you can follow us on social media. We are regularly doing blogs, LinkedIn, and we have the CXL Consortium channel. So again, follow us and look for different webinars like this and/or other announcements from the consortium.

So, we do want to thank you for listening. I'm thinking we have maybe a little bit of opportunity for some questions. Let me—so, Rob, I guess you mentioned other extended states and flows are possible with the MESI, MESI. Will those be available through CXL technology in the future?

We don't have any plans to extend those states on the CXL interface itself. The host can implement extended states as long as they're consistent with MESI, but no plans. We're trying to keep it simple for the device to implement.

I see. Okay. You might have said this, but what is the prefetching capacity with CXL technology?

So, how much prefetching would be allowed is dependent on your CXL device cache size. Cache size allowance for each device is kind of CPU host-dependent in some ways. But, the CPU only has so much tracking for caches and devices. However, we expect it to be in a few megabytes range; that is, a device could have a few megabytes of cache — coherent cache — that it can prefetch into.

Okay. Thank you. Another question is: what happens when there are too many coherent requests from the CXL device to one home agent? Does the home agent become a bottleneck?

The CXL link itself has a finite amount of bandwidth. The home agent generally should have plenty of bandwidth to keep up with a single device. Maybe more of the concern would be that if you have many internal or many devices all trying to access the same home agent, that's when you can see queuing. CXL will naturally allow queuing on the request channel from the device. So, if there's contention, the request channel—requests coming from the device—will queue and get processed in order with the other requesters.

Okay. Thanks, Rob. Maybe for you, Siamak, an example of a use case for exclusive access?

Well, again, one of the reasons that we have an exclusive state is to reduce probes that need to be sent on the network, on the CXL network. When a cache line has been tagged as exclusive, that particular device owns that and knows that nobody else in the system has that cache line available. So, all the reads, of course, are local, and even the write to that cache line will be local. And, as Rob said, the cache line can go from E to M in a silent way. So, having an E state helps remove probes and snoops on the network.

I see. All right, thank you. So, we're actually close to the top of the hour, so I'm going to basically thank everyone again for participating. We did have a few more, quite a few more questions. So, like I said, we will look at doing a blog for this and respond to some of those questions, follow up on that. So, stay tuned, follow us on our channel there. And, I don't think you'll be able to ask questions through this channel, but if you do have further questions and you can't submit them here, I'm just kurt.lender@intel.com. So, you can always get a hold of me or the marketing work group for that matter. So, again, thanks for attending. Look for future webinars and we'll talk to you soon. Stay healthy. Thank you.
