
Welcome everyone to this BrightTALK session which is brought to you by SNIA's Storage Management Initiative. SNIA works to develop data and storage related standards and has over 2,000 members from 185 companies. The SNIA SMI develops and standardizes interoperable storage management technologies and promotes them to the storage, networking, and end user communities. SMI promotes these standards, sponsors, plugfests, and performance test programs and provides education for both vendors and the IT community. We're very pleased today to be co-presenting this webcast with the Open Fabrics Alliance. If you have any questions you'd like to ask during the presentation, just type them into the BrightTALK Ask a Question box. I'll be collecting the questions and we will answer them during a Q&A session following the presentation. We expect to have about 10 or 15 minutes for the Q&A session at the end. If you'd like to get a copy of the slides you can find them as an attachment associated with the BrightTALK session. Also some of the slides in the presentation contain a lot of detail so you might find it useful to expand your view to full screen by clicking on the expand icon near the lower right hand corner of the display.

Now let me introduce my co-presenter for today. My co-presenter today is Phil Cayton, he's a senior staff engineer at Intel. He currently also serves as the OFA vice chair, the OFA marketing working group co-chair, and is co-chair of the NVMe Consortium Boot Task Group and also serves as a technical lead within the SNIA SSM or Scalable Storage Management Work Group. And he also helps set the agenda for the NVMe Consortium Technical Work Group helping to drive the NVMe spec family forward.

 And I'd like to introduce the person you just heard, Richelle, who is a storage and technology enablement architect at Intel. She's also been engaged with the industry standards initiatives for many years and she's actively engaged with many, many groups supporting manageability, which do include SNIA and DMTF and NVMe and OFA and UCIe. She's the vice chair of the SNIA board of directors, she's chair of the storage management initiative, she leads the SSM technical work group developing the scalable Swordfish management API, andshe served as the SNIA technical council chair and been engaged across the breadth of technologies. And sometimes she actually gets the time to sleep.

Please note the standard SNIA disclaimers apply here. Just go ahead and read this for a second. But basically, these are the disclaimers and we are applying to them for this presentation. Okay, let's get to the talk.

 We'll start off today with a bit of a background and an overview of composable disaggregated systems and infrastructures. We'll then discuss the challenges with resource management in heterogeneous infrastructures. And then we'll talk about how we can solve most of these problems through standards-based management before diving into the Open Fabrics Management Framework, an open ecosystem effort for vendor agnostic infrastructure management of composable disaggregated systems.And then we'll end with the traditional call to action, how to get involved, how to help out with this effort before ending with the round of Q&A that Richelle spoke of.

So first, a bit of background, I promised. Modern large-scale workloads such as machine learning require tremendous computing resources, CPUs, memory storage, that sort of thing, in order to operate. Traditional compute clusters, they're created by combining compute servers over network fabrics and they serve as the backbone for these workloads. And you see on the right where each individual compute server contains a set of CPUs, some memory, some storage, some accelerators, all of the nodes are powered up, all the nodes are drawing energy at all times. However, not all running applications and workloads need all the resources in each node. And so we are over provisioning our nodes and basically we're wasting energy. This infrastructure model, it wasn't designed for today's workload hardware requirements, which may fluctuate significantly, not just between applications and workloads, but even within the same application and workload. And this often results in cases where resources are on different compute servers than the ones that are running the workload, and thus those resources are unavailable to the workload. And this can lead to less efficient workloads or workload failure entirely. Now, as large scale computing designs are increasingly multi-node and linked together through high speed networks, one solution to addressing these provisioning and efficiency limitations, as well as the hardware and operating costs, is with Composable Disaggregated Infrastructures, or CDI. With Composable Disaggregated Infrastructures, computational resources are physically disaggregated over high speed and low latency fabrics, and they may be dynamically composed as needed into a computer system and allocated to workloads on demand. Sort of a way of looking at CDI is that it turns system architecture inside out by making pools of computational resources available to be used as needed, rather than just have a core cluster of compute nodes with individual hardware components provisioned for that node's exclusive use. And this allows real-time allocation of resources and thus dynamic construction of effective and versatile and efficient platforms that adapt to modern resource-intense workloads, such as the ones I listed here at the bottom. And they provide the right resources to the right applications at the right times. Now, this not only removes the need for over-provisioning of processors and memory and storage, etc.,but it has the added benefit of reducing electricity costs through cooling fewer idling devices. Now, CDI is not new. Network disaggregation is already the state of the art for storage devices, such as is the case with NVMe over Fabric and the SNIA Swordfish. But current trends are pushing this paradigm further and extending it to computational engines and memory elements and accelerators, eventually to all forms of compute resources that are required by modern and future HPC applications.

Okay, we keep talking about composable disaggregated infrastructure. What does this mean? Well, it's really pretty straightforward. So let's talk about composable disaggregated systems. We've actually had this concept for a long time in storage. In fact, for 20 years, 30 years, for a long time within systems in the notion of creating storage pools, creating volumes, creating storage pools. What this really is is the ability to create resources on demand and allocate them and deallocate them as needed. What we're really doing here in composable disaggregated systems is applying that same concept to other parts of the same system. So this really simple diagram is really just highlighting this concept. One of the most expensive parts of your system that really gets locked in right now is memory. And so technologies like Gen Z and now CXL are expanding to focus on and give you the ability to apply that composition capability and pooling and aggregation capabilities to your memory of your systems so that you can allocate, deallocate pool across your memory. So now you have the ability to have pools of memory and allocate that and reallocate that on demand and virtualize that. And you can do the same thing with your storage and applying composable capabilities again to your CPUs. So that's really what we're talking about here. So you can have multiple kinds of that. You can have specific constrained capabilities or you can have expandable capabilities on that depending on what your system looks like.

What that really turns into though as you think about applying that into the already complex environments as well as all of the configurations and points that Phil made in the last slide here is this takes you to a bit of a manageability headache. So if you're taking in some of the configurations that we've already talked about, you have cloud, you now have all of these composable systems, you have NVMe over Fabric. So maybe you have NVMe over TCP. You have your network services. You have potentially looking at computational storage.

What you're actually doing is bringing in these additional acceleration and virtualization technologies, a ton of flexibility and options to build these custom configurations. But you're bringing in increasing management challenges and it increases your complexity exponentially.

 Your workload management and optimization then is different for each type of technology, for every type of vendor and every type of device, sometimes even per version.

 So on top of that you're asking each of your administrators to manage an increasingly heterogeneous device and network infrastructure and each one with its own management standard and model. We're also seeing a trend in the industry towards DevOps instead of the specialized management domains. We no longer see a proliferation of storage administrators and network administrators and systems administrators. Instead we're asking DevOps to be one size fits all. So in addition to all of this increased complexity and now the ability to have all of these flexible custom pools across everything.We're now saying, "All right, admins, your one size fits all. Fix everything."

 This adds all of these increasing complexity on top of that to say different technologies, multiple vendors, all of this flexibility. That makes standards-based management more critical than ever to successfully develop, integrate, deploy and manage all of these systems and storage to scale. So what do we have to help address that?

 We have a lot of great standards-based technologies that we'll talk about here today. We're specifically talking about how to address some of these composable disaggregated infrastructures for fabric management. But we build on those by starting with the basics. Some of the basic standards-based management starts with Redfish and systems-based management. So this is what Redfish looks like. We start with the Redfish RESTful hierarchy. The basic Redfish resource map starts with service route and we have a little bit of a tuple. We split the logical view and the physical view into a pairing. This kind of shows the pairing for systems. We have the systems and chassis split. So the physical view goes in chassis and the logical view goes in systems. And then there's a bunch of other supplemental places to look. So we have a bunch of supplemental services like tasks, sessions, accounts, certificates,  eventing and then some other additional resources like registries and schemas, a copy of all the schemas that are in use, metadata and some other resources like that that are also available. And then the managers here shown is basically where you would go find the information about the system that is being used to manage. So in a system that might be your BMC, it could be a physical manager, it could be a logical manager. So this is how basically how the Redfish hierarchy is structured. That can be replicated to other domains aside from systems. That could be replicated for storage. That could be replicated for fabrics. That could be replicated for memory. So let's look at that a little bit.

 SNIA owns the storage domain. We have partnered with DMTF to develop Swordfish which covers these extending Redfish for the storage domain. So we've covered that and we have that same tuple modeled for /storage where we encompass the modeling for storage. This particular slide covers both things, both an example of the NVMe drive as well as kind of encapsulating and reflecting the basics of that storage model. So you can see that /chassis there where that same tuple I talked about covering the physical portion of the model for you as well as /storage covering the logical model. It's got exactly the things you would expect to see. And as I mentioned, we're back talking about those composable systems. Storage pools, hey, they're right there. So you've got exactly what you'd expect to see there. You've got your volumes, your pools, controllers. And then you've got on your physical side, you've got in this particular instance, you've got drives. We'd have some other things if we're modeling things slightly differently. But in this case, we're showing an NVMe drive. NVMe does things slightly differently, but we've used exactly the same objects to reflect the NVMe drive as we would for, say, a fiber channel drive or a fiber channel array or SAS or SATA or whatever. We use exactly the same model, we just map the objects onto them. So we can see here the NVMe controllers. These are logical controllers, whereas in other types of devices, it might be physical controllers. But we use the same controller objects to reflect those. So that being said, you can see these are some good fundamentals to build on top of. We have these complete models. And these fit exactly those same objects we talked about earlier in the Composable Disaggregated Systems. These storage pools reflect that and provide standardized management for those objects. 

 Now we want to talk about some of the fabrics that Phil was talking about earlier. How do we do that? Well, it's the same model. We now have, instead of just systems and chassis or storage and chassis, we also have a fabrics model. So exactly the same way within Redfish and Swordfish, we have the ability to model networks. And we also model storage fabrics as well as memory fabrics and network fabrics. So all of those are done within the Redfish Fabrics model. And they follow exactly the same pattern as you can see here. So this particular one shows endpoints, zones, and switches. So this shows connectivity. There's another use within zones of modeling access rights, which I don't have here. We can dig into in another talk. But this one basically shows how to model all that connectivity within the fabric model. So all of that is reflected within that base Redfish and Swordfish standardized interface. So what then does the Open Fabrics Alliance work on if Redfish and Swordfish have all that base instrumentation? Well what OFA actually does though, OFA doesn't actually work on standards. What they actually do is they work on reference implementations. And they work on ensuring that the standards that Redfish and Swordfish are instrumenting and that DMTF and SNIA are delivering actually are correct and complete. So that's really where that work is focused. So bringing that blue layer you can see on the bottom there plugging in, what they're doing is that's where it plugs into the models. But what they're really looking at doing is ensuring that all of those base models are complete and that they work across a lot of different technology types.

Right. Well said. What this is all working towards is to address the manageability headaches of managing pools of resources. What we need is interoperability. Interoperability through common interfaces that Richelle showed and talked about to model resources to enable composability managers to efficiently connect workloads with resources in dynamic ecosystems at scale without having to worry about the underlying hardware technology or vendor specific interfaces. And that's why as Richelle talked about the OFA and DMTF and SNIA and CXL consortiums along with several US national labs and an increasing array of companies and industry experts, we're collaborating to design and develop this open fabrics manager framework that I'll go into in the next few slides. The open fabrics manager framework or OFMF, it's easier to say that. It's an open source API, a tool set and a central repository that's designed for managing composable resources over multiple fabrics. For manipulating connected resources using client friendly abstractions and configuring fabric interconnects so that workloads can be linked with this aggregate of hardware resources over these dynamic fabric infrastructures. The goal here is to provide this common set of network and resource integration tools for configuration and management orchestration and allow clients and administrators to monitor and aggregate and subdivide and provision resources and network fabrics through the common restful interfaces that Richelle spoke about. The OFMF is not trying to replace specific management tools for individual hardware and fabric resources. No, no, no. It provides a fabric to abstract these individual resource management capabilities, a common interface and a common set of technology independent management tools that trigger hardware specific agents and we'll get into that in a bit to make the actual changes in the hardware resources wherever they reside. And this will allow clients to apply their own policies to monitor and compose and reconfigure their provisioned resources according to current workload needs. And this allows hardware vendors to keep their management intelligence proprietary if they will and maintain their own value added and market advantage. And in the next few slides, I'll go into a very high level architecture overview of the OFMF before we go into a couple of usage models.

 So the OFMF acts as a central repository for configuration and management and provisioning of hardware resources through extending Redfish and Swordfish as needed to model fabrics and available hardware resources. The OFMF represents an aggregate model of all the fabrics it manages and all the hardware  resources on all those fabrics that it has been informed of. The focus of the OFMF service is to model Redfish fabric objects and the resources they represent. And this includes abstraction of detailed fabric and hardware components into a set of common resources that are available to clients. It also includes control of abstracted resources, which then may be translated into control of detail actual components. And it includes maintaining the integrity and consistency of the aggregate Redfish model of all of the fabric resources. And it presents an integrated comprehensive view of all the resources to all the clients via Redfish API calls. It enables infrastructure clients such as applications and management software and programming frameworks to convert their user requests into the desired infrastructure by managing composition and configuring fabrics and underlying resources. So client queries and requests will go to the OFMF instead of the actual hardware. The OFMF will then gather whatever information it needs and communicate with the actual hardware manager components through technology specific agents to configure fabric and hardware resources, maintain access control, monitor performance, and provide updates on the state of the infrastructure. And this approach uses this proven trusted methodologies and simplifies integration with pre-existing manager systems that already use the Redfish structure. And it also enables those that do not currently have a Redfish RESTful interface to build lightweight agents to translate between Redfish and their own management command. We're not leaving people behind here. And that really brings me to the technology specific agents. 

 These are designed to translate Redfish operations from the OFMF into technology and vendor specific controls. And they enable the OFMF to communicate to network fabrics and hardware and manage them for composition managers. Agents thus speak Redfish to the OFMF and they speak hardware specific protocols to the actual hardware manager they're responsible for, their translators. They translate between the OFMF's Redfish URI namespaces and the hardware specific component and resource namespaces. For instance, here's an example, with a fabric manager. Clients would use a Redfish ID and the fabric manager would use a fabric manager ID. And the agent is the only entity that knows both namespaces. Now note this picture, it's a vastly simplified diagram. There might not just be a single manager for a given hardware component. Even a modest sized fabric might have multiple fabric managers instances running, each in control of their specific components. And each manager might converse with the same agent or might have its own agent. And since each agent instance is responsible for aggregating the resources from its managers as servicing and tracking the associated states and managing associated resources, it will do the translation. So now I want to go very briefly into the hardware managers before moving on to the agents that serve as the customers for the OFMF. 

So for the purpose of the OFMF architecture, hardware managers are those entities with physical access to the control space, the administration space of the hardware resources and the authority to modify those settings independent of the upstream, the OFMF for clients. They're responsible for performing fabric crawls, taking inventory of their hardware resources, the initial configuration of the resources and the actual hands-on technology specific handling of requests from the OFMF. The OFMF just exists to abstract the fabric specific administration and configuration mechanisms so clients only have to know the domain such as memory or storage and not the actual language, if you will, of the specific technology or implementation of the hardware. 

 And that brings us to the customers of the OFMF on the left side of the diagram. This is the application and administrative domains as well as the composability layer. And these are all clients of the OFMF framework. This includes workloads and applications which require hardware resources to be configured for their needs and provision to them as needed. It includes system administration tools and GUIs for composable system composition and configuration and system updates. And it includes the composability layer which includes application libraries such as libfabric or OpenFAM or OpenSHMEM and resource managers such as fabric attached memory and storage pool managers and orchestration managers. The composability layer here oversees composing of hardware resources according to client requirements and applying specific policies and updating subscribed clients with events. It can really be seen as a collection of resource managers, if you will, of policy stores and monitoring elements that track the current state of the entire system and coordinate available resources. So with the OFMF, every client calls a common fabric service to manipulate the Redfish model of the fabric resources. For instance, a brief example, clients could query the OFMF service using HTTP get to determine what resources are available. And when different clients wish to communicate with each other about fabric resources, for instance, if a memory pool manager wants to inform an orchestration manager about the location of 20 terabytes of fabric attached memory that might be needed by a workload, they can pass details about Redfish objects back and forth. And when administration tools need to make configuration changes to the computational resources or fabrics, then clients can manipulate the Redfish fabric models through HTTP post or put or patch or delete to make modifications to the Redfish fabric model to indicate the desired configuration. Now composable infrastructure works really well in HPC clusters and multi-tenant environments and with mixed workloads with shared resource pools. And so I thought we'd go into just a couple of usage models, which I think would benefit from tools such as the OFMF.

So composable storage, it enables workloads to get access to the storage capability and RAS characteristics that they need wherever they exist in the environment. But unless you over provision your servers with local storage for use as a ephemeral workload scratch space, workloads will use the remote storage as workload scratch space while computing the results they're asked for. But one side effect of all of this is that this scratch space data is unneeded after the workload completion. You've done your work, you've used your scratch space, and now you've got your final results that you send. And this might clutter the media that results in inefficient use of total available storage space. So there's this idea of an ephemeral on-demand file system that creates a scratch space for users when the job is allocated. And composable on-demand node local ephemeral file systems such as through tools of BeeGFS on demand, they serve as the local scratch space when a job allocation is created and they're provisioned to the same node as the workload to be used while the workload is generating the data. And the image on the right shows how the OFMF might compose a file system out of fabric attached memory or NVMe or fabric storage for file systems such as BeeGFS to use. And these ephemeral file systems might be used as birth buffers or temporary storage while the data set is being worked through and the final results are being calculated. But when they are no longer needed, the system can then copy the resulting final data set to a parallel shared file system such as Lustre or BeeGFS or GPFS and then delete the ephemeral file system which deletes the no longer needed intermediate data. There's one usage model. I want to go through one more.

Yes, here it is. So we're currently focused on adding support for memory fabrics to the traditional network and storage fabric models. For instance, CXL 3.0, it's introducing the notion of memory-based fabrics and it builds on PCI 6 standards to enable true system composability over full fabrics with multiple switching layers and supporting memory disaggregation. And the CXL 3.0 fabric architecture brings a multi-vendor, multi-tenant, highly scalable, memory semantic composable fabric to the data center in cloud installation servers. And what this does is it lets us dynamically allocate near local memory from the CXL memory pools to mitigate issues such as out of memory conditions and IO paid swap thrashing as well as improve network failover and security and multi-tenancy capabilities and user environment portability which is big these days. It enables us to build composable systems with larger and faster memory solutions with the ability to access vast datasets without having to transfer data from one point to another. And resource managers such as Slurm or LSF or Fuzzball or Flux which allocate resources to workloads, they can be clients to the OFMF which keeps them apprised of the resources that are available and the characteristics of each resource. And this enables these resource managers to dynamically add memory when potential out of memory conditions occur, mitigate thrashing through dynamic addition of near local memory and add other resources as needed. And this example on the screen, it really depicts a memory fabric specific resource configuration allocation. In the example here, the container engine will indicate the need for memory for its containers, the fabric attached memory manager then calls the OFMF, it will allocate the requested logical memory region and then provision it to the workload application. And so I thought, Richelle, could you take the rest of it for the NVMe over fabric stuff? 

Yeah, thank you. Thank you, Phil. So one of the things that Phil noted here was talking about the memory attached fabric configurations here. And one additional note here is for anyone who's been working with storage attached fabric or even dealing with systems and fabric configuration in the past, one of the interesting things about the memory fabrics is they have a much more tightly coupled configuration than other types of devices do. And so I just wanted to highlight that before I talk a little bit more about the split here and what we're doing. And so that's actually brought some interesting challenges into this configuration and into the work here. So they have tightly coupled in their implementations both the resource allocation and the fabric configurations. So what we see here is those fabric agents doing both tightly coupled fabric allocation, fabric management for the memory fabrics as well as the resource allocation. What you'll see there, and why I'm highlighting that, is that with the other types of resources, you then see the much more traditional split. So you'll see the fabric being much more distinct from the resource allocation. And that's why you see this picture drawn this way. So CXL and Gen Z and other things that fall into dealing much more with the memory fabrics have a much more tightly coupled resource to fabric allocation model than things like Fiber Channel or Ethernet or InfiniBand. They have a much more distinct split between managing the fabric and managing the resource. So the Swordfish portion of this picture is dealing with that much more distinct split. What you see over there and what you see in NVMe over fabrics in general is that much more distinct split. So you see there those NVMe devices, you're dealing with primarily the resource allocation and then you will separately have the fabric allocation. So when you're seeing over there, what you will do is you'll go to allocate the resource independently from the fabric. So the fabric allocation would be primarily done with the OFMF and you would separately deal with the resource allocation by talking directly to the resource and talking directly to the Swordfish elements rather than talking directly to the fabrics. So the pictures I was showing earlier where you talked to the Redfish/systems and then the Swordfish/storage and then I showed you that /fabrics. Think of those as the three discrete pieces I'm talking about here. So you would have basically a switch that had the /fabrics, the storage device that has the /storage and the system that has the /systems. Those could be three discrete devices in your environment that each had an embedded interface that would match. And the OFMF then is going to talk discreetly to each of those and each of those could then have a fabric agent and a management agent and separate from a storage agent, separate from a system agent, keeping to the vernacular you're seeing on the screen here, that would all be Redfish or Swordfish as Phil has noted here, like that red side effectively. They'd all have the red side compatible for the OFMF to talk to as an agent. And it's just a slightly different model here but basically conforming exactly to this. So the OFMF can then talk to any of those kinds of models here. And I'm sorry I can't point to this on this screen, there's just not really a good tool in BrightTALK for me to be able to point to the pieces because I'm pointing to them on my screen and you just can't see it. So but that's really how that split happens for kind of those technologies that have that more distinct split between the resource and the fabric as opposed to the memory fabrics which have brought those two more tightly coupled. 

Okay, so that's kind of what's happening in those technologies right now. So where we are today with the OFMF, as we mentioned a little bit, we've alluded to there's  a lot of different folks working on this in the OFA and partnering together. We have the, we actually have multi-organization alliance agreements in place between the OFA, SNIA, DMTF and CXL. We have three and four-way alliance agreements with these organizations working together to ensure that we're not overlapping on our work. We have multiple of the national labs together. We have Lawrence Livermore, we have Oak Ridge and we have a lot of companies engaged here. So both large companies as well as startups that are working on these smaller technologies and these new technologies working together to try and bring together these reference implementations building these agents and really looking at how we can build together this open source reference implementation. We'd love to have folks get involved. We have really have some big spots that we're looking for folks to really kind of help join us. Some of the specific things we're really still looking for here are some inputs from some of these fabric technology providers, looking at best methods and rules and ways to help us take advantage of some of the elements here of these composable computing systems. And also on the client side, we really are looking for some folks to help us on that HPC and enterprise app. Bring your knowledge in on that client side, help us look at the requirements here on the client applications. Phil talked a lot about what those client apps, what those interfaces are, come help us look and identify some of those high priority use cases. And really also looking at as we move forward in CXL 3.0, which brings in full switching, looking at how we help take advantage of this CXL 3.0 CPU designs, looking at the memory resources and the accelerators and really help flush out some of those additional use cases there. And the best way to do that is come on in, join the meetings. You don't have to actually join the OFA to participate. All the meetings are open. But everyone is welcome to also come join the OFA and participate there. Looking here, there's a ton of places to help find more info.

 We have pointers here for finding information on Swordfish, Redfish, and the Open Fabrics Management Framework. To find all things Swordfish, go to smia.org/Swordfish. To find all things Redfish, go to dmtf.org/Redfish. And on the Open Fabrics Management Framework, you can find things at openfabrics.org/working-groups. We appreciate your time today. We'll switch over here and look and see if we can't pick up and just take advantage of a few questions. And I'll just remind folks, you can go ahead and submit questions via that Ask a Question link at the bottom of your screen. All right. A couple of questions have come in, and we just have a few minutes left to take a look at those. So first one, Bill, here's the first question here is, how do the agents work? Does the OFA supply them? Well the short answer is the OFA is coming up with a set of fabric agents for a few of the fabric and hardware pieces of technology, CXL being and Gen Z being definitely examples. What we plan to do is have some examples of these agents, and we will let vendors or new technology providers come up, use those sort of as templates to determine, all right, this is how we would talk to Redfish and these are how we would plug in our vendor or technology specific management or composition commands. Okay next question, you've talked a lot about different, a couple of the different technologies, what all network technologies are you supporting? Well let's see, Richelle, you probably do this more than I do. 

 All right, yeah, so thank you. We have talked a lot, we've covered a lot of the different potential technologies. We have, and let me just pop back to this list a little bit here, we have a bunch of different companies involved, and you can see we have folks that reflect a lot of different, a lot of varied interests. So we have like these folks over here, and these startups, these guys are working on, you know, came in and were working on Gen Z and CXL and CXL consortium, so there's a lot of interest and a lot of folks that are putting forth effort towards the memory fabrics. So there really is a lot of direct interest here in the memory fabric space. That being said, you know, we have the Swordfish interest and folks are, we're really looking at NVMe over fabric support and ensuring that we have the coverage for that. NVMe over fabrics is not NVMe over CXL or anything at that point. We're really looking at how do we enable NVMe over, you know, NVMe over TCP, NVMe over RDMA at this point, and the folks, a lot of the folks coming in from the national labs are really looking at how do we enable the stuff, the technologies that they're looking at right now and they are very much InfiniBand based. They're looking at the RDMA capabilities as well. So they're bringing in a lot of concern about how do we enable the RDMA environments that they're very much concerned with. So those are really our high priority items. As we scanned over a couple of the slides had a bunch of other technologies listed. We want to enable agent plugins and capabilities for other technologies. We really need folks to come in if there's an area of interest for folks. We'd encourage you to join. But right now these are really the areas of focus for the folks that are engaged on a day-to-day basis.

All right. Next question is, is there a performance impact to using the OFMF? So let me take this one and you can chime in as well. So in the fast path data the answer is no. The OFMF it basically enables configuration, enables provisioning, enables information about what's actually available in the state of what's available in the slow path management path. So from that point of view from a fast path data path, no, there's no performance impact. We get out of the way. From a manageability point of view, well, it's an open ecosystem enabling project. The OFMF is a reference design and it's not polished. And sure, from a manageability point of view you might see some performance impact on your management interface. How much that is and how long that lasts, that depends on how many people we get to help us. Okay. Thank you. Yeah, so yeah, the, you know, there's a lot of potential for folks to look at how to take advantage of the OFMF. And it's a reference implementation which means that folks can use it, you know, in many different ways as an open source tool. We have, you know, one more question that's come in and then we'll have to look at how we're doing on time here in a second here. So I'd encourage if any folks have other questions to please pop those in chat for us here. 

One other question, I've just learned that one of the vendors has, there's a new array that's been released with NVMe over fiber channel support. What about NVMe over fiber channel? So yeah, that's another area of definite interest, right? Fiber channel is not dead. There is absolutely a ton of interest from the existing vendors and from customers in preserving their investment in maintaining their fiber channel infrastructure. And NVMe over fiber channel is definitely an absolutely critical investment area. There's a lot of people that have a huge investment in fiber channel. So NVMe over fiber channel, and Phil can talk about this from the NVMe Express perspective as well, NVMe over fiber channel is an absolutely definite area of interest. This is an area that from the standardization perspective, there's some work that needs to be done in Swordfish to flush out those models. That's what we talked about earlier, that partnership that exists is we have the base models that exist. We need those vendors to come in that do the NVMe over fiber channel, get those models in place and get the base instrumentation in and then the support will be there in the OFMF to support those products. So that's the gap. We need the products that are building those NVMe over fiber channel devices. We need their customers to ask and say we want those standards-based interfaces because we want to see the support in these open source ecosystems. And the only thing I would echo to that is we need vendors, we need technology suppliers to come to us and say these are the knobs that we need you to be able to interface with from a Redfish point of view. And we also just on the other side from the clients, we need to find out what these composability managers are actually looking for, maybe we missed something, in order for us to be able to make that part of the communication with the Open Fabrics Manager framework. All right. Well, thank you everyone. We have hit the 45 minute mark here and getting the nod that we need to wrap it up for today. If you have any additional questions for us, please feel free to send them to us offline. You can reach us at storagemanagement@snia.org. Slides again are available at SNIA, well, if I download directly from the BrightTALK if you want, they're also available at snia.org, Ed Library. And if you have any other requests for future topics, you can also reach us at storagemanagement@snia.org. Thank you very much. 
