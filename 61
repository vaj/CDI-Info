YouTube:https://www.youtube.com/watch?v=S8Rb6-oih1c
Text:
Hi, my name is Jon Jiang and I'm from MemVerge. Today we'll discuss how to optimize bioinformatics in the cloud. We will talk about some of the challenges that we've heard from users and we'll introduce MM cloud, which is a product from MemVerge. In particular, we'll talk about WaveRider, which is the feature that allows us to dynamically optimize cloud resources for bioinformatics workloads.

HPC is going to the cloud. In fact, cloud HPC is the fastest growing segment in the worldwide HPC market.

And there's a good reason for it because cloud offers agility. There's a huge amount of resources at your fingertips and it's supposed to be low cost because it works like a utility. You only pay for what you need or what you use. That's the theory or the promise. But if you asked your average bioinformatician, it's not always fun and games, is it? The reality is that there are still lots of barriers. We've talked to bioinformaticians who need to work with another organization to get access to the cloud so that it takes days and sometimes weeks to get what they need in the cloud. And the cost of cloud resources are often very expensive. In fact, if it's not used very efficiently, cloud can be much more expensive than on-prem data centers. And there's a huge amount of effort required to really optimize the cloud operations.

So let's get in a little bit deeper to talk about some of the specific challenges. Out of memory error. That is a common occurrence in bioinformatics. That happens when the cloud VM that you allocate doesn't have enough resources or doesn't have enough memory to finish the workload. You get out of memory error and you have to start all over again. So you've wasted time and also the cost to run the workload to that point. Even if you don't encounter out of memory, we find that most workloads have peaks and valleys in their utilization, which means that if you over-provision your VM resources for your workload, then you are wasting resources most of the time because the peaks are often very femoral. If you underutilize, obviously you can run into OEM errors and then you have to start over again. Another complexity is many of us use workflow managers such as Nextflow or Cromwell to run large scale workloads. And within a single workflow, there could be hundreds or thousands of jobs, each using a different amount of resources. It's virtually impossible to know what resources are needed for each of these jobs a priori. And that exacerbates the resource right sizing problem. But sometimes simpler problems occur. Like you run a job and it runs into the middle of the night and then the machine stays idle until you get to the office or wake up in the morning and check on the server again. For the period of time that the VM is sitting idle, you're paying for it. So there's a huge amount of cloud waste, so to speak, because of these simple operational inefficiencies. And then finally, a lot of users try to use spot instances to reduce the cost of cloud instances. Spot instances are often upwards of 90% of the on-demand instances prices. And the reason that it's so low cost is that it's fulfilled using excess capacity by the cloud provider. And the cloud provider has the right to reclaim these resources with minimal notice. So for workloads that cannot handle that type of fluctuation, and I want to say that's most workloads in the bioinformatics space, spot instances simply can't help. So all of these are quite complex and really outside biology or bioinformatics. So the request or the frustration that we often hear from our users is that, "Can we please just focus on the science and not deal with all these cloud operational complexities?

And that's why we designed Memory Machine Cloud to automate all of that and allow the users to focus on their science and their expertise and not worry about the operational aspect of cloud.

So Memory Machine Cloud has a simple architecture. You deploy a small machine and you would run our ops center on that machine. That's the brains behind the whole system and the central point of interaction for the user. So through the ops center, you can submit your jobs and you can manage the jobs during its execution. Ops center, once it receives a job submission, it will interact with the cloud and launch worker nodes, which will actually execute your job. Everything is automated after the job submission and essentially you just wait for the job to finish and collect the results. And Memory Machine Cloud has some very cool features that does the optimization in an automated fashion.

So at the heart of Memory Machine Cloud, we have a technology called AppCapsule. And AppCapsule is the ability to snapshot all the state information that's needed to run an application. And the state information includes all the data in memory, the CPU registers, the socket information, as well as the storage information. And we collect all these data and we put them into a capsule of sorts. And then we can store that capsule on a persistent media like S3 bucket. And then at a later point in time, you can take the AppCapsule and you can release it onto a new machine and then the execution of the application in the capsule can continue as if nothing happened. So that's AppCapsule. And with AppCapsule, we implemented a couple of very cool features, SpotSurfer and WaveRider. So SpotSurfer is the ability to run your workload on spot instances. And in the case of a spot instance reclaimed by the cloud provider, we simply take an AppCapsule of the app, we find a new spot instance, and then we then deploy the AppCapsule into the new spot instance and the workload would continue on the new spot instance. That means your workload can now take advantage of the hugely discounted spot instance prices without having to do any additional work. And WaveRider uses a similar concept except it addresses a different problem. So WaveRider will look at the resource utilization of your application and determine if the current VM is appropriate for it. If the application is about to exceed the available resources, WaveRider will move it to a larger instance. On the other hand, if your application is not consuming a significant amount of the resources available on the box, then WaveRider will move your app to a smaller instance so that you're not wasting resources.

So here's a graphical representation of WaveRider and how it dynamically right sizes the VM based on the application utilization. Here we used a load gen that monotonically increases the use of memory. And as you can see, after the first period, WaveRider detects that the machine is about to run out of memory and it migrates the app to a second machine which has double the amount of memory. And then after a while, again, it finds that there's not enough memory and then moves it again and again until the final machine reaches the final machine can handle the peak workload of the peak need of the workload.

Now I will talk about two real-life examples of how WaveRider helped our users. We worked with a user who specializes in metagenomics. So MegaHint is a popular assembly tool in metagenomics. And on the left, you see the CPU profile of the workload and you see that it's pretty constant. On the below the CPU graph, you see the memory graph, but you see the memory graph is actually bursting. So in the beginning of the execution, the program uses a lot of memory and then the memory really goes down and it goes up a little bit more during the middle of the execution and then it stays pretty even afterwards. So in our baseline, we ran with on-demand instances and without WaveRider and the cost was $9.10. Now with WaveRider turned on, still using on-demand instances, we saw a 16.9% savings. And what happened was WaveRider detected that after a while, the memory utilization has really gone down. So it moved the application to a machine with less memory, with half the memory in fact, and it covered this period and then it detected another peak or another burst in memory usage and moved the application to a slightly larger VM and then the application finished with that third instance. So just by doing that, we're saving 17%. Now we did the same thing and put it on spot instances and all of a sudden our cost went down to $2.26 and that's a 75% savings on the original baseline. So we've got great enthusiastic response from our users.

But the second example is one where I'll show that we actually have more work to do. So the second example is using MetaSPAdes. And MetaSPAdes program, the way it's written, as we've discovered, has some incompatibilities, if you will, with WaveRider. So the baseline, we ran the whole program and the cost was $24. With spot, we can reduce the cost to $7.94. That's 67% savings. However, if we turned on WaveRider, what happened was WaveRider mostly looked at CPU utilization this time because that's the part that's more bursty. And it migrated three times, so using a total of four VMs to complete the whole workload. And as it turns out, the cost was actually a little bit more. And the reason is that this program was written in a way that it's unable to take advantage of more resources when it became available during the execution. So if you look at number two over here, when the CPU was pegged, WaveRider moved the app to a larger machine, but it failed to make the execution faster. So even though the program had more CPU available, it wasn't able to take advantage of it. So that's something that we're working on to fix, to make sure that applications can take advantage of more resources during the execution. And I do want to say that this is assuming that no changes are made to the original MetaSPAdes code. So one answer obviously is to update the MetaSPAdes code to specifically take advantage of the MM Cloud or WaveRider feature. But we want to also try to be able to do it without the program's knowledge so that we have universal applicability.

And the last example I will show is one where the customer runs a very large workload using Nextflow. In this particular case, the pipeline requires something close to 10,000 jobs. And they don't have the budget to run these 10,000 jobs with on demand, so they had to run on spot. But as it turns out, there isn't enough spot instances in the region that they were in. And that's why they found that out of the 11,000 jobs that were launched, about 8,900 failed and had to be restarted. And the pipeline ran for 17 hours and finally Nextflow gave up. So with our MM Cloud, they were able to run the Nextflow pipeline as is and complete the pipeline in less than five hours. And the reason is that we handled the spot reclaims gracefully so that those jobs didn't have to be restarted. And so the customer was extremely happy with the result. You still see 131 failures, and we looked into that, and those were due to OOM errors. And that was because the customer didn't turn on Wave Rider. So if he had turned on Wave Rider, Wave Rider would have detected these out of memory conditions and moved the workload to a larger instance.

So in summary, WaveRider is an innovative approach to help bioinformaticians and data scientists to run their workload in the cloud. And we can migrate application from one VM to another without losing any progress. And this makes it possible for users to run the workload on spot instances, and this makes it possible for the users to have right-sized VMs throughout the execution of each of their workloads. And we are just at the beginning of building this product. We have lots of plans for WaveRider going forward. So we talked about expansion of application compatibility, and we will also introduce AI/ML type of logic into when and where the WaveRider migration takes place. And we also have plans to support GPU applications as well as multi-node applications. Thank you very much. And I hope that this was helpful. And if you want to try the software for free, please come to memverge.com and you can get a free copy. Thank you very much.

