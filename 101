
So I'm Chris Blackburn, a technologist on our system architecture team at TE Connectivity.

And I want to talk a little bit about Card Edge connectivity. You're in this CXL forum, so when we think about the modules, the cards, the devices, a lot of these require Card Edge connectivity. And this space has really been changing over the last couple of years. So we're all familiar with the PCIe add-in card. It's a decades-old connector and card form factor. And traditionally, that's been put into riser card architecture. So in a 1U server, you might have to put that card horizontally. In a 2U, 3U server, you might put that card vertically, like I show here in this system. But these channels are long. There's a lot of different boards and pieces. And the serviceability is not so great. You have to pull out the tray to service these. So we've seen a trend towards front faceplate pluggable devices. There's on the show floor and throughout many of these presentations, you'll see EDSFF modules, whether that's the E1S or the E3, and then also OCP NIC, DCSCM, and even some new front faceplate pluggable modules that I'll talk about here in a little bit. And some of the benefits of these-- you have a shortened electrical channel. So you now move that interconnect to the rear of the card rather than the side of the card. That can shorten the traces on the card and also in the system. The hot swap and serviceability, that's a big, big deal when we think about hyperscale. So being able to service all these devices at scale in the millions. And one of the interesting things, and especially with CXL, we've seen these devices and the use cases shift over time. So PCIe add-in cards have been this ubiquitous device. It could be an accelerator. It could be a NIC card, storage. And we've seen some customization to these new devices specific for their intent. So OCP NIC, it's typically going to be a network interface card. EDSFF, I think, is a good example. It started as an SSD. And we're seeing some use cases with DRAM and some other options there.

So to get into this a little bit, and I'm going to take us through some details and differentiation between these devices and the connectivity trade-offs. So the one new one to mention here, the SFF-TA-1034, or the PMM, this is an interesting use case. And we're going to talk about this a little bit. Because when you take the differences between these devices, you look at the size, what components you can fit on it, how do they fit into the system architectures and the servers. This is sort of the best of all of these connectors combined into one, or all of these devices combined into one.

So let's take a look at a comparison table here. So you'll see PCIe add-in cards. There's a variety of different lengths and heights, different slot counts. The EDSFF, whether it's an E1S, an E3, an E1L, OCP NIC. And then at the bottom, I have the 1034 card. And one of the biggest differences that you can see is the size. So if you look at the xy area in square millimeters here, these are really what limit your use cases. If you have a bunch of DRAM, maybe that fits in a certain xy area. When you think about accelerators and all the components, those are going to drive not only larger space, but then also additional power. So with a traditional PCIe add-in card capable of 75 watts and then some additional auxiliary power, they're getting somewhat limited with the trends that we see in power for some of these applications. So supporting up into a few hundred watts, up to 600 watts, is going to be a common requirement for many of these devices. The other difference you'll see is the number of high-speed lanes, whether it's a x4, x8, x16. Obviously, this appetite for bandwidth, we're seeing a trend and a shift and a lot of requests for x32. Now, there is one common thing with all these devices that you'll see, and we'll step through that. Everything except the PCIe add-in card uses a single connector interface, and that's TE Sliver product. It's defined in a variety of standards and has evolved over the last decade. And there's a lot of reasons why, so we're going to touch on some of those reasons. Now, I'm going to come back to the PCIe add-in card, because although I said it is the only device here that doesn't use that connector, we're starting to see a shift in that direction, too.

So quick overview of the timeline of this Sliver connector. So TE developed it in 2014, 2015, and it's had a bit of a journey. The first industry standard adoption was by Gen Z. Of course, Gen Z's now rolled into CXL, and it kind of took a life of its own since then. Gen Z, it got into and defined in SNIA, SFF-TA-1002, 1020, ultimately EDSFF, OCP NIC. So it's been quite a journey, and it's one of these things that once you get one or two devices standardized and it works in an application, when a new device form factor comes out, it's a really easy and simple decision to continue with a similar connector.

Now, it's not the only connector option, but one of the things that when we look at the options in the market, all of the connector vendors have all of these solutions. But the thing that's different is some of these are really geared towards cable applications or board to board applications. And the TA-1002 connector is really the ubiquitous connector.

There's a lot of configurability, a lot of options, and that's really why these devices have chosen this particular form factor. So if you look at the top graphic, it just shows some of the variety. There's a 28 position and a 56 position lead frame set. Those can be combined in any combination, and that's where we get these pair counts, 56, 84, 140, 168. And then when you look at the connector orientation, whether you want to be mid-board and go vertical, right angle and go out horizontal, straddle mount so you can be coplanar with the board, there's a lot of options on the connector configuration and then also on the cable exit configuration. So what you end up getting is this mix and match optionality of all of the different sizes and exits.

Now, another reasonâ€” and this is a big deal. This is why the devices are specifying this connector on the card edge. When you look at the angular gatherability and the linear gatherability, or you have a front faceplate pluggable device and you're trying to plug in from the front of the server, it's a guided but blind mate application. So you will have some angular mate. You will have some offset in the XY. And this interface is the most forgiving in that regard. If you take a look at the angular gatherability and you compare that to some of these other interfaces, having the ability to accept those cards and devices makes a robust and reliable interface compared to some of the other alternative options and legacy options.

And how do we make that possible? How do we get the linear gatherability, the angular gatherability? There's really two key differences. So if you look on the right, you'll see MCIO and MXIO. This is the connector that's defined in DCMHS. And of course, MCIO is defined in TA-1016. So if you look at the distance from the last contact to the wall and also the location of the contacts within the connector body, you'll notice that it's very small. It's tight. And that's great for density. But when we think about plugging devices into these connectors, having the contact lead frame set recessed in the housing and also enabling a greater distance from that wall, that's what gives us the robustness and the ability to support that larger angular and linear gatherability.

I want to touch on contact wipe. This is a common theme and discussion that we have. Some of these devices are biased or cammed. Some of these devices have a range that they will demate and unmate. So you have to have contact wipe. So this is the amount of distance that the contact travels on the pad. Now, this is a little bit of a conundrum because you want wipe and the tolerance range of these devices. But at the same time, introducing wipe will degrade SI performance. So we're in a situation now as we get into PCIe Gen 6, Gen 7, that reducing that wipe but also having the mechanical tolerance is critical. So you're left with what's called contact stub. It's the amount of pad that's exposed behind the contact beam point. And that can have big effects on the impedance, insertion loss, and overall performance. Looks like the battery's close to dying here, but we're getting close.

So I said I would come back to the PCIe add-in card. We've had two hyperscalers now move away from a CEM connector on their add-in card, on their SmartNICs, and utilize the TA1002 interface. There's really two main reasons that this has happened. One is the condensed pitch. When you look at a CEM connector on a 1 millimeter pitch, and you now need a x32 and higher power, you need a lot of contact pins on these card edges. So the power and the pitch, easy move away from CEM. You already have this robust ecosystem with sliver. But the other big thing that's happened here is the layer count of these cards has gone up. So they're not your traditional old NICs. They're SmartNICs. There's a lot of components and circuitry on these cards. And that really drives a higher layer count. And we need to support thicker cards. So this connector ecosystem has evolved to accommodate those thicker PCBs.

Another trend that I wanted to talk about is cabling directly to these devices. So when we think about E3 or E1L or OCP NIC, some of these device form factors that are plugging into the front, oftentimes we see the straddle mount connectors that are coplanar to the board. But one of the trends and shifts we see is really moving to panel mount connectors. So you're cabling directly from the device card edge to a connector near the chip or close to your chip. This is certainly an interesting application for TE. There's a lot of cables, a lot of opportunity here. But also when we think about the other themes within OCP, like DCMHS and some of the modular server architectures, this really gives you that building block approach where I can now pick and choose my devices. And I can build a compute memory complex and have a variety of different SKUs from those same devices and motherboards. So if I want a server that has a bunch of flash, I can use all those PCIe lanes to connect to a bunch of EDSFF drives. Perhaps I have one that uses several accelerator cards. So you get this mix and match capability.

All right, so I want to leave you with a few things. There's a lot of acronyms and a lot of numbers with these specs. So I encourage you to go out, check out the SNIA website, familiarize yourself with the SFF specifications. Also in OCP, there's a variety of work streams that are happening that specify these connectors in some fashion. So we have some links there. And if you have any connectivity questions, reach out to TE, visit our website, get in touch, and we'll be happy to answer any questions. So appreciate the time. Thanks for staying for the last presentation of the show.

All right, everybody. Anybody that's left, you're a CXL warrior. Thank you for attending today. We'll be at SC23 with another CXL forum. Oh, you have a question. Go ahead, CMA.

Hey, CMA. Hello. Thank you very much for the presentation. A comment, perhaps, for us to consider. Density will still be important. Cartridge is wonderful, and your sliver product family is wonderful. One observation, two ends of a cable need not be of the same connector family.

Sure.

If HPM from DCMHS, that's OCP spec, calls for a particular connector, the other end of it could be anything else for a number of reasons that you mentioned.

Yeah.

That's one generic observation. Another one is, of course, direct attached things are what we do today. And for that, maybe a x16 connector is good enough. But you've seen a lot of wonderful pictures that talk about many things, and memory pooling, and other things that CXL is enabling. For that, we need a little bit better density. Cartridge might still be good, but we might need to have multiple layers of it or such. A card like PMM might be appropriate today for one direct attach. But if it wants to support memory pooling, it needs more serdes.

Yeah. So a comment on that. So you talk about the linear space of a cartridge connector. The way to get more density is go to an array. So things like backplane, absolutely on the table as we get higher pair counts and more power, all those things.

Two layers of cartridge.

Yep.

As long as it is feasible, robust, and people will be happy with it.

Yep. Yeah. And the other thing here, maybe connectorizing the cartridge. So instead of going to a goldfinger edge, putting some sort of connector on the device itself. So you pay the penalty on the device, but like you're saying, use the z-height to get up.

Right. And we also shared with the team this morning and yesterday, we need a copper backbone. We need a number of these things to be linked together out of one assembly. So I keep this one, I say this one as an open, so challenge to many people. Whoever can provide a many-to-many interconnect using a copper backbone, then we can enable one card assembly or one part assembly supporting memory pooling and sharing.

I'll tell you what, maybe I'll talk about that next year.

Yes.

This is Global Summit, what we do. This is engineering to engineering talk. Every time we challenge each other, we don't wait a full year. We wait only six months. So at regional summits, please present something. And then that might be a concept. And then we will make it happen a year from today.

Sure. Will you buy my ticket to Portugal?

We'll go together.

OK.

All right, everybody, thanks.
