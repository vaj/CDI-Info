
I appreciate the opportunity to present to your audience today, and I thank the Dell team for giving us a chance to share a little bit about Liqid and what we do.

I'm Sumit Puri, and I'm the co-founder here at Liqid. As Cami mentioned, today we're going to talk about the economics of deploying GPUs and AI infrastructure. We'll start with a little bit of an overview of what we are seeing as far as general market trends. Then, we'll dive a little bit into composability, the physical layer, and how we do what we do. We'll also talk through some use cases on how we are deploying GPUs in different environments. Finally, we'd love to take questions and answer any that the audience might have.

Very quickly on Liqid: we are a startup based in Denver. We are focused on delivering disaggregation to the data center. Our primary focus is on the pooling, sharing, and disaggregation of GPUs. That's what we'll talk about today.

Just really quick before we start, I think many of us have seen slides that look like this, where we see this massive growth in the AI market and the AI market opportunity. I think it's important for us to dive a little bit deeper into what we are seeing here.

So, behind this explosive growth, where the market will grow from roughly $300 billion as an overall AI opportunity to almost $2 trillion by the end of this decade, it's primarily driven by the massive surge in AI servers that are being shipped. We're going to grow from less than a million servers in 2022 to almost 3 million AI servers shipped by 2027. The amount of GPUs that we are going to ship is tremendous. We're going to grow from 2.8 million GPUs to over 13 million GPUs in just a five-year window. And the amount of energy that we are going to consume is going to be massive: we're going to more than double the energy consumption of data centers by the end of the decade.

The thing that people are not talking about is the waste: the amount of idle time that these systems are sitting unused; the amount of oversized server and infrastructure that's being deployed; and the amount of waste and non-production environments that are out there. This leads to a tremendous amount of waste, not only in dollars but also in energy. And energy, as we'll talk about today, is going to be the limiting factor around deploying AI.

Our assertion is: there's a better way to do this. Our assertion is that, through disaggregation and the power of composability, we can really attack that utilization problem and significantly improve the economics of deploying these GPUs and accelerators into the data center. We'll also discuss how we accomplish that.

One other major trend that we are seeing, and that we are talking about, is the shift from the workload. Today, the majority of the dollars being spent in AI workflows are being spent on training. More than 90% of the infrastructure today is being deployed in a training environment. Dell, others, and us—we believe that by the end of the decade, there's going to be a massive shift. We are going to shift away from training these large foundational models to running inference—to taking these foundational models and putting them into production. Our belief is that, by the end of this decade, 90% of the infrastructure deployed for AI is going to be focused on things like inference. In our opinion, this shift will require a new level of efficiency in the infrastructure we are deploying. The way Liqid is thinking about efficiency in AI infrastructure is by maximizing the highest level of tokens we can generate per watt or the highest level of tokens we can generate per dollar. I think if we had unlimited power, unlimited energy, and unlimited dollars, there would be easier ways to solve this. However, we are all limited by those metrics, and that’s the lens through which we approach this. Our belief is that things like training are fundamental cost centers, while things like inference are fundamental profit centers. As AI makes its way into production, it will be inference workloads that generate a large portion of the dollars related to the AI market. With the cost of a single training model rising to $2 billion or even $10 billion, we don’t think most enterprises will focus on building foundational models. Instead, we think most organizations will take these foundational models, tune them with their data, and run tasks like RAG (Retrieval-Augmented Generation) and inference based on these pre-existing models. We also believe many of these environments will be limited by power. They’re going to operate at the edge or on-premises, making concepts like tokens per watt and tokens per dollar incredibly important. Finally, data is the fundamental heart of all things AI. With 83% of data not in the cloud, we believe one of two things must happen: either the data moves to the cloud so GPUs can process it, or GPUs will inevitably find their way on-premises. We think the latter will take hold, especially as inference workloads become more prominent. That’s the background we wanted to share.

The fundamental problems we are seeing related to deploying in infrastructure, specifically focused on things like inference, are multifaceted. Number one is performance. So, that is one of the major issues that our customers are seeing: GPU performance in the server. A lot of it's just driven by density. The more GPUs we can put into a server, the more AI performance we can get out of that server. And we'll show you, through composability, how we can crank the performance knob up. Cost is a big deal. Not just the CapEx cost, but the operational cost, including the cost related to power. And so, we have to find ways of deploying these GPUs into the environment, but reducing the overall deployment cost. The third problem, or the third opportunity as we see it, is flexibility. And we define flexibility in one of two ways. Number one is utilization. Right? Having GPUs locked inside of servers inevitably leads to poor utilization. And so, what we want is the ability to move these GPUs around to any server that needs them. So, number one, we can fundamentally raise the utilization of these GPUs. And then the other thing we talk about is silicon diversity: enabling an entire ecosystem of accelerators, not just from one vendor, but giving our customers the choice to be able to pick the right tool for the right job.

We think composability is at the heart of solving a lot of these problems. For those who don't really know composability, I'll take just a few seconds here to explain what we mean. On the left is traditional static servers. The way we build those is by taking devices like a GPU, plugging them into the sockets of the motherboard, and defining what our physical infrastructure looks like. That physical infrastructure is fixed for the duration of the time we deploy the server. In a disaggregated environment, think about pools of hardware: pools of servers, pools of GPUs, pools of storage, and very soon, pools of memory or any device we want. We connect these pools of hardware into a switch, and then, with software, we dynamically build our server in any shape or size we need. For example, if we need to build a server with a high level of AI performance by aggregating many GPUs through the power of composability, we can create a 10-, 20-, or even 30-way server—however large the infrastructure requires. When that server no longer needs a GPU, we can dynamically reallocate that GPU to any other server in the rack. This is a high-level overview of what composability looks like.

This is the actual physical layer of how we deploy this infrastructure. On the top are standard servers. We have a wide variety of servers available on our hardware compatibility list. We have 1U servers and 2U servers from Dell. We even support things like the MX 7000 series, where we're deploying blade servers and adding GPUs to blade environments. We also support T-Series, where you can have 2U 4-node servers. To a 2U 4-node server, we can add 5, 10, 20, or 30 GPUs and do it all at the bare metal. This is the physical overview of what it looks like. These are standardized Dell servers. We put a PCIe host bus adapter inside the server, connect it to a PCIe switch, and then connect multiple expansion chassis to that switch. These boxes on the bottom are the PCIe expansion chassis. Each one is a 4U box that can hold up to 10 full-size devices. For example, it can accommodate 10 H100s if that's what we're deploying. If I want to create a server with 10 GPUs or 20 GPUs, we come in with our software, reprogram the PCIe fabric layer, carve out 10, 20, or 30 GPUs from this resource pool, and connect them to the server at the bare metal. We don’t install any drivers, hypervisors, or modules. We don’t add any code at all on this server. It's your server, and you can run whatever operating system or application you want. Our guarantee to you is that the composed server will just work, with no code changes required. We can dynamically reallocate these resources to improve utilization. We can dynamically reallocate these resources to build performance environments. We can change the configuration on the fly. By "on the fly," we mean hot-plugging or hot-removing GPUs from running servers. The other thing to note is that today, our story is very much focused on the disaggregation of GPUs. However, we also deploy other devices into these expansion chassis. We can deploy FPGAs, NVMe devices, NICs, and DPUs. We have an entire catalog of endpoints that can be deployed into this infrastructure. It all works the same way. Our software inventories it, makes it available to the user, and the user can dynamically configure the system into any shape or size they need. In a little bit, we’ll discuss the orchestration and how we actually pull the strings of the fabric.

The first thing that we kind of comment on here is: no drivers or software modules are required. And so, as we compose, we're doing this all over native bare-metal PCIe. This is plain vanilla Windows. For this one new server, we have composed 20 GPUs to it. Inside Device Manager, those devices just show up. Windows cannot tell that the GPUs are disaggregated. If we were to hot unplug those GPUs from this running machine, what you would see in Device Manager is those GPUs just disappear. We work with Windows. We work with Linux. We work with hypervisors like VMware. In a little bit, I'll show you how we can actually integrate into some of these northbound frameworks.

We often get asked about performance, right? We often get asked, "You guys are disaggregating the GPUs. There must inherently be some loss in performance." And because we're able to disaggregate over PCIe, the answer is, there's not a loss of performance. The latency across a PCIe hop is about 100 nanoseconds. So, at the application layer, you are not going to actually notice a couple of hops across a couple hundred nanoseconds to access that GPU. Many of the servers in the market today actually have PCIe switches inside the servers as a way to expand their PCIe capability. And so, what this slide is showing is: the gray bar represents the H100 directly inside the server, while the blue bar represents the composed GPU, where the H100 is living on the fabric. The takeaway here is that there is no performance loss. It's within run-to-run variation because we are able to do it over native PCIe.

The next slide is scale. And we'll show you guys a lot of data today. We have a good amount of data that we can share with you, and this shows our ability to scale from 10. Now, we can do 20, and these days, we're even scaling all the way up to 30 GPUs and getting great linear scalability out of the solution.

And so, by taking this approach of disaggregation, we think there are a lot of benefits around composability and around deploying GPUs inside the data center. So, back to the pain points that we're trying to solve. First is performance. And so, because we are no longer bound by sheet metal, we can create a server that runs as fast as we want. If we want to create a 30-way configuration, we'll show you some data on being able to aggregate a large quantity of GPUs to a single compute node. Cost. We'll show you some real-world examples here. We believe that, by taking a disaggregated approach, there are certain environments where we can cut the overall infrastructure cost by up to 50%. We can reduce the amount of power required to deploy GPUs. We can reduce the number of networking ports that are consumed. We can reduce the amount of software licensing that's consumed. We often say, if we weren't changing the economics of deploying GPUs, I don't think anybody would care, right? It's all about the economics at the end of the day. Next one is agility, right? Being able to pool and share these resources, and then being able to give these very expensive $30,000 GPUs to any server that needs them, allows us to try to drive to 100% utilization out of this very, very expensive hardware. I think there's no bigger knob that we can turn for the economics of deploying GPUs than keeping the GPUs 100% utilized. And then, back to that theory or the idea of silicon diversity: being able to give our customers the right tool for the right task at hand.

And so, we'll give you kind of a little bit of an example of performance, cost, and agility, and how we think about it. And so, this is one where we're public about it. We kind of did a use case with Meta. This one was really interesting. As Mark Zuckerberg says, "Energy, not compute, is going to be the number one bottleneck for AI progress." And if we take a look at NVIDIA's portfolio, for example, they offer two flavors of A100 in this example. They offer the SXM A100, which is phenomenally good at things like training, and then they offer the PCIe version of A100, which is actually a lower power device, half the power of the SXM version. And so, for this particular Meta use case, what they found was there was a benefit to moving to a lower power GPU, aggregating more of those lower power devices together, and being able to deliver an absolute higher performance in the limited power window that they were given. This is not always going to be the case. It's a workload by workload basis. If we take a look at things like training, there is nothing better than an SXM module. But if we start taking a look at things like VR or we start taking a look at things like inference, it needs to be studied as to what the best token per watt and token per dollar solution is.

Here's another example. And so we talk about scaling out GPUs. And so today, we run across a lot of eight-way servers, customers just buying eight-way servers and punching a bunch of GPUs inside of them and deploying their environment in that manner. And so on the left, if we had to deploy 30, 32 GPUs, that's what it looks like with traditional eight-way servers. I buy four servers. I put eight GPUs apiece. I get to my 32 GPU deployment. On the right-hand side with Dell, what we can do is we can take 30 GPUs and connect them to a single server. Number one is I reduce the amount of infrastructure I buy, the amount of networking ports, the amount of servers, the amount of software licenses I buy. For example, on the left here, I'm probably paying for four units of software. On the right here, I'm probably paying for one unit of software. On the left, I'm paying for four units of networking. On the right, I'm paying for one unit of networking. And so we think not only can we reduce the amount of hardware that we buy, we can reduce the amount of power that we consume. This is a significant less amount of power to deploy the same number of GPUs. So, back to that operations per watt and operations per dollar, we think there's an interesting way to go out and scale these PCIe-based GPUs. And by the way, by keeping all the GPUs on a single server as opposed to distributing that workload across multiple machines, we see 20% to 30% higher performance for the same quantity of GPUs by keeping them on a single node. So, lower cost, lower power, and higher performance by not breaking the job up across multiple servers.

The next one we kind of think about is silicon diversity. We love NVIDIA. They are phenomenally good partners to us. We have qualified the entire NVIDIA portfolio on our platform. Most of their PCIe-based GPUs are available in our catalog to be able to deploy. But our customers are asking us to deploy other GPUs. And so we think it's important to be able to give people choice. And so our platform supports all the accelerator types. We work with Intel. We work with AMD. We work with Qualcomm. We work with Grok. We work with the FPGA players. At the end of the day, we don't dictate what GPU our customers want to use. We enable choice. Obviously, we will make recommendations on what we think good looks like. But at the end of the day, we do want to enable that platform that lets customers choose what GPU they want to use. And I think choice is important not only amongst different vendors, but I think choice is important, even amongst a vendor's portfolio.

And so, these days, we are really focused on NVIDIA's L40s GPU. We think it's a really, really compelling solution. NVIDIA refers to this as their universal GPU. And so, it does a large variety of tasks. The things that we like about this accelerator, number one, has a large number of cores. And so, it actually has a large quantity of CUDA cores inside of it. Number two, it actually has a reasonable memory footprint, especially when you can aggregate 16 or 30 of these things together. It has video out. So, some of our customers are interested in that. So, it does have display ports out. And in our expansion chassis, we do have access to all of the external ports on the GPU. And I think the two best things about this GPU are, number one, it's a 350-watt device. And then, number two, it's a really, I would say, lower-cost device when we're comparing it to some of the really other expensive solutions on the market. And so, we've been working closely with Dell and NVIDIA on pushing solutions built around this L40s to market. And in the next handful of slides here, we'll show you some of the data that we've collected.

And so, the first one is just against an eight-way server. And so we see this often, right? And so, if someone is buying, say, an eight-way server, they're probably buying it because they want GPU performance. And so, if somebody is getting benefit from eight GPUs inside their server, we will make the argument you'll get more benefit from going to 10, or 20, or 30. And we get great performance scaling, right? If we go from eight GPUs to 20 GPUs in this particular workload, which is MLPerf 3.1 large language model inference, we see exactly what you would expect. We see a two and a half increase in the overall performance of the system by being able to squeeze two and a half more GPUs per server. And so, we see great, I would say, linear performance on the scaling side of it. Trying to find my other slide here.

The next example that we talk about is specifically focused on inference. And we think this one's really interesting. On the left-hand side, we have an eight-way H100-based system which, I'll state again, is the best system on the planet for things like training. So, if we're going to train a trillion-parameter model, we highly recommend using something like a DGX-based system in a SuperPOD architecture, or AMD's equivalent, because that will be the best thing for training. If we're talking about inference, you know, it's an interesting analysis, right? And so, on the left, eight H100s and an SXM form factor; on the right is a 16-way L40s configuration. And so, yes, the H100 configuration scores a higher absolute number—it scores 102 on this particular benchmark; the 16-way L40s scores a 94. The difference is power. The H100-based configuration consumes roughly 12,500 watts, whereas the L40s is around 7,500 watts. So, when we go back to that tokens per watt argument, we think if we're talking about things like inference, there's a really interesting way to go off and look at this thing because a lot of environments are power-limited. A lot of environments, you're only going to be able to, you know, in a 15k rack, put one of the things on the left, where on a 15k rack, you could potentially get two of the things on the right. So, we're taking a look at rack-scale performance. Power many times is going to be the limiting factor.

Just some more data that we'll share with you guys: this is a 16-way L40s configuration compared against the eight-way H100 PCIe configuration, and again, this is inference-based, not training-based. If we were looking at training, I would say you would see a different story here. As we think about inference again, our belief is that the L40s provides a really, really compelling solution for things like inference. Under these three workloads, what we can see is, you know, 16-way L40s delivers a really, really compelling performance proposition compared to other solutions on the market.

H100 SXM, uh, SMX, is it? It's become slightly different, right? And so, absolute performance, for sure, but again, when we start taking a look at operations per watt and operations per dollar, again, we think, when we're talking about inference-centric workloads leveraging PCIe-based GPUs that have lower power, is going to become more and more important as the market begins to shift towards, uh, inference as the primary workload.

Training is different, as I mentioned before. On our platform, we do get great scaling for training workloads, meaning that when we increase this particular training workload from 8 GPUs to 16 GPUs, we actually see a 50% reduction in training time, which is exactly what we would expect, right? And so, as we are able to scale these things, we can scale to 16, we can scale to 20, we can scale all the way to 30. And again, you know, as we scale up the number of GPUs, we continue to see the performance not bottleneck, and we continue to see great, great results here. So, this is an example of us scaling from 8 L40s to 16 L40s and being able to cut the training time down by roughly half.

Um, this is the most recent data that we're publishing. Um, again, our belief is that the majority of enterprises, and I would say research organizations of the world, are not going to be focused on training multi-trillion parameter models. We just don't think that's what is going to happen. We assume the cost of a training run is going to start approaching $10 billion in the next generation of training runs. We see 100,000 GPU clusters emerging, and then, you know, in four to five years, we're seeing things like million GPU clusters beginning to emerge. And things of that scale are going to be reserved for only a handful of organizations in this world, right? You have to be a very—you know, I would say, a special company if you're going to go spend $10 billion to go train a foundational model. We think the world is going to be taking these large foundational models, at 8 billion, Llama 3A, 70 billion, Llama 3A, 300 billion, 400 billion, open-source model, and then fine-tuning it with the data that they own because data is at the heart of everything everyone's doing. And then, as mentioned, running things like inference, running things like RAG, and as we move to those deployment production type environments, token per watt and token per dollar are going to be critical. And so, here are examples of us being able to run Llama 3A, 8, and 70 billion parameter with 30 and 28 L40s, and seeing great scaling all the way through. We believe this is some of the highest performing results from a single server.

You know, often we get asked, "Is you guys focus on PCIe-based GPUs? You know, you guys are not doing the NVLink thing. You know, talk to us about peer-to-peer." And so, as you guys know, on the PCIe side of the house, we still have the ability to do peer-to-peer. And what peer-to-peer means is direct RDMA transfers between endpoints, bypassing the x86 processor, similar to NVIDIA's NVLink or AMD's, you know, Infinity Fabric technology, except it's just done over PCIe at a lower rate. But, one of the things that we exploit on our fabric is not only the ability to do peer-to-peer at scale—so take a server, attach three expansion chassis to it, have 30 GPUs going to that single server, and then enable peer-to-peer amongst all of those GPUs, so all 30 GPUs can RDMA to each other, bypassing the x86 processor, right? And so, that's a big portion of what we do, and that's a big portion of how we get performance. But we extend that to not only include GPU to GPU; we think there's this emerging set of applications that are really going to be able to benefit from what NVIDIA refers to as GPU Direct Storage, enabling the GPU and the NVMe device to have a direct RDMA data path. And so, we support this on our platform. We can have 16 GPUs, 16 NVMe drives, 16 networking cards, and we enable an any-to-any configuration. Let any GPU speak to any NVMe drive, stop to any NIC, all of it bypassing the x86 processor. And one of the things that we have found is doing this on a fabric-based approach is actually higher performing than a standard server. If we think about a standard eight-way server, for example, it's usually a dual socket system. I have two processors. Normally, half of the endpoints, half of the GPUs, for example, are hanging on CPU number one, and half of the GPUs are hanging on CPU number two. When CPU number eight needs to speak to GPU number one, many times that data path is through the CPU and through things like QPI, which actually bottlenecks the performance of the overall system. By being able to deploy all the devices on a single NUMA instance behind a single CPU, many times we can get higher levels of performance. Because we can enable all of that peer-to-peer capability and not be bottlenecked by a dual socket CPU configuration. It's a bit of a technical nuance, but it's an important one that actually has measurable performance deltas.

This is an 'if nobody cares, we're happy to send this to you.' But this is what happens when you flip peer-to-peer on GPU bandwidth in a 16-way configuration: We increase the bandwidth from the GPU-to-GPU communication from about gigabytes a second to over 50 gigabytes a second, over a 200% increase in bandwidth by being able to turn this peer-to-peer feature on.

The latency is equally good, right? And so, when we turn peer-to-peer on, there's a massive improvement in overall latency. And so, we go from something that's mid-20s to kind of low single digits. And so, there's up to a 90% reduction in latency when we're able to allow the GPUs to speak directly to each other, bypassing the x86. So, on a traditional server where you have four and four behind the CPU complex, you will not get this level of flat peer-to-peer latency across all the GPUs. This can only be done when the GPUs are behind a switch, a PCIe switch.

And NCCL performance? Again, we see great performance on things like NCCL, even as we scale up to 16 GPUs and beyond here. So, guys, that's the performance data that we wanted to share. I'll shift gears a little bit, and I'll explain a little bit about packaging—how we package it—and then I'll talk about software and the roadmap.

And so today, the way that we partner with Dell—and we deliver these solutions to the market—is there's basically two configurations: UltraStack, think about it as high-density servers, a 10-way server, a 20-way server, a 30-way server, focused on GPU performance, focused on tokens per watt, focused on tokens per dollar, really delivering the highest performing server on the market today. SmartStack, this is around composability. This is around the pooling and sharing of these devices and being able to dynamically assign GPUs in a rack to a server depending on workload. Many of our HPC deployments deploy things like SmartStack, and we'll talk about integration into frameworks like Slurm here in a moment; but being able, in an HPC environment, to dynamically configure the servers on a per-job basis, we think, has compelling value.

The other thing I'll note here is that we work closely with NVIDIA. Our UltraStack configurations are NVIDIA-certified, and so we go through the paces with Dell head nodes. We make sure to submit all of our work to NVIDIA for certification and validation. We want to give our customers confidence that this has been thoroughly tested and validated. Therefore, we work very closely not only with Dell but also with the GPU vendors.

Talk a little bit about a roadmap now, as we're approaching the last 10 minutes here. As far as Liqid, we are focused in three primary areas for our solution. And so today, what we talked a lot about was around PCIe fabrics. And so PCIe Gen 4, PCIe Gen 5, soon enough, there'll be PCIe Gen 6. We plan on supporting all of those fabric types. But we also have the ability to support things over Ethernet and InfiniBand. For example, we can disaggregate NVMe storage today over things like Ethernet and InfiniBand. We believe we are going to be one of the leaders in CXL. We have CXL working. It's in the labs. For us, very soon here, we're going to start sampling CXL solutions to the market. And again, it's all going to be driven by our Liqid Matrix software. And so, one of the hallmarks of our software is multi-fabric orchestration. We want to be able to orchestrate across whatever appropriate fabric it is. And we think multi-fabric is actually going to be a large part of the story here. The next one is multi-device. Today, we've talked a lot about disaggregation of GPUs. But our hardware compatibility list, as mentioned, we have FPGAs, we have DPUs. We have networking cards. We have storage devices. DRAM is coming with CXL. That's emerging. And so again, our mission here is to give our end users the full complement of devices that they want to orchestrate. And then the last one is frameworks that we can plug into that allow our customers to use our solution without rewriting a bunch of software. So, being able to plug into things like Slurm, plugging into things like Kubernetes, plugging into things like Dell Omnia, that is where we are focused on the software orchestration side. And we'll talk a little bit on some of the use cases there.

We see, eventually, this is what the world is going to look like. We think our vision is, eventually, most servers—many servers—are going to be composable, meaning you're going to be able to determine the amount of storage that's attached to it. And we do that today. And most storage is already disaggregated. But we believe GPUs are going to be—we believe GPUs are going to be able to determine the amount of storage that's attached to it. And we do that today, right? Most storage is already disaggregated. But we believe GPUs are going to eventually, you know, you're going to be able to determine the amount of GPU that you give to a server. You can be able to determine the amount of memory that you give to a server. We envision a world where you have PCIe fabrics, you have CXL fabrics, you have Ethernet fabrics, and you're grabbing the right device for the right job at hand and, you know, potentially even reconfiguring that server on a per-job basis in things like HPC environments.

And so, that's how we orchestrate, right? How do we pull the strings of the fabric? How do we manipulate the bare metal infrastructure? And so, there's a handful of ways. We have a very intuitive GUI. You can run it on things like an iPad app and just, you know, swipe hardware around. That GUI is actually built upon a RESTful API, and that RESTful API is scriptable. And so, some of our customers, they write scripts. They say, you know, at 10 o'clock at night, take all the GPUs and go move them to that other workload, and at eight o'clock in the morning, put all the GPUs back to where they need to be so that the researcher didn't even know that they were gone. And so, we can script based upon the time of day. We can script based upon usage. We can script based upon performance. We can script based upon the failure domain. If the DRAM module on the server fails, take all of his GPUs, his storages, his networking, whatever it might be, and move it to a spare node. And then, you know, we take that API as the way that we integrate north into the frameworks that our customers use.

And so, today, we have a plug-in for Slurm. In a normal Slurm environment, for example, someone would say, "Hey, I need to run a job," and, you know, "Go give me 10 servers with, you know, three GPUs apiece." And if the best thing that we have available is 10 DGXs, we're going to have to make a decision, right? Are we going to wait for better infrastructure to come up, or are we going to run our, you know, our three-way job on eight-way servers, and maybe virtualize or figure out some other way to do it? In a dynamically configurable world, we have a plug-in for Slurm. So, when you are scheduling the job, you can actually schedule the hardware. You can say, "I want 12 servers, and give each server 12 GPUs apiece." This is a configuration that would never exist in the wild. But through disaggregation, we can go create that 12-server, 12-GPU configuration in a couple of seconds, give it back to the Slurm orchestration engine. The Slurm orchestration engine thinks it got a bunch of static nodes, can't tell that we created those. And as soon as the job is done, we will automatically disband those servers, put the resources back into the free pools, and the end result is we get better utilization of our hardware. And so, in theory, we should be able to run more jobs in any given day. We plug into things like Kubernetes, right? Where now, when you're defining your Kubernetes cluster, you can actually define what the nodes underneath the cluster are going to look like. And then, you can modify that Kubernetes cluster over time, add nodes to the cluster, but add nodes that are specified with the exact infrastructure that you need, reconfigure nodes that are already deployed, blow apart the entire Kubernetes cluster, and recreate it as needed. And so, we plug in those nodes, and then we can plug into things like Kubernetes. We plug into things like Dell Omnia. We plug into things like Ansible. We're working on integration into things like OpenStack. And I think one of the ones that we're getting traction on a lot these days is VMware environments. If we take a look at the way that VMware kind of works right now, I buy a server, and that server has X number of storage devices and X number of GPUs. When I need the end of the server, I need the end of the server, and I need the end of the GPU. And so, the nth storage device or the nth GPU, the normal model is we buy another box, and we give VMware more money so we can get that next GPU into our mix. In a disaggregated architecture, we don't do that. We can just take that hypervisor, and at the bare metal physical layer, at PCIe layer, we can compose more GPUs underneath the hypervisor. The hypervisor doesn't know that we're pulling that GPU from a disaggregated tray. It thinks that someone just plugged another GPU into his motherboard. And a way to do that is to plug in another GPU into his server. And that's the way you go. You can add GPUs, you can add storage to the server without paying for any more VMware. And so, we do see a natural pull towards being able to optimize VMware environments, being able to optimize VMware environments and other hypervisor environments through a disaggregated architecture. Think about things like VDI, where now we have the advent of these massive core count CPUs. And what's the right GPU ratio now? The box hasn't changed. It's still a 2U box that still holds the same number of GPUs. But my cores have blown out. I have many, many more cores inside that server. Do I still have the right GPU to CPU ratio? It's a very, very difficult task for some of our customers to balance. And through disaggregation, we can fine-tune that in a much more granular way.

And for those that want to know, we're happy to dive deep into our Slurm integration. This is important for HPC environments, and we've done a lot of work here. And so, if anybody in the audience would like to learn more about what we're doing here, please do let us know. We're happy to do a deep dive.

And similarly for Kubernetes, we've had great success with our Slurm implementation. And we're starting to see really interesting things around being able to integrate a composable architecture underneath a Kubernetes framework. And we're happy to share with the team more if anybody would like to dive deeper.

And then, you know, I would be a little remiss if I didn't share this slide. We're super excited that the message around composability and disaggregation is getting out. By no means are we taking credit for this; this is not us taking credit for anything. But, we believe it's acknowledgment, right? We believe it's acknowledgment that a disaggregated world, where you can deploy accelerators anywhere in the data center and you can reconfigure and compose dynamically, is a big deal, right? And we're so excited to see that our vision is getting out there.

And, you know, we're, again, very thankful for the Dell team for being able to partner with them. Dell has a phenomenal portfolio of AI servers. We are just here to kind of augment the high-end side of, you know, the portfolio where things like 20- and 30-way GPUs might be required.

Thank you so much! What a phenomenal presentation. And I do have some questions that have come up. What I will do is, I will offer to display the questions, and I also will offer for the person who asked them to come off of mute and have a conversation and interact with the speaker. If you prefer not to do that, that's totally fine; we can read them. So, first of all, I have David Conan, who actually had a couple of questions. Dave, do you want to come off of mute and speak? Or do you want me just to read the questions?

OK. Yeah, it was about this picture. I think 17; he had a bunch of 10 GPUs on a server. Is there an expansion chassis via PCI? Is there a way to have something like a DGX or an SGX, SXM version of that, like having PCI go to essentially an NVIDIA-linked DGX or HGX?

Yeah, we've looked at that, David, in the past. And it's something that we're considering right now. So potentially, in the future, we could have trays of four-way and eight-way NV-linked or InfiniFabric-based systems out there. And so, it's something that we're looking at. We don't have it today. Today, the primary focus is PCIe-based.

OK. And the next question was, for expansion chassis, you always showed just a homogeneous—or not homogeneous, all the same type of device in it. Can you mix, like GPUs and InfiniBand cards or Ethernet cards, and SE in one expansion chassis?

Absolutely. You can even mix AMD and NVIDIA GPUs in one expansion chassis. So, we don't care what you put in there. Now, CUDA will probably frown upon you if you put an AMD and a NVIDIA GPU into the same server. But from our standpoint, it's agnostic. You can put anything in any slot. Doesn't matter.

All right, thank you.

So, the next question comes from Jacob at Tandy. Jacob, would you like to raise your hand and join?

Yeah, so when you talk about peer-to-peer communication among GPUs and notice an improvement in performance, does this imply that the CPU doesn't manage the communication between the GPUs?

Yeah, so right. Yeah, that's right. So, peer-to-peer—what it refers to is all of the devices inside of this expansion chassis; they are doing RDMA transfers amongst all of the devices and all of these chassis. And the data traffic is not going through the CPU root complex. And so, in that scenario, in a peer-to-peer scenario, you've got to think of the CPU as more of a traffic hub. It's directing things, but the data is not going through the CPU data path.

OK. So, now this quick one about composability. So, you talk about Slurm and Kubernetes. So, I believe this is the software layer. So, how do you play with the software layer in a way that you add? You access the resources when needed, as needed. And, are you just limited to Slurm, or can you use another load balancer other than Slurm?

Yeah, and so, a couple of points. So, number one is the operating system or the application that you run on the server is any application or operating system that you want. We do not dictate that, and so you can run anything that you want. From an orchestration standpoint, meaning moving things around from the composability standpoint, we have plug-ins for different software stacks.

And so today, the plug-ins that we have: we have a plug-in for Slurm, we have a plug-in for Ansible, we have a plug-in for Kubernetes, we have a plug-in for Dell Omnia, we have a plug-in for VMware, and we work with Nutanix—it doesn't actually require a plug-in. Then, we are working on OpenStack and OpenShift. Those are the frameworks that we have integrated against today. Additional frameworks are in the works.

OK, thank you.

You're welcome. Next, we have a question. There's no one who—I don't think the person put their name; it's anonymous—but I will display it. So, how does the performance of a GPU cluster via PCIe interconnect compare to the same GPU over SXM, both avoiding x86, to reduce latency? But, assuming SXM-based would still have better performance?

It's totally a function of the workload, right? And so, if the workload is training, absolutely. There is a lot of small packet communication back to back, and the bandwidth of GPU to GPU matters a lot in training environments. For things like inference, it's a lot less important. And we see that in the data that we've actually collected, right?

And so, here's MLPerf 3.1: large language model inference. What you can see here is a PCIe-based peer-to-peer scheme provides sufficient bandwidth for this particular workload. And so, the answer to your question is not black and white. It's very much a function of workload. Our data is telling us that things like training a 2 trillion parameter model, yes, NVLink will provide you great performance benefit. If you're going to run things like inference, then maybe not. You're probably, in some of those workloads, better off by choosing the lower power device, aggregating more of them together to get a better tokens per watt or tokens per dollar outcome. So, it's a function of workload.

So, again, thank you all so much. And I hope that you all have a great week. And we will, hopefully, see you next Wednesday.
