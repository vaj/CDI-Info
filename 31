YouTube:https://www.youtube.com/watch?v=z5NzjTE94to
Text:
Okay, so we've been working on the PCIe, and then of course you know PCIe and CXL would be the same Fabric Manager's idea. So we are working on the CXL now.

And today's topics that we will go is three different topics. We have a real systems, real one, based on Xconn system. Then we will talk about the software features and the API that we provide, how people to use it, how people to manage it.

I go through this very quickly. We just do the memory pooling appliance and that is appliances of course in one of the rack and being connected through CXL and being when composed it will be look like from application point of view or OS point of view it looks like their own memory, right? So we do the memory pooling appliance and a fabric management software.

This is a real one. The worldwide is a real one. This we based on the Xconn 256 CXL switch to build our list. So there is a switch to connect to the host and also the CXL memory modules. And then there's a one mCPU. The mCPU has the fabric managers. They can do the memory mapping to the host. And all these CDFP connectors are connected to either host or memory module chassis. This is the first unit. In fact, we are not just have one, we have two.

The second one is this one is another one. This one is put everything in the box. The previous one is a more flexible one. So more flexible one means that you can have your, how many hosts you want, how many memory module you want, you can flexible to adjust it. This one also can adjust it. And this one that's a, it provide the 15 PCIe slot. That slot can put either six or many modules over host adapter. And then of course there's also one mCPU to manage which one belong to which one. And that because it's a two ways you can put a front access or back access to use a different cables direction that you want to use a front access or back access. It's better for the different data center want to deploy this.

The dimension is a 2U and of course 19 inch wide, right? And it's around 890 millimeters. It could be fit into the one meters rack. And of course you see it's either to the host side or to put the memory. So you can have two different ways to use it for your cooling and your host connections.

This is also way architectures. In the light blue ones, we have collect all the information from the CXL switch and manage the port configurations, memory mapping, that kind of things. And of course we also collect the information from memory modules and present it to the API and go to the orchestrations. Not just that, we also collect some management information. For example, like each single port ingress and the egress performance through a period of time. Then it's easier for you to decide. For example, like MemVerge want to see which memories have a big capacity, have a less utilization rate. He will choose which memory he wants to be allocated to one of the hosts or to be a shared memory to get the best performance of that. So that is what we have provided software architectures.

The software features, of course we can do the memory composability. You want to allocate different memory modules to different hosts. Then we can configure each port as the device port or become a host port. Here I don't say there is a... In fact, the most important is memory sharing among hosts. But as I just mentioned, we don't care about the write-lock. We just let MemVerge to manage it. But we can create a whole mapping of the... We call it global memory address space in our chassis. We give a global memory address space, physical memory, to mapping GDT(?) tables to different hosts to see the same physical memory. So it means that they can share among different hosts. You can share among two hosts or as many hosts as you want. But of course it's being connected to a same switch. And then we can also configure the host memory address because when you ask the host, say, "Hey, please reserve from your 2TB to 4TB memory space for us." And then you probably can say, "Hey, I just want to reserve even more or smaller." Then you can choose how big size that you want to reserve from host memory space. And we collect all the performance data and we store it for at least 32 hours for you to see what's the average performance being used and how you want to use the data to do some analysis for your application or for you to choose which memory that you think is the best fit for your memory allocations.

Okay. So this is a Refresh API structure service that we provide in this one. In the root service, you have three different services. Now we provide all the fabric services. In the fabric, CXL and all the others return CXL's port configurations, memory mappings, or their ingress/egress kind of number that we provide from here.

This is a GUI. I use a GUI because it's more visible and you can imagine what feature that we provide. So when you collect host 1, you see on the top of the bar, there is a base address from 8TB to 9TB. It reserves from this host 1TB its memory address. And then you see there's MD09's 512GB. It's coming from MD09's. It's on here. Then you will see the MD09 coming from which memory modules. So you will see there's a host memory address being occupied by which memory descriptor from which memory modules. And then on here that you see all the memory descriptor list is being created. And there are binding hosts. And this is the MD memory descriptor belonging to this host 1. So it gives you a clear view from host point of view to see how the memory descriptor is being created and belongs to it. Of course, we have another, but I do not show it. It's from the device point of view. You also can see the other way around to see this device has been created. How many memory descriptors and which memory descriptor has been assigned or mapped to any host. So you can see the shared memory from which memory descriptors.

We currently work with Open Fabric Management Framework. So we put one agent, six agents in it to provide all the information that we have from our API and can let the OFMF framework orchestration to query and send information from our systems.

So we have a few different solutions that we can use two DIMMs per channel. Because our partners have to make six memory modules with four DIMMs per channel or two DIMMs per channel. So you can combine with different DIMMs to create what you want. Currently, the biggest one in this version is a 7TB, a single box, the biggest one. Then you have a smaller one if you use two DIMMs per channel.

So I know that you have lots of questions you want to ask about the latency, about the schedule, about the portfolio. We are working on the very first one. So we just bring up everything online. So I think on the Flash memory Summit you will see all our data, what comes out. But not now because now we already let the switch see the memory and can allocate to the memory. But you know that they're not finished. You need to let a 1.1 host to screen the six memory behind a six switch. So that is a bit challenging, a little bit tricky. So we work with Intel, we work with Samsung to try to make sure the BDF(?) could be read from the host's point of view. So that part is we are working on. So I think we probably need another four or five weeks we can get it ready. By then we will have some of the real data, latency, bandwidth, and all the testing systems, production systems that will be available to all the people. So I think the very first thing that we will make it ready is to remote data MemVerge to access it, to fail it. See this is right, this is not right. So we will then can provide to the access to lots of partners and customers.
