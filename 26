
OK, so the purpose of this, which is the last session. So everyone wants to leave early, so no questions and will get out there quickly. But yeah, do jump in anytime with any questions. My basic aim here is to give some quick update on where we are today. Um? What a lot of that is, what do we have implemented but not upstream? Where the focus is that sort of side of things. I'm also trying to explore things that people actually want. I mean, what are you missing for stuff that you actually want to test? Uh, kernel development side of things as I come onto it relies somewhat on QEMU. I wouldn't say it relies on it, but we found a lot of bugs. Um, so it's proved certainly useful and the early kernel development did indeed rely on QEMU 'cause there wasn't anything else. It already said that.

OK, so very quickly, why do we care about QEMU emulation of CXL at all or indeed QEMU emulation of anything? Ben did a particularly good introduction to this in the talk that he did at Plumbers last year. So if you want lots of detail on that topic and some discussion around it, go look at that. But the key thing here is it allows us a platform to do flexible emulation of really quite complex configurations. We more or less viral command line interface can emulate anything. Um? OK, we're still missing some stuff, but we're building up to that fairly quickly and will come on to what is supported today a little bit later in the talk. Um, we can poke all the parts. It looks like. A real PCI system. Um, all of the sort of flows are actually going through all the different layers of emulating different components. Um, and emulating all of the code. OK, so requiring all of the code parts in the kernel to hit those. So it turns up stuff that isn't there. Anything that was mentioned earlier, which is in tools tests. I think in the kernel there is a CXL mocking interface that. Allows you to do a lot and. So the work that Dan and team have done on that has been so very useful for making sure the kernel code does what we think it does. There's a different purpose, basically for the QEMU code. Um, it's. Something we haven't done yet, but one of the things I definitely want it for is testing and continuous integration. So over time I want to get this into some of the test farms. Mainly because even when we do have lots of CXL hardware, we're still not going to generally in the test farm have all of the different configurations. There are insanely large numbers of different types of interleave, different setups that you can have. We're only going to have a subset if we can emulate a few more of them and hit a few more of those corner cases. That would be great.

Quick you should talk to Luis about kdevops testing framework and he was asking me about reasonable configurations of CXL topologies put into an automated framework and like the exact question you should chat with him.

Excellent thanks. I will do.

So my dream is that the CXL emulation will be sufficient to exercise absolutely everything that we've got in the OS side of things until I've got some software to poke it with. But there's not a huge amount of point in emulating it. And basically yeah, just testing everything. As I'm dreaming, I'd like to do fabric management as well. We'll come on to that a little bit later, but get into the point where, and it's been discussed earlier in this, the fact we might have a nice open source fabric management layout or even orchestrator providing a platform on which to both test that and use it to drive testing of the different topologies and make sure that the host does the right thing when it's past the right configurations. One thing that we have found this useful for and I'm going to huge amount of details is using it for specification prove out. So before the specification is released, if we have a reasonable, a reasonable baseline, we can fairly quickly implement things that are being proposed for the spec. And this leads to us fixing things that would otherwise turn up either as a rotter or horrendous workarounds. So this has already been useful. The PMU RFC that came out very soon after the CXL3 spec was largely because we'd used it for spec prove out in advance and had indeed modified various bits of the spec when we discovered things that didn't work so well or weren't tightly enough defined. It's very helpful to have an implementation.

So there's a trade-off here. In order to test a feature in the kernel, what we need is a proof of concept. We need to poke it. It doesn't have to be clean. It doesn't matter if we're hacking through various layers of QEMU to get to the right places. Yeah, it's much like any other proof of concept. A number of these have been sent out as RFCs. Myself and I think a few others have also sent some of these. It's good enough for that purpose. The problem we have is that works today for adding one feature, but then when we build some other feature on top of it, we've got a steadily growing unmaintainable mess. So hence, upstreaming is good, even though we don't technically need it to verify a particular kernel feature. I'm sure plenty of you have interacted with silicon verification teams, and it is kind of the difference from the scripts they write in order to say does this thing work under this very narrow set of circumstances, and the equivalent of having a software stack that's actually going to get used. So that sort of refinement, both are useful, slightly different purposes.

So, progress report. What have we actually got upstream? So there was a QEMU release a couple of weeks ago now. We got quite a lot went in. So we have CFMWS, which is the host physical address window stuff and all of the associated description to the OS. We have host bridges, which are done using sort of extension of what was the PCI expander bridge, which exists in QEMU and doesn't represent any real hardware at all. It's mostly traditionally used for things like representing NUMA topology of devices to QEMU virtual machines. But rather helpfully, it had a nice sort of general implementation of a host bridge to which you can attach root ports. So, yep, we got CXL root ports. We've got switches, and we've got type three devices. We haven't got any type one devices. So some of the flows we've been talking about earlier, we have no way of testing them. We don't have anything that is capable of, well, okay, it's QEMU. So anything that claims to be capable of caching. So if we want to explore some of those things, that's something we could add later. We have everything very, very nearly needed to bring up interleaved persistent memory on type three devices. So you can do all sorts of things. You can do direct attached type three devices plugged into a root port. You can do a switch in the path. You can interleave across multiple host bridges. So basically everywhere the spec allows us to do interleave, we support. It's fairly simplistic. The devices, and in fact, everywhere down the path, other than the host bridge level only have one HDM decoder. It's enough to test those paths, but if we want to carry on and allow for more complicated things where you're interleaving from one device in multiple different sets, that sort of stuff, we obviously need to extend that, but it works for now. The star there is because quite late in the development of the region pack set was added the sufficient to get to the point of actually creating labels. And those include the serial number. And that is the world's smallest patch. It's like four lines of code, absolutely standard QEMU stuff wasn't there. So the reality is while you can bring up an interleaved persistent memory set, you can only do it once. After that, you have to create a new one because there's no provision for a sensible label. But yeah, that patch is tiny and we'll get that up stream fairly soon.

So other things that are done, but not upstream, ARM support, which I care about because I work on ARM servers, that's blocked by a QEMU issue, which is that they prefer for the ARM machines or from boot at least where we were using it to always have device tree support. And that's the sort of historical thing about where their main use cases are. And that means that if we want to do that, we've got to provide the provision in the kernel for, instead of using ACPI for the root device, we need to provide device tree. It's not a big job. It's not particularly useful for anything other than QEMU upstreaming for now. I'm sure there will be embedded hosts of CXL devices at some point, but at this stage, it's kind of on the list to do when things get really boring, but it would be nice to do because then we have a second architecture and can hopefully highlight any small things that aren't there. I mean, there's one thing going on in the kernel at the moment around global flushes. I mean, simply enabling that thing would allow us to go, Oh, that function isn't there. I mean, we've, we've noticed anyway, but nonetheless it is useful to have more than one architecture. DOE and CDAT support is been around actually quite a long time. That was done by some of the folk at Everydesign. The main thing I've stalled that on is the original design relied on you hand creating or using some tooling to create a CDAT table, then sticking it in as a file. Given that we have a nice configuration interface for defining all sorts of different types of device, it would be nice to automatically generate something that is plausible for whatever it is. So trying to fill that gap, which again, is not particularly big job, but has not yet been done. So I haven't attempted to upstream that. DOE compliance. So the compliance interfaces, anyone want them? I mean, from an OS point of view, we don't really, I mean, it's test suite type stuff. They're not particularly invasive. The implementation that's there is pretty much a stub. It'll sort of say, yes, everything works. But if anyone does want to develop more on that, let me know. It's not what I'm going to focus on.

The PMU stuff from CXL3. So this was written, as I mentioned, to do a bit of spec prove out and was the kernel driver that goes with it. Both have been posted as an RFC. The QEMU stuff is nowhere near controllable at the moment. It's a hard coded, entirely random set of events. That accounted a bit strangely. But it does give you enough to exercise the kernel driver. So this is one of those cases where there's a question of just how flexible a configuration do we want for PMUs in an emulated device? Or do we just want something kind of plausible that might represent what would be there? I mean, there is slight fun around that because the CPMU definition is extremely flexible. And there are quite a few interesting corner cases where you have particular combinations of fixed and configurable counters. And this sort of stuff that can come in orders that require you to do a bit of searching and removal of things from lists and that sort of thing. So trying to come up with pathological cases that will at least exercise what's going on in the kernel side of things is probably worthwhile, even if they were something you never build. Yeah. We say that today, but someone will build it. Switch CCI mailbox. So FM-API has been mentioned briefly as the sort of thing that's out of the OS control. CXL3 added the ability to access via the switch all of that infrastructure. The intent for this actually is that mostly what you'd be looking at is a BMC using a separate PCI connection, a separate upstream port on the switch to access this. Or you might have a very simple system where for some reason you need complex switch configuration, but you're only using one host. People drew diagrams for it. I have no idea if anyone will build it. But from a testing point of view, it allows us to do nice closed loop stuff because we can then control what the configuration, the runtime configuration of the CXL devices is from within the host OS. It's fairly straightforward. I think this is a sensible thing to implement. And yeah, as I say, it gives us access to that. And crucially then would allow us to do things like dynamic capacity. I think, as I mentioned earlier, we need some emulation for. We had a buzz of a feather on SPDM yesterday, and I think we have a path forward for the kernel side of things. All of that work has been done against QEMU emulation of just enough to pass through via a socket to, it's a test program that DMTF have called spdm-emu, it uses libspdm, and that provides the infrastructure to actually test that stuff. It's handy. Poison injection. Yeah, this was something that was incredibly easy just to add when the kernel patches were posted for this a while ago. So we popped it in just to check that the kernel side of things worked, but this is very much a hack. If we want to do poison injection properly, we need to do a whole load of tracking, and we actually need to return poison when someone does a read. Not totally clear to me what poison looks like in QEMU. I don't think it's a concept that's actually there today. At the moment, this doesn't do that at all. It injects it into the list of things that you read back. If you actually read the memory address, you wouldn't see anything at all. So there's potentially stuff around there that will come in handy for some of the RAS testing. If anyone wants to take any of these things on, what I'm looking for is, yeah, more contributions.

So I also listed at the end here stuff that's turned up on the kernel mailing list that we haven't got at all in QEMU today. Event logs, a particularly major thing to add to QEMU. It's not that complicated. There's a whole bunch of structures to be defined, but -- oh, sorry, defined. They need to be typed in from the spec, but it's not tricky. The security commands, that's been a fairly hot topic on the mailing list and exactly how those are handled. Again, the QEMU side of things is pretty straightforward for those. Depends on just how much we want to emulate it. We could just emulate the interface and not worry too much that nothing actually happens on the device or the emulation of the device. So if you were to read it during that phase, you'd carry on getting the memory that you're not supposed to be able to. Or we can put some really basic gating in the way and return garbage. CXL1.1 support. So this is obviously of interest to some people in the room, given the other talks. We can do it if someone wants to. It's not something I'm particularly focusing on, at least partly because there are hardware platforms available for that. But, again, if it's useful, particularly perhaps for driving some CI type situations, it's not a tricky thing to do. Although, yes, the mention earlier of EDK2 support, one thing about CXL1.1 is to exercise a lot of that stuff, we probably need an open source firmware as well. So looking at some of the stuff from that talk may provide us with some useful components to build that.

I listed a few fun things that I haven't looked at at all, don't think anyone else has. Okay, so the first one should be really straightforward, but we never really bothered because it wasn't something the kernel stack was really doing anything with yet. I know from Dan's slides from the other day, from the sync call, that the plan is to get full top capacity support into the kernel side of things fairly soon. So, yeah, we'll want to add it to the QEMU emulation as well. Along with that thing I mentioned earlier about there only being one HDM decoder, obviously if we've got both persistent and volatile on a device, we're going to want at least two HDM decoders. And obviously two is near infinity, so we'll go with quite a few when we bother to implement it. Yep, dynamic capacity, the stuff we were talking about earlier, I think that's tricky enough to work out what we're doing on the kernel side of things that we definitely want a platform to poke. Thankfully, yeah, the kernel stuff's hard, but the basics of actually wiring it up, and an incredibly inefficient implementation at least, in QEMU is not tricky at all. It's just we already have to implement all the interleave stuff, it's just another address check. Is this one here or not? If not, go away. However, it does need the FM-API side of things. From that point of view, we're going to have to use a switch front end with a mailbox, and then we're going to have to tunnel the commands down to the MLDs on the actual device which we're presenting the dynamic capacity on. Not that tricky, but we haven't done it yet. Oh, yes, for fun, this one got mentioned earlier about shared memory and the entertainment that can be had when you share the memory on a single CXL device to two different root ports on the same host, so it appears aliased. Actually, for testing all of this dynamic capacity stuff, presenting it on different root ports of the host, maybe not shared, is a bit too weird, but in all other cases, the easiest way to do it is to have it removed from one port and appear on the other port, because it gives you a very easy way to check silly things like all of the correct clearing commands have been sent as necessary, because, yeah, memory turns up over there, has it got anything in it? Or, if you want to have something in it because you're doing that virtual machine case that was asked about earlier, then, yeah. I mean, to be honest, virtual machines migrating from port 1 to port 2 is probably not that helpful, but nonetheless, we can use it to test the software stack around that. Fabric, this got mentioned in the security talk from Jerome. The complexity that CXL3 introduces around this is large. For now, the problem is that not all of the interfaces are actually designed. There's some to-dos in the CXL3 spec. So once that stuff materializes, then we'll want probably to emulate enough that we can poke the host side of things. I'm not sure it would ever make any sense to emulate the full complexity, but we'll see. If we're using something like the QEMU thing to help develop the FM-API controls, then we'll want to emulate enough that we can poke the corners of that software stack.

Oh, getting towards the end. What we need, more review. So, yeah, review is always a problem. The issue here is actually we've had great support from the community around this, but none of them know anything about CXL. As you might imagine, it's still a little bit specialist. Hopefully, hopefully they're learning, but it would be very helpful to get review of, frankly, just the CXL parts. The silly things like are the registers defined correctly? You know, the stuff that is boring to review, but not particularly tricky. You just by building the spec and making sure it's all right. So that at least we can then say to the QEMU folk, from a CXL point of view, this is correct. And then they can focus on the QEMU interfacing and the more specific parts of it. More features. So I've listed a few features that are on the list of things I'd like to have. And there are a host of other things in CXL 2 and even more in CXL 3 that we haven't really got to yet. So if people have particular things that are of interest to them, or indeed the CXL 1.1 side of things, then, yeah, please, please step up and send code. I'm certainly happy to talk to anyone about approaches for how to implement anything. So my sort of final question around this is, having heard that quick summary of where we are, does anyone have a particular focus topic? Because I'll be honest, I'll work down it at the moment in the order that makes most sense for me. But my end goal is to get the kernel support for everything around memory, at least, upstream, all of the features around that. So obviously trying to synchronize things with what other people require should speed that up. So if you're working on a kernel patch and there's no emulation, it's worth letting me know in advance so that we can hopefully land them both at the same time.

So I had some patches for the volatile capacity aspects, which, as you say, it's very simple to implement. And as we've also talked before, it's basically about having the firmware give you the HMAT distances and fabricating that sort of thing. So I never posted patches because the command line parameters are just insane, and I just left it there. So I was hoping if somebody can provide some sort of EDK2 thing that we can use for that, because that's pretty much the only difficult part in it.

That one's an interesting one, because while -- so for CXL 1.1 level stuff, we need the firmware to be involved. For CXL 2, yeah, there is, we say -- I don't know, there's some recommendations out there that you still attempt to do it in the firmware because you may have an unaware OS and you try and build as much as possible there. There's nothing requiring it, and the moment we consider hot plug, you can't do it anyway. So from that side of things, that's kind of an OS problem.

Okay, then I guess I can post the patches.

That would be excellent. Thanks.

One more slide, I remember. Crucial slide.

In your slides earlier, you were talking about what's upstream and what's not. Is the not upstream repository public, or what did you mean by that?

With one exception, yes. I haven't pushed the CCI mailbox stuff out, because I was working on it last week and didn't get quite far enough. But the rest of it is. Some of it's on the mailing list, the vast majority of it. But, yes, I've got a slightly old branch pushed out at the moment. I can, yeah, share the link to that.

Okay, yeah, I'll bug you offline.

Cool. I need to push a new version out. It's, again, one of those things that didn't happen last week. Oh, yeah, I forgot the crucial thanks slide to acknowledge some of the people who have been involved in getting this upstream, and indeed writing it.

Sorry, I have one more question. Regarding the PMU work, this is not particularly focused on -- it was not particularly focused on QEMU. I'm just more curious as to where do you see the integration with the driver and perf for the performance monitoring?

It's a perf driver, so it integrates like any other standard-ish PMU in the system.

Okay.

So I wasn't going to do it any other way. That's how all our perf devices work. I know that there are, shall we say, examples out there in the ecosystem that are not using the perf framework. This one most definitely does. It's feature-wise, as you asked about it, there's a bunch of stuff that's not there. I haven't bothered implementing fixed counters, for instance, largely because they're not much use for anything. So I don't know if people put them down, but for a perf-type situation, a counter that rolls away on its own and has no ability to stop and start becomes not useless, but not as useful. I was hoping if I implement the configurable counters, everyone would go, "Well, that's the only thing that works." So all the hardware will come with configurable counters, but I think I might be optimistic. Okay. I'll just do the quick thanks. So as I said, Ben kicked all of this off. I think he's online. Thanks for that, Ben. Chris and Huai-Cheng did the DOE CDAT stuff. That was a great help for driving some of the DOE development in the kernel. As far as I know, there isn't any other way of testing that stuff at the moment. Yep, Michale Tsirkin actually took the patches, which is the vital step of looking through them in detail and then accepting them. And a whole bunch of QEMU maintainers and reviewers who've come in with various questions, clarifications, complete design changes. But we got there. So thanks very much to everyone who was involved. Any questions online? In the room?

I'm going to make a comment. This work is absolutely amazing. Thanks to Ben for starting it and Jonathan for carrying the torch, but also getting it over the goal line. Because it totally changes the calculus on hardware testing. Because the concern was always that QEMU should be a faithful representation of -- emulation environment, but is it going to be a place where we can land just test code? And I think you've kind of broken that down and things can start flowing in. So I'm wondering now that we're over that hump, how can we get this into the kind of continuous integration flow that you talked about? So CXL test was like, hey, here's something that we can control the schedule of. QEMU is a little bit less of that. But what's your sense for, like, how much the QEMU community is willing to take? Is it just a matter of review bandwidth and we can kind of get on a cadence that's more predictable? Because, yeah, the CXL test is great for the things that can be mocked and for things that's not mockable, it's great to have QEMU. But at the same time, there's, like, QEMU is hard -- in my sense, it's difficult to do some of these topologies where you can just kind of throw them together in a mock environment. So I still think those two things coexist, but this definitely changed the calculus to be much more QEMU focused going forward for the continuous integration aspect of it. What's your sense on kind of relying on that going forward?

Thank you, speaker.
