
Okay, let's talk about the compute express link, the next generation interconnect overview and the status of Linux.

Please let me serve myself. My name is Yasunori Goto. I have worked for Linux and related OSes since 2002. I developed for the feature of Linux kernel and I worked for technical support for troubles of Linux kernel and etc. And currently I am a leader of Fujitsu Linux kernel development teams. For some years I have mainly worked for persistent memory and I presented some place about persistent memory like here. And currently my team has been working on CXL since April 2023.

Please note the following. Please refer the CXL specification for proper understanding. Anyone can download the CXL specification from official site. What you need is only registering your name and email address to the site. Although I tried to make sure there is no mistakes in my presentation, there might be misunderstandings or inaccuracies yet. The CXL specification and related specification were very huge and I could read only some parts of them. 63.1 is 1166 pages and related specification is very huge. So if you can find mistakes, please let me know. And I recommend for you to study PCI or PCI express and ACPI specification beforehand. The CXL specification is too difficult if you don't know them. Anyway I hope my presentation helps you to understand the CXL.

Here is today's contents. At first I would like to talk about overview of CXL specification until 2.0 and additional specification of CXL 3.0 and 3.1. Finally I would like to talk about the status of current Linux for CXL, memory tiering and memory hot plug or memory pool.

 Okay let's start about overview of CXL until 2.0.

What is compute express link? It's a new specification of interconnect which connects device like PCI express. CXL is abbreviation of compute express link. Official white paper says open industry standard interconnect offering high bandwidth low latency connectivity. It's suitable to connect smart device like GPGPU, SmartNIC, FPGA, computational storage and so on. In addition it's also useful to expand memory, volatile memory and persistent memory. The newest revision of specification is 3.1 which was released last month 14th. CXL seems to be winner against other competing specifications. The board of directors of CXL includes numerous vendors and service providers. Alibaba, AMD, ARM, Cisco and blah blah blah and Intel and Microsoft and so on. Other competing specifications seem not to be promising. Open copy and Gen-Z were assimilated into CXL. And CCIX is not active. There is no new information after 2019 in CCIX press release. Since the promoter companies of CCIX are also members of CXL so they can select CXL instead of CCIX.

So why CXL become necessary? The first reason is increasing demand for fast processing of data. It's influencing of current technology trends such as machine learning. The second reason is the need to offload processing due to reaching limitation of CPU performance enhancement. GPGPU, FPGA or SmartNIC must handle the processing instead. And finally memory capacity must be increased. Although the number of CPU core increases, memory capacity does not follow it. Since DDR is a parallel interface, it's difficult to increase the number of CPU pins to connect more memory. So new interconnect becomes necessary to connect devices and memories instead of PCI express or DDR. 

So what is the advantage of CXL? Here is an example of calculation of GPGPU will be more effective. So far, a CPU and device must transfer data and instructions in bulk between DDR DRAM and GPGPU memory. Not only data, but also instructions for GPGPU must be transferred. It's a bit troublesome and need time for the transfer. CXL allows that CPU and GPGPU can access other size memory interactively. It will be effective for machine learning or any other modern analysis. Similar benefits can be obtained when you offload data processing to FPGA or SmartNIC.

 To access each other's memory effectively, not only CPU but also devices require the use of cache to access memory from the other size interactively. Currently, PCI express does not allow the use of cache for transfer data. Even if device memory is mapped to the host address space, CPU must use write through access which is slow. Device need to transfer their data in bulk by DMA which is not possible to interactively load and write memory. So there are requirements against the above limitation. CPUs and devices want to have interactive access using cache. They want to write back their cache when it's necessary. So CXL is created for the above requirements.

 So CXL web page also says the following. Compute express link is industry support cache coherent interconnect for memory processor memory expansion and accelerators. But what is cache coherent here? So far, only CPUs must negotiate cache information for memory access to each other. There are some famous protocols for cache coherency. For example, MESI and MOESI. MESI means modified and E is exclusive and shared and invalid. To access each other between CPUs and devices with cache, the devices also need to coordinate cache information with CPUs. CXL realizes it with MESI protocol.

 Here is the characteristics of CXL. CXL utilize a CPU express specification generation 5.0 or later. Its physical layer is same with PCI express. But upper layer becomes CXL original protocol. PCI express generation 5.0 or later allows different protocols to get on its path. CXL protocol is mixture of the following three types of protocols. CXL.io is used for CXL device detection error report by PCI express way. CXL.cache is used to request or communicate cache information between devices and accelerators and CPUs. And CXL.mem use to request for memory access between devices, accelerators and CPUs. CXL.io is same protocol with PCI express. But others are new protocol of CXL.

Here are three definition of device type of CXL. Type 1 device is it has cache and does not have memory or its inside memory is not shown to host. For example, SmartNIC or FPGA which has above structure. It use protocol of CXL.io and CXL.cache. Type 2 device is which shows cache and memory to host. Good example is GPGPU or FPGA which shows device internal memory to host. This type of device use all of protocol CXL.io, CXL.cache and CXL .mem. Type 3 device is memory expansion which connects CXL. It's for volatile memory and/or persistent memory. It use CXL.io and CXL.mem.

Other types of device manage their cache status by device coherence engine, DCOH. It's a component in the device and it must maintain status of cache of the device and memory access. Device memory which is included in the device and it's shown to host is called host managed device memory.

How to access from type 2 device to HDM is here. A CXL device need to select the following status to access its memory which is shown to host CPU. The first one is host bias state. Device needs to request CPUs to keep cache coherency before accessing device attached memory. Like this green arrow, it must send request to host CPU once, then it can access device attached memory. Next one is device bias state. Device can access device attached memory without consulting the host coherence engines. After bias flip, this green arrow, device can access ideal latency and bandwidth. CXL 3.0 specification audit another way. I'll talk it later.

I need to talk about features of type 3 memory device. The following configuration is available. You can configure device as memory pool as follows. Use one memory device to one memory region and bind multiple device to single memory region and divide one device to multiple regions. In addition, interleave is available. This example is 8-way interleave by host bridge and CXL switches.

 CXL has binding of port of CXL switch. So far, upstream port must be only one in a PCIe switch. However, a CXL switch can have multiple upstream ports in it. You can bind a downstream port to upstream port dynamically. To configure binding, a component which is called as a fabric manager is necessary. Fabric manager can be implemented to any style like the following. Software which running on host machine embedded software running on a BMC like a server management software embedded firmware running on another CXL device and state machine running within the CXL device itself.

Type 3 memory device can be divided into multiple regions as logical device and assigned to different hosts. In this figure, type 3 device is divided into two logical devices. Logical device 1 and logical device 2 can be bound to different upstream port. These upstream ports may be connected to different hosts. The fabric manager is responsible for dividing logical device and binding them to each port.

In addition, hot plug is supported. CXL 2.0 devices will be hot pluggable like PCIe device. It means CXL type 3 memory device will be hot pluggable. Not only persistent memory but also volatile memory will be hot pluggable as the hardware specification. In past Fujitsu made special servers which support volatile memory hot plug. Memory hot plug of Linux kernel was developed for it at first. But many servers may support memory hot plug by CXL in future. Not only replacing a physical device but you can add memory area which is hot removed from another server. It will be important feature for memory pool.

Here is a memory pool use case. Memory pool distributes a part of its regions to other servers as needed. Example of all other use case of banking system is here. In daylight, it gives much memory to servers which access ATM transaction. In night, it gives memory to other servers which processes batch jobs like payload transfer. So far, this feature is only possible by special server which supports memory hot plug. Another option is to use virtual machine on the same host. But it's not possible to pass a memory area to other host. CXL might be possible by establishing an open standard specification. Next use case is failover. A server can take over regions previously used by another failed server. Not only memory but also a GPGPU may be able to take over its processing in future.

 Next section is CXL 3.0 and 3.1 specification updates.

Here is a list of new features of CXL 3.0. It was released one year ago August 1st. The right table is quoted from its white paper. Personally, notable feature is fabric capabilities and memory sharing and enhanced coherency. And there is other thing update. It is twice speed than 2.0. But it just comes from PCIe 6.0 specification. And multilevel switching is supported. It allowing CXL switch hierarchies. And direct memory access for peer-to-peer is supported by 3.0 and etc. But today I will talk about the three features of CXL 3.0.

 The first one is fabric capability. Fabric connection is supported. The topology of connection was tree structure whose root was 1 root port and CXL 2.0. Even dynamic binding is available. Tree structure is same with PCIe. CXL 3.0 allows fabric connection via CXL switch like the right figure. The number of maximum nodes is 4096. It can connect CXL devices with the shortest distance between servers. This is the most notable new feature for me. It is the basis of the next generation of distributed computing.

Port-based routing is introduced for this capability. Messages in the fabric were sent with port IDs of source and destination. Each ID is 12 bit for 4096 nodes. If a CXL switch supports port-based routing, it is called PBR switch. A fabric manager needs to distribute IDs to PBR switch via management network. What is management network? It can be SMBus, I2C, I3C or Ethernet. All of them is okay.

Next feature is enhanced coherency. CXL 3.0 allows that the device can have cache coherency information. In CXL 2.0, only CPU needs to maintain it. Devices need to ask CPUs to access its memory beforehand. In other words, CPUs and CXL devices have asymmetric relationship for cache coherency. In CXL 3.0, the relationship between CPUs and devices is symmetric. The DCOH of device watches cache coherency information on CXL. In addition, the device can request CPUs to update their cache information if necessary. For this purpose, Back Invalidation Snoop, BISnp channel is added to CXL memory protocol.

Here is an example of enhanced coherency. Specification describes a variety of access patterns and timing between a CPU and a device. This figure is simplified example of BISnp protocol. At first, CPU would like to access address X of data. But device need to flash cache address Y and request its data. Before getting data X, CPU must write address data Y. After that, device can provide its data of X. Device can actively request CPU to change cache state and transfer data depending on the device state like this figure. To confirm more correct sequence, please see this section in the spec. This is very interesting I think. It describes various sequences and you can understand how cache is managed actually. Please check it.

Next feature is memory sharing. Memory sharing between host is available. Each host can work together by shared memory. fabric manager has role of configuration of which memory regions to share and how to share them. There are two ways how to manage cache coherency. First host hardware coherency. It has feature to manage cache coherency. When a CPU request data right to device memory, device need to coordinate cache information of other host CPUs. Next one is software managed coherency. Software need to manage cache coherency between host and BISnp itself. Even if the device does not have mechanism to coordinate cache coherency, this way is available. The actual mechanism by which software coordinates cache information is out of the scope of the CXL specification.

Here is update of CXL 3.1, its summary. Today I will briefly outline some of update features. Fabric enhancements have been added for the Fabric feature. The first one is global integrated memory. It's used for enabling remote DMA and messaging across domains via CXL fabrics. Next one is dynamic routing. Message transfer can use different passes between source and destination post dynamically. I suppose personally it seems a bit similar to the IP routing of TCP/IP. It is determined by congestion avoidance and traffic distribution across multiple links or link connectivity changes. And next one is security enhancement. So far CXL has supported CXL integrity and data encryption, CXL IDE feature. In addition CXL 3.1 also supports confidential computing. So CXL TSP is defined. TSP means trusted execution environment security protocol. It defines mechanism for allowing VM guest to execute within a trusted boundary or direct attach CXL memory.

 Next section is the status of current Linux for CXL.

Summary of current status is here. Basic implementation of CXL memory driver and commands has been developed. The driver can detect CXL memory devices and you can configure memory region interweave by CXL command. The repository of CXL command is same with ndctl which is the command for persistent memory. And the solution for the memory tiering issue was developed. CXL memory makes an environment which is called as memory tiering due to variety of access latency. Since Linux memory management system did not consider it, new feature was developed. There are some difficult issues for CXL memory hot plug memory pool feature yet. Even CXL allows device hot plug feature as hardware specification, Linux has some issues for CXL memory hot plug. Today I talk about the latter two topics.

The first one is memory tiering. CXL memory has a difference of access latency compared to DDR memory. CXL persistent memory will be slower than CXL DRAM memory. The access over CXL switch is slower than direct access. As a result, memory access latency become tiering. The nearest DRAM from CPU, DRAM on another node, CXL DRAM and CXL DRAM over CXL switch. For this problem, CXL memory region is treated as a CPU less number node. Since Linux NUMA implementation considers for difference of memory latency, it's also suitable for CXL memory. There is no CPUs in the CXL memory device, so the NUMA node of CXL memory becomes CPU less.

In past, Linux memory management system did not have enough consideration for CPU less NUMA node. Currently, Linux NUMA balancing policy is to use nearest memory from CPUs. It allocates memory on the same node with the CPU which process execute if possible. If auto NUMA balancing is on, contents of memory area on a far node are moved to node  where the process is running. Since a CXL memory node does not have CPUs, process cannot execute on the CXL memory node. As a result, CXL memory may not be utilized as expected even if NUMA balancing is used. So Intel developed a new feature to solve this problem.

Its name is demotion and promotion. Instead of swap-part and swap-in, kernel migrates cold page to CPU-less node, its demotion, and it migrates hot-page to nearest NUMA node from CPUs, its promotion. So far, when a page is swap-part, CPUs cannot access its data until swap-in. However, even if a page is demoted, CPUs can still access it, its difference of swap-part. It decides which pages should be demoted in the page reclaim procedure. When a page is accessed by a threshold type, kernel promotes it. The default threshold is 1 second, but the kernel automatically adjusts it based on the amount of promotion candidates and its bandwidth. This first work was completed in kernel 6.1 once. In addition, the community continues to enhance the algorithm for selecting the demotion target. So far, kernel has only used old specifications, which provides only ratios against the nearest memory latency. Since ACPI HMAT can provide detailed performance data, they are developed to use it. And CXL has also more effective performance information by CDAT. It may be used for it in the future.

Next is issue of memory hot-plug memory pool. CXL memory hot-remove memory pool has three big issues. The first one is more software components are necessary for memory pool. Next one is the CXL specification itself causes difficulty in hot-removing a CXL memory device. Not only specification, but there are many obstacles for memory hot-remove in Linux. Unfortunately, I have not enough time to talk all of them today. So I'll talk about the last issue.What is the obstacle for memory hot-remove? Please check appendix of my presentation about other issues. You can see I'll update after this presentation. My presentation will be uploaded to speaker deck. And also, schedule.org has already my presentation. And you can see it. In addition, I'd like to recommend that you read my discussion at the community mailing list if you have more concern about these issues.

To talk about the problem, I need to introduce how memory migration works. To remove memory dynamically, contents of removing memory must be migrated to another place. It moves the contents of migration from removing memory to another memory without changing a virtual address. This is memory migration. Basically, this can work for user process memory. But it cannot work for memory used by the kernel or drivers. Because it's virtual address must be changed when the physical address is changed. Unfortunately, even if the memory is used by the user process, there are cases that memory migrate cannot work. You cannot hot-remove such area.

So long-term pinned pages are one of the big obstacles of memory migration. I read my future, like InfiniBand, pinned pages of users processing to transfer data from the pages without mediation by kernel. Kernel cannot migrate pinned pages because they may be under data transfer by the device. I guess the DPDK or any fast-performance devices features may have same problems. And such kind of feature will increase. And VM guest also tends to pinned pages to skip kernel or hypervisor for performance improvement. I think there is an ambivalent requirement like the following. There are many things which want to pinned pages and skip kernel to make better performance. But kernel has a responsibility over any resources management in OS. And memory hot plug is one of them. The difficulty of improving CPU performance causes increasing the left-sides requirements. Though kernel needs to manage total balance of system, it cannot be achieved by bypassing. So I believe the root cause is lack of communication between such feature and Linux kernel.

 Current solution of Linux is here. By opening memory areas on removable memory, the kernel migrates contents of the areas to unmovable memory like DDR memory. The current kernel can create ZONE_MOVABLE areas based on user's configuration. Zone-movable was created to ensure that removable memory is not used by the kernel and drivers. Therefore, it's beneficial for the CXL memory pool to allow hot-remove. To configure ZONE_MOVABLE, please refer the kernel documentation in the source code. If a FOLL_LONGTERM flag is specified for the area, the data transfer target, the Linux kernel migrates them from ZONE_MOVABLE to another suitable place before pinning pages  and data transfer. This is the solution for now. But it may not be the final solution. If the amount of DDR memory is relatively too small compared to CXL memory, it  may not be enough for pinning areas. And VM guest also tend to pinned pages of hypervisor. If a large number of VM guests are executed, CXL memory may not be effective due to  DDR memory shortage.

So CXL specification provides new approach. It's called dynamic capacity device. It's introduced in CXL 3.0 specification. It allows that you can gather removable small memory blocks until required capacity rather than trying to remove stuck memory blocks. Then, they can be transferred to another host. Memory pool will be available by this specification. However, I suppose it may cause other problems. A CXL memory device will have a mixture of following memory blocks. It will be very difficult. So I worry about it.

So one of my ideas for future is here. On demand paging may solve this issue. ODP is a way a device can transfer data by RDMA without pinning pages. It's communicating between a device and Linux kernel. When a kernel is going to invalidate a page for support or memory migration, it can notify the event to the driver and the device. When the device needed to access the invalidated page again, hardware asks the driver to execute a procedure like page fault and can restart the data transfer. Currently, only NVIDIA network card support it yet. However, I hope more hardware vendors will support it. To understand ODP, I recommend the following. The Mellanox presentation and the paper are very helpful to understand ODP. And since we are developing ODP support patch set for Soft-RoCE, you can check and try  it without any special hardware.

Anyway, here is the conclusion. I talked about CXL specification overview and talked about the new specification of CXL 3.0 and 3.1 and current status of Linux kernel development community. I hope that many vendors will release CXL devices and boost the market. I also hope that many people will develop future drivers for CXL in Linux community. Thank you.

 Have you done any benchmarking with the current NVIDIA product yet?
 
Currently we don't have benchmark. We can provide in public place. Internally we estimate how much performance is executed in CXL memory. Currently it's emulation.

Thank you for your detailed explanation. Could you go to the slide showing the new features of CX 3.1? OK. In this slide, CX 3.1 support RDMA and messaging across domains via CX fabric. I'd like to know what domain means in this context and also the use case of this new feature.

OK. In CXL specification, domains means often say it's servers. So you can understand domains means servers. So global integrated memory provide the future servers across RDMA via CXL fabric.

OK. So RDMA over CXL fabric is one use case.

Yeah.

OK. Thank you.
