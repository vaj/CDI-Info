
Are we up? All right. Thanks, guys. I mean, it looks like we got our critical mass back. I was a little worried after a lot of nice churros and coffee out there whether I'm going to lose all my audience, though. Thanks for being patient and joining this panel. We have an excellent panel today. I'm going to start introducing everybody, and then we'll get into it. And any time, please feel free to join the discussion. I'll try to keep it as much as possible within our time, but we definitely want a lot more discussion because we've got a very good panel here who will be very much interested in interacting with you. So let me start with Reddy. Reddy, we have from Intel. He's the co-lead for composable memory system. He's a senior principal engineer for Intel's data center platform group, and Reddy has been known in the industry for a lot of things. He has been involved in initiatives for OpenSDS, Linux Foundation. He's on the board of CEPH. And since he's working on the CXL, he's driving a lot of initiatives for getting-- make sure that you have the right adoption. So he's working on the CXL from the CPU perspective from adoption. So we'll definitely talk to him about his perspective about where we are on that adoption path and what things need to happen. So thanks, Reddy, to be here. We have a Siamak. Someone who has been working on the projects related to servers, OCP, CXL, et cetera. I don't think they need introduction of Siamak. Everybody knows him. He has been guiding us, CMS project, from its inception. He's a driving force behind CXL also. He was president. He's been advisor for the board. And he's also running CXL track tomorrow, I think, right? We have full OCP track tomorrow. So we'll surely look to him to enlighten us about what his views are about how the industry can collaborate to drive the vision of this composability that we are working on. We have Larrie. Larrie Carr is VP of engineering at Rambus. And he's actually president of CXL right now. He has been driving a lot of initiatives for memory connectivity, including CXL. So he has been involved in OpenCAPI, GenZ, CXL, RISC-V, so all of the things. He's president of CXL Consortium. So we are surely looking forward to his views from both sides of the bench as a vendor driving the technology as well as driving the industry to collaborate on our options. So I'll be very interested in taking his views on that. Samir. Samir Rajadnya is from Microsoft. He's a principal architect in Azure's hardware architecture team, which is responsible for all long-range technology pathfinding for Azure. Samir is part of the team responsible for all the future memory systems. And he has been in the industry driving a lot of these initiatives. And Samir, as you have seen through the day and the previous presentation, that he's championing the industry about use cases, what hyperscalers, why they want it, and how they want to use it, what are the key pain points. And so we'll get to talk to him more about those as we get into the discussion. And we have Sandeep from Astera Labs. He's a senior product manager and has been driving a lot of initiative in the industry. He has a lot of experience on the soft diagnostic tools, developing product strategies at Astera Labs. He's basically focusing on this product strategy for new market segments, translating the data center bottlenecks to real product opportunities. So as CXL is very key and important technology for composable memory system, as we have seen throughout the day, as a fabric. And so we are definitely looking forward to Sandeep also to share his views on what are the challenges and what are the opportunities in this space. I'm Manoj, and I work at Meta. And I take pleasure working with the whole team and learning from them and see how we can solve our problems in the composable memory systems. So with that introduction, let me start jumping into this. Let me start with Reddy. Reddy, you are the co-lead for CMS subgroup. And can you maybe give a little bit of an overview of what is the scope, who are the people involved, what are the images deliverable, just so that we can start getting-- everybody gets an idea.

Yeah. I mean, some of you probably might have attended the morning session where we talked about what we are doing in the CMS project. So as you guys know, CMS is composable memory systems. So the focus of the CMS is essentially looking at bringing in all the hardware as well as the software elements and then making sure that we essentially have a viable production solution by taking the hardware plus software elements together. So in that context, we are essentially looking at form factors, memory expansion board form factors as an example. We are also looking at all the critical elements from a switching perspective, the software perspective, some manageability, Redfish APIs, what do we need to do. And we are also doing the academia part. So we have five work streams essentially to target different facets. That includes the memory expansion as one of the focus areas in the work stream. Then there is also memory fabric. This includes the AI side of the fabric as well. And then the third one is near memory computing. This is something that you're going to see more and more going forward. So we want to kind of get ahead of the curve. So what does near memory computing will involve? Both from the enabling perspective and programming perspective so that you can seamlessly do the computations both closer to the device as well as in the host. And then we also have manageability as a key focus area. I touched upon that in my talk with Vikrant. Manageability is essentially going to be an extremely significant part because if you look at the Redfish profiles, it doesn't have a majority of the CXL constructs. So we have to go identify what are the missing pieces, go influence them with DMTF, just like what we are doing with hotness tracking as a way to influence the CXL consortium. We have to do the same thing with Redfish as well. So the manageability is going to be a very key focus area. And the last one is academia. The fact that university, the research is trending in a certain direction, we want to keep an eye on what's happening in that space and then essentially fine-tune our focus plus give the feedback. So it's bidirectional. We have approximately 40 members joining every week, weekly call. This includes pretty much diverse attendees, silicon vendors, device vendors like Sandeep and company. We have board vendors and we have essentially the cabling connectivity vendors and hyperscalers as usual, as you can see here, Manoj and Samir in the call. So we do have diverse membership, 40 people attending, somewhere around 200 folks actually.

270.

270, sorry. 270 folks actually in the CMS distribution list. So I strongly encourage you guys to actually participate in the weekly calls as well as contribute to all these five work streams, wherever you can. This includes white papers, specifications, and so on.

All right. Thanks. Let me take that now since you talked about with Samir. Samir, you have been in this industry for memory, but also now you are representing this from one of the biggest public cloud perspective. Why is this CMS important for you? Why the memory problem is solving important and then of course following with the CMS?

Yeah, I think CXL is one of the great innovations. It has been a little bit challenging initially, but I think from a technology point of view, we hardly believe, meaning we believe and we are firmly behind it. But what it really enables us is detachment from the computer roadmap. You can bring new memory technologies. We can have mix and match DDR5, DDR4, and that trend will continue. We have software as a service and we have cloud. So for cloud, there are some restrictions, but for software as a service, it's a wide big open field because we manage our software stack and we can deal with two-tier memory nicely in that space. So yeah, the memory everywhere in the keynotes we have listened, right? Memory wall, memory is not innovating fast enough, capacity problem, bandwidth problem, and all of those are helped through CXL. So we are firmly behind that because it enables us to solve all these problems. So you will see a lot of activity going forward.

All right, thanks. Since you brought up a lot of memory related, I think Larrie has a lot more experience on the memory and we're going to get very exciting answers from Larrie, I'm sure. So you see this whole Rambus has been active in this and you have been personally active in a lot of this industry. You have DRAM, HBMs. Where does CXL fit or does it fit at all? What do you think?

Well, the biggest problem is you use memory and then CXL, but CXL is fundamentally an interface, right? It's a load store interface. And if you actually look at historical where we had Gen Z and OMI, they're all load store interfaces. And what made CXL very unique is that it was the first interface to be adopted by all the major processors, right? That was the biggest problem with all the sort of problem statements that were being addressed by these other protocols, CCIX, OMI, Gen Z. And it was like, finally, we have an interface on the CPU that gives us a new way of talking into the memory hierarchy that's much better than PCIe. And as Samir stated his problem statement, he's all about memory. Like CXL is more than just memory. We have the ability to do accelerators. And there's innovation there, but because of this problem with memory, memory bandwidth, being able to address new memory types, CXL became the interface that we focused on. So without CXL, a lot of innovation on the memory hierarchy and sort of closing that access difference between direct attach and even SSD would not have occurred without CXL. So it's definitely more than just memory, but there is a lot of focus right now on memory. And there's multiple camps of new memory types, cold memory, hot memory, NUMA attached memory, new sort of built-in accelerations that people are trying to push in the market space. And it's enabled by the interface called CXL. And without it, it'd be boring.

Yeah. Yeah. I'm sure we're going to come back to some of those questions that you've read. Let me also talk to Sam, since we talked about CXL. You have been participating in CXL and OCPN, guiding a lot of projects. You have been president of CXL, board of advisors to the CXL, and now OCPN, you have been shepherding a lot of projects. How do these groups, how do they work together? What is the, why do we have, we have CXL group tomorrow, we have CMS today. How does this work together? What do you think? Do they help each other?

Of course. Actually, I have a talk tomorrow, as you said, at CXL forum at OCP. The main theme there is CXL, similar to other technologies, NVMe, PCIe, DDR from JEDEC and such, are technologies. Technologies are needed. Work needs to be done in detail, defining what those requirements are, how to specify them so they can be replicated. But OCP is the place that we as a community put things together in systems. We build systems, we power them, cool them, manage them, mechanical, thermal, all the interconnect, all the solutions, it gets realized into solutions. One specific comment about CXL, since we are talking about CXL, CXL of course is an interconnect, a very efficient, high-speed, low-latency interconnect, and memory is a very good use case for CXL. However, just like CXL building on the predominance of PCIe, we say CXL.io is just like PCIe, first do no harm, whatever we have learned from PCIe, we still do it within CXL. That's a good thing. Now what are we doing with CXL? Not only the interconnect, not only the fact that it is a coherent memory structure, but the team, the large team for CXL 3.0 is working on constructs, management, security for composability. So the work that we're doing with composable memory systems is going to generate another foundational building block, call it composable memory system architecture. Underneath of that, if it is CXL or not, that might be secondary in a year or two. You might be doing all of these concepts using a different interconnect. It might be over HBM, it might be over future Ethernet, but the control, management, security could be established through this team that we have right now.

That's very good. Thank you. Let me come to Sandeep now, because we talked about CXL from standards perspective. How do you see CXL switches play a role into these various use cases, and also from what components will come up and what kind of cadence that they can create?

Sure. I'll answer this question in two parts. The first part is what kind of products has the industry started to adopt and what kind of use cases has the industry started to have, and how does the CXL switch play into that? How can we extend the existing use cases? The second part would be what additional use cases can we implement using a switch that we don't have right now? To answer the first part, the industry has started to adopt CXL multiported memory controllers. Obviously the advantage of that is memory and bandwidth expansion, but in addition to that, because it's multiported, we can enable memory pooling and memory sharing. When we talk about VMs and containerized environment, the memory pooling that you enable would need to be larger in scale. That's where the CXL switch comes into play. What the CXL switch enables you to do is have as many as 16 hosts be able to access different memory regions simultaneously. What you end up doing with that use case is actually address what's called as memory stranding issue in the industry. Essentially in the data centers today, we have over-provisioned memory, which the CPUs are not able to efficiently utilize. By addressing the memory stranding through memory pooling, you are efficiently utilizing the available memory resources. In fact, on the CMS wiki page, there is a very interesting white paper that actually describes the topology, which includes CXL switch along with the CXL memory controller with which you can do memory pooling, which the gentleman on either side of me have authored. It's a very interesting white paper to explore. The second part of the answer I was getting to is, in addition to memory pooling and sharing, what additional use cases would a CXL switch enable? The answer to that is, if you look at use cases such as HPC or grid computing, these are environments where you need disaggregation at a large scale, at rack scale. That's exactly what CXL switch provides. To be able to disaggregate different components, when you have an HPC kind of environment where you have multiple compute nodes that are interconnected, what you need is to be able to connect fabric attached memory, FPGAs, NICs, accelerators, so on and so forth. That's exactly what CXL switch actually enables you to do. There are so many other fabric use cases that we can explore through a CXL switch. There are some challenges associated with it as well, which we'll talk about in a few minutes here.

Okay, awesome. Thanks. Actually, that has prompted some thoughts in my mind, but I'm not going to take advantage. I'm going to ask a little bit later some questions as how the AI systems are evolving and how our thoughts are going to evolve and going to change. I'll hold that thought for a second. Let me start with where we are. Reddy, I think the CXL is becoming reality from CPU perspective. You have been driving that a lot from CPU perspective, how to show that as Sandeep talked about, there's a white paper, there are proof of concepts going on, and you have been working on the Sapphire Rapids to bring up. Now we see a lot more on the floor also in Experience Center. What do you see the adoption perspective from OS, from devices, CPU?

I think probably two years ago, we were basically struggling to figure out, is there a device out there that we can actually use just to make sure the platform and the device combined together, it actually works and we can actually see the memory, CXL memory in the operating system. We were actually struggling to even validate. Fast forward now, you have so many vendors with type three devices in the market, and they're actually getting ready for a production deployment. That's the transition we have seen in the last two to three years. Early challenges were device availability, validation, operating system actually having all the hooks needed for us to actually detect this as a CXL device, memory only, NUMA domains, be able to actually run certain benchmarks. We have actually gone through that phase quite a bit. Now we have benchmarks, CacheLib, Meta actually open source that. CacheBench is a very popular one that we can use. MLC, and we have database benchmarks we can actually run on the real system with type three devices from real vendors, like what Sandeep is talking about from Astera too. We essentially have transitioned from early platform bring up to we are at a place where we can actually run the benchmarks. We can actually see the end-to-end solution working with different types of workloads, not just one. Caching, databases, training workloads can actually run. You have seen that throughout the talks. If you actually visit the Experience Center CMS station, you can actually see there are 12 demos. 10 of them are actually live demos that are actually working. That includes the switch-based demo as well. That has been major progress from where we were before two years ago to now. Having said that, I think Linux kernel, Microsoft Azure stack, as well as the VMware stack have actually evolved in supporting tiering. Is it complete and comprehensive? No. But is it at a place where it is failing in the beginning? That's not true either. These kernels are actually maturing quite a bit to be able to handle the data movement part very transparently. We are seeing that part. The kernel tiering part getting more and more mature. We are also seeing application tiering with native CXL tier support. CacheLib was one example that Intel and SK Hynix team was actually demonstrating a couple of talks in this room. There is an application tiering. I know there is the SQL-based tiering work that is also going on. We may see Redis and a bunch of other frameworks actually doing the tiering as well. So there is that transition happening in the application stacks. The thing that we need to actually focus on going forward is really bringing in the fabrics, be able to actually enable the entire ecosystem, just like what we did with Type 3 memory buffers. Type 3 is a very simple use case. When you bring in the CXL sharing and pooling type of use cases, it requires quite a bit of focus from all the industry contributors as opposed to one or two vendors. So that is going to be the focus. In fact, I'm spending most of the time in bringing the switch, actually getting trained with just one upstream port, be able to detect one downstream port as we speak. So that's where we are currently, but we are working through that transition.

I can't avoid the temptation to now bring up the topic of new areas, like AI. I'm going to ask the remaining question, but maybe I know CMS has not focused on the AI as a specific use case, nor have we brought up as much together. But where you see-- I remember two years back, we were demonstrating live one device that you had, how to detect it and show the NUMA node showing up. I think AI use case is at an early stage right now, saying that what does the role of CXL play? Is it the right solution for it? And one of the things that I'm going to bring up is that what CXL benefits from is, of course, the PCI underlying infrastructure. But the question is basically, do we think there are challenges going forward in those AI use cases? I'm going to go around, and of course, in our next question, if you can just keep that in mind, that this is my selfish interest to ask that question. Do we have right-- how do you say-- in the foundation for us to address the needs of AI also? So Samir, maybe let me start with you. We have talked about a tiered memory infrastructure and use cases that we have for virtualization, for caching, database. What are the key things that requirements that you think the CXL products need to have? I know you talked about hot tests. Maybe we can touch upon those. But while you're thinking, maybe think about how AI also, if you can, ask you in your thoughts.

OK. So I will answer this in three parts. First of all, just look at tiered memory. It's a challenge as well as an opportunity. So the challenge is there are two types of memories now, two different characteristics. So can we build some techniques which will happen under the hood? Hotness tracking is one of the examples, right? There is another one that Intel released at Hot Chips, which is a cache swap where the main memory and the CXL memory cache line swap happens, right? That's another hardware technique where software application does not have to worry about it. There is no code change required. The other side is where we have control over our software stack, where these are the people who write software today with remote memory in mind. They have been dealing with RDMA. So for them, if you give them another CXL node, which is now in say hundreds of nanoseconds versus they were dealing with microseconds, right, which is a much better thing. They have been writing their code already. For them, the transition to CXL two-tier memory is an easier transition. And it's not just writing new code, but you can get a performance boost, right? So the value prop is not just a TCO play. Now it's more of a performance play, right? And that is something we can benefit from clearing. And the third thing I will address is CXL 3.0 plus that brings a lot of new technologies such as memory sharing. That is another big leap where a lot of the softwares will benefit from this because sharing will limit or eliminate a lot of copying that happens today. So if we can do sharing with eliminate copying, that's again another performance boost. Now to come back to your AI topic, right, I'll just quickly touch on it. I think everywhere we heard memory capacity, memory bandwidth is a problem for AI. And people are, we are not saying that we need to replace any fabric. There is a place for all types of fabrics. There is UEC, there is a proprietary fabric, and there is a need for another layer of fabric. And CXL can play that role. So if you look at currently how Grace Hopper is used, right? Hopper is a GPU, Grace is a CPU. But what we know is the LPDDR on the Grace is, so the Grace is used just as a memory controller. So you're adding another tier of the memory to Hopper through Grace. So you added LPDDR as another tier of memory. Think about CXL now as another fabric which can help you to add another layer. We are not talking about HBM. We are not competing with NVLink and the bandwidth. But can we have another layer, right? So that will help. So this is how I look at AI systems.

That's a good point. And maybe let me bring that to CMS, because CMS, you have been driving this technology as a foundation for CXL, what we have. If you look at all these new use cases, what do you see CXL is driving to and how do you make sure that we address those questions? And maybe I can add a little bit of a dimension to it also, right? How do we stay competitive from the bandwidth perspective in CXL because it ties on top of PCIe? And it will, generations will continue to make sure that it serves larger market.

A very good point. So I've heard it several times that the bitrate that CXL is using is tied to PCIe. For it is today's 32 gigabits per second. And with PCIe 6.0, it goes to PAM4 64 gigabit per second. And that is, in fact, different from what you can do with Ethernet at 112 gigabits per second. NVLink uses that close to that 100 gigabits per second or so. So there is a criticism here on one lane. But I could also say that with CXL, we could have a bundle of x16. A bundle of x16 running at 64 gigabits per second can give you 256 gigabytes per second of bandwidth. But CXL also allows for interleaving. So a CPU with four x16 links can interleave and it can aggregate without software having to worry about anything and get one terabyte per second of bandwidth out of four CXL links, which is equivalent to what you can get with 1,024 signals or lanes of an HBM. So one HBM with 1,000 bits is equivalent to four CXL links. Well, that's a good bandwidth that we can have. So that's about what I can start. But that's where we are today. Of course, CXL as a specification is growing. The consortium is looking for feedback. It is very likely in response to the feedback, CXL will respond. CXL specification future versions could very well increase the bit rate per lane as well.

Yeah. Thanks. That's awesome. I think that also shows that CXL is-- maybe Larrie, as a president of CXL, maybe would you like to add about what are all the things you see in this direction that CXL is already looking at or maybe looking at?

Well, I can't comment on things that only contributors are supposed to know about. But I would definitely confirm that there is a number of people within the consortium are looking to go faster. In a lot of ways, when looking at the CPU roadmaps, the focus on going faster on these dual mode CXL PCIe slots that a lot of CPUs are doing, the number one focus on going faster is now moving from PCIe NIC bandwidths and moving closer to, again, memory bandwidth and accelerator. So the three-year PCIe SIG roadmap of following the PCIe SIG speed bump, we may go faster sooner.  There are differences already between PCIe and CXL, even at the protocol level, for certain reasons like data integrity. So it's not out of the realm. It's just, again, it's a skinny silicon to keep up because, boy, are we moving fast in a lot of these areas.

Makes sense. Makes sense. And this is the reason why CXL is the choice of fabric right now for CMS because that has not only current capabilities but also ability to move fast forward.

Again, I'm part of the team. So I'm just repeating some of the things we've said before. The architecture is layered. Even if you forget the interconnect bit rate and such, the work that we're doing to build the ecosystem around programming, security, management, orchestration, that is tremendous work. And of course, as part of the requirements from different companies, requests for doing something better, even if you pick a different interconnect, CXL is today, UCIe is coming, what happens next, we don't know. The legacy that we're creating, the technology and the framework we're creating today is very much forward thinking for composability.

Yes, absolutely. That's a very good point. Now just maybe continuing on that topic from the CXL switches perspective, I know it's early stage right now. We are just beginning on that. But as you start looking, because these new technologies take time for you to understand, test it out. And what do you see the challenges from CXL switch perspective today and tomorrow as much as you can see?

Yeah. So needless to say, every time we talk about AI and ML, within the contributing members or with customers, the first question that anybody would ask and should ask is, tell me what's the performance of CXL, especially when a switch is involved? What is the latency, to be even more specific? And that's a very relevant question because when we are targeting CXL for AI and ML kind of applications, mission-critical applications cannot wait for too long. They cannot tolerate latencies. So that is one of the major challenges with having a CXL switch. With CXL memory controllers, now we are uncovering the latency and bandwidth sensitivities across the application, meaning that now we are actually able to test and see what kind of latencies can these user-level applications tolerate. When we move towards CXL switch, the question becomes a little more amplified in the sense that can these applications tolerate additional switch hop latency? That's something that we will have to determine and figure out how to maneuver around it. Hopefully it would be tolerable, but also an intelligent way of looking at this is not all applications require the lowest latency. You could have applications that can tolerate a little bit of extra latency due to a switch hop and that's what we should be targeting in the interest of enabling rack-scale architecture. Also, the other interesting thing that CMS has done is they are standard as already touched upon, we are standardizing benchmarks that can be used. So what that means is system-level architects or designers as well as end-users can use these standard benchmarks and workloads and assess the latencies of the existing devices and the upcoming switches and then determine what application can actually utilize the end application even with a switch involved. That's the first challenge that I can think of. But thinking a little more ahead, whenever we talk about CXL 2.0 switch which enables single-level switching and/or 3.0 which enables multi-level switching, the other question that customers typically ask is how do you cable this between the host and the end devices? This is not a new problem. PCIe switches have had to deal with this problem in the past as well. So when we talk about cabling, what are the factors that you would actually think about? First and foremost, you need to make sure your signal integrity is good. You need to make sure it's low power, low latency, and don't forget the total cost of ownership. You don't want to be deploying massive cables and you take an impact on the latency and your total cost of ownership. Fortunately, we already have external PCIe cabling solutions available today that are targeting AI and ML as well as CXL switch use cases. Also, OCP in fact has what's called a PCIe extended requirement specification which actually describes a unified cabling solution. When I say unified, what I mean is this specification actually includes both NVMe and as well as CXL switch use cases. It in fact goes into depth of different configurations that CXL switch can be used for. When I say configurations, what I mean is it could be memory cluster, compute cluster, accelerator to memory, and for each of these configurations, the spec actually defines what kind of cabling should you be using. So the industry should be using these protocols and these guidelines to come up with solutions to address this problem.

Yeah. So needless to say, every time we talk about AI and ML, within the contributing members or with customers, the first question that anybody would ask and should ask is, tell me what's the performance of CXL, especially when a switch is involved? What is the latency, to be even more specific? Yeah, that's awesome. I know I've been dominating the questions, so if anybody has any questions and discussion that you want to bring up here, please feel free to join and that will definitely get priority over me right now at least. If not, I'll continue my discussion a little bit for longer and we'll close in the next three to four minutes. All right, let me go on. One thing which I wanted to bring up and I think maybe I'll just chime in a little bit, I think a question that Samir brought up, a discussion that Reddy brought up, that if I look at from AI use cases, I think there is a lot more for us to look forward to. If you look at native memory expansion, we talked about type three devices so far, right? If you want to do what Samir was bringing forward, now we're talking about something more in the type two range where the accelerators need to have access to the memory. If you talk about switched infrastructure for AI, now suddenly you see the use cases now changing from use applications that are ready to trade off bandwidth for TCO, especially cache databases and data warehouses kind of use cases that we saw. Now we suddenly see use cases start going towards more bandwidth heavy because now we are not comparing that with the natively near memory that is attached on DDR5, but now we are comparing with the accelerators that have very high bandwidth capacity with HBM. We need some kind of a tier going from there and CXL has a way of playing there. So I think we have a lot of interesting problems going forward, but I think the CXL has those or CMS has those kind of layered in a way to borrow words from CMS to where we can go and start addressing them. I'm going to maybe start, end up by putting a question and make Larrie very uncomfortable because I'm going to ask him, because I'm going to ask him what, you know, he wants to make sure that he can deliver real products, but I'm going to ask him to dream about what new things you think CXL can enable even though it will be challenging.

Oh, okay. That wasn't on the list.

No?

That wasn't on the list.

All right. Let me, yeah.

Yeah. So, you know, basically, you know, like from an innovation point of view, it's more than just memory. It's the ability to place data where you want it and use it there. I think a lot of, you know, where the future kind of needs to get into is more of the accelerator side, the compute, because there's a lot of work on, you know, storage with near, you know, near storage compute, near memory compute. Those architectures were started on PCIe. They should come over to CXL and be able to innovate. There's a scale aspect. You know, even back in Gen Z, there was the dream of, you know, a fabric of 4,000 nodes, you know, rack level kind of compute and memory all working together. And I'm seeing those architectures kind of coming today, right? So, you know, whether it's AI, and the world is more than just AI. That's just me speaking as a hardware guy. But you know, these architectures where, you know, scale of memory and compute expands beyond a 1U server, I think is where the future is. It's going to take a while to get there.

Awesome. Awesome. Thanks. Thanks, Larrie. I appreciate that. I think we are definitely over time, but I'd really like to thank the whole panel here. I know we have an excellent amount of knowledge and experience on this stage. So I appreciate you guys taking time and participating in this discussion. Thank you, guys.
