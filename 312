
There we go. My co-presenter is Fangzhi from Alibaba.

Start off with a little bit about why in the world we need one more interconnect. That's the biggest question I think I get: what do we need one more for? The answer is, AMD started looking around, saying, "Hey, we've got a great little GPU. Other people have other accelerators that they'd like to bring to market. And how do we do that where we don't have everybody creating their own ecosystem?" The answer was, let's get together. And when we got together, we started looking at all the different types of interconnects that companies had. So you see that Intel's part—we looked at what they had. Google's part—we looked at things they had available as well. What it came down to was, when we looked across all those different interconnects all the companies had, the Infinity Fabric that AMD had was a good starting point. We decided the best starting point for UALink. But it was not optimized, so there was more work to be done. That's why we came up with the idea to start a group. The goal of the group needs to be an easy interface for accelerator vendors to implement, and a way for switch vendors to come out and say, "I can make one switch that will work across a wide array of those accelerators." You can see at the bottom of the slide, we're taking on scale-up, and that's what UALink does. When you start thinking about inference models, you know, tens to hundreds of accelerators tend to work very well. When you start to think about bigger problems, a lot of the training problems, yeah, you may start to get into where you need tens of thousands of accelerators. That's where our previous presentation on UEC and UALink working together comes in. You'll see that a little bit later in the presentation.

So, what we did was the original promoters got together to say, "This is something that's important for us." And what we wanted to come down with for the Ultra Accelerator-Link was, first, it has to be open. And that's why you see it in the middle. That's the most important thing: let's make sure it's open. Anybody in the industry can join in and help shape this. Second, it has to be high performance. One of the key pieces of what we do is making sure that this can enable real-time inference and very fast training. Finally, it had to be scalable. It can start off with small models, and it can be directly connected if needed. But it can also go out through that switch and connect up to what we end up with—a thousand devices.

How do we start to think about UALink, right? We created this open ecosystem. And what we want to do is make sure that we can effectively scale the GPU. So the way to think about it is, how do I take multiple GPUs and put them into an environment that makes it, to the software, look like just one large GPU? The analogy I like to use is with CPUs. Right? Right now, if you have a server with one CPU, two CPUs, four GPUs, or even eight, you don't write different software. You write one kind of software, and the OS takes care of everything you want to do as far as talking to those CPUs. The same thing applies here with our scale-up: it's just one way to program, one way to communicate. You know, when we talk about the scale-up environment, what are we worried about? Well, we're worried about bandwidth first, right? That's the biggest thing that everybody wants. But then we also want to make sure that our latency is low because we're talking to memory. So, we need to ensure our latency stays in line with communications with memory devices. And then the other concern is power, right? I think if you've been to any of the presentations, everybody's talking about power. When you start thinking about it, it begins with the PHY. If I can decrease the size of that PHY, I can actually save power in the silicon. And when I start considering the number of lanes I want—128 lanes, maybe 256 lanes on my accelerator, and a similar setup on my switch—by saving power in that PHY, I can significantly reduce the overall power consumption in the data center. Optimizing it at the PHY level all the way up makes a big difference, and that's what drives our efficiency as well. Now, the promoter group has been working very hard to launch this. We just incorporated at the beginning of October. We will be opening for members the last week of October, so you'll be able to visit our website and sign up as a member. The specification will then be available for members to review by the end of the year.

Little bit about the timeline: we're moving very quickly. This group formed in May of this year. Since that time, we made a quick presentation at FMS. Now, we are here talking as well. We believe that we'll have those membership forms posted on the website by the end of October, and the specification by the end of the year.

Now, what is UALink? What we're doing right now is we're really focused on how to make GPUs or accelerators share memory. The models are getting large. You can't just use one GPU; you need to scale up to much larger models. With that, we came to the conclusion that we need good, low-latency communication for those models. We decided to go ahead with the 200 gigabit-per-second lane speed to ensure that our bandwidth is appropriate. Additionally, we wanted to make sure that our solution works complementarily with UEC.

And so, what you see in the picture here... I'm actually going to skip this.

But in this picture, UALink is there to help us with creating the pods, and then UEC allows those pods to be hooked together. So, when you talked earlier about the tens or hundreds of GPUs, that can be done in a pod. When you start to think about tens of thousands, that's where you take multiple pods and connect them together through UEC.

So, with that, let me bring up Fangzhi to help us with what Alibaba is doing in this area.

Thank you. Hello, everyone. I'm Fangzhi from Alibaba Cloud Server Team. Could you just give us a brief introduction about the UAL? It is definite that everybody thinks the UAL is very promising. However, from our perspective, there's still a long way to go to build a real system that works online, especially for a cloud company like ours. You know that we have many customers building their own applications on our platform, which leads to diverse requirements. That's a big challenge for us. Anyway, let me introduce our ALink system, which we call ALS. It was initiated jointly by Alibaba, AMD, and other chip vendors, several ODMs, and even IP vendors. ALS aims to build a developer system standard, ranging from interconnection standards to rack-level and fiber management levels. So, it's quite challenging for us right now. We synchronize the roadmap and align with the schedule to build up the system as soon as possible. We can’t wait for this, yeah. ALS has a data plane and a management plane, offering rich capabilities and features on the platform. The data plane uses UALink as a scale-up interconnection protocol, which natively supports memory semantic access and shared memory, including shared graphic memory, with ultra-high bandwidth and ultra-low latency. The ALS also adds in-network computing acceleration functions in the data plane. The goal of the ALS management plane is to provide standardized access for different chip vendors, enabling different chip solutions to be integrated easily into our server fleet through a unified software interface. This will especially provide cloud capabilities such as single- or multi-tenant configuration flexibility on our cloud platform.

So, okay, we designed the Pangeo AI Generation 2 servers for the next-generation hyperscale AI clusters. It's a prototype right now with open ecosystems, high energy efficiency, and high performance. It will define the AI computing node, scale-up and scale-out interconnections, the rack level, and the CDU specification, supporting mainstream AI solutions within one hardware architecture, within one hardware cloud platform. Okay, Alibaba Cloud Platform. Finally, we look forward to more and more partners embracing the UALink and joining our ALS ecosystem. We believe that we will get an ample dose of pain and suffering, as Jason said, but we also believe that we should give ourselves more options, essentially better opportunities. Thank you all.

Thank you everyone.
