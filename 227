
Hey.I'm Robert Blankenship from Intel Corporation.I'm going to be presenting on Compute Express Link CXL, going over the memory and cache protocols within that interface.I'm part of the-- I attend consortium meetings representing Intel in the protocol workgroup and the memory workgroup by workgroup.And my time at Intel, I've spent a lot of time doing server coherence protocols, memory protocols. So long 15, 20-year background in that.

So what we'll be covering today is-- I'll give a quick overview on what CXL is.There is another presentation given by Dibendra Desharma from Intel on CXL. More generally, overview presentation.I'll touch on some of that, but I recommend you take a look at that presentation if you want more overview material.I'll be going through caching as a 101.What is caching?Why is it valuable?Before getting into the CXL caching protocol itself, and then getting into the CXL memory protocol.And then finally, going over a merged use case where we're using memory and caching protocol for accelerators.

So CXL is a standard that runs on PCI Express physical layer.And we've added new protocols on top of the PCI Express definition to optimize the cache and memory flows.CXL uses the existing PCI Express port, existing physical layer, and auto negotiates to either a standard PCI Express device or a CXL device.And you can see that in the picture on the right, we basically can allow in a platform, you can plug in into the same connector a PCI Express card or a CXL card.Showing by 16 as an example, but other widths would be supported as well.The first generation of CXL aligns to the 32 giga transfer PCI Express Gen 5 definition.We expect CXL is going to be, and the usages that CXL will create, will be one of the key drivers around the PCI Express Gen 6 architecture that's forthcoming.And that will be part of the CXL 3.0 definition aligning to PCI Express Gen 6.

Now, what are these protocols again that CXL is using?Well, we have the baseline CXL.io.This is the same transaction layer, protocol layer as PCI Express.So we keep all the goodness that PCI Express provides.And we add two new protocols on top of that.We have the memory protocol.And this is protocol where the host CPU can access memory within the device.So it's a directional protocol from host to device for memory access.And then we have the cache protocol, which allows the device to directly access cacheable memory in the host CPU.And the memory and cache protocols are what will be discussed today.

We have three representative use models for CXL today.We have initially the caching devices, accelerators, which are going to be making use of the CXL.io protocol.Actually, all use models will make use of the CXL.io protocol at least to do basic configuration and error reporting, all the standard PCI Express mechanisms there.And we add to that for this caching device, we add the CXL.cache protocol.And the use model for this may be if an accelerator and NIC type accelerator would want to do a PGAS ordering model where it needs to know when writes are visible in the host.We can do that within the cache.And CXL.cache gives you direct visibility to when data is visible through the coherence protocol or through streaming writes.That will be discussed later.And you can also imagine a NIC can do atomics directly within its cache.It can do arbitrary atomics as it sees fit, bring cache line in, cache data into the cache, do atomic operations, and then flush data back to the host later.Second representative model is accelerators where we basically are adding on to the previous example in that we have a cache.We get all the same flows that we talked about with the cache model, but we also add memory.And so the memory in this case is being shown as HBM where an accelerator may have high bandwidth memory attached directly to the accelerator.It's exposing to the host as system host managed device memory is what we call it, but it's coherent memory with respect to the host processor.And it's attached to this accelerator.And in this case, accelerator still has a cache.It can do cacheable access to all of system memory, including its directly attached HBM that's exposed to system memory.And the host processor will use the CXL memory protocol to access the memory within the accelerator.And I'll discuss this more at the end of the presentation.And the final use case is for memory buffers.Memory buffers in this example are passive devices that would be primarily exposing memory to the host processor.And this could be used for memory expansion, either traditional volatile DDR type memory expansion or storage class memory that would be persistent.

So now I'll be getting into discussion of caching more generally, giving a 101 level presentation overview of why caching is valuable.

So caching conceptually is a way to bring data temporarily closer to the consumer.It improves latency and bandwidth by allowing the use of prefetching and/or locality within the cache.And when I say prefetching, referring to idea that we can load data into the cache before it's required by the consumer.This can be done either through software or hardware mechanisms.And when we say locality, we have two variants of that.We have spatial locality. Which is locality in space.And the idea would be if a device or a consumer accesses address x, it's likely going to access an address related to that address, say x plus n, in the near future.And by holding data in a cache that's close to the location of x, we have a likelihood that we will find spatial locality and access patterns from a given workload.Similarly, we have temporal locality.But in temporal locality, we say that a given piece of data may be accessed multiple times by the same consumer within a window of time such that we would leave this data in the cache after it's accessed the first time. Because we expect later access to also use that same data location.And looking at the picture on the right of the slide, it shows an example of the benefits we may expect to see where the host memory within a CPU may have an access latency of around 200 nanoseconds.It may have shared bandwidth shared between all devices or all agents accessing the memory of about 100 gigabytes per second, where if we have a local data cache, we reduce the access latency down to about 10 nanoseconds, somewhere in that range.And we end up with dedicated bandwidth now not shared between all sources, but dedicated to this source of about 100 gigabytes per second.So we get much improved access latency and much improved dedicated bandwidth.

Now looking next step, how does a cache look in a traditional CPU, or I should say a modern CPU?And where does CXL fit within that cache hierarchy?So in the picture on the left, we have an example CPU where we have CPU cores with L1 that are moderate in size, larger L2 that may be shared between multiple CPUs, and yet even larger last level cache that's shared between many CPU cores.And in addition, what CXL is doing is we're introducing allowing devices to directly engage into the cache of the CPU below the last level cache.And in this case, the device is being shown as having a cache that may be about one megabyte in size.This is the range we expect.It can be smaller, but up to about a megabyte in size within the CXL device.And it would be sitting directly as a peer to the CPUs within the CPU socket.Now above the last level cache, the CPUs often implement a private symmetric coherence link that connects between sockets in server CPU systems.So this is a proprietary interface that connects the sockets most often.And then above this proprietary interface, we have what we refer to as a home agent logic.Home agent logic is the logic that sits in front of the memory that's connected to the CPU.And it also resolves conflicts if there are multiple last level caches that are trying to access the same and trying to cache the same address at the same time.Now in this case, we're showing native DDR potentially connected to behind the home agent.And at the same time, we can have the CXL.mem protocol used to connect to memory that may be within a device connected on CXL.And so in this example I'm showing, we have combined both directly connected memory as well as CXL connected memory.And from a home agent point of view, it will manage the coherence for all of this memory in the CXL context.

So next slide.Cache consistency.Cache consistency is a concept where we need to make sure that updates that are done within the cache are visible to all other agents.So how do we do that?The normal mechanism when there's an update to a cache line, we invalidate all the peer caches prior to that update.So we make sure all the caches are don't-- are empty of that line.We update the line and make sure that that line is then at a point where it can be visible to all other caches.This can be managed either through software or hardware.The CXL assumes a hardware coherence mechanism.Now, we define a point of global observation where the new data is visible-- where new data is visible from the right.And we track-- the tracking granularity for consistency in hardware coherence mechanisms is called a cache line.And this happens to be 64 bytes in CXL.And should also note here that all addresses are assumed to be host physical address in CXL and-- in CXL cache and memory protocols.Translations from virtual addresses are done through existing address translation services from PCI Express definition.

So how do we implement this consistency policy?We use protocol states referred to as M, E, S, and I.You'll hear this referred to as the MESI protocol.And these are individual states of given cache lines in the cache hierarchy.Those going through each of these modified the M state can only be in one cache.It can be read or written.But this data is not up to date with respect to what's in memory.So it's modified with respect to memory, and it's only in one cache.Exclusive is similar to modified in that it's only in one cache.It can be read or written.But the difference is that it is up to date with respect to what's in memory.Now, shared state is a protocol that can be in all caches, it can only be read.And the data is also up to date with respect to memory.So memory-- so this-- we'll talk about the attributes of that in a minute.And then finally, we have invalid state.This means that the data is not in the cache.Now, these states are tracked within a cache for each of the different states.These states are tracked within a cache for each cache line address.And cache line address in CXL is address bits 51 down to 6.This aligns the cache to the-- this is aligned to the cache line size of 64 bytes.And we should note that each level of the CPU cache hierarchy follows the MESI protocol.The layers above must be consistent.I'm not going to talk about how this consistency is maintained, but it should be noted that this is how it does work this way.Implementations may have extended states and flows, but those aren't part of the CXL definition.So I'm leaving those out of this discussion.

Now, how are peer caches managed in a cache coherent system and specifically within CXL?The home agent logic is the logic that will know about all the peer caches.A given cache source doesn't know about peer caches directly.It only knows that it needs to check with the home agent to find out what state it's allowed to have.So what the home agent will do is it will use messages we call snoops to check on the cache state in the caches that it's responsible for.Now, and those snoops may also cause cache state changes.So just quickly summarizing the three snoop types we have in CXL, we have a Snoop Invalidate.This causes the cache to downgrade to I state, and the cache must return modified data if it had modified data.Snoop Data message, this causes the cache to degrade to shared state, and it must return any modified data as well, similar to the snoop invalidate.Snoop Current, this doesn't cause any cache state change.This is saying the home wants to know the current state in the cache.So it doesn't cause any cache state change, but the cache must return modified data if it has it.So in all cases, caches return modified data, and the snoop differences cause different state changes within the cache itself.

OK, now getting into the specifics of the CXL caching protocol.

The summary is that the protocol has 15 different request types that are reads and writes from the device to host memory.Our goal is to keep complexity of global coherence management to the host.So from a device point of view, it's just going to know that there is a home agent that's going to resolve coherence, or a host that's going to resolve coherence.But the device will only need to worry about the contents of his cache currently, and just send requests to the host.

We implement this with three channels in each direction on in the CXL cache protocol.We refer to those directions as D2H, or device to host, and host to device, H2D.Both directions have data and response channels, which are pre-allocated.From a protocol point of view, pre-allocated means they're guaranteed to sync within that source.And the other channels are the D2H Request channel, which is from the device to the host.This is where device will issue the requests that we'll be talking about to get cacheable access, either through reading or writing to host memory.And then we have H2D Requests.These are from the host.And this channel is where the snoops that we talked about on prior slides are sent.And these are from the host.Now, we do have one ordering rule within these channels.The rule is that H2D Requests, snoops, push H2D Responses, really specific responses.And they do so for the same cache line address.I'm not going to get into too many details around this.Just know that there is one ordering rule, and that aside from this ordering rule, all other messages are unordered.

OK, so now I'll start using a diagram, sometimes referred to as a ladder diagram or a Feynman diagram, to show how the messages flow over time.You'll see that in the x-axis, we have the different agent types.We have the CXL device.We have peer caches.We have the home agent and the host.We have a memory controller.And then in the y-axis, we have time.So how these transactions flow with respect to time will flow down the graph.

So looking at a read flow. In this example, we have the CXL device issuing read shared to the home.The home is responsible for managing coherence between all the caches.And in this case, the read shared comes in, and the home knows that it needs to snoop the peer cache. Because the peer cache has cache state, in this case, E state.So it sends the snoop data.The peer cache will transition its cache from E state to shared state.And once that transition has happened, it sends the response back to the home, indicating that it has transitioned to shared state.After the home has collected its snoop responses, it will then send-- oh, and I should mention, in parallel, it sent read data-- or I'm sorry, mem read to the memory controller.This memory controller could be a native memory controller or a CXL attached memory device, as we showed in the prior diagram.That memory controller will return data to the home agent.Once the home has gotten both the data and the snoop responses, it will send the data back to the CXL device.And it also includes a separate response message that's a go S state.So it's basically saying that you have shared state and global observation of that data available.And that allows the CXL device to transition his cache to shared state.Go S is the shared indication.

OK, so mapping that back, those agents, back to the CPU cache hierarchy.I want to point out where those agents would have existed in this cache hierarchy.At the bottom, we have the CXL device itself.That's generating the requests.

And then where's the home agent?The home agent location is dependent on the address that's being read.And that home agent will be associated with a given host physical address location.But that home agent logic could be within the local socket.It could be on a peer socket. Any peer socket.And then finally, the memory controllers, the memory controllers themselves could be native DDR4 memory.It could be native DDR within the local socket, native on the remote socket.Could be cxl.mem connected in the local socket or on the remote socket.Any location is possible.And location depends on the address that's being accessed.

OK, so getting to the next request type, we'll look at a write.So writes with a cache are normally done in three phases.We have the ownership phase, where we're getting on cache ownership of the line, exclusive or modified state.Then the device will do the silent write within the cache.And then at some point after that, the cache will be evicted.So let's go through how each of those flows look.I should also mention we have this legend at the bottom, just to remind you that we have the four cache states we talked about, modified, exclusive, shared.And we have these green x and orange x to indicate tracker allocation in the devices and home and tracker deallocation.

OK, so the ownership phase, the cxl device issues a read-own message.This is one of the read types that's looking to get exclusive state for the cache line.The home logic will get this request, again, issue the read to the memory controller, getting data back from a memory controller.But in parallel with that, it can do snoops to the peer caches that may have a copy.In this case, we're showing one peer cache that's in S state.So we send the snoop inval.Snoop inval causes that peer cache to downgrade its cache state from shared to invalid, sending RspI HitSE back to the home.Once the home has the snoop responses and the data, it again sends, in this case, GO-E message. Telling the device it's allowed to go to exclusive state and returning data to that device.

Now, the write phase, we don't see any messages show up in the diagram.We will see a silent write that causes the cache to go from exclusive to modified, updating the cache with the new data.

And then at some point in the future, we will evict the data from the cache.We issue DirtyEvict to the home agent.Home agent will send a GO_WritePull message asking for the data for that dirty eviction.The CXL device on getting the GO_WritePull will cause its cache to go from M to I state and return the data that it had, the M state data, back to the home.And then within the memory controller itself, we will see the memory write and completion back to cause deallocation in the home.

And then final example I have for CXL.cache is what we refer to as-- or what I refer to as streaming write transactions.These are write transactions from a CXL device where it wants to do kind of a single step write, where it wants the home to handle getting the ownership and write as a single flow rather than having to allocate space in its cache.And in this example, we will see that the request is called ItoMWr.This is sent from the device to the home.Home again resolves coherence in the same way it did before.And in this example, we don't see any read access on that initial ownership phase. Because it's not asking for the data itself.It's just telling the home it wants to do a full cache line write.So the snoops occur, basically making sure that all peer caches are invalidated.Once that's done, the home agent sends the Go_WritePull message to the device.This tells the device two things.One, that the data that it has is guaranteed to be globally observed by any future access.And second, that it wants the data to be sent.So after it gets this message, it will send the data.In this example, I'm showing that the home agent will commit the data, write it to the memory controller, and get the completion.Now, I should note that optionally, implementations may choose to install this data within the last level cache of the socket rather than flushing the data to the memory controller.That's an optional way to go.I was just trying to keep this flow simple.I wasn't showing the last level cache.And also important to note that we rely on the completion to indicate ordering.Now, a CXL device that doesn't have a lot of strict order traffic can make use of this flow and get high bandwidth.But if the CXL device has strict ordering on its transactions, it would have to do these one at a time before the next order transaction could go.And that may cause reduced bandwidth versus the prior flow where we were getting ownership and then committing the write within the cache directly.So this flow has trade-offs.Obviously, the data doesn't end up in the cache of the device either.So if there was reuse possibility, you wouldn't want to use this flow.But this could be used for bulk writes from a device to home in a way that's efficient and doesn't cause a lot of cache pollution in the device's cache.

OK, and this slide is just quickly summarizing the 15 request types that we have in CXL.We break them up into-- or at least in this case, I'm breaking them up into four categories-- regular reads that get data returned.We have RdShared, RdCurr, RdOwn.I think these ones are pretty self-explanatory.Well, RdShared and RdOwn are pretty self-explanatory trying to get E-state.RdCurr is trying to get no cache state.It's trying to get a current snapshot of the state of the line in the host, but doesn't want it-- wants to leave the cache in I-state.And then RdAny is saying that the device wants to get cache state and shared exclusive or modified.So really, any of the valid cache states it's happy with when it issues this RdAny.And it's up to the host to decide preference.The next category we have is Read-0.So Read-0 is an indication that this category of accesses don't get data return.They just have cache state that changes in the host.So RdownNoData and CLFlush both result in the host-- well, RdownNoData gives the device E-state.CLFlush would give the device-- it flushes the cache hierarchy in the host, but doesn't return any cache state to the device.And then finally, CacheFlushed is an indication that the device's cache is completely flushed of all addresses.The first two are specific to cache lines as CacheFlushed command applies to the whole cache.Now, for write types, these are writes where we had modified state within-- or I'm sorry, we had cache state within the cache.DirtyEvict would indicate we had modified state that we're trying to evict from the cache, go to I-state.CleanEvict was saying we had a clean state within the cache. Could have been exclusive or shared that we're evicting.CleanEvictNoData is saying we're evicting state from the cache, but we're not going to send it to the host.And then the final types are streaming writes.We have four primary categories for these.I won't go through the nuances of each of them, but all are similar in that streaming writes are, from a device point of view, trying to commit data into the host, but not keeping cache data in the device itself.

Now we're going to switch to the CXL memory protocol.

So the CXL memory protocol provides simple reads and writes from the host to memory.The goal is to be memory technology independent.So it can be HBM, DDR, or some alternative memory technology.CXL memory protocol doesn't have any expectation there.We have architected hooks to manage persistence of memory for memory that may be used as storage.So those have been added into the CXL 2.0 definition.We also have included two bits of metastate for every cache line.In memory-only devices, this is up to the host to define the usage of those two bits.For accelerators, the host encodes the required cache state that the host is requesting into those two bits.So two ways that those bits are used, either for host-specific use or in the accelerator context for providing some indication of cache state that the host will be using.

The channels the memory protocol provides are basically two channels in each direction.From the host side, or in this case, we use the term master and subordinate.So M to S direction, master to subordinate direction, we have requests and requests with data, two channels.And then in the other direction, subordinate to master, we have non-data response, NDR, and data response, DRS, which these channels are pre-allocated and guaranteed to sync in the host.Memory protocol has no ordering expected.With one exception, and that exception applies only for accelerator type use cases, and it applies on the request channel.I'm not going to get too deep into this ordering rule, but just know that there is one exception, one place where the M to S request channel has ordering, but that only applies to accelerators, and protocol is unordered with that exception.

OK, so now I'll walk through a flow diagram for example one, which is a write.Should also note, and this will be shown in this flow, that ECC is handled within the device.It's not exposed to the host in CXL memory protocol.So in this, looking at this example, we have the host sending memory write transaction.It includes info about the meta field indication, tells the device that it needs to update the meta value. The meta state, and the meta value is the two bit encoding itself.Now, the CXL memory device in this case will get this memory access that includes the 64 byte cache line of data, and it will do the ECC calculation that's required for the memory media that's maybe specific to the media, but not exposed to the host. And it will also store the meta state information.In this case, the value that was passed is three.The CXL memory device can return the completion as soon as it can guarantee that the data will be visible to future reads.So this may happen early before the data is actually fully committed to the media.And then you can see in this example, the write happens to the media, the completion occurs in the CXL memory device. And during this time window, the CXL memory device has to be-- after it's sent to completion, it has to make sure that any read that were to arrive is guaranteed to see the data that was written.So it has to do any necessary conflict checking or checking to make sure that that data is correct.

Next example is a read case.So in this example, we're going to show that the read is going to include a meta value.When the meta value is included on the read request, it is a new meta value.In the read completion that's returned to the host, we get the meta value that was the old meta value that was stored in the media before the new value took effect.So looking through what happens in the memory device in this case, we get the memory read from CXL.Device will do memory read to the media.It gets the data back with the ECC, does any necessary corrections and error checking within the media device, then returns the data along with the meta value, the old meta value, back to the host.And because we included a new meta value, the memory device in this case will have to do a write.And I'm showing an example case here where the meta value is stored with the data.So it read the data, and it's now going to write it back with new meta state to the media and complete that write.And this write data after the data has been returned to the host, that write data should be also visible to future reads.So any read that came in after this data was sent needs to also observe the new meta value that's stored with this write.

Now, showing one more example where the host doesn't want to update the meta value.In this case, we have a field that says the meta field is a no op, meaning it does not want to update.So in this read, it's very similar to the prior read.It causes read to the media.Data comes back from the memory device, does the ECC stuff with the meta value equal to.The data is returned, but there's no update necessary in this example.So exactly the same as prior, except there's no media update with the meta state.

CXL.mem also includes in this example a way to read and update the meta state without returning the data itself.So this allows if the host needs to either update the meta state or read the meta state, gives it a mechanism to do that without burning bandwidth on the data return itself.So you can see very similar to the prior flow where the device receives a request with a new meta value indication.Device would read the media, get the data and ECC back, get the meta state back.And in this case, once we get the meta state back, we're going to return the old meta value.In the completion message, this completion message doesn't have data, it goes on the NDR channel.And the memory device then will turn around and update the meta value with the new value that was indicated.

OK, so now I'll be switching gears to show how accelerators can use the CXL.mem protocols together.

And when we mix the protocols, these are for devices that want access to coherent memory in the host and also want access to the memory that's directly attached to the device.Now, the host manages kind of the global coherence.But in this sort of environment where-- or in this sort of use model where the accelerator has memory directly connected to it, the accelerator will take some responsibility for managing coherence with the host for the memory that is in the device.And so we refer to this memory as the device attached memory.So how can a device directly read this memory without violating coherence?Well, it has to take some responsibility for managing the coherence if we want to enable this ability for a device to directly read its own memory.And we believe this is a very valuable attribute.But we still don't want the device to be exposed to managing coherence across the whole of the coherence domain of the CPU host.We want the host to be responsible for managing the scale of the coherence that is within the host.So we want to keep it as simple as possible from the device point of view.But we still want to allow the device to directly read its device attached memory without violating coherence.

So how do we do that?We implement something we refer to as a bias table.This bias table is basically a one bit state indicating if the host potentially has a cache copy or not.So the spec will refer to this as device bias, meaning that there is no host caching.And we allow reads to go directly to the device attached memory without checking with the host.In the case where the bit indicates host bias, the host may have a cache copy.So we have to send a read basically up through the host to check the host's caches.The host is still tracking which of the peer caches may have a copy.So this is really just, has the host ever had a cache copy of this line since the device read it is really what this is tracking.

And now let's look at a couple other differences for this device attached memory use case.We also allow the host to send a snoop indication with every CXL.mem request that's sent to the device.This basically lets the host do snooping and reading of memory in the device kind of in one request.Avoids having to, it avoids two things.One, having to snoop the device's cache for the device attached memory region.And also it allows the host not to, because the device may be caching its device attached memory without the host's knowledge when it's in device bias, it allows those to avoid ever having to track device caching of the device attached memory.So the host can basically attach a snoop every time it reads to tell the device how to check its cache and what cache state it's going to be needing when it does a memory access.Now, the other indication that we added to the CXL, to the flows here is we both reads and writes from device to the host for this device attached memory region.We'll return a forward indication from the host.Instead, basically allowing the device to directly read its memory after it sees this read.And I'll show a flow of this in a second.But the important attribute here is to avoid, hey, when the device wants to read its own memory, it sends a read to the host.The host then would, we don't want the device, the host to have to read the data from the device and then send the data back.That would be wasteful in terms of bandwidth ping-ponging data from device to the host and then back to the device when that data is directly in the device already.

So showing an example of this, we have a host bias read where the device is going to read, he wants to read memory that's the device attached memory region, the memory region that he owns.But it sees that the bias bit is set to host bias.So it's going to send a read own to the host, telling the host that it wants a copy of this line.Now, the host, just like in a read to regular host memory, handles the snooping of any peer caches that is necessary.So it snoops the peer cache in this example.But the host also recognized that this read came from device's own device attached memory region.So instead of reading the memory controller, it sends this message that is called the MemRdFwd message, telling the CXL memory device, go ahead and read your own memory.I've resolved coherence.And in this example, I showed the memory coming from the device to the internal memory controller directly.And the memory controller receives this memory forward, and it resolves the data.And only after it got the memory forward and the data from the media would it then send the data back to the internal caching structures of the CXL device and transition the state from I to E.And after this, we know because it's an exclusive state within the device, we know that the bias inherently has to be device bias.If it's exclusive in the device, the host can't have a copy anymore.So bias would be allowed to go to device bias.

And just to show the example of when device does a read and we find the bias already in device bias, this is an indication that the device is allowed to directly read its own memory without checking with the host when it's in device bias.So it would read memory, get the data back from its internal memory controller, and its cache can go from I to E state.And similar, when a device cache has written data within the cache and that data is modified, it's inherently in device bias.So the device is allowed to write the data directly to its internal memory controller with no indication to the host and transition its cache state from M to I.

Now, for a streaming write case, this is where-- and specifically the streaming write case where the bias is in host bias.The device doesn't have cache state, but it wants to write memory.So we allow the device-- similar to the read, it would issue the standard write message to the host.The host streaming write message, in this case, the WOWr, weekly ordered write.The home agent will resolve coherence.In this case, one peer cache had shared state.It resolves coherence by invalidating all the peer caches and then sends this MemRdFwd message back to the device. At which point the device is allowed to do the write to its internal memory controller and complete it.And in this example, again, at the end of this flow, we know that the host had been invalidated.So the bias can be switched to device bias.

And just showing the counter example again, where we started out with bias and device bias, the device would see that, wouldn't need to check with the host at all, so it could just do a direct write to its internal memory controller and complete without any message showing up on the CXL interface.

So that's the end of my presentation.Thank you.

And we do have a plug here for Compute Express Link website.We have training videos on the website.We also have them on the CXL Consortium YouTube channel, also on LinkedIn and Twitter.Thanks for your time.
