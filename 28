
Good morning everybody, can you guys hear me in the back?So let us get started with the tutorial, Compute Express link which as we all know it is an open industry standard for composable computing.It is a small group here, please move forward, ask questions whenever you have.There are four of us here that we will walk through the material.So with that let us get rolling here.

So this is the consortium 250 plus member companies, you can read the board members names out up there.So you know very successful as a consortium.

We started in March of 2019 when we had the 1.0 spec, then September 2019 we incorporated, we also did a 1.1 spec on behalf of the consortium.November of 2020 we released the 2.0 specification and we will talk about you know what came in each and every one of these specification release and August of last year just before the FMS we released the 3.0 specification.

My name is Debendra Das Sharma and we will have each and every one of the speakers introduce themselves as they come up.You know Mahesh Wagh will go after me.I work for Intel, I am an Intel senior fellow and I am also co-chair of the CXL board technical task force.I know that's a mouthful.So generally work in the area of IO technologies, PCI Express, CXL, UCIE which is the tutorial that's coming up next among other things.You know I will do the easiest part, introduction to CXL and these guys will do all the heavy lifting.Mahesh will talk about protocols, memory pooling and sharing.Mahesh Natu, the other Mahesh will talk about software and error management and Vince will talk about fabric and fabric management and also control.

So here is the industry landscape today right.I mean we all know that there is the proliferation of cloud computing, AI and you know analytics.There is a huge demand for compute and communications from there.Network and edge is getting you know becoming more and more on the clouds.You want to do more processing over there.So all of these are causing an insatiable demand for compute.So once you have the insatiable demand for compute not just homogeneous type of compute but heterogeneous type of compute right.So you are having people deploying everything that they can possibly to solve the types of problems they have whether it is CPUs, GPUs, FPGAs, you know purpose built ASICs right.Everything that you can think of right the problems are big enough that people are deploying solutions to solve that problem.So more of those demand means that we need more memory bandwidth, we need more memory capacity, you need to feed the beast, you need more IO bandwidth right.So all of those things.So we are in this virtuous cycle of innovation and that is the context in which CXL came into being.

So CXL started off as a you know breakthrough for effectively and we will talk a little bit more about that.You know we just wanted to build something on top of PCI Express infrastructure.There were four sets of industry challenges that CXL solves and we will talk about them.By the way we have a tutorial if you are interested it is an archive you can it goes through a lot of the details of CXL.But basically we said that build on PCI infrastructure it is ubiquitous so you can take advantage of the infrastructure but fundamentally we overlaid coherency and memory protocol on top of PCI Express and delivered as a open industry standard.

Now let us talk about that particular approach.So fundamentally as I said so PCI Express if you look at the picture here you have a host processor, direct connect model with an accelerator and you have PCI Express which is effectively the CXL.io.So it is the PCIE5 all the CXL.io everything is more or less PCI Express you know we just leverage that infrastructure.On top of that we multiplexed two other protocols one is CXL.cache which is for caching smart NICs and the other one is CXL.mem for memory smart NICs.Why did we do that right?So fundamentally the challenges with this heterogeneous compute is effectively today the way in PCI the model works is you have to move your entire data set between a CPU or a GPU between GPU back to the CPU.So everything needs to move right so you carve out a certain task you send out the entire data structure let the accelerator work on that and then sends it back to the CPU.So everything basically it is like a coarse grained data movement model right.A lot of these usages they require more finer grained data movement model where you want to treat the memory as a uniform space right.And couple of things that stand on the way is effectively even though PCI Express is what we call a load store interconnect in the sense that you have direct memory access going across PCI Express.Those accesses are all non-coherent in nature right.So what it means is that devices cannot cache the system memory if there is memory attached to a PCI Express device the CPU or the other devices in the system they cannot cache it.Basically the coherency does not extend beyond that interface.So that is first set of problem that there are four sets of problems that we are looking into with CXL.So first is that heterogeneous compute model.Second is we are all we all know that we are up against the memory wall which is that if you look if you plot the amount of computation that you are getting from the core if you plot the number of cores that you can put in a CPU socket today and if you plot that against the amount of memory bandwidth available per core that is going down.So ideally that should be keeping up or going up right and it is not that we are not improving the memory speeds of course DDR speeds are going up.We are adding more and more DDR channels into the platform.But fundamentally with every DDR channel that we add it is about 300 signal pins and you get about 50 gigabytes per second right.If you look into our CPUs today we started off not too long back with just one memory channel then we went to 2 then we went to 4 then to 6 to 8 you know on and on and on like you know 16 it is still there right.You know there is and with the DRAM being placed so close to the CPU it is a thermal challenge to cool those things right.So that is the other part of the problem and then the corollary to that problem is that the cost of memory in terms of per gigabit that has flattened.So as you need more memory, memory is now becoming an increasing part of the overall system cost right.Third aspect which will I will introduce the third and the fourth when the time comes.So first these two right so these are the direct attached model that we solved with CXL.The other aspect of it so what we need is then 2 additional protocols.One is of course .cache so that devices can cache the system memory if they have a need for it and the other aspect of it is .mem which is the memory protocol which means that memory attached on CXL can be mapped into the system coherent memory space right.Now if you put caching smart NICs and memory smart NICs then of course you know your latency is need to be lower right.You cannot have very high latency and have these smart NICs and expect that the system performance will be will not be impacted right.And of course you know how low of a latency well ideally what we said is that your latency needs to be similar to that of nearby CPU sockets memory.So just like a new my right I mean so imagine you are in a multi processing system you have a certain latency if the memory is attached DDR is attached to your local CPU socket which is about a 100 nanoseconds.Again we will talk in round numbers here so that we do not have to give somebody some in particular implementations number.If you go to the nearby socket let us say it is a 200 nanoseconds right another 100 nanoseconds.So as long as the access looks similar to that of a new my node we know how to handle that right.So that is basically where the low latency requirements come from and last but not the least is asymmetric complexity and by that what we mean is in this picture if you look at it there is the notion of a home agent.What is a home agent is basically something that is very detailedly involved with the internal micro architecture but it orchestrates cache coherency between for example different caching agents in the CPU.For example any CPU today is got multiple cores multiple levels of caches right.So it is orchestrating cache coherency even for your non-coherent protocol like .io behind it there is a caching agent because somebody needs to make sure that you know your cacheable system memory looks consistent with the non-cacheable view from the device point of view right.So that is a given in any given system today right.Everybody has a home agent that is responsible for orchestrating cache coherency for resolving conflict between different coherency agents.So what we said is that the home agent functionality stays with the host processor and what happens is on the device side you get a very simple set of you know literally a dozen set of instructions which is like I want to read this cache line shared I want to read it exclusive right and any time I do a right anytime I have updated that cache line I want to do a write back a CPU can send me a snoop but then beyond that the devices do not know anything about orchestrating cache coherency.So asymmetric complexity helps ease that burden right on that entire system and this is something that if you look into the home agent functionality even for the same company over multiple generations of processors they change quite a bit for good reasons right because again your underlying micro architecture changes so things do change and this way we isolate the devices from you know the wide variety of home agents that are going to be out there and you know different architectures do things very differently even within the same architectures things get done very differently at different points of time.

So that's the fundamental approach of CXL and here are some of the representative usage cases and you know we call them as three types of devices type 1 type 2 and type 3 I am glad that yeah so type 1 type 2 type 3 that's basically what you have up there we are not marketing people so that's the most creative names we come up with you know type 1's are basically your caching devices and accelerators and fundamentally they have the .io which is the PCI Express like protocol everybody has that and then you got your .cache which is the caching smart NICs that's built into the CXL protocol right and the usages are things like accelerators smart NIC so things like for example if you are doing partition global address space nick you know there is a certain type of ordering model which is different than PCI Express so you know what happens today is that if that ordering model is different than you know what will happen is if I get a read I have to let the read complete before I am going to do the write because writes can bypass reads in the PCI hierarchy.Now with caching smart NICs I can issue a bunch of prefetches but then execute them in the program order that I desire in my local cache which has got a much lower latency right so that's basically a way for me to get better bandwidth same thing you can do with atomics like you can do a lot of very advanced atomics without having to you know make sure that your flavor of atomics is there in each and every one of the CPU once you get locally into your cache then you can do whatever atomics you want to do like you know double precision floating point atomics or whatever other type of atomics you want to innovate on.Second type of usage is around the accelerators with memory so these are things like your GPGPU all those things where not only are you know you want to cache the system memory but also you have memory that's attached to the device side and you want that memory to be mapped into the system coherent space so that way you know you can do the proper placement of data depending on who is going to use the most amount of data but then everybody can see that data without having to move all the data back and forth.Last but not the least and this is where we see a lot of interest is around the memory buffers which is type 3 devices like as I mentioned we are up all against the memory wall so memory bandwidth is not increasing now imagine if you can put memory on the SerDes based ports like PCI Express and that gets mapped into the system coherent space now remember 300 signal pins for roughly 50 gigabytes per second and now contrast that with if you look into you know a PCIe Gen 5 when we first introduced this and now Gen 6 which is twice that amount right you know your amount of bandwidth that you are getting is 128 gigabytes per second both directions for 64 signal pins and if you that's the raw number right and then if you double that with Gen 6 you get 256 right so it's per pin efficiency wise it's just much better to do serial pins now the downside is of course you know nothing beats the latency of a parallel bus okay so you're going to be latency wise at a disadvantage but there is a you know how many 300 pins are you keep are you going to keep on adding into the platform in a thermally constrained environment so the other good thing about the memory buffer here is that you know it is memory technology independent so you can you can have for example DDR5 in the in your main DIMM but you can put DDR4 over here you can put even you know some advanced type of you know more cheaper memory that you can have as a capacity expansion on CXL so you can you can use it as a bandwidth expansion as well as a capacity expansion and you can do your mix and match depending on what you want to do and Mahesh and others will talk a lot more about these 

so that was CXL the first generation right so now let me switch gears and talk about the second generation and this is the time I can introduce the third problem statement that we have and that has to do with stranded resources in a data center by that what what I mean is so let's look into the picture here right so you have the hosts and multiple hosts on the top side it imagine that each host is like a full-blown server right it's like a 2 CPU socket server connected to its own memory right even though it's a small box that's what it's supposed to represent right it's a full-blown server on a chassis by itself connected to its own set of accelerators and with 2.0 you have you basically put a CXL 2.0 switch and on the other side you have a pool of memory or even a pool of accelerators okay now what's the problem with the stranded resources well each and every one of the hosts today right it's a tightly coupled model the load store right so if you put memory next to for example a CPU so let's say I have a 2 socket system for argument's sake I have a terabyte of memory most of the time we find that people don't use that full amount of memory and generally people have to over provision because if you need more memory there is no way for you to generate more memory in the field I know Siamak is working on that but that will take him a few decades probably no just you know jokes aside right because you need to over provision that so but on the other hand if I get a task that's let's say needs 2 terabytes of memory what do I do well I have no choice but to go through page misses and suffer the performance consequences because again I cannot create memory out of thin air right so I got the problem of stranded resources I got the problem that you know I cannot run tasks efficiently that efficiently that do not fit within that profile and you can make the same argument even about accelerators right I have a certain task that might need let's say X number of accelerators I put let's say 8 accelerators let's say in my server and you know somebody needs 16 what are they going to do well you know you just have to make do with 8 right now the challenge that pooling serves helps us overcome is that you have resources that are captive to those posts that doesn't go away but in addition to that you put some additional resources on this other side of the switch and what that does is that whenever there is a need right whenever there is a bandwidth demand or a capacity demand right or a processing demand you can come to that other side of the switch and get the resources that you need from the pool once you're done with that you release it back to the pool so now maybe the memory that you have a terabyte of memory for every server instead of that you can put half a terabyte and then you can put the rest of them over here on the pool side and now you got a better usage of your you know resources that you can do so that's the problem that you know 2.0 solved and not only so then the question is what is the unit right in which these resources move of course you can move the entire thing by a device basis but for memory what we did also was we define things called a multi logical device and you see that another color combination right so you can see like d1 is completely assigned to h1 but d2 is assigned partly to h1 and partly to h3 so you can do different locations as get assigned to different servers up to 16 that's CXL 2.0 so this is the first time that you see a load store kind of tightly coupled interconnect has been used across multiple sets of servers right and there is that are challenges with that of course there is a latency challenge your latency will be higher but like anything else it's a trade off right your latency will be higher and also from the things that are below the switch those are you need to be careful to make sure that you are maintaining isolation between the different hosts let's say host goes down you need to make sure that not everybody else will go down with it because they are pulling some resources right so and we built all of that into the cxl2.0 specification same thing with the quality of service all of those things you need to work through right.

Of course with 2.0 we went with single level of switching 1.0 was a straight direct attach model 2.0 was a switching model that we enabled so this enables for memory expansion and of course connecting more of the devices 

talked about 2.0 enabling resource pooling resource disaggregation and you know effectively we have this managed hot plug flows to move the resources around right so whenever you need more it looks like a hot add effectively it's like online right but it goes through the same process so you add resources once you are done with that you release it back so it's like a remove which is offlining so all of those things we can do and then we also added things like persistence flows with 2.0 for persistent memory we define things called like fabric manager and API which this gentleman will talk about for managing the resources there were security enhancement security is a never ending thing you always keep doing more and more right so authentication and encryption is what we did and effectively as I said right 2.0 is the first time when we went from for a load store interconnect from a node level to a rack level of connectivity.

So far we talked about the three challenges right that we have solved the third one with CXL 2.0 I will talk a little bit about the fourth challenge and so this one is so 3.0 you know what we saw when we started 3.0 is that we have the you know nobody is ever happy with the amount of bandwidth they get okay people will always want to keep doubling paying half the power so that the power remains flat with the same latency right I mean these are all good things and you know the different standards bodies are here to solve those problems and those are hard problems so yeah of course we double the bandwidth on a per pin basis and you know we talked about reduced memory capacity bandwidth per core that was part of the second problem statement so that still remains so that's being helped by the bandwidth doubling and then efficient peer-to-peer resource sharing across multiple domains so this is talking a little bit about the fourth problem and then we talked about memory bottlenecks right so which was the second problem.So fundamentally the problem is the fourth problem is that yes we have pooling right we talked so that's the third problem let's say you know no more stranded resources but then the question is for every access if I go through the CPU then I have a bottleneck there ideally you want things to be distributed and yet that flies on the face of saying hey you know everything is asymmetric in nature so you know you can't have your cake and eat it too well turns out you can so you know we want to this you know only when needed what we want to do is involve the CPU otherwise you want to basically have multiple paths between any pair of source destination nodes and you know doesn't matter whether it is the memory that is mapped into the system memory space or whether it is a device that you are trying to access right you should be able to access them without having to go through the same path right because if you always take a fixed path between source destination where if that forces you to be in a tree topology which we know it's good but it's constraining right you want to go to more of a fabric topology which is the fourth problem statement that CXL 3.0 solves.So what we did is with 3.0 is we introduced fabric capability which is that yes tree topology will be there for people that really need it and it will be there for the initial discovery and all of that but beyond that there is a fabric capability that we introduced we need better improved capability for better scalability and resource utilization in other words single level switching if we can expand that then we can construct larger composable systems we talked about doubling the bandwidth latency being zero add and of course maintain full backward compatibility.

So you can take a look into this this goes through some of the but features right that we will talk about some of these but effectively you will see that there is a set of things that we introduced for the first time in CXL 3.0 we will talk more about that.

So how much time do I have?I am doing good in time.So you know CXL 3.0 we double the bandwidth with the same latency and effectively you know what happens is that going from you know what we have done a lot of analysis NRZ signaling pretty much becomes ineffective beyond you know some will argue 40 gig some will argue 38 gig some will say 36 gig okay fine beyond 32 gig I think it is pretty unanimous that NRZ becomes a challenge right.So when we went with PCIe 6.0 at 64 gig it was PAM4 signaling you know PAM4 is nothing new in the networking world has done it you know they tend to be thankfully at the bleeding edge of technology we are at the leading edge of technology that is how I call things.So you know PAM4 signaling means high bit error rate and you know the thing that we did not want to inherit from the networking side is the high latency of the forward error correction.So if we add 100 nanosecond for FEC latency FEC decoding and all of that for a lot of these applications it is just a non-starter.So you will notice that you know the amount of time we allot for a read to the data coming back or a memory type 3 device that is 80 nanoseconds right I cannot add 100 on each direction to that right and still be viable.So what we did was so there are three pictures here and we can go through this in a lot of detail but I do not want to lose my bet here.So on the top you have the 6.0 flit layout which is you will see that it is a 6 byte FEC which basically it is three levels three independent forward error correcting codes which are interleaved so that way you are taking care of the burst right effectively.So that results in a low latency forward error correction you are correcting a single symbol within each FEC group and then you got 8 bytes of CRC which is fairly it is fairly robust right 8 bytes of CRC guarantees 8 bytes of errors that will be detected and beyond that more than 8 bytes you have a 2 to the power minus 64 is the statistical probability that an aliasing will happen.So with that you know your fit rate is practically 10 to the power minus 10 which is you are going to have that many escapes in a billion hours right which is practically 0.Your silicon has 3 digits 4 digits fit to begin with so this is 10 to the power minus 10 or my in the case of CXL the latency optimized on its minus 3 which is still pretty good.So excuse me so what we did with CXL is that there are two of those right so the second one and then the third one there are two sets of flit layout the first one you will notice is similar to that of PCI 6.0 so it works on a 256 byte granularity and then the one below that is a more optimized version that we did for CXL you will notice that there is 6 bytes of CRC for every 128 byte.So what we do is that you accumulate 128 byte you just do the CRC check if it passes you use the data you do not wait for the next 128 byte.If there is an error then you wait for the next 128 byte you do the forward error correction then you apply the CRC and if it is good you accept it otherwise you go for a replay.So what we are really trying to save here is couple of nanoseconds really literally one of them is of course there is the accumulation time of the entire 256 bytes so with the latency optimized flit that you have a 128 byte accumulation is half the time of a 256 byte accumulation and if you did the math you know with respect to a 68 byte flit at 32 gig or 128 byte flit at 64 gig right accumulation time is slightly better not by whole much but you know fine it is the same as what we had with you know 32 gig right.And then I am not doing any effect for the most part right I am just doing a CRC check how many levels of logic is CRC it is about 6 to 8 levels of logic which is practically nothing for you know doing the thing.So anytime there is no error you are going to accept the packet anytime there is an error you have to now pay for the accumulation latency and then the FEC correction latency which is like I said we are trying to save on those couple of nanoseconds in case FEC needs to be involved ok.And then if that still fails then of course you have to go through the replay ok.So that is basically what we did with CXL 3.0 there are two types of flit is the key take away one of them is the 256 byte standard flit looks very similar to PCI express the other one is a 256 byte flit but with some latency optimization flavor.So that is how we went to 64 gig PAM 4 and kept our latency flat.

Next one is around next one is around the protocol enhancements and there are two simple constructs but these are very powerful constructs.One of them is known as unordered IO and the other one is known as back invalidate.So remember I said about the fourth problem statement was direct access not everything need needing to go through the CPU.So unordered IO fundamentally what it says is that hey I can access memory or I should be allowed to access memory directly by going from the device if it is coherent memory right I can access them directly by going to the place where the memory is if there is a way for me to figure out if that memory is not is being held in a coherent state that is consistent with what I want to do and what I mean by that is for example if I am reading a memory location I should be able to go and access that memory directly without involving the CPU if I am if I know that that memory location is either invalid in the rest of the cases which means nobody has it cast or at most their share in which case I should be able to go and get it right.The only time I can do it and I need to involve the CPU to resolve the coherency is if I want to read a copy but then that is being held as exclusive because I do not know whether that is the latest or the greatest copy right.So unordered IO fundamentally what it says is you should be able to send your IOs and these are IO coherent right these are not they are not allowing you to cast it if you are if you have to cast it then you go through the proper you know CPU hierarchy go through all the resolve the cast coherency but those take quite a bit of bandwidth just to resolve coherency actions but I should be able to go and get that location and I should be able to process it.The question is ok what happens when you go to that memory and let us say the memory location it maintains directory anyway because that is part of CXL it finds that you know it is a couple of directory bits it finds that well somebody might have this location it is not a guarantee that they will have it but they might have this location and we need to resolve your coherency conflict.Well remember these are these can be type 3 devices they do not even participate in cast coherency actions right why does a memory device need to understand anything about cast coherency other than it looked at the directory and said hey I think somebody has it.So what we did was we introduced the notion of this thing called back invalidate which is part of the CXL dot memory flow all that it does is the memory device tells the host processor saying I have this access that is coming on the side and I think there is a conflict why did not you why did not you get into action do something.So then the CPU of course knows how to resolve coherency it is going to do a bunch of things and then tell the memory side that hey now you can go and do the transaction in the meanwhile what has happened is that particular transaction the memory device or type 2 can it can also be a type 2 device right has put it on the side while it is processing other requests and off you go right and the reason you can do that is because these are all unordered.So what happens to the ordering model well the ordering model moves to the source by that what we mean is if it is a read of course you know when you got the read completion so no prizes for guessing that now if it is a write what we did is instead of writes being posted you get a completion back for that write.So that tells you that that write made it and then you can post whatever flag and whatever else you need to do so in other words before you tell the rest of the system that this writes are all committed you have waited for that handshake to come back to you.So those are the two things right very simple construct takes advantage of all the existing mechanisms that we have right same kind of virtual channels same kind of everything right but very powerful construct.Now what it did is it enabled me to do direct access and also it enables me to do fabric topologies right with two simple things we are able to you know remove the cycles of the restrictions of a tree based topology while still maintaining all the architectural ordering smart NICs that we like to do yeah.So let us say your question is what would be a use case for you I/O so let us say that you have a something coming from your nick let us say okay so let us say you got a packet coming in from the nick it is targeting a memory location and you know you might have to do some amount of processing now question is do you want to take all of that into the main CPU and then do the processing or can you do some processing next to the device let us say could be a GPGPU for example that is hosting the memory.So now the NIC it looks at the address it says hey I know where this location is it sends it to the to the proper GPGPU or the type 3 memory device the memory device does the processing and once it is done the processing it basically loads it into the memory.Now contrast that with what would happen today you would send it to the processor it would you are going to queue it up there it is going to go back to that location get that data right it is a lot of movement back and forth you just have removed all of that movement back and forth do the processor will do the processing and then finally do the write back into the memory right so you have just got everything out of it just gone directly now there is a small chance that you know that memory location might be held by somebody in their cache but that is very rare right that it is unlikely that you are writing to something from let us say coming from the nick side that somebody else has it it is possible that they cannot take you can have a still copy but even if it is a 1% 2% case you will still if you do the math you are going to come out way ahead in terms of your bandwidth utilization so that is an example right.And then the other other usage is like I said I can do treat I can do non-tree topologies that is the bigger one right.

So this particular one talks about how to build even for the UIO and the back invalid how can I build you know snoop filters in my type 2 devices.So today for the amount of memory that you are you have mapped into the system memory space right with CXL 1 and 2 you had to have either a directory to track or you limit the number of entries that are part of the system coherency space because otherwise there is no way for you to really invoke something and with with 3.0 what we did was we enable this notion of back invalidate.So you can now have a snoop filter and anytime you are having a capacity miss you can basically issue a back invalidate back to the processor so that your snoop filter is consistent with whatever is there in the processor right.So this just goes through an example flow of that effectively you know it is like a capacity miss you are going to issue a back invalidate.

With 3.0 again we support multiple levels of switching so this is just showing an example of multiple levels of switching and you know the right side one is a tree topology so that is of course supported but then the left one is a non-tree kind of topology so we support that also.

 So multiple devices of all types per root port that is again an expansion of what we support.

The other aspect of CXL 3.0 in addition to the pooling right which we did in 2.0 and this is like pooling at a much more larger level of system we also have this notion of what we call shared coherent memory across multiple domains.So far pooling means that you know each of those locations if you want to look into those right they belong to one given host right in other words no two hosts see the same physical location of memory that is pooling.Now there are lots of applications where you want to have a set of memory locations that are shared between in a coherent manner between multiple hosts so that way you can pass that as semaphores or whatever you can pass data structures back and forth between them if you are working collaboratively on a problem and 3.0 introduced this notion of you know the shared coherent memory again because the fundamental problem is that each of the host is an independent coherency domain.They don't know about other hosts right for example every host thinks that you know address location 0 to whatever belongs to them.They are in charge right and you can't have multiple sets of people thinking that they are in charge of the whole thing right.So it's much easier for on the other hand for a device because we have the back invalidate the device can be told that hey these locations are shared between these hosts.So now it can track them on its own local directory.So anytime there is a conflict between multiple hosts it can just independently issue back invalidates to those hosts and resolve coherency without really orchestrating the coherency conflict directly that way within each and every one of the hosts.So that's basically what we did and we defined this thing called global fabric attached memory that this gentleman will talk about.

I think more of the same stuff let's see here.

So again CXL hardware coherency between is enforced for this shared coherent memory location.

Now 3.0 introduces this notion of as I said fabric right.So what you see in this picture here is a non-tree topology right.In other words between pick any source destination where you got multiple sets of paths and that way you can do better a better job in terms of doing load balancing.You can do a better job in terms of getting more bisection bandwidth right.So all of those things you are going to benefit tremendously.And you know you can basically now use CXL truly to attach to you know have something like you know these switches can be on the they can be leave switches or the spline switches right.You can be on the top of the rack or middle of the rack if especially given that copper cables are about a meter is what they are extending.Maybe you can do it in the middle of the rack right or if you want to do an optical transition you can be few racks away.But fundamentally now what can happen is that all of these are resources right at the bottom right you got accelerators, memory, CPUs all of those you got GFAM.Not only can you pool resources that you can go within your rack or even across racks and get access to it and build composable system right.I can build a system with let us say I got a two socket server I can I need more memory I can go somewhere get the memory attached to it.I need more accelerators I can go you know to another chassis get some amount of accelerators and I can attach to these in a dynamic fashion right.You can do this as a dynamically composable system.I can also have things like an HPC problem where I got multiple independent servers that are working on a problem and then I just have a global fabric attached memory that I can use as a area in which I can pass intermediate results back and forth between all of these different hosts.So that is what CXL 3.0 enables right.So it is as you can see it is a crawl walk run philosophy right.Start with something get the traction you know do the learning.We are still in the learning process by the way.We are not past this thing because if you look into the ecosystem people are people have started deploying CXL and so there will be more learnings and we will incorporate right.We are not any learning organization but you know gradually we are solving these challenges one after the other surely but you know definitely in a timely fashion.

So that brings to the end of my presentation.Any quick questions?

Can I just talk for a second?

Sure.

Thanks I am not part of the show here but I am the chairman of the CXL consortium.But anyway you know Debendra did a great job.I have presented these slides many times but nowhere near as well as he does.But you know I have been involved with CXL from the inception and it has been fun to watch and to help lead.And I just want to make a point that CXL is an open industry consortium.So any company is free to join, able to join I should say.Not necessarily free.But we actually, one thing that is very very unique in this industry is CXL, the adopter tier is free.Most organizations do charge.So we have a free tier actually to be a member of the CXL consortium and get all of the IP benefits that you know these large consortia employ.So if you are not a member, well if you are a member thank you.And if you are not a member you can join.And even at the contributor level where you can work in the groups.It is one of the least expensive.One of the reasons being is because it was very very clear that CXL was going to take off like a rocket.And when you have a small consortium you need to charge a lot of money because there is a fixed cost for any consortium.As it gets larger the cost per company decreases and we saw that happening and everything else.So I will reiterate, Deventra talked about crawl, walk, run.And then we have sprint.And then we have driving cars and then plane streams and automobiles.We are, you know this is going crazy.And it is one of the hottest technologies I think in the industry.So I am glad you are here to learn about it and thank you for coming.Thank you Mahesh.

So quick introduction.My name is Mahesh Wagh.I am a co-chair for the CXL consortium's technical task force.I am an AMD senior fellow so I drive all of the Epic server system architecture and my team drives some of the CXL execution.So we are going to dive deeper into memory pooling and sharing, go into some of the aspects, especially cover some of the things that were introduced in 3.0.

So we will start with, you know I will be focusing more on memory devices and sort of CXL.mem but some of those concepts are applicable to the other device types as well.Quick background.So I think this topic to do justice to that, let us just start with a quick background of what you know an SLD means.This is single logical device, right.This is your memory device.It has got only one link coming into it.It has got memory resources.This is what was defined in CXL 1.1, right.It is a primary function.You can identify that device through CXL, you know DVSEC ID.Lots of, you know you get lots of questions in terms of, you know can this device be a multi-function device and yes.Any of the CXL devices can be multi-function device.You have the non-CXL function map DVSEC that just says what the other capabilities of those devices are.And this device, you know from its definition must support 1.1 so that you have the backwards compatibility.All of your investments into it, you know you recoup your investments by supporting backwards compatibility.And the type 3 device component registers define the HDM.This is the host defined memory, right.So the key aspect is all of the memory media that is behind CXL, you have to map into some memory space, right.As we talked about, you map that memory in the host physical address.So you need a way for the device to understand where that memory is mapped.So you do that using the HDM decoders.And such a device typically is connected to a single virtual hierarchy.What we mean by that is it belongs to a given virtual hierarchy domain and gets mapped into it.So the key things about talking about this device was as the definition was starting for CXL, you had an understanding of how to map CXL memory devices into a given host system address space, right.A lot of infrastructure was getting built on top of that.But at the same time, there were a lot of use cases that said we want to extend this use case.Devendra talked about all of the motivations to get into pooling and sharing.So important aspect was how do you keep something that you have defined about how to map this memory in a system address space and then extend on top of that by adding pooling and sharing, right.So the concepts get started defined by SLD and then we build on top of that.So that was sort of the story behind that.

And then pooling was introduced in 2.0.Basically pooling, think about it this way, you have a device with media.You want to partition that media into, you know, n number of partitions.And then you want to assign that partition exclusively to given host image, in which case you're mapping that partition's memory into a given host images system address space.That's what pooling was about.We defined a device that was called multi logical device, in which case you can take a single device and share it across multiple hosts.And there were smart NICs that were added to the protocol to support that.And we'll go deeper into that from a scalability perspective, it was supporting up to 16 hosts.And you know, if you're attaching an MLD device, then you needed a switch in between, because the switch is what understood that it can connect to multiple different hosts and then pool this device, right.So that was a concept that was introduced with CXL2.0.Now as we were doing this definition, there were a lot of pioneers, a lot of them are in this room, who are looking at it and saying, well, even with 2.0, you can really do pooling, you don't need a switch, you can define devices that could directly attach to a host without requiring a switch.There were also concepts of, hey, if I'm pooling a device, isn't it just a natural extension for me to share a device, right.So all of those things were getting discussed in 2.0, but we had to get the 2.0 spec out, and then look into the developments that are happening in the ecosystem, and bring those concepts that people have already talked about into 3.0, right.

So that sort of starts the stage into, you know, what's 3.0 and yes.We'll talk about that.Yeah.So we'll get more into, but as we get into that, it's really good to ground ourselves on all of the definitions, because now you're getting into the space of what does pooling mean, what does sharing mean, you know, what's the scale, right.It was really important to define some terms, right.So we'll talk a little bit about what those terms are, and then get into that, right.So basically, if you're taking it, looking at a memory resource, and if that resource is connected to multiple hosts, we call such a device or that memory as fabric attached memory, right, which means you have more than one host that are accessing that memory.And that memory is called FAM.We expose that FAM via logical devices, because now you have some resource that's virtualized and handed over to a given domain.So you look at that and say it's FAM, and then every host image is going to have a logical representation of that device.So that's why it's exposed through logical devices.And we talked about scale, you can get up to 16 partitions with, you know, just CXL2.0.What if I want to get to 1000s of partitions, right, and share it across 1000s of nodes, if you want to get to that level of scale, then it's exposed in a more scalable manner, it requires you to have different sorts of routing, which is called port based routing.Vince is going to go talk a lot more about fabrics and how it is.But if you have memory that is supporting scalability at that range, we call that type of memory as GFAM, which is global fabric attachment, right, it's just talking about the scale.Now, when the fabric attached memory is dedicated exclusively to a single host, then we call that particular memory pooled memory or pooled FAM.So pooling means it's a resource that's exclusively mapped into a given host domain, right.An example for that is that S1 that's in partition, and you have that pooled across, I mean, S2, which is pooled across, you know, the host edge two, right, so it has a copy of that.If it's shared FAM, then it's only attached to one host, right.So that's, that's what it is.

Now when we talk about pooling and sharing, especially when we talk about sharing, you also have to talk about the concepts that are important to guarantee the coherency model for that.And you can support sort of two coherency models associated with any resource that you take that is shared between multiple hosts, right.What I mean by that is, you have the same physical address range, right, like the device's physical media mapped into two different host images, and they can concurrently access that memory, right.Now when you have such a thing, you're going to talk about coherency, we have two ways of solving that, one is software coherency, and the other one is hardware coherency.Software managed coherency means that you're managing the ownership of that particular region, through some means that are outside the scope of specification, in which case, you're defining some data structures, some locks, some semaphores that you can say have exclusive access.And then anybody who has an exclusive access can write to that memory, everybody else can read, right.So it requires some very close coordinating at the software level, but you can deploy those models.And you can see, hopefully, in the event, and in the past events, a lot of companies have explored that space and talked about how software coherency can be enabled with devices and systems that are available to them.When we get into the next level, which is, we really want to get into supporting multi-host hardware coherency, right.This is where you're saying, you know, we're not going to rely on software-based coherency mechanisms, we want hardware to provide the guarantee for coherency, then that requires some unique hardware flows.There are some specific flows that you have to go into and make sure that those are required for coherency, right.The shared region must be mapped as HDMDB, right.This is where you map it as HDMH or HDMDB.And then you support the back-end validate channel.So this is where you can, Devinder talked about what BI does.So when you're having something that is multi-host hardware coherency, you need to support BI.And if you are interested in your use model requires you to support direct peer-to-peer, we talked about the advantages of direct peer-to-peer, then you need to support unordered I/Os so that you have access, so that you can get some of the benefits of direct peer-to-peer access.

Yeah, I realized there's an issue in this one.I need to change that diagram.But yeah, S2 should just show, go map to H2, right.In that case, it's just one exclusive copy of that.

Okay, so let's get into then, you know, what is the definition of a multi-logical device, right.And then we'll get into MLDs and other things.So when you look at something that's a multi-logical device, in which case it's a, you know, FAM type 3 device, and it can partition all of its resources, and it can partition up to 16 resources.We call each of those partitions as LDs, logical devices.And you need a fabric manager.And you need a fabric manager, FM owned LD.And the reason for that is, when this device comes up, you need something that comes in and programs this device, programs its partitions, and then allocates it to a given host, right.All of that functionality is done by a fabric manager.And the fabric manager accesses this device through the FM owned LD to go program this particular device.Now, as this device appears in a given systems image, it appears as if it's a type 3 SLD device, right.So the switch does all of the magic in between to map the logical device to a given virtual hierarchy.And then that particular device appears to the host image as an SLD device, which means anytime you're doing a config access, an IO access, an HDM access, at the software level, you won't be able to distinguish between an SLD or, you know, an MLD device, right, that's just exposing its SLD partition, right.The FM owned LD is accessible with a specific logical device ID of all ones, right.So on cxr.io, you send that out on TLP prefix.And that way you can access the FM owned LD.You use the cxr.io path to go program all of the partitions and then configure the MLD device.The MLD link, so how do you know this device is an MLD device?So when you go through the auto protocol negotiation as part of that, you advertise that you're MLD capable, and then you negotiate the MLD operation.So now you know that you're an MLD device.One of the questions that I get usually in this particular section is, you know, after you configure, how do you know are all the accesses done with the LD ID?And the answer is yes.Once you negotiate to operate in an MLD, then all your accesses, whether it's cxr.io or if it is cxr.mem, then you're carrying the logical ID on all of your transactions so that the device can identify that.And same thing with cxr.io, you have to carry that in your cxr.io so that the appropriate logical device can access it.So that's how these things are carried forward.

Now we'll introduce the cxr3.0, a lot of things that were possible in the ecosystem, we wanted to make sure that the specification covers it, which is if you're thinking about the shared or pool device, one way to think about it is like what we talked about the MLD, right?You have one link and then through that link, you're accessing this device and you're partitioning this device, right?But you could think about just saying I can expose these multiple heads to, you know, to a given host directly and I don't need to put a switch in between.You can save some amount of latency by doing a system like that, right?So in that case, you have multiple links coming out of the device, each link directly attached to a given host, and then you pool the device, right?Or share that device between those hosts.Again, the same concepts apply, your partitions would still be available to each host.From a host perspective, it would look like you're connected to an SLD from your image perspective.A couple of things that were added to this was there was a LD pool CCI, there's a command interface that was defined so that the fabric manager can identify that and then go program these devices, right?So as you can see, now we've introduced these concepts of I have an MLD, which is single link and I can partition the resources behind it or I can give you multiple links, right?

From a multi-headed device perspective, this is just getting into more aspects of it.Just to illustrate, right, you have, as an example, four links coming out of the device, you have a host physical address, each of those links will have its associated HDM decoders, and then you have memory resources that you partitioned and assigned to a given host, right?So if it's a pooling example, this is a four ported CXL memory device, it got four links.And then in this example, very clearly allocating those pools across the four hosts, right?But you can allocate in any fashion that you want, depending on the needs of the system.

Now there is this concept of a multi-headed SLD.And now this is adding more terms, beyond terms, in terms of what's possible, what are the systems you can build, right?And then there is a multi-headed MLD device, right?So if it's MH-SLD, then in that case, you have multiple heads, each head has a logical device associated to it, and then you're connecting to a FAM, right?And heads can directly attach to the hosts, right?So that's a multi-headed device, MH-SLD.

Now if it's a MH-MLD, this is a multi-headed, multi-logical device, in which case, you have those single heads, right, single ports, and then each of the links that you form with that port can be an MLD device, right?So in this example, you have a head zero connected to logical device zero, it's an SLD, right, MH-SLD sort of thing.But head one has three logical devices, and it's looking like a multi-logical device, right, because it's got three partitions in it, right?And that's where you can connect that device.So so on and so forth.And you know, if you're using a device like this, then it requires a switch for that MLD functionality, right?So any link that you have, that's an MLD link, you will have to connect to a switch because the switch is the one that understands MLD.The other ports, they could directly connect, you know, if it's MH-SLD port, then that can directly attach to a host, because all it understands is there's one logical device behind it, right?So that's sort of the way to think about these devices and think about how you can construct one, right?

The question is, how do you manage the multi-headed devices, right?A lot of work was done for using the MLD framework, right?So MLD framework was defined in 2.0 that just said, you know, you understand how multi-logical devices are, you understand the partitions, and then you have ways to manage and map those partitions.So that same thing was extended with this LD pool CCI.So with that LD pool CCI, you can identify the logical devices and then do the mapping of the logical devices to the associated heads, right?So I think in the programming section and in the fabric section, we might touch on some aspects of that.

And then finally, we'll come to an example of a GFAM device, right?This is what we talked about.You want to get the scalability really high, you have a memory resource, and you want this resource to be accessed with many, many nodes, right?How do you enable that?In that case, you need a fabric.And Vince is going to talk about how this fabric is formed, what's the scale of that fabric, right?And the same concepts apply.You can have this GFD device that could be single host mapped, in which case, you know, some portions of its memory, you're exclusively mapping.And it could be multi-host shared, right?In which case, that same memory region can be allocated and shared across multiple hosts.The same concepts, you know, peer-to-peer access, if you want to provide access to this from a peer device, you can support UIO getting to this device.There are some unique differences between how a GFD device appears versus, you know, the other GFAM devices, right?A GFD configuration space is only accessed by FM.And the memory capacity for a GFD device can exclusively be managed using dynamic capacity mechanism.So we'll touch a little bit about the dynamic capacity management, how it is used, and so on and so forth.A lot of host client management of GFDs, it's expected to be covered in future ECNs.We should see some of that come up with the upcoming CXL 3.1.So today, we're not going to go a lot into the GFD management, but just wanted to introduce the concept of what a GFD device is from its scalability perspective.And the key thing is, if you're a GFD device, you're using a different routing mechanism, you're using port-based routing to access the device, right?So that's what it is.

At a high level, what is the differences between, you know, an LDFAM and a GFAM, right?If you look into it, there is the introduction of what's called a device media partition.So you take a device as GFD, you take its media and divide it into partitions, and then you map those partitions into the host physical address.And then within those dynamic capacity regions, your device physical address, you uniquely tag it as either it's shared or it is, you know, exclusive to a given host domain.And that way you can map it to a host domain.I won't read through all of this table, it's there in the spec.But just some differences there like with LDFAM, you know, a given device, how many hosts can you support up to 16?GFAM gives you, you know, thousands architecturally, probably hundreds from a realistic implementation perspective.And then there are some other differences in there in terms of just how that memory is done.I think I touched on the routing and address decoders, right?That's you have to support, you know, port-based routing if you're using GFAM versus the other schemes that are defined in CXL2 data applied to the LDFAM devices.

We'll touch on Fabric Manager as we look at, you know, any device that is pooled or shared, you need some resource that is coming in and configuring those devices, right?That resource is typically a Fabric Manager.And that connects to a Fabric Manager endpoint in the switch or a device.Depending on the type of device, you'll need one connection.Definitely a switch that is supporting pooling and sharing will also require that.The functions of the Fabric Manager are to identify pooled devices and, you know, shared resources, and then map them on to a given sort of virtual hierarchy within the switch, right?So all of that function of binding, you know, resources to a given virtual hierarchy happens through the Fabric Manager.There are FMAPIs that are defined, and it's media agnostic in the sense that we use MCTP.So you can get to that Fabric endpoint through different media, right?It could be, you know, USB, BMC, SMBus, internal, right?Many different ways of accessing that.In general, you're looking at this for setup and management of that.So I think we've said it need to be performant, in which case, you know, it cannot be horrendously slow, but you really don't requiring it to be, you know, using it, you know, highly performant because these are tasks that you're using infrequently for memory, either allocation or management of that device, right?

I think I touched on most aspects of it.We have some examples that we will go through that talk about, you know, the Fabric Manager and some aspects of how, you know, memory pooling and sharing can work with the FM involved in it, right?

Yeah, I think that is something that is there is a lot of work that is ongoing, right?In terms of what FM is.I don't know if Vincy can share about any general public.

Yeah, yeah, there is a, as part of the Linux community, there is work underway.There's actually a talk tomorrow afternoon from someone at ByteDance that's been spending a lot of time implementing an open source Fabric Manager.Yeah.

All right, so we'll go through some examples of all of the different device types that we talked about, and then how does that work, right?So in this example, we're showing, you know, you have a CXL3 data switch.Just simplistically in this case, you know, it was a type three device that was, you know, solely all of its resources were allocated to host two, right?And then the host two, you know, notifies to the FM that, you know, it doesn't need the memory anymore.So it just says that D4 memory is no longer needed.And you know, you're relinquishing your resources, right?

The Fabric Manager will then, using the FM API, talk to the switch, tells the switch to unbind that D4, right?So in which case it says, you know, you have memory that was connected to host two resources mapped in that virtual hierarchy.You know, it says, you know, do a hot remove of that device, of that memory, and send the notification to host two that, you know, the memory is offline, right?So there's a lot of work that is ongoing, even with CXL2.0 in terms of memory online, offline.So use those flows to notify the host that, you know, you've offlined the memory.

Then the switch, FM tells the switch that, you know, it wants to bind that D4 resource to a host one.So it notifies the host one to hot add H1, right?So it's hot add this D4 memory, and then H1 enumerates, finds its memory, and then configures accesses to D4, right?So that's a way to dynamically unbind and then bind that memory to a different host.

The same concepts apply now if you have sort of a pooled memory, right?And in this example, host two is connected, has memory mapped in D4 and D2.H2 notifies that some D2 memory is no longer needed, right?So as I had that pooled memory, I want to give up, I don't need those resources anymore.You notify the FM.

It tells the switch that, you know, deallocate some blue memory.Now, in this case, the FM has to also talk to the device, right?And say, you know, it's going through that device and saying, you know, I'm not using that LD partition anymore, and then it's going to be mapped to some other device, right?And then D2 then notifies H2 saying, you know, I have access to that particular memory, right?And then you can map it.

Now you want to allocate that memory to a different host.So the same thing, you inform D2, and then, you know, D2 notifies the host that I have memory available, and then the H1 updates its HDM ranges and maps that memory, right?So all of this coordination happens through the device and through the switch, through the FM and making use of the hot plug flows that are defined.

So you have to change the stream page or generally it could be done using DCD?

Okay.So a lot of this was DCD.You can definitely, the mechanism to do that would be DCD.And we'll talk a little bit about that in terms of you can do that today.There are proprietary ways of doing things today.And in the CXL 2.0, there's a lot of work that is ongoing in the industry.And then when we get into DCD, then, you know, if you have a DCD implemented, then you would follow the DCD flows, right?In this case, you would use the DCD flows to say, okay, I'm going to allocate those partitions.So a lot of things were also done in 2.0 and there are some work that is predates DCD.So we have to see how the ecosystem is going to handle that.But in the long term, DCD is the way to go because it's scalable and then it supports a lot of the things that you would then extend to GFAM and other things, right?So that's a long term, you know, should focus on DCD.

Now, host 2 notifies, this is just an example, if you, you know, same sort of scenarios, but you're direct attached.And in that case, you know, you attach, you notify, you know, to the FM that, you know, some D2 memory is no longer needed.

And the same thing applies, you know, the FM will talk to D2, right?In this case, it uses the LD CCI and say, you know, this is where it's mapped.And then you can map those resources and notify H2 of the memory that's available, right?

And you know, similar examples of mapping that memory in there, right?

So overall, right?I mean, when you think about a FAM with CXL, you have to do it with FM, right?The fabric manager becomes a key piece of that solution.It's really responsible to participate in all of the bind and bind, you know, allocating LD allocations across hosts and managing those, right?We didn't touch on a lot of other aspects that are related to MLD QoS.As you're sharing a particular device, or pulling it across multiple hosts, you have to think about some of the QoS settings.So all of those parameters exist for managing QoS.So the FM participates in that.And then the benefits of having a FAM are, right again, you're using it for, you know, better allocation of resources, you need that mechanism.The FAM gets the fabric manager, I mean, FAM gets involved in, you know, dynamic allocation, deallocation of memory resources.And overall, from a motivation perspective, right, you're looking at TCO savings going on with FAM, right?So those are some of the benefits that you would use with that.

Okay, we'll touch on DCD devices, right?So before DCD, changing memory allocation was very disruptive.Before that, you know, you could take the devices of today and say I have partitions, you know, you could enable a static configuration, or you could go in and enable a configuration where you can say I want to dynamically allocate.But there wasn't a lot of, you know, architecturally defined ways and mechanisms to do that.So a lot of things that you would have to do that were device specific, you can work in at the kernel level to figure out how you're going to do memory allocation and deallocation, right.And it required you to do some amount of, you know, HDM decoder reprogramming, right?Because now I changed the range, and you need to be able to quiet traffic, system reset more likely if you're thinking about in this space, right.

But with DCD, what happens now is, you know, you allow the capability for dynamically changing the device memory allocation, right.What it means is that now you have that particular memory provision and map through HDM decoders.And within that memory, you have regions that you can dynamically map to a given host, right.So the HDM decoders are mapped to a full DPA range.So you say, okay, this is how it is mapped.And within that, then you use the DCD command set to say, you know, which allocations are active and which allocations are mapped, right.I think in the next sessions in the programming side of things, Mahesh Nathu will walk through how you can use DCD allocations.He has a good flow that talks about how DCD flows would work once you have this particular mechanism.

But when we talk about pooling and sharing, I wanted to sort of introduce this concept, right.So basically, what is defined is per host, there's a block-based extent list.So that just says, where do you have a, you know, device physical address start length, and then you have eight, you know, dynamic DC regions with different properties.So you can talk about that and say, you know, what's the coherency smart NICs of that region?Is it sharing?What's the block size, so on and so forth.And the fabric manager is the one that comes in and programs this DCD regions, right.And that's sort of is the formation of DCD.It's really important as we're talking about, you know, pooling and sharing for the future, these would be some of the things that we want to apply, right.So DCD is defined as part of the three-dotter spec.And we expect this would be very critical as pooling and sharing solutions that are deployed.

All right, so that was my section on just, you know, pooling and sharing.I think I got some time back.So that's good.Yeah, this would be time to just open up for any questions for what we've covered up to this section.Yeah.

I have a question on your multi-head device.So you have multiple poles sharing the same memory.Do you allow the poles mapping to the same physical address or they have to be isolated?

Yeah, so the question was, I'm just repeating for the benefit of, you know, this recording time, right.I think the question was if you have multiple, you know, multi-headed devices and your question was, you know, does the physical address has to be mapped to that, right.So let's talk about the LDFAM and then let's talk about the GFAM, right.If it's an LDFAM in which case, you know, you have two different host images and I'm assuming that you want to go down to sharing of that memory region, right.If your question is, I want to share that memory region between the two hosts, then what it means is that each host has a host physical address, right.And yet, you know, the same partition now needs to be mapped into two different host physical addresses, right.So I have host physical address A, host physical and host A physical address and host B physical address.They're unique from their perspective, right.Each one will get a HDM decoder range, right.So you will program host A's physical address in that HDM decoder that's mapped through that link and then you have another host, you will map that host physical address in that HDM decoder.But on the backside on the device, you will map that host physical address to the same device physical address, right.That way now you have that same region that is shared between the two regions, right.And then if it's through DCD, then you're mapping it to the same device physical address.But now your properties for DCD would say that this region is shared.So that's how you get that mapping.Now when we get into, you know, GFAM type of devices, there is a different addressing scheme in which case you're mapping all of the regions in an address space.So in my address space, I know what is my local node and also I have access to all of the remote nodes, right.So looking at the address space, I know which address space belongs to which particular domain and if I map request into that domain, then it's getting into, you know, accessing a remote sort of a, you know, GFAM region, right.So that's how that would be.I don't know if that answered your question.

That answers it, yeah.So would you say like for the GFAM, it's better to just use the CXL fabric since what you just described?

Yeah, for GFAM, you would have to use the CXL fabric because it's the port-based routing.But I think the key question there would be once you're using GFAM, it also goes into each system how you're going to map the fabric and the GFAM device in your system address space.So it kind of extends into that as well.

Thank you.

Any other questions?Yeah.

Can you give some kind of feeling for the penetration in the marketplace right now of CXL and what you see for the future?

Yeah.The question was what is, you know, what do we see penetration in the ecosystem?I would say, you know, the crawl, walk and run, right, a lot of things.So I'll talk from a consortium perspective and all of these concepts, it was important to get these defined because there's a lot of interest in terms of enabling these capabilities.My take on it is we're looking at it as a paradigm shift in memory architecture.So all of these concepts are important.Lot of things will start with CXL 1.1 2.0 to get memory expansion.We got to make sure that it's functional and performant.So we're at that point now where there's a lot of, you know, deployment, there are solutions out there for memory expansion.And those solutions will get deployed.There's a lot of interest and development in getting into the pooling and sharing concepts.So the infrastructure for fabric management, as Vince was saying, is getting developed, but it builds on all of these capabilities that you do.So I do see that, you know, the early adoption of that would be memory expansion.And then what follows after that would be some of these concepts, right?For bringing something, technology like this will require us to have a lot of patience and persistence, right?And just being at it, right, just day in, day out, just getting all of the capabilities out there.So when I look into the ecosystem from enabling a memory expansion device, you have everything that you need to do today, right?And the capabilities are there for people to try out on, you know, use as prototypes and POC, explore all of the concepts that we've defined.And pretty much you will see that everything from pooling, sharing, you know, the functional proof points are all there, right?Now it gets into, you know, how do you make a business case?How do you get solutions and how do you drive that?And that's the challenge in front of us.

So if I walk out to the show floor tomorrow, how many boosts will I find that have CXL products, you know, CXL capable?

I haven't done a survey, but I would say it's mind-blowing.

So I think if you look into, just to add to what Mahesh has said, if you look into, we did like compliance workshops on behalf of the consortium, very well attended, significant number of devices.Pretty much, you know, if you look into the mainstream CPUs, you know, my company has CXL on our volume CPUs, is done, right?So, you know, it's out there.And then, you know, there are, I haven't counted, but probably at least a dozen plus devices that are out there.So it's, you will have a lot of people that should be there at the floor.I mean, supercomputing, when we went there, it was like a huge amount of demos going on with CXL.So yes, it's real.

And, you know, in the upcoming months, we expect to see production-level devices out there.So, yeah.Any other questions?

So this is the year of CXL, is that right?

There will be multiple years.

Last year was the year of CXL.

Yeah.Any other questions?All right.We'll end for break two minutes early.Yeah, I think...10-15.Yeah, 10-15, we'll give a 15-minute break and then start the other...Yeah, I think that's a good idea.Yeah.So, you know, we'll see you all later.Yeah.So, yeah, thank you very much.Thank you.Any other questions? All right. We'll end for break two minutes early.I think 10.15, we'll give a 15 minute break and then start the other two sessions. Thank you.Okay, it's 10.15 so we'll get started. Again, welcome back everyone for the third section of this training.A couple of disclaimers, right? One is that the slides you're about to see, I'm kind of old fashioned, right?So I made them by hand. I didn't consult any chat bot, chat GPT. So if you have questions, you can ask me directly.Don't ask chat GPT. You won't get an answer. And the second thing is there's just a lot of work going on in the software area today.I think Mahesh and Devyan have both mentioned about the ecosystem really likes CXL, gangbusters.So just a ton of development and innovation going in the software space.You will see some of that in the different booths later here. So I'm not going to try and cover all of that.But I'm hoping you will get a feel of a bunch of things that are happening right in the industry over the next few days.So I'll stick to the very high level summary and not get into details. So with that, let's get started.

So this is sort of the starting diagram for CXL software. So this shows the CXL hierarchy as seen by the software.And this is a simple CXL 2.0 timeframe. It doesn't include CXL Fabric, but this is where we start.So a few concepts to introduce. One, again, I also labeled them so you can consult them later.So the first concept to understand is the RCD or RCH. RCD stands for Reciprocated CXL Device.And H stands for the host. So that's the host and device pair that is as defined in the CXL 1.1 spec.It's restricted because it has a few capabilities than what CXL 2.0 spec enabled. For example, the RCDs cannot do hot-plug.There's a bunch of things it cannot do. But it's maintained there for backwards compatibility promise.As CXL spec has said, we'll maintain backwards compatibility, provide protection, investment protection going forward.So even today's systems, even modern systems, right now the spec allows backwards compatibility with this older devices and older host.The other term to understand is Virtual Hierarchy, VH. Essentially what it is, CXL root port and everything below that, switches, devices, etc.Those we would call CXL VH, Virtual Hierarchy. A pretty open-ended name, right, for a reason.But that's what we'll see everywhere in the CXL spec that's being used to represent this.The next phase is CXL Host Wages. It's a software construct that will combine a number of root ports together.So software can discover the CXL hierarchy, walk the hierarchy. Just like PCI Express, the base of the hierarchy is always described through ACPI.It's something that is not a replug-and-play, can't be animated. But once that is discovered, everything below that can pretty much be discovered through standard animation rules, just like PCI.So the first level is always ACPI. The root is identified by a specific ACPI device ID that is listed here.So the software will look for that ACPI device ID. Once it sees that, it knows the hierarchy is a CXL hierarchy in relation to PCI compatible.It will walk the tree using the standard rules that PCI-8 and CXL has defined to find discovered devices.We leverage PCI Express whenever possible. So what we've done is the hierarchy discovers basically like PCI Express.But to differentiate PCI devices from CXL devices, we take the devices with a specific register structure.We call CXL DEVSEC, Windows Specific Extensions. So there's a number of CXL structures defined to describe different attributes of CXL devices that are unique to CXL.So software can look at the devices and know it's a CXL device or it's a CXL switch, and then discover these different capabilities and then configure them or enumerate them independently.So we have different capabilities for, for example, switches. We have many features that are introduced.One of the features that just comes to mind is CXL isolation. So we have registers defined for that that show up in the root port.So software can know, "Hey, this CXL root port can also support this capability. I can configure it. I can manage it." So that's really in a nutshell how the CXL discovery works.Yes?

Yeah, I think that's really showing backwards compatibility, meaning that legacy software will discover this and look at PCI Express RP root port.It's very familiar to it, but if it knows CXL, it will see the other thing and can configure CXL.So in theory, legacy software can just walk through, look at the tree and then, right, we'll be able to discover C devices, PCI Express, and configure them as PCI Express.It probably won't be able to use some of the advanced features CXL provides, but it can enumerate the tree just fine.That's a good question. Anything else?

Okay. The other part that was mentioned earlier is where a lot of industries focused on, a lot of excitement is about using CXL for memory expansion.So when you use CXL for memory expansion, you have to understand that CXL systems are by nature heterogeneous systems.What I mean by that is that CXL devices have different attributes in terms of memory bandwidth that are not going to match what you know for, let's say, DDI dash.It's going to a CXL fabric, one interconnect, it's going to go to another device, and you could have DDI, some other media behind that.So latency band is going to be a little bit different. So what we have to really do is make sure software understands the characteristics of the CXL memory device as a separate part than the DDI. And when it's doing memory placement, allocation, etc., it must take into account.So what we've done is, again, everywhere we've tried to extend what's out there.Today, the way software discovers heterogeneous memory is using this ACPI concept called SRAT and HMAT.So SRAT has been around for a long time. It sort of describes how many NUMA node system has and the interrelationship, right, whether a NUMA node or a proximity node has memory or only compute. So that's what SRAT says.HMAT is sort of a new introduction, and software is starting to use that. That is much more detailed and allows software to figure out, "Okay, what's my bandwidth from this compute node, compute resource to this memory," for example, right?So if you are allocating memory for, let's say, a GPGPU usage or a CPU usage, software can say, "Okay, here's the best memory node I can use to allocate memory for best performance," right?So all of that. And these systems are pretty complex, right? So now systems have lots of compute units, right?There's GPGPUs, different accelerators, lots of processing units, and lots of memory nodes, etc., right?So all of that matrix, right, is what HMAT describes in terms of the latency as well as bandwidth.So the challenge that we ran into CXL was that CXL, HMAT and SRAT work really good if your system is static, right?At boot time, you know what system has and BIOS can, right, set up the tables and tell operating systems, "Here's the characteristics. Here's what you look for." CXL breaks everything, right?It's essentially an open ecosystem, right? It adds a hot block, right, where systems can add, remove capacity dynamically.So what we ended up doing was--but even then, right, the OS still needs information.It needs to know what the bandwidth is, what latency is, how to allocate, right?But it's a much more dynamic system. So what we came up with is a structure called CDAT, Coherent Device Attribute Table, that each of the CXL components will expose to software, saying, "Here's my attributes, right? Here's the latency from my ingress to my memory," right?The switch will say, "Here's the latency of bandwidth from my upstream port to downstream port." And what we teach software is to look at all the data, look at the entire system construction and the topology, and then construct what it needs to figure out latency from point A to point B in the system, right?So that's really how CXL has become a plug-and-play, and I think this scheme has worked really well.

Any questions?Yeah, so again, HMAT essentially defines it to be the best-case latency, best-case bandwidth.So it is idle latency, and it is bandwidth that you can -- the most optimal -- best you can get, right?Those are not, like, great numbers, meaning that you will not see them in real life, but for the most part, software needs to sort of know which is slow, which is fast, right?So, I mean, to that, it serves that purpose.There have been discussions about whether we should also add loaded latency, right?So software can look at the system and say, "Okay, when the system is loaded, right, what do I expect?" But that's just a lot of work, a lot of things to get right, so we are not gone there, but, yeah, this keeps coming up every now and then, whether we should give software something that's, like, more realistic than these ideal numbers, right?Any other questions?

And, of course, if any of you have proposals, ideas as to how to improve these things that are built, right, please bring them up in the consortium, and we're always looking for how we can push the ecosystem forward, right?C-Skill brings a lot of new things that we have not thought about earlier, so it brings a lot of things, like I mentioned, and sometimes bringing fresh ideas allows us to do things more effectively in the new paradigm, right, versus just propagating the old concepts.

Okay.So the other one that is sort of the fun thing that we had to solve with CXL was interleaving.So if you're familiar for standard DDI channels, this is just very common, basic stuff, right?You get -- in order to get best bandwidth characteristics, right, we end up interleaving all the memory channels, right, or maybe multiple of them together.With CXL being used for memory expansion, we needed to find something very similar, except that the CXL system doesn't have to be -- is a tree, right, unlike DDI, which is generally, like, just a direct connect, right?CXL tends to be a tree.So we came up with a scheme that allows it to be interleaved in a hierarchy, right?So if you look at the picture, we have this concept called interleave set, right?An interleave set is identified as a bunch of devices that are connected together.The set has a base HPA, or host physical address, and it has a limit, right, and both are multiple of 256 megabytes, right?And there's a concept called interleave weights, which means how many devices are part of the interleave set, right?It can be 248.We essentially increase that to, I think, 12 and 16 and 3 and 6, right?I think that was -- I should have probably corrected the slide.That came in as an ECN after 2.0.The next attribute of the interleave set is the interleave granularity, and that can range from 250 bytes all the way to 4K as a power of 2.So that's what is shown here.And for each of the root complex and downstream switchboard, we need to identify where -- what the targets are, meaning if I get an access, get an address, right, where does the root port send it to, right?Does it send it to this port or that port, right?Same thing with the switch, right?If the switch is an access and it's interleaved below that, how does the switch figure out which of the downstream port this will go to?So that's the target list.So this shows an example, right?So in the picture on the right, right, it shows memory range 16 to 22 bytes.That is interleaved two-way at 4K at the host bridge level, which means when the processor gets the access, right, it's going to pick at the 4K granularity whether it goes to the left root port or the right root port, right?Once it goes to the left root port, it lands at the switch.The switch has its own decoders.I think Mahesh mentioned the HDM decoder concept, which I used to figure out where to route the address.So the switch has its own decoders, and it's going to figure out, okay, it's the same address range, 16 to 22 bytes, and for me, it's interleaved four-way at 1K granularity.So every 1K address is going to go to a different downstream switch port, and it will keep repeating that, right, after 4K, right?And at the end, it will land up in a device, right?So this one doesn't show all the devices, but there should be eight devices below this, and the device on the leftmost, right, for example, will get one-eighth of the address chunk.And from device perspective, if you do the math, right, it looks like the same address is interleaved eight-way, two-way at the CPU level and four-way at the switch level, and this can just go on.If you had multiple switches, like two-level switches, which is probably not very common, this math keeps quite working that way, right?So that's really the relatively simple scheme that we came up with that allows us to do a flexible interleaving, and obviously this works really well for non-Fabric or non-PBR topologies.For Fabric, it's a very different address.This scheme doesn't really scale very well when you think about GFAM, but I don't have a picture to show that right now.Any questions on the interleaving?

I comment, underneath that, multiple DDR channels could also be, right?

Actually, yeah, that's a really good point, right?So the question was, underneath that is multiple DDR channels that could be there.So CXL, because it abstracts the device in terms of the CXL.mem access, it doesn't really look into how the device is going to handle all these access, right? Very often, this device on the left side, right, may have, I don't know, four DDR channels behind it, and it could be interleaving those access across those four to get the best bandwidth, right?But that's not exposed at the CXL level.From the software perspective, it just sees the device with some capacity, interleaving at 1K.With the device, it could be interleaving at, I don't know, 128 bytes or even 64 bytes, up to 16 devices, right?Who knows, right?That's really a device implementation choice, and all that will factor into the CDAT thing we mentioned, right?Device has four channels.It's going to provide more bandwidth to the host, and it's going to report that to the CDAT structure I just mentioned in the last slide.Actually, that's a really good way.That keeps coming up.I agree, there's confusion, right, again and again.Okay, anything else?

All right, I'll keep going.So this is another sort of a topic that we spend a lot of time with in the consortium, and I'll summarize this in one slide.It's kind of tricky.So the problem that we discovered was that Type III devices, especially ones that have persistent memory, require a lot of management from the system software.They don't just work without any help, right?So they need simple things like provision, needs to be configured.They often need firmware update.If you look at these devices, they're pretty complex, right?So traditionally all the things like memory training that was done by the boot firmware is now done in a device, right?So if you look at just the training code, it's a lot of code, right?So that means a lot of firmware.It needs updates, so we need someone to push the updates to the device, right?And what we didn't want to do was leave this as a problem for the vendors to solve, because we've seen some other industries like in gaming where we've seen the success of having a standard interface that allows a generic class-based driver to manage devices.It's worked wonders for them, right?So we decided to take the same approach in CXL, where for memory devices, Type III devices specifically, we defined a standard interface by which a standard driver could access the device and manage the device and configure it, right?And so the different OSs can just carry a class driver for memory devices and be done with it, right?If they want to do a vendor driver, that's fine, but the class driver will provide enough functionality so the device can be managed.The things like DCD that Mahesh just earlier mentioned, right, those are also called the spec, right?So DCD is a standard thing. This driver can manage that.So if you have a device that supports DCD, you can just plug into a system that has this class driver, and there you go.You don't need to have any vendor-specific driver, manager driver, certified driver, all those things just go away.So in terms of what we've done is we have defined a few elements to enable this, right?So one is this number of, again, discoverable capabilities that each device can say.First device can say, "I am the device that follows the interface," right?There's a specific class code that we defined for this, so you can bind a generic driver to that device.Once the driver binds it, it can figure out all its capabilities, a bunch of registers through memory map, and then once we get to the memory map, right, we have a number of mailbox commands, right?This was really not--we looked at whether we can just expose everything as registers, but it just became too cumbersome.There's a lot of data back and forth that needs to go between device and software.For example, the firmware image has to be downloaded to device, so, I mean, mailbox seemed to be like a right thing to do for that.So we went with that, and obviously we allow vendor extensions, and those come in handy.If you are doing something unique that the spec hasn't caught up with, but the expectation is that over time, right, a lot of these things will flow back into Cicso specs so they can be managed in a standard way.But the spec does allow vendor extensions.Okay, any questions?

Okay, so this is sort of an example of software stack, and this is sort of a tricky slide to draw, because no matter how I draw it, someone is going to say, "Oh, that doesn't look like my stack." So example, right, so I'll just take it in the spirit of that.So like I mentioned, right, Cicso is really going to expand--essentially extends PCIe.So the stack looks very much like PCIe, right?In many places, we've taken PCIe stack and just added a few things on top of it, right, and get going, right?And you can add more and more things and make the system more and more Cicso-aware.But to get started, you just need very, very minimal stuff to be added to PCIe stack.So if you look at all that in the bottom, right, so I'm showing two hardware components.One is a PCIe hardware component.Let's just pick an AV device, for example, right?It's really well-known and appropriate for this conference.And the Cicso device.In the Cicso device, right, I mentioned a Cicso memory device.That's a standard interface, right, that define.So both are at the bottom.At the--above there would be the ACPI or the UEFI there that describes what these devices are.And there is ACPI constant for the whole space that allows the OS to discover CXL hierarchy.So those will show up at the next level.On top of that, you could take a PCIe bus driver that enumerates PCIe really well today, add some logic to it, and turn it into PCIe plus CXL bus driver, right?And so this driver will now understand CXL, can configure CXL fabric, CXL interconnect, right?So that's probably an easy way to go from today to the, right, CXL-enabled device systems.And on top of the bus driver, you see the NVMe class driver that's there today, no change, right?So because PCIe plus CXL driver is basically like a superset, all the PCIe stuff is maintained.You can have a vendor driver that talks to a vendor CXL accelerator device that may be, right, built by vendor A or vendor B.And then we have a CXL memory class driver that I mentioned earlier that is now managing the CXL memory device as a standard interface.The different, one unique thing that CXL has that PCIe doesn't have is the interaction memory manager.PCIe doesn't have coherent memory, so because CXL adds memory, it has to now interoperate the memory manager that the OSes have.And that's often one of the tricky part, right?Memory managers are very specialized and very, what should I say, difficult, right, sort of tricky to get, right?So this has been some of the struggle that we had is how to interact between the CXL bus driver and memory manager and keep both the parties happy, right?So, but I think a lot of progress has been made, and we're seeing a lot of more and more prototypes, more and more, right, sort of the software out there that's doing these things.Obviously, CXL can be, supports hot plug, right, so there's another level of interaction that you need between the CXL, right, and the memory manager where memory can go and come and go, right?And that's something that they were really not, today's software doesn't really handle very well, right?Today, memory hot plug is not that common, right?CXL makes it really, really easy to do it, right?And DCD makes it even more easy.So those are some of the things that we are currently working through to kind of make sure that memory management, right, is now able to handle those, right, without giving up all the nice things that people have developed over time in terms of management techniques of memory, right?Any questions?Sure.

So, a nice block diagram, if you want to draw the fabric manager, like, how, which box it will tap into?

So this is really showing a really simple system, it's not in the fabric system, but if I were to draw, fabric manager will be sitting somewhere outside there constructing this very system, right?So in a fabric kind of scenario, all these devices that are shown in the hardware, right, they could be on the fabric, and fabric manager is actually attaching them to a system and saying, okay, this device belongs to this system, so it's actually constructing this view, right?And once it constructs this view, then the software on the local node can go and enumerate and do discovery.That's probably one way to think about it.Okay.

So, things in a hot plug, right?So, CXL protocol does support hot add, also manage hot remove, and we also have the support for serverless hot remove, right, with an asterisk, right?And I'll go into what the asterisk means for serverless hot remove.So managed hot remove is sort of straightforward, right?So software is told that now I'm going to remove this memory device, please get ready, so please, right, ask your memory manager very nicely, can I take the memory away?If the memory manager says okay, then you can, right, support go to the remove, right, process.And this is, again, a lot of steps to do that because CXL is coherent, so you've got to create a sponge or accelerator that accesses the memory, right, maybe flush caches to make sure when the memory goes away, you're not, like, left with some dirty lines, right, in the caches that map the memory.So a lot of the preparation needs to be done by software, but software is told ahead of time, so you can do all of those things, right, in order to -- it also has to offline pages, right, tell applications, hey, this page is going away, please, please, please, right, please, please, please, free the page because I'll give you another page, right, in return.So all those things happen, right, in the background when managed hot remove happens.Hot add is really easy because memory is coming in, so it's less of a hassle for the software to handle that.Software sees more memory and can start allocating to applications, right, so that's less sort of a tricky kind of flow, right.Software remove is what we call -- goes by the name of CXL isolation, right.So CXL isolation was an ECN that was added after 2.0 that allows software remove with an asterisk.So it does require support in the root ports in order to handle that.It does require significant software support and the software recovery after that is sort of the best effort, right.I mean, if we do really well in terms of getting the software ready, I think there's a good hope that we can handle it most of the time, but there could still be scenarios when the software is hard to move, right.If it's done in the wrong time, if there's an application using memory, it could still result in failures.So this is really an exciting area where we want to spend more time trying to figure out how far we can push the envelope, right.Software is hard to move is a fact of life, right, and then we learned it hard way that devices, right, the links go down, right, devices get yanked, so we needed a solution for that, and I think we have an architecture solution.How far industry can take this, right, to innovations and different ideas, right, is something that we obviously are very eagerly watching.Clearly, right, I mean, this goes without saying, PCI has a long lead over us, right, in terms of enabling hot block, right, and they have a pretty mature model in terms of the hot block model, all the indicators and everything, right, and different form factors have adopted them, right.So CXL just happily innovate all of that, right, as is and not even bother.A lot of slots are, I mean, most slots are going to be common, PCI and CXL, right, so therefore, right, we have to really follow PCI in terms of what they do for form factors and some of these things, right, because from system design perspective, right, there is no unique CXL slot, it's the same, right, as PCI.Any questions?Okay, fine.

So this sort of shows the type 3 device hot end, and it'll show some of the-- I'll talk about some of the points I made earlier about it looks easy but, right, it can be pretty complicated, right.So I divided it in three steps, right.One is the steps that are just like PCI, that we know, right, software understands, right.So the first step is system firmware boot time has to prepare the system for hot end future.It has to know a system can take hot end, so it may have to sort of reserve some address space, right, for the hot end devices to show up.Somewhere along the line, the user will hot add a type 3 device, right.The PCI hot plug interface generated because we leverage PCI Express, and then the PCI hot plug software will come in and say, "Hey, I have a device now there.I will assign all of the IO.IO resources to it," just like it would do for PCI hot end.It will assign the base addresses, it will assign memory, IO, all of those things, as it would do for PCI.So that's the PCI-like portion of it.Now we need to bring in a CXL aware software that can do rest of the steps, right, that are really CXL specific.It will look at the HDM decoders on the device and say, "Okay, how much memory do you need?" It will configure the decoder that I mentioned earlier all the way from the root port to the switch to the device, say, "Okay, now you know how to decode this address range, right.Here's the address that you can do." It can configure a CXL DVSEC that are based on which features are enabled, that's a part of the device initialization.And then it will use the CDAT table that we mentioned to extract the information from the device, saying, "Okay, what's your capacity, what's your latency, what's your bandwidth, right?" And so that way, software can allocate, make intelligent software allocation decisions.So that's the CXL aware software part.And then it goes into memory manager saying, "Okay, standard capacity add steps from the manager would be to, okay, now you have new capacity." The memory manager will process the request saying, "Okay, I have new capacity.This is the address that I'm getting.Now I'll add it to the pool that's available to applications to allocate from." And then basically we're done at that point, right?So applications can now allocate on new memory.If they are aware, they can say, "Okay, I want memory of this characteristic, this bandwidth, this latency. Can you give it to me?" So all those things can happen after that.And this is a simplification.DCD makes it a lot easier because there's no physical hot add that's involved in there.And I'll go into some of that in a minute.

So dynamic DCD, right?So Mahesh Wagh covered some of this, right?So I will probably skip some of the details.But we needed something that's like--hot plug is like really, really a big hammer, right?We need something that's like lighter weight, more efficient, that allows things like efficient page reallocation from one node to another node, right?So multiple hosts that are running, right?And workload dynamically changes, right?We needed like a second granularity, take page away from one node, give it to another node, right?Something that's really, really quick, right?Not something that takes hours or days, right, or months, right, for example.So we had defined events that the device can notify the host saying, "Okay, I have more capacity that I can give you," or "I can take capacity away from you," right?There's handshake from the host because the host really has to accept those requests, right?You can't just push them away to the host, right, without his consent or take it away from the host without his consent.The system will crash if you do that.And then allowing the FM, right, the Fabric Manager, right, to manage the device capacity and say, "Okay, this host is asking for more memory. I'm going to give it to him." This host is done using his memory. It's like running, has fewer workloads running. It doesn't need that.So I take it away from this host and keep it in my back pocket in case someone else asks, right?So all that magic is done by the FM, right?So we support up to eight regions in a DCD device. Each can have different characteristics.So some can be like high bandwidth, some can be persistent, some can be, right, volatile.All of those things, combinations are allowed.And each region essentially will span what we call the device physical address range in the device, right, so DPA range.So it can also expose shared memory region.So again, that was discussed earlier is that a device can say, "This region can be shared." So software can set up sharing between two hosts in that region.Each host will see a different physical address, but in the end, it will resort to the same physical location or memory ranges in the device, right?So the data can be exchanged between the hosts so that we can have them very efficiently.And there are two types of DCDs. One is, again, what was mentioned was memory pooling device or the multi-logical device or a GFAM device.A GFAM device is extremely flexible, and the only way it operates is through DCD.You can allocate only through DCD.The multi-pooling device, you can do other things, but DCD is a preferred mechanism to sort of allocate, deallocate memory and make the system more dynamic, right, in terms of agile and responsive to the workload needs.And I have a picture that sort of shows that.So this is a simple flow, right?

So if you look at this, there's an entity called orchestrator, maybe data center level entity, that is looking at all the workloads, saying, "Okay, I'm going to schedule this workload here.I'm going to move this workload there." All of that is the orchestrator, right?Again, these are terms that are sort of loosely defined. They mean different things for different installations, but so take it with that.The fabric manager we talked about, that's the magic, right, entity that's going to do the allocation.And I'm showing two hosts for simplicity. There could be multiple hosts, right?Each host has a generic CXL type 2 driver that we talked about earlier.And you have memory pool device that I'm showing here, a simple device, not a GFAM.And it exposes two logical devices, right? Both have basically DCD.So let's say the orchestrator decides to schedule some workload W onto H2, right, host number 2.And that workload requires 512 gigs of memory.Now O will see H2 is using all the memory. It doesn't really have any memory left, right, for this new workload.But H1 is not, right? So O will ask FM, right, to say, "Can you get 512 gigs back from LD1?" FM will ask LD1 to give memory back. LD will ask its host to say, "Can you release the memory?" The OS on the host 1 will say, "Yeah, I think I can do that," right?And it will return to FM. FM will write all the orchestrator. The task is done, right?So now O knows there's a 512 gig of memory that's available that it can give it to H2.It will give it to LD2, which is assigned to H2.And this attack mechanism that's used to figure out what to reference the memory.I'll skip that for now. And once memory is given to H2, the O can now schedule the workload, the new workload onto H2.H2 has enough memory, and H2 is happy, right? So this is sort of simple.You can do something with this for also sharing if you need to. Any questions?

Okay. Other topic that I want to cover is performance management, right?In terms of the CXL device, if you look at it, it's very tightly bound to the host, right, unlike PCI Express.So what we figured out was that any glitches or anything that can happen on the CXL device side can greatly affect system performance.And today there's existing infrastructure for managing system performance, monitoring performance, right, that a lot of the architectures have.There's an infrastructure that Linux has that is pretty well developed and very widely used to monitor performance, right, provide feedback to applications, help with debug, or even just, right, even in the system that's deployed, right?So we essentially defined a register interface that software can now use to access performance data from the device and then feed that back into the infrastructure.So it looks at the P1 infrastructure can now look at the data from the CPU and these devices and put it together and see where the bottleneck is, right?What's wrong? Why is my app so slow? Right? So those types of things can be done with this.

So CXL device has what we call the CXL PMU, Performance Monitoring Unit, that can essentially expose a lot of the data, right?So device can have multiple of these. Each can say, "Okay, I have these many counters. I can support these many events." CXL spec defines certain events and we allow vendors to extend that as needed, right?So again, this is really trying to fit into the existing software infrastructure and not having to ask software, right, because we need to build something unique for CXL, right? So that's really where a lot of these decisions were made on.

Okay, I'll keep going. There's no question. So CXL error handling, right?So there's two, if you look at the CXL device, there's two domains if you think about it, right, in error management.One is the device, part of the device that's facing the CXL link, right? We call them the protocol, right, aspect.Other one is the back end of the device, which is really not tied to the CXL protocol, but let's say a memory device, this could be a DDI channel that Jem was mentioning, right? So if you divide it into two, the protocol errors are handled and are sent to the host by the device through PCIe-based mechanism, right? We decided to leverage PCIe because it's pretty robust and again, it can, so what we do is the errors that are happening on the protocol side are reported to PCIe mechanisms using either corrected internal errors or uncorrected internal errors. So those are two types of errors that PCIe defines, so we sort of leverage that. It gives us good compatibility with PCIe, really good starting point, loses some details, but those are filled in by the CXL. And on the memory device side, right, we mentioned standard interface, so those are also used to extend the memory event loss, right? So if there's DDI errors, right, we define formats for those so software can say, "Okay, behind this device I have these many DDI channels and here's what's going wrong with my device," right?So all of those things are both signaling, as well as logging is defined to CXL spec, so that can be managed for a memory device.For, let's say, third-party devices, you can still have a vendor-specific mechanism, right, that can be handled by a vendor if need be.

Okay, any questions? Okay, I think that was the end of my material, so if you have questions after the session, I'm probably going to hang around for a while, so please reach out and I thank you for attending.

So, good morning, still, I guess, everyone. My name is Vincent Hache. I work for Rambus.My background, I spent a lot of time looking at PCIe fabrics.Fabric topologies, getting away from a strict tree structure is definitely a desirable architecture to move towards in, like an enterprise and a data center, a large-scale type environment, but there are some limitations in PCIe that limit you to a tree structure.There are some very cool technologies developed for PCIe fabrics, but it's all proprietary. It hasn't been standardized.And so that's a key industry need that CXL has now addressed, and I'm excited to talk about what we've done to enable fabric-type topologies, leveraging the PCIe backbone, and now introducing these caching and memory smart NICs on top of it.So, we're going to talk about fabrics. So, this is based on CXL 3.0, and there are some, if you've had a chance to spend some time with the spec, you'll note there are a few places it mentions, you know, certain key aspects of the architecture are coming in a future specification.So, we'll talk about them. I'll talk about the needs, and I look forward, when those details get published, I look forward to holding a follow-on training session where we really dig into the details.So, we'll start with an introduction of fabrics, how we're going to enable those sorts of architectures, and then we'll visit fabric management again, just to really make sure there's a good understanding of the roles and responsibilities.When you move from memory management, which conventionally a host completely owns its own memory, it's in complete control because it's DRAM sitting in a box right beside the CPU, and then you take those DIMMs and you put them outside in a separate box with some cables in the middle, and there are a bunch of other hosts there. Half of that architecture belongs to that host, but half of it's shared, and so you need to start dividing up the responsibilities to make sure that a host isn't going to do something on a management front that's going to impact other hosts. So, we'll talk about all that.

So, we needed a fabric to be able to enable some of these disaggregated composable systems.So, you've got a pool of hosts, a pool of devices, a pool of memory resources.How can you compose those into working systems at runtime as needs arise?This is a movement that's been going on in industry for a long time now.Things are getting broken apart from just self-contained servers into these pooled boxes of resources.This is something that I spent a lot of time working on in storage, and now we're seeing CXL enable this on the memory front.Scale-out systems as well, HPC, deep learning, machine learning, analytics, large systems where a number of nodes are all collaborating together on one larger body of work. There needs to be a lot of communication through the hosts, and yes, there's Ethernet, we already made available to pass data back and forth, but the bandwidths are high, but the overhead and latency is also high, so getting a more tightly integrated communication between those systems.So, basically, we wanted to add capabilities to expand CXL from just a node.There are arguments for adding CXL to a single node just on the bandwidth per I/O pin argument, the cost of DRAM.There are benefits just within the box, but it does allow us to start to scale up to a full rack system, or maybe a number of racks all connected together, all with native CXL and not needing to go to Ethernet.Being able to grow to that scale is very desirable.So, what we decided was a good trade-off between overhead in the transactions in terms of an addressable ID space versus being able to hit that scale is a 12-bit ID space.So, it's not the scale that something like Ethernet would give you, but it gets you up to 4,000 IDs, which is going to hit that small number of racks type scale that we're looking for.And as I mentioned before, there are some serious limitations to just sticking to tree-based topologies.Basically, it kind of just removes the ability to do anything multi-host, because how do you get the trees to align?So, we needed a technology to get us out of the tree-based topology architecture.But an important design goal for all of this is that we did not want to compromise node-level properties.We didn't want to start throwing in overhead in transaction headers, for example.That would mean that a single host that is not using a fabric, no fabric requirements or technologies involved, suddenly its bandwidth drops or latency increases or something like that.So, that was a design principle that we maintained throughout this development.

So, here's how we addressed composability.The idea of composability is basically you have your host and your endpoint at different locations in the fabric, and you just kind of select and connect them together. You bind them together.But as Mahesh covered, we're trying to do minimal extensions upon what PCIe established.We want to be able to reuse all of the existing host software that's there for device discovery and the baseline that's used for device management.So, instead of exposing all the gory details of the interconnect across the fabric, there's a virtualization layer that's introduced.So, you can see in the center of the slide, there's a physical topology that has a couple of hosts, host 0 and host 1, and a handful of endpoints scattered around.One of them is an MLD, and just a cloud of fabric interconnect.And it's a very messy, jumbled up physical topology.But when a host goes out and enumerates its topology, the role that the switches that are connected to the hosts and the endpoints, a role we call edge switch, these edge switches, they virtualize what has happened.So, the logical view that's presented to the host when it does a PCIe enumeration is up to two layers of standard switches.So, host 0 sees its local one, and endpoint 0 is connected directly to that switch, because there's no need to virtualize an artificial second layer of switching.And then endpoints 1 and 3 are connected to two other switches, but we don't see any of the intermediate fabric links.All of those details are abstracted out, and what's presented is a virtual link that is representative of the routing path throughout the fabric from the host edge switch to the endpoint edge switch, or the downstream edge switch.And that's that connection between those two layers of switches.So, that's actually a virtualized downstream port and a virtualized upstream port that you're seeing in the center of that logical topology.And host 1 is seeing a similar view, where endpoint 3 is local on the host edge switch, and then we've got another layer of switching down.So, it will only ever be up to two layers of standard switching.There's huge benefits. I hope it's obvious.The benefits we get from being able to reuse the decades of PCIe system discovery and enumeration software development in taking an approach like this.

So, that's how we address composability for scale-out.The technology that was introduced is, as Maheshwag covered, global fabric attach memory.So, this is a highly scalable memory pool.So, an extreme example is, you know, this could enable over 2,000 hosts accessing a memory pool of over 2,000 memory devices, all at the same time.Practically speaking, you will see a much smaller system than this.Also, that's taking the theoretical limit of those 4,000 IDs.When it actually comes to implementing real systems, the ID consumption depends on your specific interconnecting configuration.There's port counts to consider and switch node size.But basically, this is certainly a much larger pool than one host with its DIMMs and an RDMA to talk to other devices.So, the way that this pool is accessible by the host is something called the Fabric Address Segment Table, or FAST.So, this is a block of address space that the host assigns in his physical address range.And it gets broken up into identically sized segments of a power of two size.And each of those segments maps to a memory range within one of these GFDs.The host goes ahead and reads from that large pool using their HPA, their host physical address.And the GFD does the translation into what that means in the DPA.So, the GFD is maintaining an HPA to DPA decoding for each of the hosts that will be accessing it.

Okay, so let's talk about how this sort of scale and this addressing scheme is achieved with the transport level changes in 3.0.

So, in CXL 1.1 and 2.0, we call the routing model employed there, and also in the non-fabric ones, the non-fabric deployments of CXL 3.0, we call it host-based routing.Fundamentally because a lot of the transactions are routed by host physical address.So, it's a host-centric routing architecture.The problem is when you're looking at a switched environment, if you are routing a switch link based on address, that restricts those links to a single virtual hierarchy.Because the hosts are choosing their physical address in a vacuum.And so, there's the risk of address collisions.So, there's just no way for a switch-to-switch link to resolve routing based on address alone, which is why we needed to move to this ID space.But in the legacy HBR transaction packets, address is the only routing information that's defined.

So, 3.0 introduced a new routing model to enable CXL Fabrics, which is called port-based routing.And the term port is used as opposed to host, because you're routing now based on your destination port, not based on a particular host address range.So, this is part of the new flit mode defined in 3.0, the 256-byte flit mode.So, you'll see highlighted here on the right are new fields that were added in previously reserved space with some IDs.So, there's a destination PBR ID or D-PID, and that's carried by all transactions.All transactions in a Fabric are going to have a destination PID.And select transactions as needed will also carry with them a source PBR ID or S-PID.And so that informs, if you can imagine, for a request, the request requires a destination and a source, so that when the target of the request completes the request, it knows who to send the completion to.The completion, however, does not require a source because it'll have a transaction tag.So, this is how we allow inter-switch links to carry traffic from multiple virtual hierarchies, because each host is going to have a unique PID.Each target is going to have a unique PID.So, the combination of S-PID and D-PID is going to allow us to isolate the traffic and route independently.But again, this is only enabled in the 256-byte flit mode in the PBR packing.And I'll show you how links figure out what mode and thereby what transactions they'll be using on the next slide.

So, at link up, when a link's being negotiated, both ends of the link, during alternate protocol negotiation, they exchange information on their capability set.And that's how the links are going to figure out how they're going to bring themselves up.So, part of these symbols that are exchanged, you can see in the full list, there are things like PCIe capable, .IO capable, .MEM capable, .Cache capable.And so, those sorts of capability indicators are how a component will detect.So, a switch, for example, will detect on the far end of a link, whether it's a Type 1, Type 2, or Type 3 device.Can it do .MEM? Can it do .Cache? Can it do both?They'll exchange what type of flit modes they support.An MLD will advertise whether or not it requires the MLD link mode.And then, at bit 18, is where it advertises whether it's PBR flit capable.And so, this is how the PBR capable links, or fabric link, if you want to think about it in those terms, will advertise to the far end.If both sides are PBR capable, they'll negotiate into PBR mode.Otherwise, they'll fall back to HBR mode.So, if we think of the previous block diagram, where there were a bunch of intermediate switch links, and those edge switches had a combination of endpoint links, host links, and links to the purple cloud of fabric interconnect.So, the switch links can figure out whether or not they're linking as an intermediate fabric link, or whether they're linking as an edge to an endpoint or an edge to a host, based on the advertisement of this bit on the far end of the link at negotiation.

Okay, now we'll talk routing model.I've spoken in general terms that we use ID to route, but the host isn't using IDs.We want to be backwards compatible.The endpoints, or the legacy endpoints, the non-GFAM endpoints, they're not using ID to link.They're just regular type 3, type 1, type 2 endpoints that you could plug directly into the host.So, we'll walk through a transaction passing through a fabric to show you where these transitions take place, how traffic passes through a fabric.

So, I've kind of simplified the representation of the fabric into just a path from a host on the left to the components it's working with on the right.And so, we've got three different types of target component.So, there's a GFD, a device, and that could be anything.What we mean by device is not a GFD.So, this could be a PCIe endpoint, this could be an SLD, this could be an MLD, this could be type 1, type 2, doesn't matter. It's a device, it's a non-fabric device.Or, an HBR switch.That could also mean a PCIe switch, but basically not a target device.It's going to do its own decoding and routing.The host, as I said, is HBR.We don't want to impact the host and force the host to be fabric capable.So, it's operating as it would were there no fabric there.And we've got the PBR switch there, which is the technical term for a fabric switch.And it's operating as a host edge.It's that host edge's responsibility.I mentioned the virtualization of the topology as one of the roles that it fulfills.The other is, it's responsible for converting the host transactions from HBR format into the PBR format that's going to be used inside the fabric for routing.So, the host request is sent in HBR format to the host edge switch.The host edge switch converts it to PBR.And then at that point, that conversion, on a low-level basis, that means decoding the address, discovering what target endpoint that is, and applying a D-PID.Now we've got a PBR format with a D-PID and an S-PID, because the host edge knows which host that came from, so it can apply the S-PID as well.And it's just going to pass that off.If there are any intermediate switches, they'll just route based on D-PID.So, a PBR switch, one of the requirements for a PBR switch, is basically a 4K table whose index is D-PID and output is egress port, basically.It's a routing table.So, all those intermediate switches are going to decode the D-PID and choose a routing port.If the D-PID is a GFD, you'll note from the dotted boundary of the fabric, the GFD is actually inside the fabric.We mark the fabric border as where HBR turns to PBR or vice versa.But the GFD is dealing with PBR FLIT.So, that intermediate PBR switch connected to the GFD, it doesn't actually really know that it's sending it to a GFD.It just knows that the D-PID is at that port and the GFD is going to take care of the rest of the decoding.So, if the decoding that happened at the host edge meant the transaction gets routed to the GFD, then it just gets straight routed to the GFD in PBR FLIT mode.For all other endpoint types, however, you need a downstream edge switch because you need to now go back into HBR mode.So, the downstream edge switch, it also has binding information.What endpoints have been bound to which host virtual hierarchies?And what that informs the downstream edge switch is of the S-PID, basically.So, it'll receive a request. It knows the D-PID.It will also be able to evaluate the S-PID to understand what was the source host and how do I reconcile that with the binding information that I've been configured with.Based on that information, it will convert the PBR FLIT mode transaction back into HBR mode where it'll pass it to the device.So, now the device receives an address-targeted transaction.It'll process it, provide a completion, and the whole flow happens in reverse.For an HBR switch, it's basically the same premise.An address-routed transaction pops out into the HBR switch, and it's got its own decoding and routing logic, and that can filter down into whatever the switch topology looks like downstream of that switch.Any questions there?We'll do the back first. Yeah?

Yeah, so the routing table versus couples like earlier you mentioned about FAST, are they the same thing or they're different?

They're different. They're different.

Can you elaborate on this?

Yeah, yeah, absolutely.

Yes, yeah. So, the question was the routing table that I referenced for the intermediate PBR switches and the FAST that was mentioned previously in the GFAM, are they the same or are they different?They're different.The FAST is--the index into the FAST is a host physical address, and the output is a D-PID.Whereas the routing table in the PBR switches, the input is a D-PID, and the output is an egress port.So, there'll be a bit of--in the host edge switch, there'll be a bit of--likely as a design optimization, there'll be a merging of those capabilities where the input will be address, the D-PID will be applied, but the output will be an egress port.But for the intermediate switches, they don't have a FAST.The FAST is specifically at a host edge port.

Got it.

Yeah. Another question?

Why is the typical delay for you at those switches?

Ah, the question is, what is the typical delay through those switches?Well, I'm sure some switch vendors are going to have some booze at the exhibition hall.That's a really--that's a very interesting question.I would say latency is one of the biggest concerns that I've certainly heard people voice through the development of these systems.A really good PCIe switch, Gen 4, Gen 5, I mean, you're talking anywhere from 75 to 100 nanoseconds, one way through the switch.So, round trip, that's a big number.And that's based on a certain node size, a certain number of ports, right?The larger the switch becomes, the bigger the silicon, the bigger the crossbar.Like, it starts to impact latency.So, I can't give you a hard answer, but that's definitely an area of intense focus in the development of these, because there's a big tradeoff between increasing the switch size, enabling larger fabrics, but also impacting latency, and what's an acceptable round trip latency.I will say, though, that for a memory type such as GFAM, the use case would be such that it would be expected to be a bit more of a cooler memory tier.I don't want to say it's cold memory, but you're not going to put your OS image in GFAM, right?This is for large pools of shared data whose interaction model can tolerate a larger latency head.

Okay, let's spend some time talking about the fabric management architecture now.

So, as we disaggregate this memory, there are some new challenges in terms of how to manage it all, but that also presents some pretty good opportunities for new levels of capability.So, the first thing is, as much as possible, we want to be able to leverage existing DRAM management.There's some memory management software that has existed for a very long time that no one is in a rush to rewrite, but just to make it CXL specific.So, we really want to minimize the software and firmware development requirements.We want to leverage existing concepts that Mahesh has talked about, SRAT, HMAT, we've got the CDAT now.We want to be able to reuse SPD and PMIC type management entities for memory management.But we do need to extend these concepts because it's no longer as simple as a DIMM connected directly to a CPU.There was really only one physical architecture for memory in the past.You had the DRAM, the DIMM, the DIMM connector, and the memory channel, right?But now you can put any sort of media type on any sort of form factor and just connect it to a CXL port.And what does that port look like? Is that a connector? Is that a cable? Is it an enclosure?Is the DRAM chipped down or is it DIMMs on the other side of the CXL controller?There's a lot of variables and maybe over the next 10 years the industry converges on one specific form factor.And we can just assume that it's going to look like that, but we can't dictate a form factor at this stage.So, we need to extend what is reported in terms of topology.So, the PLDM Type 2 platform data records are used now in CXL for reporting the architecture, the physical architecture, physical topology, CXL memory module.And we need Redfish models to be able to advertise that to higher levels of management software.And this really, it's become clear over the years, and I'm sure many of you have seen the announcements that have come out over the years about CXL Consortium working together with JEDEC, working together with DMTF.It's become clear that standardization and really pulling together as a broader industry is the key to being successful in this area.So, in the consortium we rely on DMTF's support for development of Redfish models, for finding good ways to track CXL, to, sorry, port CXL concepts into DMTF standard-based concepts.So, it will really help unify the industry under one model of management.

So, what is PLDM?PLDM? PLDM is a platform level data model.It's a family of specifications published by the DMTF for various aspects of management.So, for example, Type 5 is a definition for firmware update.So, independent of what the device type is, a lot of different types of devices run firmware.And so, there is a firmware update standard.Type 2 is a generic way of reporting out sensors and effectors.So, that could mean temperature sensors, that could mean voltage sensors, that could mean fan control, things like that.So, standardizing generic common management type capabilities.

So, what is a fabric manager?Ultimately, a fabric manager, there may in the future, or there will in the future, be an open source fabric manager software project that can be reused and deployed.But from the CXL specification definition of a fabric manager, it's not limited to that one project.It's not dictating development requirements for one body of software.The fabric manager is a conceptual term, really.It refers to application specific logic that is responsible for composing systems, allocating pooled resources, managing platforms, and the sorts of activities like that, that are required when it's no longer just the host's responsibility for managing the components that are exposed to it.And that can take a lot of different forms.So, that can be a BMC inside a rack mount appliance.So, the picture I've got here is a JBOM, just a bunch of memory, which is some sort of rack mount.If you can imagine a rack mount appliance that's got a bunch of host ports on the front, and a bunch of hot pluggable memory slots.Maybe that's DIMM, maybe that's an EDSFF form factor, E3.S, maybe it's a ruler, who knows, right?Maybe it's something that's yet to be defined.But some kind of BMC running in there, doing some management activities, tracking errors, tracking temperatures, and communicating.Mahesh talked about the orchestrator taking some sort of external cues, maybe, about the decisions it needs to make, the processes it needs to kick off.In a smaller, more integrated system, it could just be management software running on the host, making decisions about how to manage a platform.Or, if you have a very capable CXL switch with embedded firmware, maybe you don't need the BMC anymore.Maybe it's just running the CXL switch.But in terms of how it concerns the specification, the components just need to know when an external management entity is accessing them in a fabric management role.That's what we specify. How the components need to respond to a "FM".

The reason we defined it so flexibly is because we need to enable a variety of applications.It would be easy to just get focused on data center or enterprise or HPC and think about these large systems where you do have a big body of software like an orchestrator talking to hundreds of BMCs, helping to deploy workloads and things like that.But this is, generically speaking, the future of memory.We need to be able to support industrial or automotive embedded type use cases.We certainly want to address hyperscale, but we want to leave as much flexibility as possible.To enable that, most management capabilities are optional.You don't require an FM just to deploy CXL.But there are certain CXL-defined capabilities that are on the more advanced side that are going to require an FM.Composable systems. You can't really dynamically compose a system without an FM.The FM is the entity responsible for assigning the resources to the host.You need somebody to do the binding and unbinding, the allocation of memory and deallocation of memory.Memory pooling as well. When you have a pool of memory, that's no one host's memory.That's a shared resource, so you need a manager of that shared memory.So those are the sorts of responsibilities that fall to the FM when you can't rely on an individual host to take care of them.

Would you please remind us what CCI is again?Yes, I've got a whole section on it next.

The CCI is the Component Command Interface. I didn't plank that question.I don't believe that.The Component Command Interface or the CCI are the target of the management commands.That's the term we use to refer to the target of those commands.There are two types. There's a mailbox CCI, which is presented through MMIO space.And then there's an MCTP-based CCI.That's presented as an MCTP endpoint over whatever physical transport it is as long as an MCTP binding spec has been defined for it.It's not a queued interface. For those of you that are familiar with NVMe, it's not like an NVMe admin queue interface. It's one at a time.But that does not mean that lengthy operations are going to back up the interface.There is a concept of a background operation.Firmware update can take seconds, even minutes, depending on the size of your image, to complete.We don't want all management activities blocked until that operation has completed.There's a class of operation called background operations where the command that is sent is not "Go and update the firmware." The command that is sent is "Start the firmware update." And there's a nuanced distinction there where the completion of the command is "Yes, I have started the update." That's running in the background.Now I can service requests until you want to send a command where the command is "Tell me the status of the firmware update." The guidance that we use within the consortium for when a defined command should be designated as a background operation or as a foreground-- we don't call them foreground operations, but the opposite would be foreground-- is about if the operation is going to take-- if you expect the operation to take two seconds or longer to complete, it should be defined by spec as a background operation.That's the other key distinction or keynote to take care of, is that the specification defines when an operation is background versus foreground.It's not up to the implementer. The read log command is a foreground operation.You should be able to produce your log data.If I do a bad job designing a device and it takes me five seconds to spit out the log data, I don't report the read log as a background operation.I really should just fix my device.It's the specification that decides what the interaction model is for a command.A component can support multiple CCIs with varying capabilities.This is how you would enable an architecture where, for example, the host sees the endpoint, sees a mailbox, which is a CCI, and when it checks the capabilities of that mailbox, they're pretty limited.They're really just limited to working with the memory.The host is not allowed to do a firmware update.It's not allowed to get internal logging information.It's a limited interface to just what the host needs to do.Then there's a secure BMC interface, and the BMC is the trusted firmware update entity, which is a model that would make sense when the device is pooled.If you have an MLD, you don't want to allow one host to reset that MLD, run a firmware update, wipe the contents.That's a bit of a problem for the other hosts.There are also deployments where you don't really trust the BMC.The BMC is checking temperatures and errors, and that's all that it's good for.It's not allowed running firmware update, and it's not pooled.The host is going to do the pooled update.There's some flexibility in how the component defines its CCIs and what capabilities it presents on each of the CCIs.

The mailbox, as I said, is located in PCIe MMIO space.There are two types of mailbox.The primary is designed for use by the generic drivers that Mahesh referenced.It's intended for privileged operations, as far as the mailboxes go.You can optionally add a secondary mailbox, which is its own CCI.It maintains its own login context.That's designed for just pulling logs and event records if you have some diagnostic software that you want to run, but you don't want to interfere with the operation of the driver.The key distinction on the secondary mailbox is that it does not have interrupt support.There are ways to configure the primary mailbox to, for example, send an MSIX interrupt when there's a new event record, or on the completion of a background operation.The secondary mailbox doesn't support interrupts.It also doesn't support background operations.The way that a driver would locate the mailbox is first in the PCIe configuration space for the endpoint.There is an extended capability called the register locator DVSEC.This is a DVSEC that is defined in the CXL specification.It's going to specify the bar number, the register identifier type, and the address.If the bar identifier is 3, it means it's referring to a CXL memory device register set.Then you'll use the bar number and the address, which is an offset into that bar, which will give you the location of the CXL memory device registers.The memory device registers start off with a device capabilities array.In that array, there will be a capability header.The capability ID is either going to be 2 for a primary mailbox or 3 for a secondary mailbox.Then an address, which will be an offset into that same bar, for the location of those mailbox registers.Then you have the mailbox.There's a capabilities register where the mailbox advertises its capabilities.There's a control register where things like the trigger bit are set or configuring some options.There's a command register where the opcode for the command that you want to run gets written.There's a mailbox status register that gives you the run time status of a command that's in the work or if there are any errors in the mailbox.There is a separate status register for background commands.Then you have your command payload region.Basically, you write your command inputs into the payload region.You write your opcode in. You trigger it to run.When it's complete, you go and you read the outputs from that same command payload region.It's through the mailbox control register where you optionally configure the generation of interrupts on the completion of commands and completion of background operations and things like that.

Next, we've got MCTP-based CCI.Basically, all of this just gets packetized.The FM is going to discover all MCTP endpoints based on MCTP spec-defined discovery.As part of that, those endpoints advertise the message types that they're capable of.If it detects that an endpoint is capable of type 7 or type 8, type 7 is for the FM API commands, which is used primarily by switches and MLDs.Type 8 is the general device and memory device commands.They use type 8. That's how it will know it's talking to a type 3 device.As I said before, it's supported over any physical interface for which an MCTP binding spec is defined.Much of the control and obviously the payload, those are defined in the bottom here.Those just get dropped into the MCTP packet payload.

If you look at this shared fabric, as I said, any of the shared resources, it's not really responsible to have one host controlling it.The FM really needs to take care of the responsibilities that impact multiple host hierarchies.Of course, the biggest and the first management activity as a system is brought up is the fabric needs to be discovered and initialized.Go out and discover the physical topology and get everything ready to run.A big portion of that obviously is composition, so binding endpoints.Once you have your inventory of endpoints and your inventory of hosts, you need to start assigning resources to those hosts.When we talked about the virtualization layer that's presented, how all the fabric details are extracted from the hosts, the hosts can't even see the inter-switch links between intermediate fabric links.They're clearly not good candidates for managing them.Errors that take place on those links, any sort of monitoring and management of those links, that falls to the fabric manager.The GFDs, I would argue more so than MLDs, are designed purely as shared resources.The host's level of management capability on a GFD is very low.It's almost entirely up to the FM to configure the GFD and things like access controls.It's the FM's responsibility to configure the GFD to allow a particular host access to a particular region of memory, as one example.And then any unbound endpoints.Maybe when a particular type of endpoint is unbound, meaning not assigned to any host, it's just powered off and unused, but it's the FM's responsibility to manage those endpoints.Once they're bound to a host, the host has full visibility and control of them.But until that time, it's up to the FM.The edge link to the host and the link from a downstream edge and a bound endpoint, those are fully visible, fully managed by the hosts, because they have been bound and designated in that host's virtual hierarchy.

So the sorts of things that need to be managed in the fabric resources, errors in any shared media, scrubbing media, firmware update, things like that for the GFDs and the switches, and as I said, binding and access control.I also wanted to touch on access protection in a fabric, because that's an obvious concern.How do I stop host 0 from accessing endpoint 2, which has been assigned to host 1?There are a number of layers in which that access protection is in place.At the host edge, there's obviously whether the switch has been configured to expose that point.At the switch ingress, at the host edge switch ingress, if a host has not been configured for accessing a particular endpoint, there just shouldn't be any logic configured for routing to that endpoint.But that's not enough. There's also the routing path.The FM needs to explicitly go in and program a routing path.Each link, if you see D-PID X, use port Y.If that entry hasn't been programmed, then there's just no way for a switch to route to it.That offers some protection as well, because only known usable routing paths should be configured through the fabric.And then for GFDs, they get access control configured on an S-PID perspective.So when they get a transaction, they look at the S-PID, and they know whether or not they're allowed to process that combination of target address and S-PID.Endpoints don't have that same luxury because they don't see the S-PID, but we do have a number of other ways to protect those accesses.

I want to talk a little bit about the roadmap, what's coming in future specifications.

We've kind of touched a little bit on some of this already.A big body of work that we'll be publishing in an updated release of the spec is the details of the fabric management.We talked a lot about who does what and what types of roles and responsibilities there are, but we haven't actually defined all the opcodes, all the specifics of PBR switch management and GFD management.So there's a big body of work there to be done.There are also some communication paths that we'll be enabling in the fabric.We talked about host to GFD, which is memory access, and we talked about connecting hosts to endpoints.The communication model there is analogous to what direct connect would look like for type 1, type 2, type 3 devices.But there are some new communication paths that we'll be looking at, including host to host communication.How do you allow one host to access another host's memory directly, which is a technology that you need a fabric to enable.Same with device to device, and I'm not talking about just like .UIO transactions that a 3.0 HBR switch can just write from device to device.I'm talking about a device in one virtual hierarchy communicating with a device in another virtual hierarchy.So this cropped domain traffic presents some very interesting possibilities in terms of system capabilities.

Okay, so I'll do a quick conclusion, and then we'll do some Q&A.So 3.0 really introduced a lot of new capabilities.We've introduced fabric capabilities and really expanded upon the fabric management architecture that was introduced in 2.0.Expanded up the switching topologies to go beyond one level to multiple levels of switching.The symmetric coherency capabilities, basically the back and validate flows, are really going to help with pooling and sharing type use cases, and cases where endpoints are doing peer-to-peer access in memory.Peer-to-peer resource sharing, doubling the bandwidth with zero added latency, really speeds up the system, and we're still maintaining full backwards compatibility with 2.0, 1.1, and 1.0.A lot of new usage models enabled using the dynamic capacity device framework for enabling memory sharing between hosts and peer devices.Formal definition of multi-headed devices as pooling and sharing components.Through our switch support, expanding the capability for connecting type 1 and type 2 devices.And really expanding the scope and scale of the memory pool through GFAM enabled through the fabric architecture.So the call to action.CXL 3.0 has been published for a while now.If you haven't grabbed a copy of the specification, I'd encourage you to do so.We're always looking for more bright minds to help us out in defining the future of CXL, so you're all encouraged to join the consortium and participate in the calls.And follow us on Twitter and LinkedIn for ongoing updates on our progress and activities.Thank you everyone.So I guess we'll open it up to, you know, we've got a few minutes, so any questions spanning anything that was discussed today, or more importantly anything that wasn't discussed today.Yes, Sanjay.

As far as the higher link rate in PAM4, you said there will be, bit rate is, hardware is going lower, so probability of getting the bit errors, which means that more retries will be more likely.Is there some work done to see practically what are we observing in terms of the retries happening on the wire?

Yeah, so let me take that question, let me repeat the question.The question was, with PAM4 signaling and higher bit error rate, you are more likely to see replays happening.And is there any studies done in terms of what is the real impact and all of that.So yes, there have been very extensive studies.In fact, if you look at the presentation, there is a reference to a paper in there, you can go and look into that.And, you know, as the, it's a fact of life that once you go for something like a PAM4 signaling, you are going to of course have higher number of bit errors, which means, you know, try to correct some of them through the FEC mechanism, but you are going to have a higher incidence of replays.There are two ways of handling those things.One of them is, the good news is that PCI Express, CXL, all of them have a link level replay.And we have tightened the specifications so that the replay time is going to be sub-100 nanoseconds.So it should be a don't care from that point of view.And you can look into, like I said, the real bandwidth impact of that is less than 0.1%.So it should be fairly seamless, but of course there will be more replays.Because the fundamental bit error rate went up by six orders of magnitude.

There was another question?

How do you get a copy of this presentation slide?

Oh, a question on how do you get a copy of this presentation slide.I don't actually know. I'm going to defer to Elza.

We, if you reach out to us, we can send you the presentation slides.I'll also share the slides with FMS organizers and they can pull it to us as well.

Okay, just to repeat. So if you reach out, Elza can send you the slide, but then, Brian, do you want to say anything on that?

Well, we always put the presentations on our website and I'm sure we can do it for these presentations as well.

Yeah, so Brian just said that they're going to put the presentation material on the website so that you can download it.

So you guys should all have a link that you can upload your files to. Have you got that?

No, not yet. So we'll take care of that. Elza will take care of that on our website.

And just so everybody knows, there is a section of our website with presentations going back many years.So these can be added to that, just like the other presentations for the next three days.

So what is the difference between the DRAM CXL and versus the Flash CXL deployments?

Oh, what is the difference between, so the question is, what is the difference between DRAM CXL and Flash CXL deployments?So a Flash CXL deployment would advertise itself as PMEM.So there is a persistent memory capability framework defined in CXL.And through the CDAT table, reported out for a memory device, it will report things like its bandwidth and latency.So for Flash, it's going to be a very different number than compared to DRAM.And when the OS goes in and discovers all of the media available to it, it's going to make memory tiering decisions based on those numbers.So for Flash, it's going to be a very cold memory tier, and it will be treated by the OS accordingly.But in terms of the other fundamentals of how a device shows up, how the addresses are mapped, the transactions that are passed, that's all the same.

Just to add to that, I think about CXL, it's an agnostic interface.So the media that you put behind it could be DDR, DDR4, DDR5, could be Flash.And then it's up to the controller vendors to see how they're going to productize their solutions.As you can see, there are a lot of innovations in this space.Like what I was talking about, it opens up for people to innovate.You can front it with DRAM, with Flash behind it, and come up with some new innovative solutions.So it's not one media type.It'll start with typically DDR4, DDR5, but very quickly, there are a lot of innovations happening there where it's kind of bringing all of these things towards a solution.

So is CXL still pretty in its infancy? It's not a mature product development yet?I mean, I don't see 3.0, right? Most of the 2.0 products?

Yeah, probably.

So the question was that, you know, is CXL in its infancy? We don't see CXL 3.0 devices. We only see CXL 2.0 devices.CXL 3.0 came out last year. Generally, by the time a spec comes out and products get designed, validated, and you're going to do them for -- these are all at the data center level, right?So they have a lot longer of a cylindrical validation cycle. It takes a few years for products to show up.So it's unlikely that we're going to have a 3.0 spec come out and products come out at the same time.But yeah, people -- that's -- you know, we've seen this kind of a lag in other standards also, which is expected.

Thank you.

