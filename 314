
First of all, I want to thank Frank and the Memory Fabric Forum for inviting me to speak here at OCP. And secondly, I want to say that NVIDIA is a member of the Ultra Ethernet standard group. Not long ago, when the computer industry was not really interested in these little graphics processors, what could they do? We have this new technology called SSDs. So let's all focus there and design standards around that, which was probably the right thing to do. But back then, we had customers at NVIDIA that were saying, "Hey, we want to hook these little graphics processors together and do AI." So at that time, we had to leverage existing standards and develop solutions, which today, I'm going to talk about. Secondly, I want to say that, or thirdly, I guess, that our vision at NVIDIA is to follow the new standards that are going to evolve around Ethernet. And when they become mature. And if there's customer demand for them, of course, to support them. So it's not like we're running off without paying any attention to what the industry is up to. And we have, like I said, we're part of the standards group doing Ultra Ethernet.

So with that said, let's look at what we are doing today. So this is a very ingenious chart that was designed by Dan. And I'm going to butcher his name, I'm sure. Rabanovits. Anyway. Very smart guy. And he gave a talk that you can go look at. That's the link to it. Less than a year ago. And basically, what he came up with was a way of showing the different AI workloads and their requirements from a performance perspective for these five different areas. So, for example, if you look at large language model training, which is the purple Pentagon here, it needs network bandwidth. Yeah. Network bandwidth performance is important to it. And compute performance is important to it. In a different area, in inferencing, so if you look at large language model inferencing the decode part, which is the green Pentagon, you can see that network latency and memory bandwidth and memory capacity are all important characteristics of the performance. So what I'm going to do through this talk is show you the different, there's actually three areas. So besides InfiniBand, we actually started a new technology more than a year ago called Spectrum X, which is kind of, I'd call it an early version of AI-accelerated Ethernet. Anyway, I'm going to show those three technologies and how they fit into this really cool performance graph.

So, as Charles said, there's NVLink, there's InfiniBand, and there's Spectrum X. They fit into different deployment strategies. So, for example, there's the AI factory, and I'm going to go into more details on that, where NVLink and InfiniBand go together. And then there's AI cloud, which is a different beast altogether in many areas. And then there's actually a regular cloud where traditional Ethernet has thrived, but also traditional Ethernet with RDMA. And so, the AI-enhanced Ethernet. Advanced Ethernet can also play there.

So, what do these two types look like? So, the AI factory—think ChatGPT—all that data center does is that application, right? And that application requires super high performance, super low latency. So, that's typically where you're going to find the NVLink in the rack, and the InfiniBand for scaling it out. Now, if you look at an AI cloud—think, you know, when you go onto Amazon and get AI systems or any of the cloud providers that do that—you've got multi-tenants. So, all kinds of different tenants there, all kinds of different applications. And there, basically, it's an Ethernet world. It's always been an Ethernet world. Now, there's a lot of InfiniBand there, just sort of building little AI factories within them. But the whole data center basically runs on Ethernet. And that's where Spectrum X comes in.

So, in a little more detail: a regular cloud, multi-tenant variety of small workloads, traditional Ethernet. Now, AI Ethernet can actually run there. And if Ethernet becomes more AI-friendly, it will probably play there too because regular clouds can take advantage just as well of those new capabilities. And then, you get into the AI, the general, you know, the regular AI clouds that I described earlier—that's a space for the Spectrum X technology that I'll go into, and InfiniBand at the top end of that. Then, in the AI factories, it's really the fastest technology they can get, at least if they have big pocketbooks. They could also use InfiniBand in that space, and even Spectrum X, but they're going to get less performance for less cost because, in technology, it's usually a cost and performance trade-off.

So, let's dig into these three technologies, starting with NVLink. As Charles said, it basically connects the memory of all the GPUs in the memory fabric together, so you get one big chunk of memory that all the GPUs can access. And it's just like regular memory—you know, load, store. The technology takes care of any intervention, so there you don't have to worry about that, and you can do DMA as well.

So, if we take Dan's cool chart, we've covered memory capacity.

NVLink is tried and true; it's been around for 10 years. Each year, or each generation of GPU it's built into, there's no Nix for NVLink—it's basically pins. Think of it like QPI for Intel CPUs; it's just there to connect, in the case of CPUs, Intel processors together, and in the case of GPUs, Nvidia GPUs together, but it's on steroids, right? You can connect a whole lot more than QPI can connect, and you can see that with every generation, performance got faster and faster and faster. And, um, I heard Charles go into the performance at the link layer.

And so, if we go back to Dan's cool chart, now we've got memory bandwidth covered.

So, it's a switch technology; there's a switch ASIC in the middle, like all switch technologies. And this—they have 72 ports, they're 100 gigabytes per second each. There's total cross-sectional bandwidth, so 7.2 terabytes of cross-sectional bandwidth, so no blocking. And it's rack scale, so it can—as I think Charles said that too—it can. It's basically designed to go in a rack. You can maybe do two racks. It can handle up to 576 GPUs in one domain. There's management software for it, just like any network; it needs to be managed. All the kinds of things that are done there, and then one really interesting thing is what we call in-network computing.

And what, in network computing, does is actually offload this to the GPU. In this case, not a CPU, the GPU. And so, I've got some sort of a dotted line around "compute" because it saves the GPU, which it's from having to handle the network management features. For example, in the different applications where you have multiple GPUs working on a single system, there's a synchronization phase that occurs, and all that's just done in the network, right? So, there's compute power in the network that keeps you from having to deal with that from a GPU programmer perspective.

I think Charles also showed this rack. He—I should have checked with him on slides, but, uh, this is the Blackwell rack. So, there's over a thousand—over 1,200, actually—connections. It supports 72 GPUs, all connected together. So, you have 72 GPUs all sharing one big memory chunk. Any program running on any of them has access to all that memory. You can see how it's put into the rack, split up so that you have the switches in the middle, so you can use copper connections. Very, very super—like, uh, Charles said, "a supercomputer in a rack."

So, that's NVLink. Um, think of NVLink like the Ferrari of these memory fabric technologies. InfiniBand's maybe like the BMW M5 or something, so not quite as cool, but really fast and powerful. Um, right now, the switches have 800 gigabit port speed. Um, and the latency—there's also um, NICs in the InfiniBand world are called HBAs. So, from an HBA to an HBA across the switch is about 1.3 microseconds, so it's not shabby. Full cross-sectional bandwidth and 4x the amount of income in network computing, so lots of processing power actually in the switch to handle, um, clustering issues that you don't want to interrupt the GPU to solve.

So, if we look at our fancy chart here from the meta guys—um, you've got network bandwidth, you've got network latency, and you've got some compute offload. So, pretty powerful technology!

And if we want to connect the memories together using InfiniBand, there's lots of ways. Charles mentioned a few, but one that he didn't talk about was something called GPU Direct with RDMA. This is part of CUDA, which is the programming framework for GPUs. Think of it like the iOS or the Windows for your laptop; it's what all the programs run on. Um, and basically, what it allows you to do is share memory, not with load/store, but with DMA. It's called RDMA because it's across the network, but basically, without it, you can connect the memory of one GPU to another. You can move data from one GPU or write data to one from another GPU across the network at the speeds we talked about earlier.

So, now we also have, um, added memory capacity to our fancy chart.

And InfiniBand has been around a really long time—not as long as Ethernet, but a really long time. I see a couple of faces in the room that were involved with the standard back in, I think, '98; that's when it started. Version one came out in 2000. It was 10 gigabits per second then, and it's been around 800 now. What, almost 20 years later? So, it's been around a long time, a very mature technology.

And it scales to literally hundreds of thousands of nodes—um, 10,000 or over 10,000 nodes in like a leaf-spine architecture—and um, you can go over well over a hundred thousand with leaf-spine-core architectures. It has adaptive routing for congestion control, so in these um, AI environments, you get a lot of congestion, so you need congestion control to handle that, and—with adaptive routing to handle that—and congestion control, as well. In case you have different speeds, different nodes with different speeds, and other issues that can come up in just regular AI traffic, congestion control is actually much easier here on InfiniBand because it has positive flow control, meaning you can't send data into an InfiniBand network unless it allows you to, um, unlike Ethernet where you just get data and have to figure out how to deal with it. So, it's easier to do, but it's still—it's still very needed. And then, it has self-healing, and that actually kind of comes from adaptive routing. So, what self-healing means, if you lose a switch or you lose links, it just routes around them. That's basically what adaptive routing does for congestion control, as well. And then, like NVLink, you can do copper between switches up to 1.5 meters. So, I'm—put your switches in the middle of the rack.

And last but not least, to stay in my time slot for Frank, is what we're doing with high-speed RoCE, basically on standard Ethernet, to enhance it for AI. We call the technology Spectrum-X. Not quite as old as InfiniBand or NVLink, but it's based on RoCE technology, which is very old, or at least, you know, 14 years old. Basically, what we're doing here is enhancing—adding capabilities—basically taking what's great about InfiniBand and applying it to Ethernet, so as to get Ethernet performance as close as possible.

So, what happens—why we're doing this—is what happens when you run AI workloads on traditional Ethernet: you run into significant congestion, latency, and bandwidth unfairness issues. And so, what Spectrum X does is go after those, and I'll show you in a minute how it does that, or at least some of the ways it does that. But by implementing it, and this testing, where I show these performance numbers, was done at scale in a data center with GPU applications and not—you know—no change was made between running Spectrum X and not running Spectrum X, meaning the hardware was all the same. All we did was switch from standard RoCE, as good as we can tune it—and I'm from the Mellanox acquisition, so we spent a lot of years learning how to tune it—um, and then we just switch on Spectrum X, and you get an improvement. You get 95% efficiency versus, you'll see as I show more data, 60% efficiency over the best-tuned RoCE we can do, and you get a 1.6 performance improvement in the AI applications.

So, a lot of that has to do with taking that InfiniBand adaptive routing technology and applying it to Ethernet. Um, so basically what happens is, when you know in standard Ethernet for adaptive routing, you're working on flows. In this technology, and Spectrum X technology, you're working at the packet level. Now, this causes a big problem because, as you can see in the chart in the top right, when you spray the packets across all the possible links, you end up with them out of order when they get into the NICs at the other end. But, we have special technology in the NICs that, at line speed, allows them to re-order. So, that from the AI applications' perspective, it gets the same packet in the right order as it comes out of the NIC.

And this is how we're able to increase the efficiency of the existing Ethernet. So, you can see 95 percent efficiency when the adaptive routing is turned on, and there's also some other things like congestion controls that we also adopted from Ethernet. But, this is where the big story is, and you can see the difference from the perspective of utilization of the network. Utilization is super important because in all those gaps in the gray chart, the GPUs are sitting there idle, and you paid a lot of money for those GPUs to have them sitting there idle. They're burning lots of power and cooling at the same time. Oh, I'm sorry, Frank, you're supposed to tell me about these things. Should I start over? So, your GPUs are sitting there idling, right? And you paid a lot of money for them, so you don't want to do that.

The other cool thing about this technology is the latency improvement. So, without Spectrum X, you can see the gray chart; it's spread across from 17 to almost 24 microseconds of latency. Once Spectrum X is turned on, it's right between 17 and 18, so the tail latency is dramatically improved. And this is also important for efficiency because now, for collectives, you don't have to wait for the slowest member before you can move forward.

And if we look at our special chart, you know, we don't cover all the boxes like we did earlier with InfiniBand, but we covered most of them. I didn't go into the cost of these because I'm a technology guy, and they don't let me talk about pricing at all. But you can imagine that, from NVLink to NVIDIA, you know, from Ferrari to BMW, there's a difference in price. So, here we're probably talking about a souped-up stock car or something.

So, um, we were supposed to all do call-to-actions. I didn't see one from Charles, but we won't hold him to that. Um, so a call to action is that, um, these technologies are out there, they're mature, they're already in the market, and you know, scaling the Spectrum X technology has only been out for a year, but it's already at scale. I'm sure you'll see a lot of PR about that from us in the future. The solutions are here today, so um, you know, get out there and try them and give us feedback, and we'll update them. Um, you know, we're always looking for feedback from our customers. If you have any questions, I don't know if we have time for questions. Okay, um, we'll do that now, but also, you can contact me; I'm just r.davis@nvidia.com. You can find total tons of information on all these technologies here on our website, and um, they do a really much better job than we did at Mellanox at documenting things, so I think you'll find them very interesting.

I had a quick question: when you say 'turn on Spectrum X,' is that like in the hardware, or is that software?

I wasn't really, so that's another cool thing about Spectrum X. Um, you know, we had this—I don't know how many are familiar with RoCE—but we had, in our switches at least, a switch where you could just turn on RoCE, and it did it across the network. But RoCE has, you know, 60 capability, which is really cool compared to regular Ethernet without RDMA. But it's very tunable; you have to sort of tune it. You have to do kind of adapter routing on your own. The cool thing about Spectrum X is, once you turn it on, and it's just like, um, I don't know, we had a marketing term for it; I think it was 'RoCE Now' or something. When you do Spectrum X now, or whatever they're going to call it from a marketing perspective, the whole network—assuming that it's the right level of components—it's automatically doing it at the background. It's super, super high-speed. That's why you have to have the latest NICs and the latest switches because they're working together at very high speed. And I didn't mention this, but you can run regular Ethernet at the same time. There's no, you know, change out everything in the whole network. You can still support other switches connected to it; they won't do Spectrum X, but they can connect and send regular IP traffic at the same time.

Um, so the question was: When we do the spraying at the packet level to do adaptive routing across the network, where do we reassemble them? It's in the NICs. That's why the NICs have to be the latest NICs, and it's done at line speed—so, 400 gig today, 800 gig tomorrow. Yeah.

Other questions? I see Frank standing here. So, all right, thanks a lot, Rob. You're welcome. Thank you.
