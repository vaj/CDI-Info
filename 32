YouTube:https://www.youtube.com/watch?v=2bjssuOaHS4
Text:
Thank you. As was stated, I'm the chief architect for Micron's Scalable Memory Systems Pathfinding Group. So I lead a group of architects and researchers in this area of essentially large memory systems. And based on, you know, over the last few years with CXL, it's based on the CXL 3.0 disaggregated memory, you know, CXL GFAM type of space. So exactly what everybody's talking about here. So a second thing that our group looks at has been looking at for a number of years is near memory computing. In the past, before CXL, there really wasn't a vehicle to bring near memory computing to the market. You know, in order to do that, you would have to have some specialized interfaces to processors to something. And, you know, some company really can't pull that off. It has to be a standardized thing. Well, with CXL, voila, you've got this vehicle to bring into market with this scalable memory CXL fabric. It's, you know, an ideal solution here.

Okay. So what I'm talking about here today is primarily use cases applications on this scalable memory system. So modern big data applications execute on many servers with Data Lake for pipelining application storage. And so this is similar to what, you know, Charles just talked about at NEMBRD. So this data move up from the Data Lake to and forth to Data Lake as you're pipelining these applications, you know, takes considerable resources, latency. And one of the big things is this whole software stack that you have to go through to get to that storage. And so, of course, what's been, you know, described is that you can avoid that with the zero copy with direct access to memory. So system architectures need to be rethought to address this class of application. And so that's where CXL3 comes in. You know, this entire industry is looking at this and 3.0 is really centered on the shared memory space. And so even, you know, this, these new servers, these new systems with multiple hosts, memory, you know, direct fit. So with the CXL3 shared memory capability, it's ideally suited for these applications. This intermediate data set can be leveraged for pipelining these connections throughout your application space, as well as you can have these very large in-memory data sets, whether that's graph analytics or in this space here in this conference, these large analytic or large scientific applications that, you know, just require huge in-memory data sets. And one of the things that we've seen is, you know, when you're able to pipeline through these, this shared memory and reduce this latency, where that really impacts is time to insight. And so where you have these real-time applications or you have large simulations that are, that you're trying to minimize the time, that's where we see these kind of platforms can really make an impact.

So back in August when CXL3 was rolled out, the specification, I was part of the group there at Hot Chips that did a tutorial on CXL. And the area that I was assigned was a CXL3 fabric space. So at that time, it was a, you know, the brand new was being rolled out. And, you know, what's interesting about it is clearly that you can have up to 4096 ports on these fabrics. And by the time you get to CXL3, you're going to be at least at PCIe Gen 6 rates. And at PCIe Gen 6 rates, you know, if a full-width interface there is 128 gigabytes per second, huge amount of bandwidth, 4096 of those ports in the system, you know, you're talking some real bandwidth there. And so clearly that's not one switch. You know, you've got cascaded switches, but still it's a huge amount. And so with that, you're able to have these very large scalable systems with, you know, hundreds of CPU or other types of root ports with petabytes of memory. They do have a limitation. I think it's on the order of 416 petabytes of memory in their CXL.mem. But still, that's a very large amount. Now, within the next, you know, 10 years, we'll exceed that in what you can put in a system, but that's still a huge amount. And, you know, it may be that a lot of commercial systems won't be able to afford that. That's just not practical. But certainly, you know, the government space, they certainly have data sets and deep pockets enough to do that. And then, you know, another aspect here that we've, if you listen to the conference here, you know, AI, chat GPT, right? These huge models, petabytes, you know, those go away real quickly in those kind of models. So, I think we'll see limitations on memory sooner than it's expected with even with CXL 3.0.

So, CXL enables this, these memory-centric system architectures. So, certainly from a memory company, we love to see that. But here, what I show is this global address space, this global shared memory on the top with all these devices. And in a system with 4096 endpoints, you could easily have half of those as your memory. So, you could have a couple of thousand CXL devices for memory space. You have, in most of these systems, you're going to have some number of general-purpose compute hosts. And in discussions, is that, you know, a dozen? Is it a few dozen? It's in that order for these large systems. And then, you'll have these specialized compute resources. And if you listen to Hyperion this morning, their research, they presented, they said that, what, 87% of systems in this high-performance space all have accelerators today. I think, well, rather, it was 94% and 87% was the GPGPUs. So, you know, that specialized space there is just going to grow over time. And so, what I show there are these AI devices, GPGPUs, and then other specialized devices. And I think with this CXL 3.0, we're going to see that space grow more and more over time. One of the things I want to point out here is that with, say, a couple thousand memory devices that hit your petabytes, if that's, you know, what you're looking for, and you've got your general-purpose compute and your special-purpose compute, those are really more on the root port side. In general, you're going to support that amount of memory. You're going to have way more bandwidth from those memory devices than what your compute, your root ports, can actually access. It can be on the order of, you know, order of magnitude more bandwidth on the memory side. So, that's something to keep in mind here.

So, on this slide, I show a GFAM module that has within it memory-to-memory compute. And so, if you have an order of magnitude more bandwidth in your memory system, even if your memory just can support the interface rate, it still implies you still have an order of magnitude more memory bandwidth to deal with in your memory system, make positive use of, than what your processors could access or your specialized devices. The reason I have this portion on the right, A, Micron is a memory vendor, and marketing wanted to show this kind of information. But at the same time, what it shows is that with this DDR5, the kind of changes they made in it, you know, they've got twice as many banks, they have bank groups, they have the burst length, all these things, the bus rates higher.

It's resulting in a much more efficient access of that, of the bus. So, if you can get 89% utilization of the memory bus, you know, if you've worked in memory systems, that's really pushing that really high, compared to 66% on DDR4. What you're looking at there is you have significant bandwidth, enough within these CXL form factors in order to support the kind of bandwidth you're able to pull out of that CXL3 device. Now, what we're seeing is that most of these CXL memory controllers are going to have, at most, an 8-bit wide, 8-lane link, possibly a pair of them, but more likely going forward, just 4-bits wide.

But with that, you know, you're able to support that kind of bandwidth with DDR5 memory and have it sustain that kind of bandwidth.

So, why heterogeneity? Of course, one size does not fit all. So, today, you know, as was said, 87% of the systems have these GPGPUs, and those are really focused on applications with high compute density. But there's other types of things you might want to accelerate, focus on. So, in the future, with CXL3 Fabric, memory latency, as was noted probably many times today, it's going to impact application performance. And so, how do you deal with that latency in order to get the kind of optimization, the kind of bandwidth performance you want out of these systems? So, what I have here is four different ways that you could mitigate this higher latency to optimize this performance.

So, first off, modern processors have prefetch instructions. Well, those prefetch instructions really were set to pull in accesses in advance so that you don't run out of your reorder buffer space. But if you look at your reorder buffer space in modern processors, it's really set to cover just the local memory latency. And so, you know, these prefetch instructions, they may be used in some situations where you really don't have to pull in much data, maybe just a single cache line or so out in CXL, this FAM space. But, you know, it's one tool but probably not the one that you're going to choose.

A second one here is most modern processors now have data movers built into them. And talking to the processor vendors, they've designed them in such a way that you can have multi-microsecond tolerance to latency with them. And they did this because of, you know, the 3D crosspoint, this, you know, this new type of memory, it had longer latencies. And so, they put those data movers in to deal with that latency. Of course, that memory technology has now gone away. But what that really did was enable CXL with these longer latencies to GFAM memory to have a mechanism to mitigate that latency. Now, data movers are good if you have enough data you're pulling in in one block. Because in general, you'll start it up and you won't get a response back. You won't free up that data mover until you've completed it. Well, if the amount of data you're moving is just a couple of cache lines, that data mover is going to be sitting idle while it's waiting for the entire latency path to pull back. So, data movers will be very effective if you have a block of memory to pull in. But if it's just a small amount of memory, maybe not so much.

All right. So, the third thing here is new memory compute. So, where does that come into play? Well, if you have sharded data sets, so some portion of your data resides strictly on one of these CXL devices, and you want to do some type of compute on that data, and hopefully it results in a reduction of data once you've completed the compute, then you can leverage that, maybe an order of magnitude more bandwidth out in your memory system to provide that operation and provide the result to either leave it there on the device in that shared memory or pull it into your, DMA it into your processor space so it can then leverage that once that new memory compute is complete.

Now, at the end, I have a few use cases where, and one of them highlights where that could be, I think, very useful. And the fourth one here is a specialized engine that focuses on latency-tolerant compute model. So, latency-tolerant compute model, what does that mean? That means somehow that compute device was architected so it can have enough outstanding requests to memory to cover that latency and keep your memory system busy as you're performing things. So maybe that would be ideal for a graph analytics application where you've got this relatively sparse data set across your hundreds of terabytes of whatever size memory you have, and you're wanting to pull nodes, search edges, and so forth, and you want to do this all in parallel with all these various threads going on simultaneously. So, you know, today there's 87% GPUs, but there's other, the other kind of accelerators. For this kind, these kind of systems, I think there's a space where these specialized engines that can be latency-tolerant, where you don't have to use a data mover, where you can have effective single-line access and really keep that bandwidth to the memory system running, has a place. So I think that'll be an interesting thing to see going forward.

Okay. So, with this near-memory compute, with these accelerators, what you have is a three-tier compute model. So tier one is your host systems, and all systems will have a host in it today, as well as, you know, going forward here. And that general-purpose compute, it handles generally the input and output for your system. So all systems will have some type of I/O operations, and this is generally, you know, it's not high-throughput or parallelized kind of operations. It's, you know, go out and access it, interrupts, it's spaghetti kind of code. And so that'll be done very well on a host processor, where they've really put in place these capabilities to support that kind of operation. But those host processors will also manage all these other types of compute resources. So the second type I show here, tier two, are these specialized engines. And these are in systems today, these GPGPU, AI devices, other accelerators. And then the third one is this near-memory compute, which should be out in the memory space. So processing within those memory modules, leveraging that higher bandwidth.

Okay. So something that was been mentioned also is a software versus hardware-based coherency. CXL3 supports for shared memory, both hardware and software-based coherency. So they have two different modes, and they're both supported from a specification point of view. But what that really means, it's up to the ecosystem to decide what direction systems will actually be built, architected around. So for small systems, in this case, the number of root port participating in coherency will be relatively small. And so it's feasible to do hardware-based coherency. And also in those small systems, maybe you have just one layer of switches in there. So yes, you have additional latency, but it's not too bad. And so maybe you don't have to change your applications to tolerate maybe another 50 or 100 nanoseconds of latency to this kind of memory. In which case, hardware-based coherency, so all of these hosts that are working together don't have to realize the threads are on different hosts, probably would make a lot of sense. But the group I'm working in looks at scalable memory systems, systems that have, say, 100 terabytes or larger. And you're probably going to have at least two layers of switches there. In which case, hardware-based coherency in this situation really doesn't work that well, because once you start having hundreds of root ports for all these accelerators and hosts all working in the same system, all of a sudden your overhead in the memory for this coherency can be as much as the size of the data in memory itself. And I don't know too many folks that are willing to pay 2x for their memory in a system. Now, if it has the value, sure, they will. But if there's an alternative, software-based coherency, where you don't have to pay that additional and you can get the same performance or more, then that's the approach generally the industry would end up going. So how will this play out, this decision, whether the ecosystem go hardware versus software? For the smaller systems, yeah, I think you'll have hardware-based coherency. For the large ones, though, here these vendors are driven by use cases. And so if a use case doesn't come up where the value of hardware-based coherency is apparent, then it'll stay software coherent for these very large systems. And I lived through the period of time back in the late '80s, I architected a system for convex a long time ago. And that was 128 HP PA-RISC processors, and we made it fully hardware coherent. Lots of complexity, lots of extra hardware. The industry moved away from that, and I don't see it going back to that. We have these scale-out systems now, cloud, they figured out how to do it without hardware-based coherency. I don't see that complexity coming back. So maybe in these small systems you'll have hardware-based coherency, but I don't think the large ones, especially the size systems I'm talking about here.

All right, the next thing here is in these large systems, it'll require a new class of middleware. And so it's been talked about, Gismo is one of them, I think. And what we're looking at here is how do you manage the shared memory? So Gismo layers on top of shared memory in general. For shared memory, you're going to have to have fabric manager allocators, deallocators. You have to be able to have multiple hosts sharing the same memory. Now, if you look at what's in the CXL specification, they're starting to define, essentially it's a tagged DAX capability. And so that's kind of like a key and a value or a named object kind of model. And in those kind of situations, you know, it'd be probably fairly large granular chunks of memory. So that's at the very lowest level. But then you can layer on top of something that gives you more granularity. And I think, you know, we'll see all those layers being used at various points in these different applications. So that's one dimension. But as soon as you have all of these hosts in this large system, you know, you can see hot spots in your system. And so you're going to have to have visibility within these systems in order to make sure that you can get the performance out of it. You know, there's nothing like having a system, you've ported your application, and it doesn't run as you expect it, and you have no visibility. Well, in that situation, you can't improve it. And so I think these libraries are going to have to provide some way of getting this visibility in these large systems just to be able to optimize for it.

Oh, okay. So some examples of applications benefiting from large shared memory. So I've listed nine here. And like MemVerge and others, we are looking at applications and trying to understand how they would fit on these systems. Now, Micron, of course, looks at these memory modules. And in order to really make a business case, you have to see volume. So all of the applications I show here are in the commercial space. That's where you've got to get the volume. And if you get the volume, then we can leverage it in this, you know, research community, in this supercomputing space. And a lot of the work we're doing is from, you know, government partnerships. And of course, they're looking at scientific applications. They're looking at these very large sensor systems, you know. So, but we have to be successful on the commercial side in order to have the product for the scientific side. Now, we are looking at all of these. I can't say that from a commercial perspective, each of these different market spaces will see the value to go through the effort of porting their applications and tuning them for this space. You know, we're talking about brand new middleware libraries. We're talking about shared coherency support. You know, you've got to flush the data out of a processor in order to make sure it gets back to memory. So there's, you know, there has to be value to these market spaces in order for them to go forward. Now, time to insight, generally, for a lot of cases, will be that value. Now, I've picked out three different of these applications that I'll show in a little more detail.

So this first one is machine learning training pipeline. And so this shows, you know, the same memory-centric system organization, but it shows how the compute and specialized resources could be used to implement some of these kind of pipeline kind of solutions. So in this first one, you have on the left, the pre-processing of the data. And then on the right, you have the actual training portion of it. And what I show for the pre-process is in cleaning, reduction, transformation. So if you're doing, you know, a training on images, well, you have to, say, identify the faces. You have to then size it and transform it so it's, you know, the right shape and size for you can do training on it. And generally, you know, if you got cell phones and you take your picture, what's that, 20, 30 megapixels? Huge, right? And then once you do this transform, oftentimes you get that down to something much smaller. Is it 64 by 64 pixels, 256 by 256, whatever it is, but it's much smaller. And so you have that reduction in size. So I picked this one specifically because you could see the value of doing near memory computing for this kind of place. So we've talked to, you know, some vendors that are in this training pipeline space. And what they say is that sometimes this pre-processing can take as much time as the machine learning itself. And so if you're doing all this image manipulation, you can see that. Well, if you can use the 10x order of magnitude more bandwidth in the memory system to do the pre-processing and having your host and your GPU resources doing actual machine learning where the compute intensity is very high and this pre-processing that computed tensor usually is much, much lower, you can see that making a lot of sense. So this particular one I selected because of that value in the near memory computing side of it.

This next one is a recommendation system. And here you've got, you know, this first one is pulling, the first stage of the pipeline is pulling in these events, whether that be clicks or views or purchase, whatever. And so it's pulled in that cluster. And then actually go into the Spark streaming portion of the pipeline. And that's where, you know, Spark, if you look into it, it's really its own set of stages of work. And between every stage they usually push data through in mini-batches. And then each mini-batch, once it gets through a stage, is put into your SSD. And it does that so that if the system fails, then you can resume right where you left off right there. And so this is, again, that producer/consumer Gismo, potentially kind of a solution. So a lot of steps going through that SSD data lake. And if you can reduce that, you can reduce the overall time to insight. And you can see in a recommender system, if you're looking at online kind of things, it is real time. You want to make sure that the person that's, you know, interacting with your sales system has the best experience. And if you can have something that reduces the time to insight, maybe you put more compute in so that you can give them better results. You can, you know, you can use that extra time to have better results out of it. So this may be one place that could be really high value because that directly relates to how many purchases they're observing, right? So I think this will be a very interesting space potentially as well.

And the last one I picked was this graph analytics system space. And the reason I picked this one, yes, there's a lot of pipelining going on. You can avoid going to SSDs, you know, the data lake quite a bit here in these connectors. But what I really picked this one for was this graph analytics platform where today, if you talk to these in-memory database companies, they generally are limited to the amount of memory you can put in a single system. Is that, you know, a few terabytes? It's on that order. But with a system like this, you could scale that up to a petabyte. And yeah, they're looking at how do you do that in a distributed fashion, those in-memory databases. But there's nothing that's going to compete with just one large shared memory space for these applications. And if you can have, you know, I showed here this specialized, well, if you had a latency tolerant capability, so you could go out and randomly access your data and keep the memory system busy, that would be, I think, an interesting solution, potentially really speed up time to insight applications like this as well.

So, as a summary, so continuing to scale this big data application requires minimizing the data movement. And as has been said, certainly in MemVerge, and I assume before I came here an hour or so ago, you know, with the shared memory, you can do these zero copies. That's going to have a huge impact on time to insight. And this can be applied whether you have sharded data, so you have data within a single module, or uncharted like these graph analytics kind of applications. So, CXL3 was envisioned and architected to support applications scaling through very large usable memory space.

So what is Micron's plan, particularly in this space? 

So, certainly, yeah, so certainly Micron, being a memory vendor, they currently have today CXL memory modules, and they will continue to go forward as CXL evolves. So, CXL3 with this GFD, global fabric devices, it will certainly have memory devices there. And then beyond that, you know, our group is looking at memory computing. So, is that a possibility? You know, the business units have to decide if there's enough value there to go forward with that. We're certainly doing that, the pathfinding in that space. And we are working, as I said, with government entities. And we are essentially through these contracts, standing up proof of concept systems and leveraging, you know, trying to pull in like CXL3.0, trying to pull in systems, prototype systems early so we can port these large applications and understand the tradeoffs. Is there value in different types of access models, key value store, named objects? Is there value in memory computing? So, we really want to enable the software environment to make progress so that when there really is CXL3 hardware, the software is ready to go as well. And we, as an ecosystem, can go forward with products and, you know, stand-up systems. And we're not waiting years for software to catch up, which is often the case.

Thank you.
