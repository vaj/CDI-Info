YouTube:https://www.youtube.com/watch?v=6-pSIGzncbs
Text:
Hello, folks. This is Nilesh. I'm part of ZeroPoint Technologies, and we are an IP licensing company.And I'm going to talk today about the hyperscale composable memory systems with dynamically adjusting compressed here.

So, we'll jump right in. I want to start with the challenge. The hyperscalers are spending a significant amount of CPU cycles on software-based compression at scale in their infrastructure. This is widely published in various papers by Google, Meta. So, they're using upwards of 4% of the CPU cycles on compression. In some of the workloads, the CPU usage could go as high as 50%. That's what they're... Analysis across the fleet has demonstrated. So, as a response to this, of course, you know, we know CXL. There is a hyperscaler CXL tiered memory expander spec that was put out through the Open Compute project. And this was put out by the hyperscalers themselves. One of the key features they are asking for is inline memory compression at line rate. Now, there are... Several other features, of course, the key one being DDR4 decommissioned use on these CXL deployments. And there are also a cache for decompressed pages. So, this is a very critical piece of the whole TCO, total cost of ownership reduction, that the hyperscalers are trying to achieve with the help of CXL.

Now, not only are they spending CPU cycles, what the various other papers have demonstrated—if you look at the figure on the left—is that, first of all, cold data specifically has great compressibility, almost a ratio of 3 to 1. And this is, again, from a study at hyperscale. So, this is analyzing a variety of workloads that are running in data centers in the cloud. So, what are you looking at out here? There are two things. The first one is the compression ratio, which is on the x-axis, is about 3. That's if you look at the cumulative distribution function. And then, the decompression latencies actually are maxed out at about a few microseconds. So, what this shows is the compression, decompression is a pretty prime target for a tier 2 memory like CXL. And so... That's one key takeaway. And then, if you look at some of the other publications looking at traces and workloads running at scale, at warehouse scale, another interesting observation was that cold data typically requires a very small fraction of the total system bandwidth. So, again, what this shows is if you're going to do some kind of accelerated compression, decompression operation, closer to the CXL memory then—or in a tier 2 type of memory—then the CXL is actually a pretty good spot to perform these operations without interfering too much with the bandwidth performance.

So, as a result of this problem that we're seeing, and some of the opportunities for cold page compression compressibility and also with the low bandwidth utilization, what we saw recently on the left, actually very recently at the Future of Memory and Storage Summit, is the hyperscalers actually came out talking about a compressed DRAM tier in between NAND and DDR as a pretty compelling solution to increase and enhance the TCO value of CXL. So, this is a pretty key message that was delivered at the Future of Memory and Storage, jointly by Google and Meta. And then, of course, matching this is the OCP spec, which is a call to action to the CXL controller vendor to actually include this kind of an inline compression capability within the CXL controller itself to do a couple of things. One is this enables, first of all, removes the adoption barrier for CXL. And then, it enables a diversity of customers, not just the hyperscalers, because once you put this feature in the controller itself, now it's accessible to anyone who wants to use it. And this also unlocks a key CXL value proposition, which is the total cost of ownership reduction, which is what everyone is trying to achieve with CXL. And that's the promise of CXL.

So, I'll show you the TCO model a little bit later. But in the meanwhile, I pulled out some of the requirements from the OCP spec. And by the way, the link below actually takes you directly to the OCP spec, where you can look up all of the key details. But some of the key artifacts of this specification, the first one, of course, is latency. So, now that you might have compressed data and uncompressed data, there is a very tight spec on the latency of accesses, which is in the order of 100 nanoseconds. And this is for uncompressed data. If you look at compressed data, there is a range of latency, which is tolerable, which is from 250 nanoseconds to less than a microsecond, especially for tail type of performances. And then, of course, there are specifications on the decompression bandwidth, the block size algorithms, and configurability. So, end customers want complete control, and they want configurability over these types of capabilities, which actually frees them up to perform trade-offs between compression ratio, which means more capacity versus trading off latency. So, that's focusing more on performance. So they want these knobs so they can adjust or make these settings for themselves. Now, if you look at the right side, what you're looking at is a depiction of some of the key compression/decompression algorithms in the market out there today. And on the x-axis is my time in terms of time to compress and decompress. And then on the y-axis is the compression ratio. How much can you compress the data in terms of compression ratio factor? So, if you look at some of the open standard compression algorithms like LZ4 and zstd, these are pretty well-known open in the industry. And they work great for storage because in storage, you can tolerate microseconds of latency. But when we talk about CXL, now you're talking nanoseconds. So, a lot of these algorithms actually work well at the page granularity level, but not so much at the cache line granularity level. So, that's where one of the algorithms that we've implemented, it's a proprietary algorithm operating at the cache line. This actually satisfies the low latency requirements and provides a pretty competitive compression ratio. So that's a key thing. You want compelling compression ratios without fading off the latency. So, that's some of the things that the spec is calling for is LZ4 is required. So, this is something you would need to implement, but then there is room for innovation.

Looking, maybe a double clicking a little bit into the controller design itself, and how a compressed tier might fit in. So, the vision, and actually what the spec calls for, is the ability to manage and offer multiple tiers within a single CXL device. So you might have your standard uncompressed tier within the CXL type three device address space. But now, we want to introduce a dynamically adjusting compressed tier. So depending on the data flying into the CXL device, the compression ratio might change, but then the device, the CXL controller should have the ability to dynamically adjust. And present the capacity dynamically back to the host. So that's where this inline compression, decompression, uh, accelerator within the CXL controller device itself can offer this capability by doing not just compression and decompression, because that's only step one, but then you need to compact the data in order to benefit from the additional capacity. And then, of course, you have to transparently manage the memory because we don't want to burden the whole software having to be aware of and managing this address space. This is something the device and the compression architecture should already take care of automatically for the host.

One of the key questions that we get asked about compression, and where there seems to be some confusion, is the compression that we know and love today in software is all a look-aside compression. And there are also hardware implementations of this compression technology for storage. Now, the big difference, and I call this a look-aside accelerator, is in the domain of CXL. You would actually store the data first uncompressed into the media, and then you would invoke the compression or decompression engine to compress the data or decompress it before delivering it back to the host. So, this requires an explicit software control command to perform compression, decompression. This is not what the OCP spec is looking for. Actually, what the OCP spec is looking for is an inline solution, a transparent solution, where as the data is flying in over .mem on the CXL interface, the compression, decompression should automatically take place. The address, remapping, managing all these different cache lines which may now have been compressed together, and the capacity which may now have gone, let's say, for an example, from one terabyte to two terabytes, all of this capacity is automatically managed on the fly without any software intervention.

So, ZeroPoint does have an IP solution which actually complies and actually exceeds the OCP specification, and this piece of IP solution is ready to be integrated pretty much into any CXL controller, ASIC device, because this is a piece of soft IP that can be integrated into the most common buses that exist on your SOC, like AXI4 or CHI, and all of the leading process nodes are supported. And there might be some controllers on older nodes; they can take advantage of this IP as well. So the great thing about this IP solution, it's not just an IP; it's a complete solution that seamlessly integrates into the CXL flow and offers 2 to 4x transparent compression, decompression in line, which is what the OCP spec calls for. And it actually implements not just the LZ4 algorithm, which is a requirement, but also the proprietary cache line algorithm, which offers the end customers the knobs that they're looking for. So they can fade off latency versus compression ratio as and when they require.

In terms of the total cost of ownership reduction—I spoke briefly about this before—we modeled the total cost of ownership over a 40-server rack over a three-year lifetime. And what we found today is that CXL itself provides CAPEX and OPEX benefits, but compression then actually also offers an additional benefit because you effectively lower the dollar per gigabyte cost of the same capacity or physical capacity of memory. And when you take compression on top of CXL, now you're starting to get significant CAPEX and OPEX reduction, which is what customers are really looking for as they decide and look at how and where it makes sense to deploy CXL. So, one key message is that the compressed memory significantly not only reduces the TCO but also reduces the carbon footprint. And in this TCO model, actually, we have not even looked at decommissioned DDR4, which is a key thing the hyperscalers are looking for. That would actually further improve the TCO model, and this is something we'll update in the coming weeks and we'll share. But this is a key value proposition of compression: reducing significantly the total cost of ownership for the adoption of CXL scale.

Now, I spoke about the fact that the compression ratio varies by workload, and this is actually the nature of lossless compression. So everything I spoke about is actually lossless compression. So, of course, there are many lossy compressions, compression solutions out there, which work fine for video and internet streaming and things like that. But, of course, in the data center, we don't know what the data type is, so we have to assume compression has to be lossless. So having said that, if you look at this chart out here, we measured the performance of these algorithms across a variety of benchmarks like SPEC, and, of course, MLPerf for AI-style workloads, and then MonetDB for more database-style applications. So what we found is our proprietary algorithm, which operates at a cache-line granularity, which means extremely low latencies. That's the purple bars. That's offering significantly competitive compression ratios compared to LZ4, which, by the way, we've also implemented LZ4 and the cache-line compression algorithm. But if you look at LZ4 at the 4KB data point, which means you have to wait for an entire 4K page before you can start decompressing a cache-line. So the latencies are extremely high. But, of course, the compression ratios are better in many of the cases. But you can see the cache-line compression is pretty competitive in most of the scenarios and, in some scenarios, actually outperforms the LZ4 algorithm. So this is the solution where, by offering both these algorithms within the CXL IP solution, now end customers have a choice whether they want high compression ratio or they want to trade off and get lower latency, compress or decompress at the cache-line or page granularity.

Now, one of the things that I mentioned before is this: the entire capability is transparent. So, there should not be any software management required. But when you're deploying this capability at scale, customers do require some telemetry and configuration information. So, this is something that we've implemented as APIs over .io. So, you'll notice none of the critical data path is over .io. This is all configuration and telemetry-style information that is communicated over .io back to the host. It's more for reporting and fleet management purposes. But, one of the key things we are working on is to upstream these APIs to the Linux CXL driver so that it becomes part of the standard CXL solution and it becomes easy to adopt this solution. And you don't need to do any custom software changes or installation. It's all transparent to the host applications.

So, with that, I'd like to summarize and state some calls to action. First of all, the compression IP solution that we have is already compliant with the OCP spec. It's ready for integration into your controller design. It is portable across process nodes and various interfaces that might exist on your SOC, and the performance has already been verified. This is something that we believe can enable and democratize access to compression capability for all and make for a solution that can be adopted by the hyperscale customer. In terms of calls to action, we want to work with the software vendors because all of this hardware is no use without compelling software solutions. We'd like to work to integrate into various host software to leverage and demonstrate some of these workloads, which right now we've measured on benchmarks, but we'd like to demonstrate these on real-world use cases. We've actually worked with some hyperscalers who've given us their data center traces. But now, as we start to integrate into real devices, we'd like to have real workloads running on these kinds of compressed memory tiers. In terms of calls to action for controller manufacturers, we'd love to collaborate and work together to address these opportunities that the hyperscalers have laid out and put out in a very clear spec to achieve lower latency, decompression, providing optimizations for small block sizes, and then supporting multiple tiers, including compressed DRAM on CXL. So that's a call to action. We do want to collaborate with controller vendors to integrate this capability, make it compelling for end users, and accelerate the deployment of CXL together.

And with that, I'd like to conclude the presentation. Thanks so much again for the time. And please do reach out at my email or over LinkedIn, and I would love to discuss in more detail this solution. And also, I'd like to understand if there are other use cases where we can collaborate together. So, thanks once again.
