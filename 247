
Good morning. Thanks all for joining today. I'm here to talk about optics and AI clusters from Meta's perspective. I hope that we'll have a good conversation. I'll try not to talk too fast, but fast enough to leave some room for questions at the end,  but of course I'll be here.

I don't think anything in this chart is new. I think we've been talking about these sorts of things as an industry for quite some time  now. I think what I'll do today is try to talk about and motivate the growth of this model  size. Why does the model continue to grow and what does that mean for I/O and for the optical  industry in general? The trend is clear. The trend isn't stopping. I think we had a gentleman from Google yesterday who spoke about this, I think in some detail  and quite eloquently. I will try to add some color to that same trend. As Vlad answered in that last question, what about PCIe over optics? Where is that going? I think PCIe over optics is the same as any other PHY. It's essentially increasing, doubling maybe every two generation or every generation,  every couple of years, but the requirements on the AI/ML model size certainly are growing  significantly faster than that. 2x every four months versus 2x every 24 months. As a hardware industry, we're falling behind the requirements or the desires. I think,  that we have in the AI model size, which is causing the challenge I think we have as providers  on our side. In the same trend, flops per GPU isn't increasing as quickly as we would like either. Same physical trends are occurring there. We're seeing the opening gap between what we want to do or what we need to do to reach  this artificial general intelligence, AGI inflection point or something, or increased  or better LLM models. In order to be able to do that, we have to innovate across the entire stack. This is the call that I think Meta and others have to innovate software all the way down  to hardware. We'll talk about that in just a second.

This seems to be a new chart. I'm surprised a bit because it's out there. This is Wikipedia in general. In general, I think the message that people should be taking is larger models give better  results and we don't see an end to that trend. If that's the case, then we need larger models. Larger models trend to or require larger amounts of flops. If you look at this curve here on the left, I'll point you to this reference. Essentially, if you look at the amount of time you put into compute or flops, so flops  per day, so length of time over a number of compute, and you look at the loss signal,  your loss signal plateaus. Meaning essentially for whatever given model size you had, you can get so good, but it  doesn't matter how much longer you train it after that, that's as good as it gets. You have enough parameters and then you're done. Our goal, of course, is to get the best test loss possible, the best model possible. Given the fact that we can't train our models, well, it doesn't matter how long you train  your model. At some point, you reach an asymptote. The question is really how do you get more parameters in your model? How much data can you get into your model to get better results and hence the better  result at the end? Turns out in the end that the loss is inversely proportional to the size of the dataset and  the number of parameters that you put into it, and the compute is linearly related to  the size of the dataset and the number of parameters. We want more parameters, we want bigger datasets so we get better results, and that requires  more compute. It's essentially that simple, and I'd encourage you to look at this. But really what that means is as providers, what we need is flops. We need more compute to be able to get these models trained faster, to be able to get the  results out faster, to build onto the next dataset. But flops obviously change the AI cluster design, and they have significant implications  on the amount of I/O that we need.
 
On top of that, the previous chart really is, this is a training chart, right? Sort of built, I think, loosely on the Chinchilla model from Google. So this is how we sort of train the large language models, Llama2, Llama3, and all the PaLMs and ChatGPTs. However, training, I think, as those of us in the AI industry sort of know or are learning,  is only a single portion of the entire sort of stack or value proposition that we provide. There's obviously large language model training, which sort of results in these models, but  then we have to use them, right? When you put your query in, it has to be sort of consumed and then it has to be sort of  spit out. So these are the infill and the decode, essentially, right? The sort of inference portion of the stack. And you can see how they put on different strains on the AI network that we put together. In the ideal world, we would have one set of hardware which would be reconfigurable  to meet all of these things. If we needed a new GPU or a new AI cluster for every single one of these, and you end  up bifurcating sort of your investment. So this sense of complexity across the stack, both for large language models, but also for  some of Meta's workloads like ranking and recommendation, push us to have more flexibility  in our AI cluster design and by extension, the hardware design. Which sort of feeds in  the end to more efficient capital investment across our stack, which is, of course, always  a good thing from our perspective. Maybe not from the hardware perspective, but that's what we want.

So okay, here we are. We want more flops, which means we want more GPUs, right? We want more GPUs across the entire cluster, and we would like some sort of flexibility  in that to meet the diversity of workloads across the training and the inference stack. These clusters, and I think Vlad showed some nice pictures from NVIDIA, and I think those  are, I would reference those and continue to encourage people to look at the publicity  that NVIDIA's put out around their designs as very good reference points. You end up with connectivity models or sort of architectures which look loosely like this.  Meaning in general you have a bunch of GPUs very tightly coupled within a rack or in subsections  of a rack, potentially even between racks, then you build a scale out connectivity cluster  on top of that. In general, this will be things like the InfiniBand and the RoCE types of interconnects, and  the scale up will be this sort of CXL or NVLink type of connections. On top of that, of course, we need a sort of front-end network. This is the sort of network which connects us not only to the data, but to the rest of  the world so that we can pull things out. So there's quite a lot of sort of data being pushed across here. I think the important thing to understand is that the datasets and the model parameters  are so large right now they don't fit on individual GPUs anymore. And so what we're trying to do as a community is figure out how to shard this data across  different collections of our stack in order to train faster, train better, train with  less data, become more efficient, sort of bend the curve a little bit so it doesn't  become linear with the dataset. In general, that means that some of these parallelizations will drive the amount of  bandwidth required and scale out networks, or these InfiniBand ones, where some will  drive the scale up network. So there's sort of connections within the rack, you can think. Generally any one of these single vectors sort of gives us an end scaling law, which  means if you get n sort of GPUs in the scale up network, the scale out network is an nth  of that sort of fabric. So Flops drives I/O, but close copper-based in general, sort of NVLink type bandwidth  drives down the bandwidth required in the upper levels of the network. But everything's scaling. So Flops per accelerator really in the end drives up these sort of synchronization steps  and more data, creates more data, more data per unit time is more bandwidth. And so these  sort of scale up requirements or higher Flops in each accelerator then drives up the I/O  bandwidth per GPU. Whereas these larger number of GPUs across the cluster drives up sort of the number of  links, so it drives radix and a number of links that we have to put out there. But in general both of these are going up, so we need more links and we need faster links. So sort of music I hope to the hardware industry's ears.

I'm trying to call myself to account here and admit to the fact that my crystal ball  is maybe a little bit better than Vlad's for at least Meta's applications, but it's not  perfect either. This is the type of slide that I had been showing last year sort of significantly, trying  to give a concept of where we saw the sort of sets of requirements being needed across  a single GPU node, and I'm going to sort of call myself out and show you where I missed. The idea here being that AI/ML GPU nodes aren't monolithic anymore, and we've seen the Grace  Blackwell announcements from NVIDIA and from Jensen, and clearly that's true now. We're going to need more HBMs, so more HBM stacks across that. And what we saw, at least last year, was that network GPUs, so this is a sort of scale up  and scale out network, was on the order of 900 gigabytes per second. You know, 18, 400  gigabit per second sort of logical links distributed through copper and optics. But scaling, you know, if you thought 50 percent generation over generation, we were going  to get to two terabytes per second per GPU, you know, pretty soon. GPU to GPU, this is the sort of PCIe fabric, you know, fractions of a terabyte, but still,  you know, not insignificant. And then these GPU to HBM links are trending to, well, certainly being deployed now at  a terabyte per second per stack, and there's multiple of these stacks, so each one of these  GPU cores has a very, very significant amount of bandwidth. And I'm suggesting 10 terabytes  per second, and, you know, boy, isn't that scary? That's a lot of I/O.

So, okay, so let's look at this a little more carefully, right? I mean, they're no longer monolithic, that's absolutely true. In fact, as we've seen from the Grace Blackwell design, it's not only sort of single GPUs  anymore, there's multiple GPU dies in a single package. Which means that now there's GPU  to GPU communications that's inside the package as well, so if we were going to account for  all the I/O, it's exploding, right? Certainly more HBM stacks, wow, you know, that wasn't sort of too significant a call,  but even so. The I/O bandwidth, massively under-called, right? I'm going to give myself a real sort of thumbs down on that one. Here we are talking about 900 gigabits per second, you know, maybe trending to two. And  Jensen's already announced four terabytes per second per accelerator, so under-called  by 2x in sort of half a year, a year's worth of sort of trending here. I'm not going to talk too much about GPU to CPU. I don't know that came out too clearly in what Grace Blackwell is, but I can tell you  that it's much more significant here than this sort of PCI link, so I don't think we're  clear there either. And you know, GPU to HBM, I'm going to give that a sort of plus, certainly more is better,  but it's trending to 16 terabytes per second per accelerator, at least from the, if we  count one package as one accelerator. So we've already got 16 terabytes per second, and so here I am thinking, oh, you know, I'm  scaring people by talking about 10 terabytes per second per package in the next couple  of generations, and we're exceeding that significantly already with the latest announcements of 22  terabytes per second per accelerator in the Grace Blackwell generation, which is essentially  targeted for mass production next year. So okay, not so great. So here I am thinking 16,000 GPUs, that was what Meta had announced last year in a single  cluster, you know, 10 terabytes per second for each accelerator was going to push us  to 160 petabytes per second per cluster, which is a staggering amount of data, and again,  massively under-called, right? 24,000 nodes per cluster is what we announced just a few weeks ago, and that's a small step  on the path of sort of where we're going, and at significantly more than, you know,  10 terabytes per second per accelerator, we're going to massively blow through that as well. So the amount of I/O that's needed just in the next generation, which is, this goes back  to sort of Vlad's comments and some of the questions, the next generation's designed.  You've seen the design, it's based on sort of known technologies and copper and sort  of optics where it needs to be. We're designing the generations after that right now, which have to be orders of magnitude  better than this to meet the trend.

So the I/O fabric demands are truly significant. Meta has announced, and I would push you or show you towards this sort of link here, 224,000  GPU clusters just in the last few weeks. You can see the details here. There's one cluster based on RoCE, one cluster based on InfiniBand. We've published details on the hardware, the network storage, design, performance, and  software for various portions of these workloads, and we're training the Llama3 models on this  now. So this is based on previous hardware, not sort of the Grace Blackwell that we talked  about previously. Meta, as a long-time sponsor, I think, and co-innovator around OCP, continues to be strongly  committed to this sort of open community here. And so we built these clusters on the Grand Teton open rack and PyTorch technologies that  we've published and shared specs with at OCP, and we continue to sort of push this innovation  and collaboration to ensure that we can meet these requirements moving forward. And I suppose the really important thing to understand is that 24,000 node clusters this  year don't mean that we're going to top out at 24,000 node clusters. We're going further than that. The models need to be bigger to meet the sort of requirements that we need.
 
We're really at a very early and exciting portion of the entire sort of LLM and AGI  journey. We need larger software models. We need larger training sets. We need sort of more innovation to try and bend this curve so that we don't have to be  up at planet-sized sort of GPU clusters to create AGI. But these demands to push for more flops are going to require more I/O, and as Lad sort  of talked about before, right, the more I/O you need, the more investment you need across  the size of these clusters, the more efficiency matters. Efficiency matters in area. Efficiency matters in power. Efficiency matters in dollars. Efficiency matters in latency. All of these things sort of have to be innovated on aggressively in order to create these large  clusters to meet the requirements and demands that we have. And then I'll sort of hit this one again. There's innovation across the entire stack. Hardware, model parameters, compression of the models, compression of inference, software  paradigms, memory architectures, system I/O, et cetera. And we're going to continue to innovate across all of that. The hardware and the optical hardware and I/O is a part of that sort of discussion. We look forward to continuing the conversation. So thanks very much.

We have two minutes for questions. And there's a microphone in the back. But I think the room is small enough. It's recorded. Oh, it's recorded. So we need to use a microphone. But I don't see any questions. So I guess there's one in the front. Let's get you a microphone first. So it needs to be.

Hi, good morning. Just a question. With the evolution here and learning, as I said one month ago, it was OFC and in the  executive from Optica, it was clear that CPO was far away to be ready for the requirement  of the market. So with the speed of the evolution we go today, how do you see the impact as the optics is  not ready to follow the speed of the higher level?

So let me try and repeat back and then answer. So what's the impact of optics not being able to meet the speed requirements or the demands  of the requirement? Quite frankly, that means that we'll re-architect around it. There was a famous phrase used back in the day. Don't shoot behind the duck, which means essentially, right, I think this is very important in a  sort of new technology. If Vlad's right and the first OFC demo was two years away from mass production and then  you have to do HVM sort of scale up past then, it means really you're three years from your  first demonstration to the point at which you can meet the volume demands that we require  to put out these large clusters very, very quickly. How long does it take you from where we are now to the first OFC demo? One year? Two years? So if we're looking at a development cycle, which is four or five years out, my crystal  ball was sort of wrong, right? A year out. Vlad's, you know, maybe he's more accurate because he's been doing it better and longer,  but he's only got sort of two years and he costs us beyond that. So if we're looking at a development cycle where you're starting with a new technology  and you have to innovate from there, you should be looking at what you think the targets are  five years out. Otherwise, you're shooting behind the duck and you'll miss it. The only other point I'll make, right, and this goes back to the comment I think Hong  made about sort of CPO optics is always sort of two years out. The decisions that we're making on the sort of next-gen clusters have been made last year,  and the ones that we're making about sort of clusters from two years or three years  from now are sort of being made now. And so if your new technology is sort of coming now and you think it's going to be ready in  three years, I can't bet on that. We can't bet on that for the next cluster, right? We can't put however many billions of marks, dollars into this cluster and hope that some  optics will sort of work sometime, right, regardless of all the reliability associated  with actually running the thing. It has to be deployed. It has to be bomber. So we need optics earlier, which means we need sort of co-investment, co-discussion  around where the technology is, how mature it is, how scalable to volume it is, and how  well it can be sort of operated and serviced in these large clusters in order to be able  to build on it. So I guess my comment is however much I/O you think you need, sort of multiply it by  five or ten and let's get it ready earlier so that we can have these discussions and  make it real. But I hope it will be a snowball effect, right? Once we get it done once, then everybody will start to see. But it's faster, lower power, more efficient. That's what we mean.

All right. Let us thank the speaker one more time.
