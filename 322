
Hello everyone, my name is Khurram Malik. I'm the Director of Product Marketing for the CXL Product Portfolio here at Marvell. So, let's get started.

So, you might have heard and seen our big product announcement at FMS, and I'm here to talk a little bit more about it—what we are doing, what the product looks like, and why we are actually building this product, right? So, first of all, what we have seen is that the number of compute core accounts has been growing significantly, but the memory bandwidth per core has been going down. And recently, what we have started to notice is that the gap continues to increase over time; CPUs are becoming more and more inefficient for memory-centric applications.

The second thing we have started to notice is: What does it take to get more memory pins connected to the CPU? These constraints include packet size and thermals. There are certain limitations which prevent these CPUs from having as much memory as required for memory-centric applications.

Also, what we have started to notice is that the second most expensive component spent on in the data center is memory, right? Meta has published a paper which openly said that half of the platform cost is spent on memory. Just think about it—half of the platform costs, right? One of the most challenging parts for these hyperscalers and customers is anticipating how we can recoup those costs. What does it require? We can monetize those costs for a much longer period of time, right? And that's where sustainability comes into play—recycling DDR4, decommissioning DDR4—and how the overall bigger picture fits together and what can be done to leverage that. What CXL product portfolio needs to be offered to obtain that?

So, these are the three use cases that have been very well defined, and, and, and you have been hearing probably since, since many years. The first one is a memory expander. It simply adds memory capacity and memory performance to the CPU, and it enables the flexibility. When we talk about flexibility, it means it enables our customers to mix and match different types of DRAM sitting inside the enclosure. For example, a CPU, which can only support DDR5, will be directly attached to the memory. But with CXL, controllers sitting inside the appliance will have a DDR4 connected to it to provide more memory capacity. And then we have also some customers who would like to use DDR5 for memory expansion if they have an application required for high memory bandwidth. It depends upon what application you're looking for, what pain point you are trying to address. The middle one has been talked about a lot as well. The memory pooling, what does it do? It enables multi-dynamic memory allocation where multiple hosts can access the same memory and take advantage of it. A lot has been talked about on that angle, but there are many. I think there are many POCs that have been happening, but still, yet to see what application and what TCO will take that use case and go into mass production. On the very right side, what I'm sharing here is a memory accelerator. And this is a very interesting use case. What has been happening here is the industry has seen a great benefit of bringing accelerators closer to storage for SR-IOV virtualization. But also there has been discussion about how we can bring accelerators closer to the memory. When we talk about accelerators, it means how we can bring the compute capability closer to the memory so that the data get processed on the card sitting right next to the memory, and then shove the processed data back to the host.

So, with that said, what we have done—our thorough analysis—and what we have constructed, there are two main pain points we'd like to address. And what shows here—that the x-axis is the capacity, the y-axis is the bandwidth per core. What we have seen, after talking to many customers and evaluating thoroughly from the application side of view, is one use case which shows on the very top left side: higher memory bandwidth per core, right? And this is what I mentioned; that's where one of the biggest challenges in the industry has been happening. And how we're going to address that is because one of the common questions could be asked, "OK, what ratio is the best one, right?" Again, it depends upon the different workloads. But when we navigate through and say what application can be targeted, what we anticipated to see—10 gigabytes per second per core—is a rough estimate where the industry has started to realize that that could be a good starting point to go with, right? And the second is a higher capacity. And higher capacity is like, how much capacity do I need for my application? That needs to be decided inside the server, right? And the capacity we have seen, from roughly from three terabytes to six to eight terabytes. And now we are talking about capacity in the storage world, yes, three to four terabytes is nothing. But we are talking about capacity in the most expensive memory capacities sitting inside the server. So, in order to address these two core pain points, Marvell has developed a product portfolio with a product brand name called Structera.

One of the common questions asked by me is, "Okay, why Structera and what does Structera mean?" So, just a quick definition: Structera, I mean, "structured of New Era." The CXL product portfolio defines how to develop the chip with the new structural definition of the product. So, we have addressed those two pain points using Structera A and Structera X. Structera A is focused on near-memory accelerator, and Structera X is focused on memory expansion controller.

So, what we are trying—what we are offering using Structera A—is a PCI Gen 5 1 by 16 lanes and CXL 2.0. And the memory capacity, memory bandwidth, is a critical one which I highlighted here: 200 gigabytes. And when we talk about 200 gigabytes, the capacity bandwidth is the capacity of the PCI Gen 5. And earlier, I mentioned that industry would like to bring compute capabilities closer to the memory. So, we have 16 new V2 cores embedded inside the controller. And the purpose of these V2 cores is that customers can run some of the applications to get defined data on top of it. We also support inline LZ4 compression algorithms, and the purpose of that one is, if for certain applications, for certain workloads, for different customers, they would like to have the compressed data and would like to utilize more memory based off of different workloads, they can utilize that. And of course, this is built off of a five-nanometer processor node, which means the power consumptions are the most critical, one of the critical parts where we have heard since morning how critical the power is for these chips. And that's where the power consumption is less than 100 watts, right? And if you think about it, what we are trying to address is how to provide higher memory bandwidth per core sitting inside the card, closer to the memory, for all the data needs to be processed in this controller, and refined data goes back to the host from that card. That's what—that's what the critical pinpoint is, what we are trying to address with this product.

One of the common questions asked is, "Okay, how—which application can take advantage of it and what do we have so far?" So if we think about it, a deep learning recommendation engine model is one of the applications which can be utilized for this particular purpose. And just a quick update: Deep learning recommendation engine is all we have used. What happens is, when we go to any media and click on one short video or one video, they can understand what information we are looking at, and they can recommend all the other videos. That is a part of deep learning recommendation engine models. And what we are trying to show here, in a standard deployment, is that you will have a backplane connected to a compute fabric switch. Then you have XPUs—it could be CPU, XPU, GPU—connected to the CPU, which has a smartNIC connected for outside the box. And then, it has its own memory attached, DIMMs connected to it. The important part which I'm trying to show here is, if we think about a server which has 64 cores, right? And which gives 400 gigabytes of memory bandwidth and consumes around 400 watts of power, right? If we do the math, it turns out it's giving you 6.25 gigabytes per second. And that translates into one watt per gigabyte per second. This is how it is used, without any of the components.

Okay, what we are trying to address with the product is, now we have one card memory accelerator. As I mentioned, it has 16 cores. Now, the total of cores—16 plus 64—is 80 cores, with and added up to a 600 gigabyte DRAM bandwidth, and consumes 500 watts, which translates into a 25% higher cores and around 50% higher memory bandwidth added into the server.

This is another application where we want more bandwidth per core. What do we need to do? We can add another card, and these cards are like an add-in card form factor type of a card with CXL support, right? So, now, in this case, what we are showing here is, it has 96 cores. The server has sitting on 96 cores with 800 gigabytes of DRAM bandwidth roughly. It increases the 50% cores and doubling the memory bandwidth where it was, and which applications can take advantage. This is one application; the second application could be graph analytics. Sparse database and vector database are a few of the applications where has been very much focused on to do a lot of work, which can take advantage of these ones to provide higher memory bandwidth per core to address this challenge.

Okay, next one: I am jumping into memory expansion. As I mentioned, one of the main pain points we have been hearing over and over is how we can have better control of our memory on our CapEx and how we can use somehow have better utilization of the memory which has been used on a previous generation CPU and how we can enable those to the next one. That's where the CXL is going to enable that, and that's where we have a product called Structera X2404. It's a, it's again, PCI Gen 5.0, CXL 2.0 support. This product enables 3 DIMMs per channel, a total of 12 DIMMs sitting behind the controller, and these are DDR4 DIMMs. The capacity, you can think of it on a single card with 12 with 12 games and 512 gigabytes of a DIMM, you can get 6 terabytes of DRAM capacity from this card. One of the important factors here is, we have seen and heard that how critical it is to use games to get the higher capacity behind the CXL, right? And this is where it's going to solve the problem. This product also supports LZ4 inline compression algorithm. Right, and when we talk about CXL, and when we talk about compression algorithms, the question comes into play: What benefit will it bring and what does it require to enable these compression algorithms? Right, and in a memory wall, when we all know we talk about cold pages and hot pages, and all the hot pages industry would prefer to move those to the main memory and the cold pages to the CXL, or it could be a possibility that within the CXL card you can have different tiers of memory. You can have hot pages, cold pages, and then move those pages accordingly as per your requirements. Google, it's public information, has published great data that the cold pages can be compressed at 3x the capacity, 3x the ratio, right? And when you have a mixture of cold pages and hot pages, the best algorithm and most optimization method you can get is at least a minimum of 2x the ratio. So, that's where the value prop comes into play. And Meta has a public paper, also open paper, that one in a rack, half of the memory is untouched in a rack for two minutes. Think about it: half of the memory is untouched in a rack for two minutes. What does that mean? It translates that these memories do not need high bandwidth all the time. These memories, you don't need to have very high bandwidth memory all the time. If for two minutes, half of the memory is sitting idle, this means the user can immediately know which applications are taking more memory or require higher memory bandwidth and can work and separate those memories from the ones which do not require more memory, and then use that memory to compress to get even a higher memory capacity. So, with that said, we have seen a lot has been talked about recertified DDR4, and we use DDR4, and this is one of the key applications that can be used where you need more capacity to utilize for your applications to provide the required capacity.

And how is this just displaying here in the end? Quickly, you have a CPU connected to the PC or CXL PCI bus, and with a PCI box, you have a card connected. With that, 12 DIMMs total—and how you can connect to all of them—is a DDR4 four-channel controller with 3 DIMMs per channel, giving a total of 12 DIMMs, right? In fact, the industry would like to see how you can get even better, more and more of those.

Sorry, okay, there is another application. Another application, even though, which I mentioned previously, reuse of DDR4 for memory expansion, is the one case, but there are other applications where the industry would like to use DDR5 for memory expansion, and we have a product called Structera X2504. Again, it's a four-channel controller, two DIMMs per channel, totaling eight DIMMs connected to a single controller. It also supports the LZ4 compression algorithm, as well as being built off of a five-nanometer processor node, giving a total of four terabytes capacity for a single memory card.

So, this is, in a nutshell, what we are trying to address. This one involves two use cases. Each use case includes a memory accelerator and memory expansion. With memory expansion, we have DDR4 and DDR5. With the memory accelerator, the focus is on DDR5 to give you the higher memory bandwidth per core. This addresses some of the pain points of how to balance the bandwidth per core within the server. Four channels of DDR, PCIe Gen 5.0, and CXL 2.0 support.

Key takeaway: I will say, okay, if you have more questions, please come and see us at the booth and talk to me. We are pioneering and leading in this segment based off of three DIMMs per channel controller, based off of memory accelerators, as well as providing two DIMMs per channel controller with a total of eight DIMMs for DDR5 enablement. A lot has been done; there's great momentum. I think the industry, at this point, has looked up what is required and what it needs to take into mass production of a deployment rather than what can be done. So, with that said, if you have any questions, please ask.

I'm sorry, Khurram. We're going to have to move forward without any questions, but thank you very much for your presentation.
