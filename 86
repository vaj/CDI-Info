
Yeah, thank you very much. Like I said, I'm Steve Scargall. I'm the product manager at MemVerge. We're being in the fortunate position of being the software vendor for CXL. The company started over six years ago working with Intel Optane products. So that work that we learned from the Intel Optane world has now transitioned seamlessly into CXL. So we're continuing to innovate. We love working with all the partners, either directly through open forums like this one. It's great to see everybody here and talking about the innovation that's coming. But I want to talk more on the software ecosphere, where we are today, a little bit of where we're going, some of the innovation in the space, and some of what we're working on right now.

So we'll talk a little bit about the architecture. And I'm going to focus mostly on Linux for this presentation and focus mostly on the Type 3, the memory expansion device, although a lot of this will actually transition into some of the Type 2 and eventually Type 1 work as well. So it's a bit of an eye chart. But effectively, what we're trying to show here is on the lowest level, the hardware level, there are all the different hardware types, all the hardware devices that the CXL specification defines. They're here today for the most part. On top of that, we now get into the kernel area. That's the purple area, where we're saying that all the drivers are there today. They've been there for quite some time, actually. And they expose the memory capacity of the underlying physical hardware through different device drivers, effectively. Now, the two common ones that you'll see if you plug a device into your server today are going to be what we call a device DAX. This is a character device, just a big blob of memory that we give out. And you, as the application developer, can take that and do with it what you want. You manage the memory that's sitting behind that device. But that isn't for everybody, right? Not everybody has SDKs that they want to go and utilize, modify your applications to go do that. So if you want to run your unmodified application using CXL, the typical way we would do that is we would change the namespace type to what we call system RAM. This exposes the CXL memory as a new memory NUMA node, effectively. Looks like just DRAM, just without any attached CPUs. And your application can use that, right? If it's a NUMA-aware application, again, it's just a NUMA node, just memory. However, there are other things in there, like middleware. I mean, we class ourselves as a middleware. We have some SDKs as well for tiering. Tiering, we have both latency and bandwidth tiering coming very soon, actually. But for the most part, the industry has been focused on latency tiering. That is, having two types of memory, DRAM and CXL, and keeping the hot data in DRAM and the cooler data in CXL. So the kernel has this functionality today. If you run a fairly modern kernel, you'll have tiering if you go and enable it. We ourselves have latency tiering. That's our memory machine product that we have. And then on top of that, we have the application that, again, can just utilize this stuff. So we're focusing on unmodified applications. Whatever that application might be, it could be a database, could be machine learning or AI-based. On the very far right-hand side is persistent CXL, kind of like it was with Optane, with persistent memory. Those devices are coming. Samsung has announced their memory semantic SSDs. That is really just using that and Flash as the persistent media, and fronting that with a DRAM cache. But effectively, the drivers are still there. It's the same drivers that we use for Optane that we'll continue to use for these new devices. And that exposes the memory as a file system. So again, applications that can understand either memory map files or that just want to read and write file semantics will be or could be using memory speed devices in the very near future here.

So here's a little bit of a timeline. Again, it kind of starts from the CXL version 1.1 realm on the left-hand side. And we're going to progress onto CXL 3.0 towards the right-hand side. Now, if I was to give you the eye chart of what we're doing inside of the kernel and what happened, we just couldn't fit it on one slide. So what I did was I went back and looked at the history of the ndctl utility framework. And that is ndctl cxl CLI, which was new to support the CXL devices and daxctl, which existed previously. So that's a little easier to comprehend, because when the command line interface changes, that means we've made some significant progress inside of the kernel that we want to expose to you as a system administrator or maybe as an application developer. So starting with version 72 on the left, that's when we introduced the cxl CLI, as we call it, or just CXL for the most part. So that allows you to see what devices you have inside of your system. And as we progress to the right-hand side, now we can do clever things like monitoring and update firmware and things like that. So the software is definitely leading ahead of hardware, although there is some synergy there.

Now, if I was to layer on top of this the CXL specifications, you can kind of see where we're lining up. Now, for CXL 1.1, it's somewhat simple. There was a hard introduction of when we started putting these patches and features upstream. The CXL 2.0 and 3.0, you'll see that they're a little bit fuzzy on the left-hand side. And that's really because, as a kernel community, we don't just dump a whole specification into the kernel. It takes time to deliver these features. So it might be that for CXL 2.0, the version 74 of the ndctl utility started to introduce CXL 2.0 features in. And that was part of the kernel work. And if you were to go back and look through all the patches, you'll probably see that, well, actually, the work started a little earlier than this. But for the most part, these are the granularity of when CXL lines up with the kernel versions. So if you go run, like I say, 6.5, which is the current mainline kernel, 6.6, it's probably going to come out this weekend or be in well. That's when you can start to see these features come in. So it really depends on what your device needs, what is it capable of, what is it exposing up to user space, as to which kernel version you're really probably going to want to target. So it's worth just checking with your device vendor as to what they recommend on this side.

So here's a little crib sheet. Again, I just summarized a lot of detail that's out there already, anywhere from administration tools and utilities, like your CXL, your daxctl, lspci. Just go check, see what's in there. lstopo, if you use version 3.0, go compile it yourself. You can actually see the tree, the physical PCI tree, and it does decode CXL devices very well these days. Fabric management, with vendor-specific tools and APIs. fm_cxl is not really a tool right now. It's a patch set that is currently under review in the Linux kernel mailing list. But if you have a switch and you want to go play with the mailbox commands and MTCP and all that type of stuff, have at it. Go compile it yourself and see what you can do. The same with memory tiering. AutoNUMA has been around forever in the kernel. So again, as you're exposing CXL devices up to the kernel, you can just let the kernel manage it if you want to. You don't have to do anything at all. Similarly, depending on the BIOS options that you have set, you can just expose CXL as regular memory. We call it special purpose memory. So depending on the BIOS option, if you're on AMD or Intel, you can enable and disable special purpose memory. If it's disabled, the kernel just sees it as though it's regular DRAM, and the BIOS is handling all of this stuff underneath. So you don't even have to touch any of the administration tools on the left-hand side. More recently, the TPP, transparent page placement, is integration of latency tiering inside of the kernel. So again, you can do promotion and demotion of pages between DRAM and CXL. All that stuff's been in for what is relatively a long time for me, because we're all bleeding edge, but you'd probably need a fairly recent Linux distribution to have that feature. And then of course, we have our own Memory Machine. So we're working on n-level tiering, meaning if I have local CXL, if I have CXL attached via a switch, or in CXL 3.0, it could be behind many switches, that different types of memory have different types of latency and bandwidth characteristics. CXL is not just CXL. It could be different media types. It could be different device vendors. They all have different characteristics that we need to understand when we're doing the intelligent tiering. So we do latency and bandwidth tiering, right? And then numactl, you can manage what your application gets access to in terms of NUMA nodes. The telemetry side, there's always CPU-specific tools, Intel PCM, AMD uProf, microprof. We've got our own memory viewer being demonstrated outside the door here. perf, if you want to get down into the details. Emulation, so not only do we do proprietary work, we also contribute back. So a lot of the kernel work that we contribute is public domain stuff. We push that back. Emulation, it's the same thing. Our lead engineer, Greg, is here today. He did a tremendous amount of work recently with QEMU. And that allows anybody today, if you want to, go download the QEMU, what we call the flight simulator. You can go play with CXL. You don't have to have physical devices. So you can go start your research today. We do expansion. We do sharing. So you can have two QEMU VM instances, let them share some memory on the back end. It could be a file. It could be some DRAM that you have on the main host. And you can start playing with some of the 2.0, 3.0 features. You don't have to wait for hardware to arrive. And DCD, DCD patches are sitting there. The kernel has some patches and branches available. In fact, Intel, at Intel Innovation, just last month, demonstrated a DCD device in one of the hands-on labs that I gave with Dan Williams. So this stuff is coming. I mean, it's pretty phenomenal what you can do. Just with software.

And then we get into the fabric management. We've heard a lot about it. But what really is it? I mean, the CXL talks about fabric management. We all talk about fabric management when it comes to switches. But it's kind of an ephemeral term. It's conceptual. It's not one thing. FM, fabric management, kind of refers to just the logic of how do we compose components behind a switch. What does that switch do? Can it aggregate? Can we do-- what's a logical device? Is it a single logical device? Is it a dynamic capacity device? The switch has to manage all of this stuff. And we need to tell the switch what you want to do with the devices that are sitting behind the switch and what hosts you have in front of the switch. Now, one of the beauties of CXL is that it's forwards and backwards compatible, meaning that we can take a 1.1 server, which is available today, we can attach that to a 2.0 or even a 3.0 switch. So long as the switch and the host can communicate together, we can offer that capabilities. So it means that the fabric management itself can live pretty much anywhere in the stack. There are components that are going to live on the switch. We need to communicate with the switch and tell it how to provision. Well, what port is connected to a host? Is it connected to a back end JBOM, just a bunch of memory or a smart appliance? How much memory are we going to pass through each of those ports? Similarly, on the host side, you've heard talks this morning about this. So from a host side, is there stuff in the BMC? What in the operating system do we need to do? Is it in-band? Is it out-of-band communication?

But the FM, as far as the specification defines it, this is very flexible. And it's flexible by design, meaning that there are very minimal components that are mandatory to stand up an FM environment from an API perspective, allowing us to not have this monolithic solution that has to sit on a switch that might only have a few megabytes of memory and a very small x86 or ARM-based processor. So you just take the building blocks that you need, and you can implement just those for whatever device or devices you have. We already talked about security and configuring them and things like that.

So then there's the orchestration piece. So now that we have the APIs, we have some tools, somebody has to tell those things to go and set up this environment. And this could be a Kubernetes environment, for example, where you want to instantiate a cluster or a pod or whatever it might be, and that pod has some resource requirements. Well, do you just go and launch it and hope that it works? Ideally not. You'd want to go and provision the hardware first. So there's work going on in Kubernetes. Node resource infrastructure stuff, we're doing some work in there as well. And then we get back to the OS tools. So the local host, does it have a management interface with cxl, daxctl, and the proposition of fm_cxl in the near future here? And then obviously, we don't want to just give you a whole bunch of new tools. This has got to work seamlessly with your current infrastructure. So all of this stuff has to be integrated into your existing DCIM software management stuff. Could be a plug-in, could be a module, whatever software you use.

So all of that work needs to be done right now. And then telemetry is very important. So now that we have CXL mapped out to a host or many hosts, in the case of a cluster, and 2.0 and 3.0 fabrics, what information could we get out of that system? And I gave a talk yesterday about telemetry in more detail, so you're welcome to go rewatch that. But ultimately, we want to be able to expose throughputs. How do we deal with events where a host goes down, or maybe a switch goes down, or a link goes down? We've got all the RAS features that we need to bake in. Now, the 3.0 specification for CXL introduced this concept of CPMU, so CXL Performance Monitoring Unit, which is, again, a very loosely-defined specification, meaning that device vendors can go implement device-level statistics that are exposed up to the host, again, either in-band or out-of-band, depending on how they're connected. Switches can also take advantage of this. So we should be able to get port-level statistics, throughput, latency, that type of stuff. And this is all part of section 3.2-- 13.2, I beg your pardon-- of the CXL 3.0 specification. But actually, more recently, in the current 6.5 kernel, this has actually been started to be implemented. So going back to that fuzzy starting bar period that we were talking about earlier, this stuff lives out there today. So if you have a very modern kernel, you can go start seeing what performance counters will be available by your hardware. It is going to be hardware-dependent. So you're going to need fairly recent firmware, fairly recent devices, and a very recent kernel.

So we definitely encourage you to continue your learning experience. Hopefully, forums like this will continue to do throughout the years. It's very educational for us as software guys. And it's great to get together with everybody. So as we announced in the keynote, the CXL forum, which is what we call this thing, is now going online. We encourage everybody to get involved. We've got Discord servers. We've got events, calendars. So you can look ahead and see where we're going to be and what we're going to be doing. We've got solutions. So some of the stuff that we've been doing with our partners, that'll be on the website. If you want to know who's who, there's a CXL vendor directory there. The CXL Academy is kind of a Udemy, Coursera type of thing that we'll be setting up. So we'll be doing tutorials and showing people how to do lots of different things in the kernel, in devices, BIOSes, different platforms. So we encourage everybody to contribute to this. It's not just going to be a memverge thing. This is a community effort now. We're just leading the charge and giving the platforms available for everybody. So if you want to get involved in any of the Linux stuff, I mean, again, that is completely open source. So feel free. There is a dedicated CXL community. We meet once a month, virtually. The Linux Plumbers Conference is coming up next month. We'll be talking very heavily on CXL Futuristics at that conference. There's the mailing list. And of course, there's the CMS OCP subgroup that we're also leading some of the efforts on as well. So with that, I thank you very much for your time. I finished a little earlier, so we can go to lunch when it's ready.
