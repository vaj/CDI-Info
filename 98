
My name is Parag, and I'm responsible for segment marketing within one of the business units at ARM. So today, I'm going to cover CXL devices within the data center.

So from ARM point of view, I think many people are probably aware that we have the CPUs for the data center. I think it's pretty well known currently. We have three swim lanes of products from the CPU side, the V series, N series, and the E series. But on top of that, we also have interconnect and system IP. So the main product that we're going to talk a little bit deeper today is the scalable coherent mesh fabric that you see here, which we call it as CMN. Then on top of that, we also have non-coherent interconnects as well. So we call it the NI here. And then we obviously provide interrupt controllers and memory management unit as well. So what we do is actually we combine the CPUs and the interconnect and system IP that we are showing here into these reference designs. So we have two reference designs, V1 reference design and N2 reference design. And just this week, we announced what we call ARM total design, where we have taken these compute subsystems and opened it up for other partners to build silicon that is custom to their own needs. So we've been kind of doing quite a few number of things in the data center for the last few years.

So specifically talking about CXL, if you see these platforms-- so these are the same V, N, and E series that I showed in the previous slide. So we have been enabling a lot of CPUs since 2019 for the data center. And almost we have been providing different releases every year or every other year, depending on which swim lane we are talking about. But let's just go over maybe one of the platforms here. So let's see the V2 platform that you see here. So the blue box that you have is a combination of our CPUs and the CMN, which is the coherent mesh network IP that I showed in the previous slide. And then we work with quite a few number of third party PHY and IP vendors to really have a full platform solution. So this is what we call as a compute subsystem design. Now, because the CXL part is in our CMN IP that you see here in the blue box, it is really scalable across all of our V series, N series, or E series. So V series is our most higher performance and optimal TCO. Then N is for scale out. So this is our most efficient performance line of series. And E is more for efficient throughput, primarily deployed in heavy edge kind of devices. So I think we have been working on CXL for quite a few number of years. As you can see here from an IP point of view, we've had our CXL 2.0 already enabled last year. So based on IP and our customers' back end and all that, you should expect a silicon based on CXL 2.0 from some of our partners later this year and early next year.

So now moving to CXL itself, I think many people have talked about the whole disaggregation problem. I think one of the number one reasons for CXL in from a memory disaggregation point of view is the memory stranding problem. So the inefficiency that you get from the DRAM memory utilization, so that's definitely the number one cause that we also think. But at the same time, we also see that the memory channel bandwidth and capacity per core is declining. I think you have seen the NVIDIA Grace has like 144 ARM CPUs, and then even Ampere announced 192. And we see the core count going up and up. So with 512 gigabytes of DRAM, the 4 gigabyte per core number is no longer being able to meet. Similarly, the same issue persists on the 4 gigabytes per second per core on the channel bandwidth as well. So this is why we think CXL for memory is really important. And this is why ARM has been investing in enabling CXL in its portfolio. And the last two reasons are reducing the total cost of ownership. I think many people are aware, memory is one of the largest components of a server bomb. So anything that can be done to reduce the TCO, I think, is beneficial. And the last one is the workload's becoming more and more divergent. So this is also one of the reasons why we think deploying CXL solutions will help. So I think many people have seen about the memory expansion part. I think many companies have also released their products. But for us, as in ARM, we are more focused on the memory pooling side. So today's talk will go into the detail of how we can enable a memory pooling solution.

So this is a solution proposal that we have been sharing with a couple of our partners for the last few months. And you can see here we have a host and a device. And the device is a CXL device with a memory pooling controller, which has four to eight CXL inputs and eight DDR controllers with, obviously, ARM's coherent mesh network fabric. So we think we are in a unique position to provide this CXL memory pooling solution. And this is because our coherent mesh network product is based on a lot of feedback that we get from hyperscalers. So the reference designs that we were showing before-- so these are high core count systems with the mesh fabric already integrated. So we get a lot of feedback from our hyperscaler customers. So that's why, obviously, latency is a key part. So we do think we have a very good product with the lowest latency possible. And then, obviously, ARM-- so we have CPUs. So we can provide the CPUs for fabric and SoC management. So really, when you look at it from a host and device side, we think we can provide an end-to-end CXL solution that's optimized across the host and device.

So let me go one level down into what the details of the solution architecture are. So here, you can see the four CXL 3.x on one side with the CMN interconnect in between and DDR5. And then we have a fabric manager and an SoC manager. So these are primarily the ARM products. And then we also have-- if you want to add compute next to memory, so we have our R82 or any newer scores that you can add. All of them support CHI even now or will support CHI in the future. So that way, you have compute also next to memory. Now, on the port capabilities themselves-- so we do work with quite a few number of third parties to make sure we are compatible, which we'll talk about. But at the same time, we think we can enable these kinds of architectures in the future. And lastly, we do get a lot of requests on making these into chiplets. So that's why we have been looking at trying to enable UCIe as well. So we are engaged with a couple of vendors to see how we can add in UCIe into this as well.

So one of the questions that we get asked quite a bit is, OK, where is actually ARM implementing CXL in its product portfolio? So this is just one example of an internal block diagram where you can see the CMN interconnect. And depending on which side you are-- so the PCIe bus and all is mostly a third party. So you can see the third party CXL and PCIe IP there. And you can see the main block here, which is CCG. So this is where we have all the upper link layer for CXL.mem and .cache. So all of the flipped information gets passed into the CMN interconnect. So this is how we maintain the CXL protocol. So this takes care of all the credit management that is needed, packing and packing, et cetera. Then at the same time, for all the .io part, we have the regular AXI bus that can help get that information into the CMN. And if you have any VMs that you want to also translate that information, we do have an MMU, as I mentioned, that can also take care of that. So as I mentioned, we do have ongoing interoperability with third party CXL IP vendors and PHY vendors.

Here is just an example of how that validation happens. Currently on the host side, we have a CMN host with a few ARM Neoverse cores and an interrupt controller. And SCP stands for just a system control manager with just two memory controllers, and then with a third-party CXL controller, both on the host side and the device side in between. And then on the device side, we have a CMN device, which has CXL support. And same, we have two memory controllers. So this is a setup that we validate on our side, because even though we are an IP company, we work with quite a variety of vendors, and everyone has their own requests. So sometimes doing this kind of validation actually helps us to provide a level of certainty to many of our customers. So this is also a work in progress.

Now, one of the questions that people ask us quite a bit is, why ARM, and why do we need to come to ARM? I think the key thing that we provide in our mesh network is the capability for memory pooling with hardware coherency. So here is an example that we are showing, where you have a set of CPUs. And let's assume that the CMN interconnect is an 8 CXL 3.0 input interconnect. And then you have 8 DDR channels, like how we showed in the previous slides. So if you take that memory and then divide it up into shared memory region and then pooled memory region, this interconnect that we are providing, which is the CMN, so this can provide full coherency in such a system. So basically, all the HDMDB features will be implemented in our IP in the future. So this way, all the back invalidate information that is there coming in from the host, and the device can be translated back and forth. So this is the reason why we provide this unique IP that can really support hardware coherency in the future. Now, we do expect that in the initial few deployments, probably many companies would use software-based coherency. But as many people know, the overhead becomes too much if you're trying to scale, especially with software-based coherency.

Now, from a portfolio point of view, currently we already have a couple of products with CXL 1.1 and 2.0, as I mentioned before. From a feature set point of view, we have SLD and P2P support already. But we are actively working on implementing new features in upcoming releases. So we'll have a couple of releases next year. And the features that we are actively looking at are the list that you see here. So obviously, security, then MemSpecRd. I think this is a feature that we do get asked quite a bit to implement. Then telemetry data. I think if you want to know if any of the DRAM channels are loaded or not loaded, how busy they are. So we do plan to implement telemetry data. Then support for multi-host and single logical device, and also eventually multi-host multiple logical devices. So these are features that are being planned currently to be implemented and will be ready in the future. Similarly, I think one of the other features that we get asked quite a bit is memory sharing with HDMDB for back invalidation on the device side, again. So this is a feature, again, that we are actively looking into implementing. So with this set of features, we do think we can provide a very good product that can go into silicon for future memory pooling deployments with hardware coherency.

And the last one I think we want to touch upon is on a standards basis, we do promote quite a bit of standards as ARM, because we cater to a wide, broad set of customers. So from our side, as many of you might know, AMBA has been public for many years. And over the last few years, we even publicly supported AMBA CHI as the main interface. So here, what we are showing is, from our side, we do expect more and more chiplets to emerge over time. And that's why we are very active in UCIe consortium and also CXL consortium. But what we are saying is, at the higher layers, we would recommend to use AMBA CHI. So whatever the transport layer could be-- the transport layer could be CXL, PCIe, or UCIe. But on the higher layers, we are recommending to use AMBA CHI.

And primarily, the reasons are it really extends ARM architecture to multi-chip. So it's a single interface to enable capabilities, really to provide diverse set of solutions. And with this unified interface, you can really simplify your compute, device and memory attach, your coherent traffic, I/O, coherent accelerator traffic, or even architectural features extended across chiplets. So anything related to virtualization, telemetry-- so AMBA CHI has all the features to enable this. So that's why we have been recommending to do this. And then, obviously, localized managed memory, and then composable with die reuse. Lastly, the software reuse across multiple platforms.

So from a call-to-action point of view, I think ARM is very active in the Composable Memory Systems group within OCP subproject. So a lot of folks from ARM attend these CMS meetings. So if you have any questions, feel free to bring them to that subgroup. And to summarize, I think we do think we have a good solution for future memory pooling solutions. So if you have any questions, then we'll be outside if you want to reach.

So it looks like your solution is between Intel AMD and CXL switches. So you are kind of in the middle. You are providing, supporting target and client both. So from your point of view, what kind of application and what kind of market are you targeting? So you are not totally CXL switch. You are not totally CXL client. So you're in the middle. So what's your application target and what's the market target?

Yeah, I think the market target is definitely the data center side. Now, we can be behind a CXL switch as well. We don't need to be directly connected to the CPU. So it depends on-- because the reason I'm saying this is data center architectures are very different across different companies. So if companies want to use a CXL switch, and then they can still use the memory pooling behind it. Or if they do not want to use a CXL switch, they can still use the same solution. So that's the way we are looking at it. But at the end of the day, it's definitely the data center and the cloud service provider. 

Thank you.
