YouTube:https://www.youtube.com/watch?v=VtsXOKZv80o
Text:
So, we'll switch gears back from the software, right? We'll talk a little bit more about the hardware. And for those of you not familiar with what we do, I'll give you a little bit of an introduction. CXL is near and dear to our heart. You know, as a little tiny organization, we don't get quite the exposure that the intels and other people that work on CXL do. We were one of the original founding members of CXL. We've got guys that are deeply involved in the technical side of it. I would say we get probably some outsized references within CXL because our guys are really quite knowledgeable. As I say, as a small company, you tend to not have the big voice, but there are definitely times they come to our guys and say, "Hey, we need some help on this issue or that issue.

So, we look at it, we call this the march of composability, and that the server itself is really just a bunch of Lego blocks. You've got processors, you've got memory, you've got drives, you've got accelerators increasingly, and you've got network cards. Today, they all fit within the sheet metal of a given server, and you talk from one server to another over a conventional store and forward network. We would say, 'Okay, that's good, but if you could take that construct and basically turn it on its head.' What we've been able to do is we now have the capability of being able to route PCI, and we'll talk about that in a minute. So, we can turn PCI into both effectively an extension of that server. So, we can take endpoints and we can attach them to physically separate servers, but we can also allow the servers to talk to each other over the same fabric. It is a memory construct already. So, a conventional network, you know the legacy networks today in terms of composition really do a nice job with storage. And storage is slow enough that the existing networks are fast enough. If you use InfiniBand or Ethernet, they're really fast enough to be able to compose storage and do it pretty well. The storage industry NVMeOF running over Ethernet actually runs really well. Now, we run NVMeOF, we've written a transport to run this over fabric, what we call fabrics, and it's actually considerably faster. And back when Intel was selling Optane, it actually mattered, right, because you had a storage medium which was considerably faster. And if you put it with an InfiniBand network or an Ethernet network, it hid the performance advantage of Optane. And so, they found they couldn't network it, but with fabrics they could network it. And so today with fabrics, we touch into memory, and I'll talk about memory because we can actually do memory constructs, but it's not really the sweet spot for where we are. It's really around accelerators, GPUs, FPGAs, and the associated memory movement and storage movement into and out of those accelerators. But with CXL, you know we look forward to CXL because then it becomes really a dynamic memory environment.

Alright, like I said, we're the only routable PCI, alright, and soon to be CXL network in the world. We do no translations, we do no bounce buffers, there's no kernel calls. Typically from GPU movement to GPU movement, it's one GPU to another. It could be a GPU to an FPGA, it could be a bank of GPUs to a bank of FPGAs like they're doing at San Diego Supercomputer right now. And so you don't have to go back through. It's a, think of it this way, people would say, well it's like an RDMA. It is, except it's not an RDMA, it's a DMA, anywhere in the network. So there's no bounce buffers you can write directly from processor to memory.

So I use this slide, it was presented last year at Hot Chips, CXL consortium, and there were a couple of different vendors, and it's really quite good in terms of separating speeds of memory. And it would say, alright, so if you look at memory tiers today, this is where you think it is, this is where most of the industry would say, look you've got register cache and main memory, which is all attached to the CPU. You're running in the 40, you know, 40 to 100 nanosecond kind of range. The operating systems, everything is aligned to be able to support the tiering around this, and it all works really well. There are people that we know of that are trying to do what they would call disaggregated memory, running it across InfiniBand, they're getting 2 to 4 microsecond sort of latency. You have to be really desirous of large memory to be able to do this. It's slow, but we know there are players that do it, and then of course you get to storage.

Now where we say where it really is today is this, right, that we can actually run the same sort of disaggregated memory concept, we can just do it a lot faster. Instead of being 2 to 4 microseconds, we're in the 500 nanoseconds to a microsecond and a half. Processor number one can write to the memory of processor two, and so you can do these constructs. Now, there's two caveats, as we like to say, to the model.

Caveat number one is we tell people, remember, you're on the I/O bus, you're not on the memory bus. So the speed, right, it's not 100 nanoseconds, it's going to be 500, or it's going to be a microsecond. That's caveat number one. Caveat number two is there is no cache coherency. So if you're not careful, if you don't understand what you're doing, if you're writing directly into another processor's memory, there's nothing that's going to protect you. So you can overwrite things you don't want to overwrite. You can cause problems for yourself if you're not careful because there's nothing that manages the memory for you. You have to do it yourself. And number two, you have to have ownership of both ends. And so in certain cases, particularly the financial services guys, right, you mentioned the financial services guys, they'll do just about anything to get a couple nanosecond advantage. And they do own both ends, and so this becomes really interesting to them that they can do this. In fact, if it's hard, that's even better for them because then it's hard for their competitor to do it. And so it becomes an interesting construct for them. But we get asked that a lot. The HPC community in particular would love to do this, but in a lot of cases, they don't own the application code itself, so they can't really deal with the memory problem, and they certainly don't own both ends. And so it's not a very good solution if you're running an off-the-shelf software.

Okay, you guys probably know all of this in terms of CXL. It's built alongside of and on top of PCI, the two constructs run side-by-side. 

With the key aspect now of CXL memory, one of the really key aspects, in servers today, a lot of times people are adding a second processor, not because they actually need more processor, it's they want the extra memory. And so you want to be able to get a group of more DRAM that's attached to that secondary processor. The speed to get to the secondary processor through the UPI or through the Infinity Link is typically 150 to 200 nanoseconds. So the design goals for the CXL community has been to effectively make remote memory via CXL look like at least the performance level of getting out to secondary memory. So the design goal has been to get to 170 to 250. We're not quite there yet. From the performance testing that we've seen, it's close-ish, but it's not particularly there. But it'll get there, right? This is a lot of people working.

So one of the key benefits of CXL is to move memory to a serial link. And so it becomes a processor-to-controller communication. Let the controller then manage the memory that sits behind that. The key advantage is becoming the controller now manages the memory. You can use different kinds of memory types. And the two sort of constraints that people talk around is I can either get a lot of memory and/or I can get cheap memory. And the controller can manage that. So you could take fast memory and just put a lot of it, or you could use a lot of cheap memory. And the controller can manage that.

And once again, the goal has to be for the construct to really work, the goal has to be that that access time to CXL memory needs to be roughly equal to the access time to getting to secondary memory. And like I say, there's lots of work to make that occur. It'll probably take some time, but it will certainly occur. 

So the four effective stages that we see the development of CXL with memory, you're starting to see servers now that are coming out with what's called CXL 1.1. And it works within a given server. And so it's a CPU to memory controller construct. And so you'll have memory cards, you'll have various formats of things that will put memory behind this controller. And again, you'll get much more memory and/or cheaper memory that you can put there. Servers are just coming out now. Controllers are coming out. Testing is going on. So in the late 23, 24 timeframe, these things will start to become, we think, pretty widespread. Next step will be with CXL 2.0. It introduces the notion of one switch. And in a lot of cases, people will put the switch out near the memory. So you could have a separate memory box. And you can connect it to one or more servers. It's probably going to be two, three, four servers, a smallish number. And we'll start with pools of memory. So you'll put a couple terabytes, let's just say 10 terabytes in that box. And you can allocate a chunk of it to a given server. More than one server can use the pool, but individual discrete chunks go to one server and the other servers can't see that chunk. With 2.0+, some of those chunks can now be shared. So more than one server can be using the same chunk of memory. So you could use it to pass messages. You could use it to share memory. You could do a lot of interesting things when you get to the shared pool. And more than one server, again, cache coherency built in. So it will help manage, make sure that people don't overwrite. And it keeps, if there's two servers, it keeps them both coherent with that memory. And then finally, the goal for us as a company is to get out to fabric, we'll call it fabric memory, where you introduce more than one switch. And so you can have multitudes of servers connected to a variety of memory. And now you can share this memory quite easily. Timeframes that we're seeing, CXL 2.0, 24/25, looks like a lot of the roadmaps now are lining up for CXL 3 or 3.1 in the late 25, early 26 timeframe.

So with that then, early, it sort of helps to fill the gap in the hierarchy. And now CXL, CXL memory, let's call it the CXL with expensive memory, alright, just much larger chunks of it, alright. And let's say we hit the design goals and we get into the 200 nanosecond kind of range. Obviously, if you put slower memory but cheaper memory, alright, the speeds will be less. And a term that I like, sort of this reference here for up to about a microsecond is high latency memory.

And then the question becomes, okay, if I've got high latency memory, you know, there becomes the considerations here about how is the, you know, there's a lot of work here in the operating system to be able to deal with this. Alright, the folks at MemVerge are well down this path in terms of being able to say, alright, we've got certain tiering constructs and high latency memory will be a different tier. And we see there will be a tension between the desire to share, you know, if you look at particularly the big cloud players, they see this desire to share big, huge pools of memory because it's going to save globs of money because they're going to take memory out of, you know, a rack at a time or a row at a time. And a lot of our design team comes out of Sun Microsystems. They were doing rack scale memory but with very expensive sort of solutions. And our guys would say, the more it becomes exponentially more difficult to keep cache coherency. So, four servers is infinitely more complex than two. And most of our guys, and it seems like people would tend to agree with us, the more things you're trying to keep cache coherent, the more difficult it will become. And so sort of the notion of, I'm going to try to keep an entire row of 200 servers cache coherent, okay, I won't say never. It's probably just not likely in the near term. We'll just say that. It will become increasingly difficult to keep things cache coherent across a lot. So, we think there's a natural tension between people desiring performance, which will keep things close, less numbers of switches, less distance of cable, because the speed of light has speed. And the more switches you put it through, every switch you're going to put in, you know, 50 nanoseconds, 100 nanoseconds. So, you want to keep distances down, we think, for performance versus, hey, I can pull a whole bunch of this and I can get a lot of savings. So, as CXL develops over the next handful of years, this is going to be one of the natural tension points, we believe. We think it drives the notion of zones. So, storage, storage, because it's relatively slow, can be shared across a huge number of servers. We already see with accelerators, it generally tends to be a smaller number, and we think memory will be smaller yet. And so, the right construct for the fabric is to be able to manage, not just the fact that you can compose and aggregate different resources, but you can do it in the appropriate effective zone.

But, you know, undoubtedly CXL is an enormously important construct for the industry in general. Memory, particularly in processors of all types, has become super important, right? The ability to move to much larger memory models. We're always amused, I guess is the right word, when people say, "Oh, this is going to open the door. We're going to save so much money because we're not going to need as much memory." And history would suggest otherwise. That the applications will develop, people will desire a lot more memory, not less of it. And so, there may be a period of time in which, yes, that's true, I've saved a bunch of memory, but we think it's the natural extension. People will come up with new things. You'll come up with a model that says, "I need 100 terabytes of memory," and it'll be available. And so, naturally, we think in the long run, memory will become a much larger resource, not a smaller resource.

And then lastly, once more about GigaIO, we right now handle all of the compute side traffic. So, we can take resources we can attach to servers, but we also run MPI, for example, natively across fabrics. Just at a performance level, it's much better than InfiniBand. We run NVMeOF. We can run what's called GPU direct RDMA. So, we run all the compute side. And the second thing is, we were told way back in the beginning when we started, we went out and visited customers. They told us, 'Please do not give me a management orchestration layer that you've spent a lot of time on that I have no desire to use. I don't want to paint a glass, really. I have Slurm. I have Bright Cluster Manager. I have Altair. I have Jupiter Hub. I have my constructs already. You do the work to integrate with them. Don't make me learn a new interface because I really don't want to.' And that's what we've done.

Okay? Any questions? Kept it mercifully short.

Yeah, you indicated the CXL 2.0 plus, and from what you showed, you indicated some coherency or shaered memory in the 2.0 plus time frame. Is that pushing the envelope of what's actually in the specification, you mean?

Yes.

Yeah. So, do you have a plan for that?

So, we've seen it's largely going to be driven by the server manufacturers that will put some additional capability into, you know, they won't stick to, the CXL, you know, I have to say I'm amazed at how rapidly the CXL consortium has advanced the standard. And when they got to 2.0, there was a tremendous amount of discussion about 3.0, and so there was probably work that should have gone into 2.0 that just got shuttled to 3.0. And so, particularly around this notion of being able to share the memory and not just pool it. And so, we've seen that certain server manufacturers are putting some, let's say, additions to the official 2.0, because nobody's working on 2.0 anymore. Everybody's working on 3.0, 3.1, maybe even 4.0, and there's probably some things that should have been added to 2.0, but nobody wants to deal with 2.0 anymore. But there are some server folks that are putting some extensions to 2.0 that will allow sharing.
