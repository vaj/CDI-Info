
I'm going to be co-presenting with Navneet who's online, so you may hear a few instances of next slide please later in this set. So what we're going to talk about here is kind of on the basis that CXL2 is basically solved. I mean, there's not really any problems. The rest of the talk is today on that stuff. It's fine. Let's move on to the new stuff. So this talk is going to be all about things that are in CXL3, which has been out for, what, a month, something like that. So yeah, we had fun writing the abstract for this one because we couldn't actually talk about the content, but now we can. So it's all about the stuff that's just been mentioned, memory pooling and the features that come into CXL3 that make this a bit easier. You can do it in CXL2, but there's an awful lot of basically hot plug-in whole devices to enable it if you're doing it in CXL2 because you don't have the ability to resize things easily.

So what are we actually after from this session? It's plumbers. We don't just want to present a set of slides. We want some feedback. There is an element of this presentation where I would say the main aim is actually to scare people and say there's some really nasty stuff that has implications in quite a lot of memory management code, DAX. I'm sure Dan will chip in on some of that. And it's not obvious what the answer is. It's going to take us a while, but the spec's only been out for a month. We've got a bit of time. So we obviously want some discussion. So just anyone in the room or online, just jump right in. I can't even see if anyone puts their hand up. I'll rely on someone telling me if they do. It's unlikely we'll get to any conclusions today. There isn't any code. There's the very first outlines of some emulation. But no more than that. It may well be, actually, that was the question I was going to ask in the previous talk, there might be something coming in that ARM FVP that will give us a good platform for some of this. So there's also a broad strategy question that will come up in this, which is that we have a number of features, some of which are reasonably lightweight to implement. They're not particularly nasty. And some of which are very nasty. They add a lot of extra complexities. And the problem we have is that if we try and solve the simple ones first, we may well end up with an architecture that's just not extensible to take the complex ones in. And the question then becomes, do we actually care? I mean, it's not necessarily a problem if we have to do something different when it gets tricky.

So, a quick overview. As ever, to Plumber's Talk, nowhere near enough time. So we'll be going quick for some of this. We're going to give a -- well, my colleague -- my co-presenter online, Navneet, is going to give an introduction to DCD in general, which is dynamic capacity devices. We'll have a little tiny bit on use cases. But if you're actually interested in this stuff, there are various papers that have come out now that talk about uses for memory pooling. We're just going to touch on it to give us a bit of motivation to the rest. Then we're going to get to the fun topic, which is memory sharing. We'll come back to that later. Hopefully we'll have time. And then -- yeah, okay. Plan is probably too much of a description. I have to come back.

Okay. So, Navneet, over to you.

Thanks, Jonathan. Hello, everyone. This is Navneet here from Intel. I'm basically working on the memory pooling solution. Today I'm going to talk about -- I'm going to give basic DCD overview and also going to touch a bit upon the use cases of it. So let's start with the DCD overview. CXL 3.0 introduces dynamic capacity device. It's a CXL memory device which allows device memory capacity to change without resetting it. Each dynamic capacity device can have one to eight DCD region. Each DCD region is basically divided into fixed size blocks. And these are the blocks used to allocate to the host. And at boot, the host software is responsible for querying the dynamic capacity region, the capacity, and to map it to the host physical address, which is provided by CDT, CXL fixed window memory structure in CDT ACPI table. During boot, once this is mapped, this is mapped using one or more HDM decoders. And during boot, there's no extent presence. Extents are basically a continuous region which consists of the dynamic capacity block base address and the size. And extents are used to allocate and release memory to the host. So host are informed using the extents what memory region they are allowed to access. And once they are done or they want to release the memory, they can release some of the memories using the extents. Next slide, please.

So whenever the host is in under pressure or a host need more memory, it can talk to the out of band agent to add some of the memory extents. It can talk to the CXL device. CXL device can allocate some of the DCD memory blocks. It can create some extents. When extents are created, it can inform or it can raise the event to the device. The device driver can handle this event or it can handle the interoperability provided by the or raised by the CXL device. And it will pass the, it will get the event log. It will get the events out of the event log. And event extents consist of the DPA. It will translate the DPA into the HPA and it will provision that memory to the host by different means. It can be behind the text device or it can give that memory to the CXL memory manager, which can map it to the system address space. So once a block is created, a driver can, driver has consumed that extent. It can notify back to the device what memory has been consumed and it can also create an extent and give it back to the device so that device can update the existing extent. So DCD basically creates or maintains one extent list per host. And it can, it can basically provide different DPA ranges to different devices, to different hosts. Next slide please.

This slide, DCD is going to provide. DCD is providing more extents to the host in the red blocks in the same manner as before. And once this is, more extents are provided to the host, the extent list are updated for that particular host. Next slide.

A driver can be notified for the release of the extent in the similar fashion to the add of the memory, adding of the memory, right? In that case, an extent can be created and sent to the, to the host, informed by the interrupt. A driver can find out what are the extents or what are the DPA base and sizes are requested to be released. It's a scary force because some of the pages can be pinned by the kernel and can't release that memory synchronously, or it may release the memory in the chunks in an asynchronous manner. So in that case, a driver can inform the Linux kernel memory manager to, you know, not to use this memory because we are going to release this memory back to the device. Next slide.

Yeah, so if Linux can release the extent, it can stop using it and it can asynchronously call the mailbox commands like release capacity, dynamic capacity, and provides the extent what are the, what are the DPA and the sizes has been released and, you know, so that dynamic device, DCD can update its extent list maintained for that particular host. Next slide.

So here we have covered some of the high-level use cases of the DCD. So, you know, as discussed previously also, DCD can enable memory pooling. It can provide really lightweight memory hotplug sort of feature, right? So let's say if there is some workload running on a host and it temporarily needs more memory due to a spike, right? DCD can provide that memory from the pool, which can lead to, which can help us to reduce the TCO of that, right? Dynamic aspects bring repurpose of the memory. So the same memory can be provisioned to multiple hosts. A basic example is if there are multiple hosts and we need to do the software upgrade or the database upgrade of all the hosts in a round-robin fashion, then, you know, each host can use the same memory in a round-robin fashion and instead of over provisioning the memory. Next slide, please.

So DCD can also help in make use of the idle compute by provisioning the memory from the pool using the DCD. So let's say we have a server, in the server there is some workload running and which has exhausted the local memory completely. And we still have some of the compute left. So with the DCD, we can provision memory from the pool to the new application. And once this application is done, we can release the whole memory back to the pool, right? And memory sharing, it basically enables the memory sharing among compute hosts. So as mentioned, DCD basically maintains a separate action list per host and it can assign different DPA ranges to different hosts and it also provides some of the sequence numbers and to take care of them to handle the memory sharing among multiple compute hosts. Yeah, that's all about the use cases and the brief introduction of the DCD. Over to you, Jonathan.

Okay, before we move on, have we got any questions about the sort of basic concepts of what DCD is? Anything about that? Someone want to provide--get a microphone. Sorry. I'll just say one-handed is optimistic. He was holding a laptop in the other hand, if you can't see.

I just have a quick—I'm probably not thinking about this right away but when I read memory sharing among compute hosts, that kind of reminds me of old concepts like swapping over TCP. Is this—like that—it's obviously completely different but is that kind of like where we're going?

No, not really. I mean that--yeah, the concept of memory pooling absolutely could be used in some of those environments. I would postpone sharing--detailed sharing discussion because we haven't really introduced it yet and there was a bit on it a bit later. So I don't know if Jerome is going for that. But we wanted to just start with the non-sharing case more or less but just mention it was there.

My next question was like, with this pool of memory, what's--like, how are we going to ensure that we're not actually like wasting memory in a pool?

Effectively, the same problem as wasting memory anywhere.

But we don't have a pool, that's why like...

Yes. We--I mean, the nature of it is, is you've pushed some of this stuff out to what is effectively an appliance. I mean, the memory pool devices are typically a box at the top of your rack with an awful lot of memory in it. It can do an awful lot of monitoring in theory. In practice, much like some of the earlier discussions we've had doing plumbers on hot page tracking, cold page tracking, any understanding of memory usage, there's a whole load of questions around what hardware do we need, how do we actually go about monitoring this. And at the moment, there isn't really an answer. Absolutely, some of the software techniques that have been referred to during various other talks will work fine. It's just memory. And there certainly are use cases particularly when--so there's a--I can't remember whose paper it was. There's a paper on memory pooling usage. It was--is it Meta? Someone put one out that basically talked about this case of stranded compute and the savings they could get. And I think they were doing it on a fairly static basis because it was a question of they knew what VMs they were loading, what the work--

Yes.

The more dynamic case is tricky but we do have the tools to do something about it if we know the answer because we can scale things up and down very dynamically. Which is a bit more--I mean, it's a bit like the virtualization case of virtual OMEM where you can poke little tiny bits of extra memory into VMs if they need it. And you have very good sort of granularity of control but you still got the question of how do you know what to do.

So I see the problem is unplug. It's an unplug problem, right? And we--like, we have good ways of getting at that now like with like DAX KMEM and then like while attempting to work on the problem, I guess, all I'm saying is like the whole start simple approach, right? I think we just got to get better at the unplug in the beginning, right? And build upon that for anything else here. 

I think--is that the question about-- The unplug thing, absolutely. So, I'm trying to remember what we actually have on the slides on this. I think we have some material, amazingly enough, on DAX and how it applies to this. So we'll move on and hopefully we'll answer the sort of question as we go along. Yeah, as mentioned here, dynamic capacity devices as summarized do have the ability to allocate new physical capacity to a host and as you mentioned, request it back again. Now the get it back again thing has always been a bit of a problem because hot remove of normal RAM, you've got all of the usual problems of there's something pinned in it. There is massive work going on to enable ways of improving that and hopefully we can take full advantage of that if we're treating it as normal memory. Now the other option is what we've done for most CXL memory that hasn't been handled by the firmware and hasn't been set up already is that the sort of standard approach of the kernel is to treat it as from ACPI point of view, specific purpose memory, which means that we're shoving it into DAX by default. We have the option of KMEM. If you use KMEM, then stuff might get pinned and bad luck, you can't have it back. The one key thing on the dynamic capacity stuff is in most cases, the OS and the host is allowed to say no. No, you can't have that memory back. I know you want it, but no. And that does give us some flexibility. Now obviously this doesn't scale perfectly because you get the host that's crashed, you need to recover it. So there is a forced path. And from a software point of view, that probably means going out and killing software and possibly panicking the machine, depending on exactly what memory you're being forced to remove. It's the same problem that occurred with traditional hot plug and indeed hot plug into virtual machines. DAX makes this easier. There are some sort of mapping questions of how we map it to DAX. So DAX already has a concept of regions. You can have multiple carved out chunks of memory within a given DAX device. Yeah, we do need to add some stuff. It doesn't really, well, as far as I know, it doesn't have a concept of sparse memory. It doesn't have this idea that the physical addresses are not all continuous. You've got gaps punched in there. Those gaps are going to change because you'll get releases for chunks of memory in the middle of your range. So we will need some new infrastructure to deal with that. I have a question. This is music to my ears, if I'm understanding it correctly. From the point of virtualization, I could potentially live migrate a VM from host memory into this NUMA node and then potentially pick it up on another host out of that NUMA.

Again, let's leave, because that would be, okay, that's not technically a sharing problem. So yeah, okay, let's discuss it now. Absolutely, you can indeed do those sort of transfers. There are security issues around it. So you have to very much have your orchestrator opt in. So when it issues the commands to the fabric manager, there's a whole API for controlling the device. It has to say, normally it would assume it needs to wipe it because it's passing memory to a different host. But yeah, you absolutely have the potential just to hand it over. You'll need to be a little bit careful, because as was touched on by Navneet earlier, there isn't actually a guarantee that you would present the underlying physical memory at a memory address range that would then map onto the new machine. But again, if you're doing it for memory in a VM, you've probably got the option in a stage two translation or whatever, to map it to somewhere that makes sense so that you could indeed do that precise application.

One question, does the DCD also involve the Gen-Z memory? You mentioned here the DCD is just for the CXL type 3, or could it also serve for the other memory pool technologies, such as the Gen-Z memory module?

The software solutions being developed could apply to any other hardware solution or any of this stuff. Absolutely, if you've got something that looks a bit like this, that's not this, then we need to have that conversation as soon as possible so that we can try and share some resources and work and hopefully get a common solution that works for any system where you've got the same properties.

There are some extra challenges. Sorry, I'm just going to move on because we won't get to the end otherwise. There are some extra challenges around orchestration of this. We have interleave going on, so although you've got dynamic capacity, it might be coming from up to 16 separate devices, all of which are presenting an individual event saying this new bit's available. Obviously, there's a bit of orchestration there to say, 'Well, we can't plug it in yet because we're missing some of the interleave set.' Once it all turns up, you can then instantiate that. There's a bunch of policy around this to be developed, like if you get a new chunk of dynamic capacity, do you try and stitch it into an existing DAX region and say, 'Well, your region got bigger,' or just create a new one? There is a bunch of infrastructure built into this as well, which is intended for tagging memory such that the idea is the orchestrator says, 'I'm launching this application on these servers. They need some more memory,' and you can pass this memory with a tag that says, 'It was for this purpose.' It's completely opaque. It's just a magic--is it UUID or--it's a big number. And that allows you to have applications then come down and map the memory that was intended for them. And so, there's a bunch of stuff to be done to get that information to the right places. Yeah. And if the last note here is the one that already exists for CXL Type 3 devices, if they are memory that is suitable for normal use, you have a question of whether it makes any sense to map this stuff through as normal memory. In this particular case, because of the sparsity side of things, I think this is--if you did it, you would need something that looks a bit like is already done for VertIO mem where you have this option to have a sparse physical address range but still map it as normal memory.

So I already did tags, so we can skip that one. Data leaving, we kind of touched on with the many events there. The last one is the one that makes us all a bit exciting for how we go about doing it. So how long have I got?

So I'll give a very quick introduction to this. So this is sharing. So in sharing, we are presenting the memory to multiple hosts. Same underlying memory. It can be hardware coherent. So you can have no need to do any special maintenance, any flushing, any of that stuff because CXL3 introduced back invalidate and all of the usual stuff to enable a coherent protocol. Now, there are some quirks. One of which is because you have, we say, a problem with fragmentation after you've been running one of these devices for a while, you may well have a situation where you want to share some memory to two different hosts. Both of them are using host physical address space windows to access the thing. And you don't actually have a common physical address space that you can use. There's nothing in both hosts that is available on both of them. So this means that you end up in a situation where you have to map them to different addresses. So the view seen from each host is different. And then just for extra comedy, there's no particular reason they have to either be the same size chunks or in the same order, which means that you get this situation where your host is getting presented with stuff that you're sharing across multiple hosts. The physical address ranges that they're seeing on each host are effectively scrambled. But you do have the information to rebuild that. So when this was getting defined, one of the requirements was that it would be possible on each host to build a contiguous mapping. So you can map it up into virtual address space and have a contiguous region. Whether that makes sense is an implementation choice. But you can at least ensure it looks the same on each host to some extent. So the issue that -- and the reason I raise this is this is the complex case. And there is a question for if we're getting into questions like the sparse case for KMEM and whether we're trying to handle that, how we handle things in DAX when we can have holes punched in the middle of the memory range, all of that side of things. And now we have a question where we actually have a use case where we need to be able to reconstruct the order of the things. The order matters. We do at least have the advantage that, as far as I'm aware, for sharing, you're not allowed to free random chunks of it. It's freed on a tag basis. So you can take out a whole thing or none of it. Or if you want to create some more, you create a new region that's tagged separately. So we don't have the complete insanity of ending up with shuffling what's in the share. This is kind of the question was, do people think that our best approach to this is basically to ignore sharing for now? It brings a lot of extra complexity with the possible disadvantage that we end up with an entirely different solution for the shared case. Or is it worth trying to architect the whole thing from the start and possibly getting nowhere for quite some time?

We have one final slide, which is very loosely termed, the plan. The very first thing we need, and I'd certainly be interested to hear if that FVP model we had in the previous talk is going to support any of this stuff, is we need a platform. It's been in the spec for a month. It's going to be a little while before there's any hardware. And more to the point, it's going to be even longer before there's any sort of hardware generally available to anyone who's developing for these platforms. I'm sure there'll be a number of prototypes in different places. They're useful for testing, but we can't really develop against them. So we need some emulation. So step one, in my view at least, is emulate it. FVP's great from ARM, but we'll also do something in QEMU. And then the question becomes, my proposal is to do the simple case first. Let's get something working. We might not actually merge it, but at least check that we have an RFC that works, that allows people to look at how this is going to be used, start developing applications against it, and then later move on through extra complexity, make it dynamic, have the ability to punch holes in the middle of an existing region that's been provided to a given host, and then only then hit the complex cases. Shared is one thing. It is possible people are going to do this with non-volatile memory, in which case you get into fun questions of mapping the extents the same as you did last time. So there's extra complexity there, but I'm certainly not hearing anyone caring about that use case just yet. Pools of non-volatile memory don't seem to be high priority. If anyone has a different view, then join in and try and support that use case. So we are running pretty much to the end of time, but we maybe have time for one question or two questions. Anyone online?

A question on the slide 14, yeah, just the last slide. And you separated the DCD region and the CXL type 3 device. So in hardware, the type 3 device is out of the system, and you still put a DCD device on every server. Is that right? So here the DCD is mentioned as a device and a Linux driver to enable this device and connect to the CXL type 3 device.

No. So what we haven't touched on here is what the actual model to providing memory pools for CXL devices is. But the complexity of management of the presentation of the devices is actually handled in the switches in the path between you. Or you can have what's known as a multi-head device, which is -- I mean, it's kind of -- it looks like a PCI device that goes into a slot in two different machines at the same time. So you can do dynamic capacity across those as well. But there is no explicit hardware supporting this on the CXL hosts. So it's in the memory pooling device, wherever that is. Or the switch in front of it.

Okay.

That's a whole other topic.

I have one comment. Maybe it's not a question, but a comment. I think it needs to consider sharing in the first place. It could provide something like more clear abstraction. I know many cases when people don't consider something like some idea from the beginning. And that was big trouble. I think maybe it makes sense to think about sharing anyway from the start.

The only thing I would say about that is the software model for anything that's making use of shared memory, it has to be -- whatever the actual application is that's on top of it, has to be fully aware that it's shared memory. Because much like a couple of processes sharing some memory, you need to know it's going to change on you, or potentially. Whereas for all of the other use cases, you know that's not the case. So if we did end up with, I don't know, a different way of mounting it or something to expose it all the way down the stack differently, we'd probably be in a situation where we wouldn't run into the problem of trying to extend the underlying thing. Because there's no application that you would write to deal with both cases. So maybe we can get away with it. But I take your point that, yeah, don't ignore the complex case. Because it might bite you.

If it's simple, start from the simplest one. It's good. Simply go ahead. It might make sense to consider maybe sharing. Keep it in mind.

Yeah, there isn't a complete separation. Because there are things that are more complicated if you aren't sharing. And there are things that are more complicated if you are. But I get your point. Go in for the hard cases at the beginning.

Thank the speakers.
Thank you.

