
Okay, so we'll get started. Yeah, I really enjoyed the last session. It was very good information, really good education. My name is Charles Fan. I'm part of MemVerge. It's a startup company doing software-defined memory. And we participate in the SDM work stream of OCP as well, and working more closely with Menology and others there. And it'll be great to share with you what we're doing with regard to CXL. And it's great to see a lot of familiar faces. You know, we'll just start saying hi to a few people. Last time we met was before the pandemic at this conference. So it's good to be back in person.
Maybe before I, you know, get into the content, just kind of one minute about MemVerge. It is a startup company. We started about five years ago. And our vision was that applications are driving more data at faster speed, and memory will become a more important element of data center. And we believe as new innovations continue to happen on the hardware front, there will start to be need for software in memory, which will be the first time in history that happens. Because memory traditionally has been purely a hardware technology. You know, all the software you need are done in BIOS or in operating systems. And there is not really a need for an independent tier of software managing memory. But as memory becomes more complex, you know, there are more tiers, high bandwidth memory, DDR, and CXL, as we're seeing it. So the, you know, as it, with HBM, CXL, as well as potentially new memory media being created, there is an increased heterogeneity in memory, and there may emerge a need for an independent layer of software that can help manage memory, and in particular, a fabric of memory. And that's what we set out to do, started from five years ago. And our first use case was focusing on Optane, the 3D cross-point memory, because it's the first sort of another tier of memory that we can help to perform auto-tiering from the user space. We also started working on interesting data services that can apply on top of memory, starting with what we call in-memory snapshot, which is really a special kind of transparent checkpointing technology that happens real time for the application processes. Now as we have heard about six weeks ago that Intel has changed their plan, Optane, and we were fortunate to sort of have a hint of this started about a year ago. So since then, now the good news is that around the same time, CXL is gaining momentum as a new technology that's emerging. And so we have been essentially, over the last year, developing new technologies related to the CXL, and as I will go into more details in terms of what work we have been doing. So that's a short history on the company. And it's great to see over the last year, the whole industry kind of came together behind CXL. If you were at the Flash Memory Summit, there were a lot of announcements related to it. And I think it is also becoming more obvious, and there are more people agreeing with us on the emergence of this new software layer that is helping managing the increased importance and complexities associated with memory.

And so let me go into the content here. The fundamental motivation, or the fundamental challenges that we try to solve are two things. I call them, or rather the industry called them the memory wall and the I/O wall. And these are two of the fundamental bottlenecks for applications, modern applications particularly, but really for any applications. Memory wall is referring to the interface between compute and memory. And I/O wall is referring to the interface between memory and storage, or between memory and memory on another server over a network. So it's a storage I/O and network I/O, as well as a memory bandwidth, and often what slow down the applications, sometimes more so than the bottleneck in compute itself. And certainly there are innovations that are happening to accelerate the compute. But in some ways, these two walls have been a bigger obstacle for the application to overcome. And we believe new memory technologies potentially can overcome, or can help overcome both of these challenges. So here are just some graphs you may have seen before. The left one is from Meta, from the OCP Summit last year, and showing while the number of cores are increasing to solving the compute problem, the amount of memory bandwidth per core is not increasing, and even decreasing over the last 10 years. So really the memory wall problems are not being solved, even though the amount of compute is increasing. The right-hand picture is from Intel. And those happen to be the two presenters from the last session. So that's good to be the first slide here. And from 2019, it's showing the amount of data that need to have fast access is growing very quickly with some of the modern workload. And this example is for AI workload. And in particular, the models are getting bigger and bigger. There are more and more parameters for those AI models. And this is showing they are increasing roughly 10 times per year. So it is hard for the memory capacity to keep up with it. All right, so as the model become bigger, now if the memory cannot take all of it, it needs to go to storage. And that's when the performance starts to drop very quickly. So then instead, people have to have very distributed parameter servers and so on. And I talked to a customer. They have 8,000 parameter servers just to be able to fit a model in memory so that they don't have to go to the storage I/O. And that really slows things down. So if we can scale the memory bandwidth, that can solve the memory wall problem. If we can scale the memory capacity, that can avoid the I/O wall problem. And so now the question is, what kind of architecture in memory that can help scale both bandwidth and capacity to do that?

So we think CXL as a new interconnect, as a new fabric, does have effect of alleviate problems in both the memory wall and I/O area. So this is a picture. The CXL is kind of a circle. And this essentially allows different kind of computing chips to be connected to it. There could be CPUs, GPUs, various accelerators, type 1 devices, type 2 devices be connected to the CXL. This could also have various kind of memory that's connecting to it. And this can be inside a server. This can be outside a server, interconnected either directly through a full mesh or over a CXL switch. And this essentially, I think, enables higher level heterogeneity and allows a full composability between the various computing devices and various memory devices. And because of the effect of disaggregation between memory and compute, which is really the last mile of disaggregation. So if you look at computing resources, storage, GPUs, and networking are largely disaggregated. You can add more GPU cards. You can dynamically add more GPU cards. You can add more storage SSDs. You can add more networking cards and networking ports independent to compute. But memory is the last thing that's tightly coupled to compute. And it's really difficult to manage that separately and scale that separately from CPUs. And what CXL will allow is that last disaggregation between memory and the CPU so that both can scale independently and managed independently. And we think because of disaggregation, it can lead to a more memory-centric architecture for the data center because data is the one with the highest amount of gravity. And it's hard to move. And if the data is to be placed on memory, then you could have different processors coming to the data to process it rather than moving the data between the processors for that to be processed.

And so the benefit of this is that, number one, on the memory bandwidth front, it has a higher amount of ping efficiency, CPU ping efficiency, than the DDR memory. About three times the amount of bandwidth can go through PCIe to the CPU than the DDR. And this is additive to the DDR. So if your DDR memory presents you a memory bandwidth limitation, then CXL can enlarge it and enlarge it to the extent beyond what DDR provides. Now, obviously, with PCIe, you need to share this bandwidth with the other PCIe devices, such as your computing, storage, or networking devices. But this is something you can decide. How do they share the PCIe devices that's available? And PCIe bandwidth is expected to continue to increase as Gen 5, Gen 6, and future versions of PCIe come along.

And now, with high bandwidth memory, you can also get higher bandwidth. But that's kind of more fully attached to the CPU. It does not have the capacity, scalability, and the flexibility that CXL memory has. So HBM is necessary. It's critical. It's important. But it's not sufficient by itself. There needs to be other memory tiers that's added to deliver that capacity and scalability. So that's how this can break through the memory wall, and it can scale the capacity of all the I/O wall, et cetera.

So I believe many of you probably have read the paper from Azure that's published back in March. And it's demonstrating one of the use cases. So I'm going to first start looking at some of the possible use cases, specific use cases, that CXL can bring to us beyond what's possible today. So the Azure use case is really a pooling and utilization, more efficient utilization use case, to reduce the overall TCO of your environment. And this is particularly applicable to hyperscalers, but also applicable to other data centers. So today, for a hyperscaler like Azure, they have many servers, hundreds of thousands or millions of servers in their data center. And the servers are typically pretty fully provisioned in terms of amount of memory. In fact, memory has become the most expensive component of a server. In their study, over about 50% of the server cost is the memory cost. So you pay more for memory than for the CPU or any other parts of your server. It is very expensive. And then what they found out is up to 25% of memory is stranded. What does stranded memory mean? It's a memory that's never being used or allocated to any of the virtual machines that they are supplying to their customers. So those are just wasted memory that's sitting aside, not being touched at all. And then, in addition to the 25% of the stranded memory, there are also more memory they call frigid. And what that memory, frigid memory, means is a memory that's allocated to a VM, but not really used either. So it's off the books from a hyperscaler perspective, but their customer never used the memory either. So they saw 50% of the memory, 50% of the VM never gets used, even for those that's being allocated. So overall, about half of the memory never gets used. And that's a lot of unutilized resources there. And the reason is because the memory now is constrained within a server box. So you really have to be pessimistic in terms of how much workload will be used in the memory. So you need to have the box to be full of the memory. And many of the servers, their cores get used up before the memory gets used. That's how they are stranded or frigid.

And the way to solve this, if you have a way to pool the memory together with CXL 2.0, which allows the memory pooling, then you can be a bit more aggressive or optimistic in provisioning or procuring your servers. So there could be less memory procured within the server, but having those memory to be pooled together in a CXL pooled architecture and supplying those memory on demand to the servers. And by doing this, more memory will be used, less memory will be stranded, overall utilization will be increased. And in that paper, they show if you just have -- they build a model with 16 or 32 servers sharing a pool across the data center, then they can increase the memory utilization by 10%. And by that 10%, that translates to just for that Azure data center, hundreds of millions of dollars per year of cost saving. And this can turn into billions of dollars for the industry, how much cost they can save. Now, for the memory makers, I think what this means is not less memory gets sold, but rather more efficient use of memory. And the demand will drive more memory usage as well. But overall, I think there is a higher efficiency and higher utilization of the memory resources. So this is sort of the basic benefit of pooling is always the higher amount of utilization. And the same is true when you look at virtualization, has the same effect there. It's always the first value proposition there. And even if you go back 30-some years, when storage was -- this is SDC storage developer conference. If you go back 30-some years, when storage is being disaggregated from server and pooled into SAN or NAS devices, there are many benefits, but it starts from utilization and the efficiency as well.

And now there are more data services that can be added and do other wonderful things. And I think similar thing will happen to memory as well. And the second use case here is I call it the memory auto-healing that you can create it on top of CXL and with smart software. So memory has been made to be pretty reliable with ECC and so on that's on the memory. But if you have a lot of them, they do fail. And when they fail, there could be signals they can be giving out so that you can do something about it.

And if you have a CXL pool, when the local memory failed, even if you don't have free memory locally, you could have remote memory or pooled memory being available for you to perform with the right software a transparent migration of the data on that failing memory. When error rates start going up, you can move them off to another device and doing so without impacting the applications.

And at the end, you would retire the failing memory and have this remote memory to plug in and for the application to continue. So by doing auto-detection of failing memories, initiating transparent migrations, you can have auto-healing capability for your environment.

And the third use case is I think perhaps over the long term the most powerful and interesting use case for CXL. And so essentially what CXL allows is to have multiple hosts. CXL 2.0 and 3.0 allows you to have multiple hosts to have access to the same pool of memory. And what 3.0 allows in the spec is to have shared memory, meaning you can have a cache coherence being deployed, being implemented either in hardware or in software to allow multiple hosts to access the same area of memory at the same time with full cache coherency between them. Now the implementation of that will take some time. It's not easy to implement the fully cache coherent distributed shared memory there. But I think just by the fact of having low latency access to the same area of memory will allow the implementation of some level of memory sharing even before the full implementation of CXL 3.0. And this will become a very powerful feature for many distributed applications that are using I/Os to do data shuffling or data sharing between multiple nodes. So here is just one example, and this is far beyond this one example, which is a Spark. When you are performing a Spark job, it is a distributed application where multiple servers are performing computing tasks. And after, now the data is often sharded between those nodes. And after the computation is done, you would have intermediate results. And then those intermediate results will need to be shuffled to other nodes which will need to use that data for the next stage of computation. So it often goes through computation, shuffling, computation, shuffling, and repeat this many times for this whole job to complete. And the shuffling process sometimes takes more time than the computation itself because it involves saving the data onto your local storage, which incurs storage I/O, and then moving them over the network to the other node, which incurs network I/O. And this is slow and often the area of most complaint from the users of Spark.

And then now if, based on the CXL, shared memory paradigm, then you could redesign your Spark modules in such that instead of using storage and network to do the data shuffling, basically this typical scatter and gather process of data, you would use the memory as a media, which would really serve as the fastest transport for your intermediate data so that one writer can be writing it to a certain area, the other one will be reading it from the other nodes, and you would create a much faster and efficient mechanism for data shuffling and data sharing across multiple nodes in a distributed application. And the similar mechanism can be potentially applied to databases, especially parallel distributed databases, where the data is sharded that today the join is a notoriously slow process in MPP or distributed databases, and you can design it using a shared memory mechanism to dramatically accelerate your join operations for those databases. So that's where we think this is a paradigm shifting in terms of from infrastructure that can enable new innovative application designs, both in massive data processing and the relational databases.

Okay. Now, so those are the use cases, and what does a system look like? So a typical CXL system, if we take an old view of the hardware view, it's going to look something like this. Now, the actual look may look a little different. I think that, you know, as opposed to storage 30 years ago, this time it will probably be delivered not as one vendor, like an EMC or NetApp or something give you racks full of hardware plus with all the software integrated. It will probably be software defined or even service defined where you would buy hardware from the people who are selling hardware, and then the software can be layered on top of it. But still it's kind of good visual if you just look at the hardware boxes where if you have a rack, you would have a switch on top of the rack, and a switch is allowed by the CXL 2.0 specification, and there are companies, large and small, working on the chips for that switch right now. And we expect probably the first switch will come into customers' hands in about two years, 2024. And then you would have pools of memory that can come in the form of memory appliances or just box of memory that is available to be provisioned to the other servers or computing hosts. And then you would have a manager. I call it resource manager here, and I think it starts with what's being called a fabric manager. That is allowed by the specification that manages the CXL fabric as well as the memory media that's attached to CXL and manages a provisioning of this pool of memory to the computing servers. Now, this manager can have other capabilities which I'll go into more detail, adding data services in terms of to this memory pool. But what is a little different, you might be thinking about storage controllers of the storage systems. I think the difference here, this manager are not always on the data path for the memory access, because memory access is very fast, very low latency, and you don't want to have another layer of software to be on the data path. So this is more handling the control path of things. It can do many different things in terms of control paths, the allocation and other capabilities. And I think any data path capability will probably be implemented in the hardware inside of switch or inside of the flash controllers. Now, in addition to this layer of software, they need to have another layer of software on the computing hosts. It will talk to the resource manager to handling the allocation and deallocation dynamic provisioning of the resources, and then there's another layer of capability that can be delivered on the computing host side. And the difference between CXL 2.0 and 3.0, another difference between 2.0 and 3.0, 3.0 allows multiple switches to be connected to each other, therefore increase the scalability of your memory pooling domain. And so that will allow multiple switches to be connected, multiple racks, more servers to be able to share the same memory pool.

And now, as I mentioned, the software will exist both on the resource manager that's on the pool side, on the rack end side, as well as on the host side. And there can be different capabilities enabled on either side of the CXL, both on the computing side and on the memory device side. First, maybe just starting from the left-hand side, on the host side, I think the last talk, it really went through the tiering mechanism. I think one way to enable the tiering is from the kernel, and there's a Linux project that's working on that right now. And now there are other ways of doing tiering, and in particular, MemVerge, as I go into, has a user space-based tiering software that has certain differentiations. You can do tiering per process, you can have different ratio, different policies for each different application running on the server. There are some other benefits in software. I mean, the user space, it allows faster iterations and more modular design. You can have essentially various policies that's available that can be plugged in, and different applications could use different type of tiering strategies there. So there are various ways of doing tiering, and there are a good amount of data services that's better implemented on the computing host side, because it is closer to the application, and memory is very close to the application, so better knowledge of the processes would help. And this, including the snapshotting capability I mentioned, as well as the migration capability, like I mentioned in one of the use cases. And this software could help monitor the local memory devices, and it could also help profile the applications that's running on the host, that help you decide on the best policies to use for that particular application. And this would also allow some of the more advanced data sharing or fast memory-based transport capabilities between the hosts, as I mentioned in one of the use cases before, as well. And I think various software can be implemented on the pool side as well. So, for sure, you need a software there to manage the pooling and sharing by talking to all of the hosts. You can implement potentially different ways to optimize your capacity through compression or dedupe or other mechanisms to providing a larger logical space of your memory pool. You could implement various data protection mechanisms on that pool, potentially as well, especially if that pool involves both memory as well as NAND inside of the same system, or if there are future persistent memory media exist inside of that pool as well. And you could implement various security capabilities. There are basic ones like access control and so on, making sure people are only accessing the memory they are supposed to access, and there could be more advanced security capabilities. And then this could be your management central. By talking to all the computing hosts, you can have a holistic view of how the overall memory resources are being used and gain insights and potentially lead to certain actions as part of this management central.

So, it will be living -- it could live on -- it will need to live on some CPUs. It could be a server that's attached to the switch that's responsible for the management. Now, there could be switch switches that has ARM or, you know, running a Linux inside that this can run inside a switch. And there could also be an overall memory system that has a switch and media and this software all inside. So, like I said, the physical form factor can come in many shapes, but the software layer is the same. So, I think this is all part of the next gen composable infrastructure makeup. But I think it in itself is, you know, is hugely valuable for applications.

Yes?

So, that's a great question. Just to share with you, I think I have about five minutes in my time I want to finish. And I want to -- I'll answer this question right now and maybe I'll cover one thing and I'll open up to some questions. So, we see this as an opportunity for the industry to come together. And what we are planning to do, what we are working on now, and we are working on that with a number of partners, hardware partners, who are making the switches, who are making the devices, who are making the appliances. We are not doing hardware ourselves. We are only doing software. And they are not really interested in doing software. They're doing it only because they have to, to make it demoable. So, what we are working on is a fabric manager to start with. And we plan to make it open source, Apache, just for anyone to be able to use. So, this will be able to be used by any of the hardware vendors to work for their devices over the next couple years. I think in the next couple years we're going to see a number of different hardware devices that will come to market. And we hope to provide a free open source version of the base fabric manager that allows the base provisioning and all that to happen. And hopefully there will be less of this fragmentation. And then, so that's how the, and then potentially we'll build more higher value add capabilities that can plug into that so that we can survive as a company. And then we are also making software on the host side. Starting with the memory machine, that's a product that we have been selling to many customers over the last couple years. And that is working on CXL already.

So, maybe let me show you, let me just jump to, it doesn't have it here. Okay, here. So, I think we have our memory machine, which is a tearing and snapshot software that runs on host. And we demoed it at Flash Memory Summit. It's running on the next gen, Intel next gen server. And also works on AMD next gen server that supports CXL. And it's connecting to DDR5 memory. It also has a new CXL memory expander card. This one's from Montage, which makes a controller. And they have their own test card.

And we get some initial numbers that runs on top of, between DDR and CXL memory. You know, the latency is actually not bad. We use MLC, micro benchmark from Intel. The latency to the DDR5 is 108 nanosecond. And to the CXL card, this particular card is 272 nanosecond. So, it's roughly about the same as one NUMA hop away. So, you get similar latencies if you're accessing DRAM on the other NUMA nodes.

And then, yeah.

No, no, it's not just the DIMM latency. It's also the controller latency and everything else. So, yeah, it's the, it's overall latency end to end.

And so, this is, I'm sure they're going to make it even better. Because this is still a sample controller. But this is already pretty good in terms of latency. And then, not sure, let me try to do the thing, see if it auto advance. So, on the graph there, you see the first bar is running various type of workload, all read, a different read/write ratios for MLC. And then, we also did with stream, which is another benchmark. You see the performance of DDR5 only and CXL only, with the exception of all read, are all within 5 to 10% with each other. In some of them, CXL is actually a little higher from the bandwidth perspective. And then, the third bar is actually running our current software, just out of the box, running tiering between the two type of memory. Which we are seeing an increase in the bandwidth performance, because it's using both devices. So, that's the kind of early results. The first CXL devices is here, in terms of sample, it's working. We are now running various application workload on it with our tiering software. And our snapshot also works on top of it. And so, essentially, it's real, it's coming. And we are working hard to continue building software, both on the computing server side and on the pool side. To really enable this new architecture, we think is gonna be paradigm shifting for the industry. Okay, so that's, I think, the main content. Maybe a few minutes, couple minutes for questions here.

Actually, let me just show some of the other content. I think the material will be on the website. But it's a very broad ecosystem. From the memory to the CPU to servers to switches to hyperscalers, applications. A lot of people are working on technologies related to CXL. And we are trying to contribute from this kind of memory middleware layer to providing a common platform that people can use.

And I believe a new industry could emerge for us, who I worked in the last 20 years in the storage industry. And so, my view might be a little tainted or biased, but I really see a parallel here, where with storage, once it's disaggregated with fiber channel and later on with IP network, that really create a new industry from nothing. Before this, the storage industry was the industry of hard drives, and people making better and better hard drives, and that's it. After this, people creating innovative software, and that's how their storage developers start to exist.

And I think we are in the same period, like 1990 for storage, where I think memory developers will start to exist. And people can develop interesting data services on top of memory. Many of them I have no idea about, that could be invented once this new fabric starts to exist, and so on. So I think that's where all of us could have some interesting work to do in the next 10 years.

And in summary, CXL is a key enabler. I think for the first time in history, software will become a critical component of a new memory system. And the combination of hardware and software, I think, will create a new $10 billion plus market from zero today. Okay, questions?

I think 1.1, good stuff will start to happen. I believe the first 1.1 device will become available commercially at the end of next year. So we are about a year away from the 1.1 devices. And that has the effect of scale, both memory bandwidth and memory capacity. And like I said, our memory machine, as well as kernel tiering, and all those capabilities in software will be needed for a successful adoption of 1.1 devices. But 2.0 is where I think things will become really exciting. Because that created a new architecture. 1.1 is incremental innovation. And I think many customers will buy it and so on. But 2.0 for the developers will become more exciting because that opened up many new possibilities. Any other questions? Okay, thank you very much.

