
So, hi everyone! My name is Greg Price. I'll be talking about mempolicy extensions to better support heterogeneous memory systems. Sorry, I couldn't be there in person. If you have any questions either after the talk, feel free to reach out to me on the mailing list, or just reach out to me directly; it's also fine.

So let's—here we go. So let's talk about what we're going to talk about. I'm going to dive into a little bit about what we mean by heterogeneity because it's a little complicated. And I'm sure some of you are asking the question, 'Why are we still talking about mempolicy?' Everyone's talking about tiering. They're somewhat orthogonal, and I'll discuss why that's important. We'll talk about the new—there's a new mempolicy that we just released and got upstream in version 6.9. Way to dinner leave. I learned a bunch of lessons from that and got a bunch of good feedback from a lot of interested parties. And that's going to lead to some additional future work. There's some new policy ideas that have been thrown around, including some new syscalls and some new migrate on mbind stuff as well. And then probably I saved the more contentious piece for last, which is—there's been a bunch of interested parties who have been suggesting, 'Well, what if we just have mempolicy in cgroup?' And there's a bunch of reasons why that's maybe good, bad, and ugly. And we'll kind of dive into it a little bit. Feel free, if there's any questions along the way, to just interrupt me. It'll be easier for me to field questions in that regard, because this is really going to be a smattering of ideas in this talk.

So, heterogeneity. This is not a talk about PMEM, or CXL, or GPUs, or whatever. It's more general than that, right? So, if you use a two-socket server, I contend that you are using a heterogeneous memory system. And you really have to think about this in terms of where you have a task executing on your system. If you have a task executing on socket zero and you have DRAM attached to socket zero and socket one, well, if you're accessing memory across the sockets, you are actually accessing heterogeneous memory because the DRAM on the remote socket has different latency and bandwidth than the local socket. So, that's really what we mean by heterogeneity.

In a more future-looking perspective, you can expect to see systems that are, like it or not, somewhat of a monstrosity like this. Yeah. Where you might have HBM, DRAM, local CXL memory, CXL memory that's behind a switch. You might have HBM on a GPU that's directly mapped and managed. And it's going to get complicated rather fast. And so, the current mempolicy subsystem is not well-built to reason about a system like this. And I would contend that it's also not well-built to reason about a system like this, either. And so, that's what we've started to look at in building new policies.

So, second question: why am I focused on mempolicy? Shouldn't I be focused on tiering, like everyone else is kind of looking at? And I kind of separate these two logically into allocation policy versus movement policy. So, mempolicy is a per-task or per-VMA NUMA policy that controls node selection at allocation time. So, when I say mempolicy, you should think allocation policy or node selection policy at the time a page is faulted in. Tiering is more migration-focused, generally speaking. It does go into the allocation side a little bit, but generally speaking, when you hear tiering, you should be thinking movement policy. Where am I moving data to? Is it this tier, that tier, et cetera? In a perfect world, we would always kind of pick the right placement on allocation and then never move that memory again. But in reality, that's not really feasible. The allocation policy will pretty much always be wrong for some definition of "wrong" in space-time, right? Memory tends to get hot and then cold, and then hot and then cold, and fill up, and maybe we want to demote stuff. And so, tiering is useful. I would also say that an effective allocation policy is just as useful, but the two will need to be used effectively to manage heterogeneous systems.

So, where are we at right now with mempolicy? As I mentioned, mempolicy is task-local, which means every single thread can have a separate mempolicy for itself that covers all of its VMAs. It can also be VMA-local, which means an individual VMA, including shared VMAs or memory regions, can also have an explicit mempolicy. And I have some things highlighted in red here, like interleave accounting and ref counts, that are alluding to the fact that there are some warts in the system that probably need to be handled into the future if we're going to be more effective with this. But mempolicy is basically a mode and a node mask. We'll talk a little bit about those modes in a second. And you'll note that there are some interactions with CPU sets or cgroups as well. There's a ref count. We'll get into that. But really, it's not all that complicated. The system is relatively small, but there are a lot of interconnected components that we have to worry about.

A quick mempolicy crash course, and we'll get right into it: there are basically four different types of policies. The default, or local policy, basically says, "On allocation, if local memory is available, use that." The preferred one basically says, "If memory is available on the preferred node, use that." Interleave is a round-robin distribution of memory; basically, every node gets one page in round-robin mode. And then, weighted interleave is basically round-robin, but with different weights applied. So, you can have different distributions of memory based on the different node, kind of using nodes as a tier stand-in, more or less.

So, let's take a talk about the new mem policy that just launched in 6.9, and then we'll talk about some of the lessons learned from that. Weighted interleave is basically interleave, but it's weighted. It just went upstream in 6.9, and there's one set of global, quote-unquote, "weights" that were added to sysfs that you can modify. The observation was basically, "Oh, the local DRAM channel, for example, might have 128 gigabytes a second of bandwidth, but in order to go to a remote socket, it has to go across a cross-socket interconnect." And that cross-socket interconnect could be lower in bandwidth than the remote DRAM, which really means your effective DRAM or your effective bandwidth is whatever that cross-socket interconnect is. And that means, in this setup, for example, this very simple setup, the optimal bandwidth would be a two-to-one distribution of weights across these two nodes. Latency is a little more complicated; you know, we have to ask the question, "Is bandwidth actively saturated? Is the capacity saturated?" And that can cause you to say, "Well, if the bandwidth is saturated, it might be better for latency to put hot pages on a remote node to allow more concurrent memory requests, for example." So, until weighted interleave showed up, it wasn't really possible to do this, at least not easily. Weighted interleave allows us to do this for relatively simple forms of tasks, uh, and to be able to capture this type of topological nuance directly in the mempolicy system. So, if you want to check that out, it's all upstream; it's all in the man pages, and it's been documented.

So, we learned some lessons while implementing this; however, mempolicy hasn't really been, uh, brought forward as the computing environment has evolved. So, this is just one test I'm going to show you. It was a single-socket system with a CXL memory expander. You can think of it as two tiers: the bandwidth ratio for this system was nine to one. Right? I used the stream benchmark, which very simply just uses a bunch of heap memory and drives bandwidth to those regions. If we use the default interleave, the round-robin one-to-one interleave, you can see that it was 78% slower than DRAM. Right? And this is one of the reasons why, if you talk to a ton of people about, "Oh, are you using NUMA control? Are you using NUMA policies?" etc., they'll say, "No, NUMA sucks." And this is one of those reasons. Is it intuitively? It doesn't seem like a good idea to just use interleave because you're not going to have some of those topological complexities captured. Weighted interleave, when you use it from the task perspective—not the VMA perspective but the task-wide weighted interleave—you get better results, and that can be plus or minus five percent of the time you're going to be able to get that four percent, give or take. This still kind of sucks, right? I'd really like to get that four percent consistently, if possible, but it is significantly better, and it lets you use more of the resources that you have available. And this is one of the biggest takeaways: if I use mbind instead and apply it only to the VMA regions that are going to drive bandwidth, I get a consistent two and a half to four percent increase in performance. Now, obviously, this is going to be workload dependent, but this is a stand-in for kind of a general workload. And really, these last two points should tell you the problem with the existing mempolicy system, right? There's a gap between, "Can we do something better than just applying a mempolicy to every single VMA?" or requiring a task to be NUMA aware. Right? So, in order to use mbind, a task has to be NUMA aware. Can we make the targeted approach a little more available and in a natural way? And we'll jump into that. Um, so this is where we—this is some major lessons learned from implementing weighted interleave.

There is some future work that is, uh, still pending for the weighted interleave. Uh, "a" is, uh, default to global sysfs weights. Right now, the default operates pretty much the same as standard interleave, and an administrator, or some kind of daemon, is expected to go in and toggle those weights to the, uh, to the, to the defaults that you expect to apply to various tiers. Uh, we do have an RFC out based on various HMAT info, for example, provided by the CXL driver or other, other forms, but this isn't trivial. Uh, in particular, large NUMA systems—systems that might have a thousand nodes, or two thousand nodes, or three thousand nodes, what have you—uh, calculating default weights is actually relatively complex because you're doing some complex work, and you're doing complex divisions, and if you happen to hit a prime number, the whole thing falls apart. Um, so if you have a use case for something like this, and you have a large NUMA system, please reach out to me. I would love to see an example setup of that, and how we might mitigate that issue. Um, but there is an RFC out, and I expect that to move along, uh, pretty, pretty quickly. The other one is task local weights; this was dropped from the initial, uh, feature support, and that was because it required, uh, additional syscalls or maybe procfs extensions. Um, but it was relayed to me that that would probably hold up the feature as a whole, and it really needs users to justify such an extension. So, if you have a need for something like task local weights, where each individual task has different usages of different, uh, tiers, uh, please reach out. Please hit the RFC, and we can discuss.

So, let's get back to talking about mempolicy in general in a heterogeneous world. Uh, there are a few warts on the system, and some observations that have held over the assessment. Um, the biggest one is that large NUMA systems have become very unwieldy in trying to apply these like task-local or global mempolicies, especially when it comes to sub-NUMA clustering. It's really, really hard to use these things for real-world benefit. It's hard to reason about these systems, and—this is just a general statement for you know, CXL folk or anyone that is working on a system or portions of the system that generate new nodes—you should probably consider consolidating wherever is possible or feasible. An example of that would be CXL interleave sets. If you have a bunch of local CXL memory expanders, it would be a lot easier to reason about those if there's fewer nodes involved. So, using interleave sets is preferential to end users, uh, to be able to apply better policies. Um, but there—there's a whole conversation that has to be had around that, that side. Um, in terms of maintaining the subsystem, the biggest wart on the mempolicy subsystem is the fact that it is written in a very current task-centric way and by that, I mean only the current task can change its own mempolicy. As you can see, the syscall that I included in here doesn't tell you—doesn't provide a PID FD, doesn't provide you a way to toggle another task’s uh mempolicy. And this is problematic for a variety of reasons. And by extension, the same thing. VMA policies can only be changed by tasks with access to that VMA, so shared memory regions. Changing this, the fact that only the current task can toggle its system is actually really rather annoying because it utilizes the current, I guess, mnemonic that says, hey, check the current task permissions to do this particular operation. And ripping that reference to current out and changing it to a task-driven thing, it requires replumbing through at least two or three other subsystems: shared memory subsystem, VMA subsystem, and a couple others. And that current task-centric design is actually performance-related. Mempolicy as it stands today is actually completely lockless in the allocation path for a very good reason. Locks are expensive, right? But there's also reference counting that is present to track those shared memory policies. And now, today, we're using RCU, depending on the different policies applied. So, there's a really complex locking mechanism, depending on which direction you are accessing mempolicy from. It can come in from cgroups. It can come in from the local task. And it can come in through shared memory regions. And so this probably will end up requiring some level of a refactor to drive everything towards a more RCU-driven update and update policy. So that's a major wart on the system and something we're still struggling with, even though the component itself is really not that big. And the last one is, and I alluded to this, there are cgroup interactions. From this perspective, mempolicy cannot be used to violate cgroup restrictions. mems_allowed is one of these. You can call this CPU sets, if you will. But I shouldn't be able to tell mempolicy, "Hey, allocate from node 4 if CPU sets limits me to accessing only nodes 0 through 3," as an example. And so there are some complex interactions that go on with, well, what if cgroups changes its mems_allowed at runtime? And that's something that there's a complex locking structure in cgroups, so we can't just probe cgroups easily, either. Right. They're on changes. So, cgroup-driven migrations, for example, may cause weird rebindings. And we'll talk about how that looks. And it is somewhat rather nasty.

Can I have a question, Gregory? So here's Michael. Yeah, what you're saying is correct. But my question is mostly, how do you envision that external party could manipulate per VMA memory policies without cooperation, without application? Because that just sounds to me like solving a problem by creating tons of other problems.

Of course. Yeah, of course. And I actually agree with you. And there's an RFC out there right now about having process and bind work. And you're not wrong. It is very complicated, and it can potentially be, A, very security-sensitive. And I think the key observation right now for me is it should be possible without causing problems, if only because shared memory regions can have policies. And so it is technically already possible for one task to change another task's VMA policy by nature of a shared memory region having a mempolicy. And so I haven't done all of the digging into the specific mechanics there. But since that's already possible to do, and I don't know whether if that's an intended feature or just something that is. But if it was an intended feature and it is something that is, then probably we can use the same mechanisms to allow for that, right? Obviously, this type of interface is going to use an administrator at a minimum, if not nice, and etcetera, because you're changing allocation policy. But hopefully, I answered your question enough.

Well, I'm not sure.

Yeah, I think the core of the answer really is, "I'm not sure entirely." However, it does appear that there is a way to go about doing this that isn't going to be a complete upheaval, because the shared memory regions already allow for shared policies.

Yeah. No question about shared memory. Yeah. Also, I think that PIDF, the interface to change the global policy for the task, is something that is reasonable, because that's what cgroups do for all the processes that are living in that cgroup. But I'm not really sure that we want to have that per VMA, because that just opens a lot of weird cases where... And I clearly do not see a use case for that, because you, you really have to have an intimate understanding of the application in order to do that. And if you do have that kind of understanding, then probably something like a user fold FD might be a better way to approach that. But I don't want to hijack more of that, and there is another question.

Sure. Go ahead. I do have a slide on that specifically, so we can discuss it at that point. So, there's another question.

Hi, this is David. So, I was just wondering: How do we envision that in the long term? That like, some application will go around and change random policies for VMAs, or would it be an option that an application explicitly opts in to having a, let's call it, a managed VMA policy, however you would want to call that. That like, for example, the application allocates some unique ID and sets the VMA policy to that unique ID, and then, like, you have a clean interface where somebody can just modify that across multiple, multiple VMAs. I don't know, but like, have some kind of abstraction and an explicit opt-in from the application that a certain memory area can be changed by an external authority.

And I think—so the key observation, one, is just allowing it to happen from the first case, as Michal was saying—is that's the question we have to answer first. And then, how do you design the interfaces is second. And I think—I think where we are right now with that RFC is it's not in a state that I think anyone can agree on. And I think all of these are absolutely valid questions. So, in the long term, I think it's more likely, as Michal said, it's more likely not we're going to reach directly in with mbind to change mappings for a particular VMA. I think it's more reasonable to try and enrich the policy system to use existing information, like madvice flags and mmap flags, VMA information, to just make decisions about placement without having specific mbind calls required. And I actually have a few slides on that coming up here. So, this is the existing observations of words. And then we're going to get into some of the more proposals.

So, this is Liam. So, the mempolicy right now is static. So, if you have this weighted interleave, if the scheduler switches where the task is, how does that affect the layout of the memory, and what occurs next? Do we need to change the mempolicy based on the CPU it's running on?

Very good observation. And that is something we spent a lot of time talking about. And the reason why we settled on those global weights in Sysfs was because of this migration issue, right? If you have, for example, task local weights and you have a migration from one socket to another or what have you, yeah, all of your weighting gets really messed up. And in particular, some of these rebinding issues already happen. And you're right. We need a way to kind of clean that up and get a little smarter about it. Because you can end up with really weird re-bindings, or you can end up with your task running on socket one and all of your memory being on socket zero. And there's no good way right now using the mempolicy system on a rebind to trigger all of those migrations to occur in some scenarios. So, I'm actually going to talk about that.

I don't know if you really want to migrate, but you want to change your policy to be accurate for future running. And it almost seems like every policy will have a reflection policy based on each node in the system. So, depending on which node you end up running on, your policy is changed. And if it's not, it's going to change to be a flip, or, I don't know, if you think of four or multiple CPUs, it gets even messier.

Well, so right now with the weighted interleave, because they're global, when a migration occurs, it'll start using the task-local perspective and weight appropriately. So, for example, if it just moves from socket zero to socket one, it'll see that socket one has a higher weight than a remote task. But yes, you're right. It is, right now, we don't really have a good topological way to capture cross-socket interconnects, in particular with this system. And this is where, as you get more and more NUMA nodes and more and more compute nodes in particular, it becomes unwieldy. And that's kind of the first bullet point on this: large NUMA systems get to become very unwieldy in that regard. So yeah, we need to have that discussion more explicitly. And we have discussed it to some extent. And that's one of the reasons why task-local weighting got pushed off to a future release, because it was hard to reason about what happens on migrations.

Also, in the mobile space, we have lower-powered cores that are managing the freeing of memory. And so, if you think about what's going on there in respect to what the policies are, then basically all of the memory it is accessing is remote. It's slower.

Yep. And then I would love to get more input from the mobile side for sure. So I'm going to push along, because I got a couple more slides to get through, and I'm running short on time here. So, a couple of feature proposals that have been talked about. I already mentioned this earlier. Is it possible for us to get closer to that VMA mempolicy result by using task of global mempolicies? The idea here would be, can we use existing information like MMAT flags? Or madvice? Or VMA info? Or cgroup data to avoid things like interleaving code mappings or stack mappings, for example? And that's basically what I did using the VMA or mbind policies with the stream benchmark; you're basically avoiding interleaving stuff that isn't really driving bandwidth. And so there are some open questions here. It seems like a reasonable lift for a reasonable amount of pain, but there's some research and some plumbing that needs to be assessed here. This is one thing that doesn't really have an RFC yet.

Process mbind; we basically just discussed this. There is an RFC for this available. There are a lot of warts because of the current task-centric design of mempolicy at the moment. But it does, to an extent, work in a prototype phase. It could be thought of as a, quote-unquote, crude memory tiering mechanism. I'm not sure how, as others alluded to, I'm not sure how much we want to support that as a tier one feature. But you can think of it that way. But job schedulers might have some interest in this type of feature, though, for things like resource use compaction, gathering stuff in, or reclaim and such. And the core problem is the fact that it's all current task. And it can open up a can of worms in that regard. So there are security implications. There are performance implications because this code is in the allocation hot path. So if this is something that is in the foreseeable future, this is going to be very careful work to make a possibility.

Migrate and mbind for interleave. This is something I wanted to touch on to give you an example of how migration is complicated with mempolicy. Mbind, right now, only migrates memory when a node is removed. So, if I'm currently interleaving over set 1, 2, and I move to set 3, 4, then I'll migrate everything on node 1 to node 3 and everything on node 3 to node 4. But, if I'm currently, for whatever reason, only interleaving on one node and I add a node, no migrations will occur. So, there's no redistribution of memory. That might actually be nice, even though it is expensive to do. But so is the existing migration. So, if the migration in case 1 is supported, then maybe the migration in case 2 might be nice to have as an opt-in, maybe not as a default, for example. So, the two proposals that are out there, can we allow redistribution of interleave regions? And allow the application of weights on distribution? You can think of this as another crude tiering mechanism. Not sure, again, not sure how much I want to support that as a feature, but it is an idea. And mostly, this is useful for things like job schedulers so that they can have reliable and predictable results when they migrate cgroups or tasks from one place to another and bind them.

And this is really the end, one that's going to have the most contention. Is there a reason to have a cgroup mempolicy, right? We've had mempolicy, but what about a second mempolicy? What if we just add a mempolicy to a cgroup and have all of the tasks in that cgroup either inherit or use it directly? So, mempolicy being task-local allows it to be mostly lockless. And cgroups is not lockless, in effect. It's iteratively locked. There are iterative locks that you can end up going through. I know I'm running short on time.

One reason that this has been a problem in the past is that it's really hard to define reasonable hierarchical behavior of that. And that's a hard requirement for any features. So, that's where you might really hit that end.

Yeah. And I have one last slide that talks about some of the feedback. So, the two motivations here are: we have an allocation policy management of a set of processes and threads. That would be really nice. And it would allow the simplification of the allocation policy when jobs are migrated. So, if you're migrated from one cgroup to another, you just get the new allocation policy. That would be really nice.

But this has gotten some skepticism and feedback. From Tejun, I've already said he's not sure that this really belongs in cgroup at all. Explicit point: it's unclear what the hierarchical relationship means. And that is not something I've fully worked through. And I'm not sure I will be able to work through it. But it is. It is worth exploring, I think. A quote from Mikhail: 'You know, its CPU set seems like a good fit, but there was some contention there.' So the too long; didn't read of this is: there's no real consensus on this particular idea yet. Although there is, in talking to quite a few people, quite a few people that are like, 'Man, I wish this existed. It would be so nice from a Kubernetes perspective or for whatever my container system.' It would be really nice to be able to just redefine where allocations occur without having to do that, having to reach in and change things or make my software NUMA aware. So I'm going to continue pushing down this path and looking for solutions. And I'd love it for you to join me. And that's kind of where I'm at. I think I'm at time.

Any other last questions before we hop over to the next?

There seems to be no other questions. Thanks. Thanks a lot, and good luck. I mean, this is a hard topic. And even though it seems that there are many dead ends, I believe that it's worth pursuing. So, thanks a lot.

I think so, too. Thank you.
