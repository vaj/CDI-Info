YouTube:https://www.youtube.com/watch?v=JiZWNj0mPjk
Text:
Okay, so as Frank mentioned,  this talk is about how we can build large single node memory  with memory expansion using CXL without breaking the budget.

First, I'd like to talk about the motivation. Why do we need to scale up these large memory servers? I think Ron probably touched a little bit on that in his previous talk on AI use case. But even beyond AI, we see a lot of HPC,  high-performance, generalized high-performance computing,  and database use cases for scaling up a server. There's three primary reasons if I were to break it up. It's one is to enable the processing of ever larger dataset. I mean, we see it in AI,  we see it in genomics,  we see it in computational fluid dynamics,  EDA, and all type of simulations in general. Number two is a faster time to result. Then this is comparing the scale up approach versus scale out approach,  where you try to shard one large problem  so that it can be processed in a distributed fashion. But if there's opportunity to bring it back into a larger server,  we actually see a faster time to result. Then the third one is this higher performance to the cost ratio  that goes back to something Frank mentioned earlier,  how do we bring down the cost down here? I have two examples. One example is a work from genomic space called MetaBat. It does a lot of genomic analysis of a large set of data. We're comparing here the two running environment. One is this scale out approach with a cluster of 100 nodes. Each node is about 64 gigabyte instance. It took about two weeks to finish one run or one analysis. Then just because there's so much coordination between these nodes,  there is a high probability of failure for this run. Now, we were able to do this on a single four terabyte node. That reduced the runtime significantly from two weeks to four days. Still long, but much sooner than the two weeks. It's a significantly lower chance of failure as well. The second example I have here,  again, it's to our deck of this performance cost ratio here. Then this is for our analytics workload based off a SQL Server. Baseline here is that we have one single 64 gigabyte server. That's a baseline queries per second QPS. Then if we were to support higher queries per second,  there's two ways you can do it. You can scale out just doubling the server. But then what you have here is that you will run into  the additional server cost as well as  software license associated with this additional server. Usually, that's just as much as a hardware cost, if not more. Then the other approach that we took is that we scaled  up this individual server by expanding the memory using CXL. We're able to get a similar result,  2X the queries per second metric,  and with only cost being the memory expansion module. I'll go into a little bit later about that. That's the motivation of why we want to scale up on the server.

 Now, there's challenges. Of course,  when you look at server in general,  there's CPU core count, GPU now as well. Then the other big side of this is memory. Our talk is more focused on the memory side. In terms of the memory scale challenges,  it's number one is that there's  a limited set of DIMM slots on the motherboard. It's usually fixed as well. There's only so many DIMM slots that you can plug in. Number two is that the DIMM slots themselves,  they have certain capacity and bandwidth characteristic. In general, you have to follow certain patterns  when you populate these DIMM slots. There's only a few ways that you can expand your memory. Then third is the actual cost,  especially if you get up to the high-density DIMM slots,  like the 128 gigabyte DIMM slots. The cost of these DIMM modules,  they go up significantly. The per unit cost goes up,  per gigabyte cost goes up more than three times. That's why it's not that easy to scale up a server,  especially on the memory side. What we're presenting here and then the rest of  the slide will follow through is how we can do it. One way we can do it using CXL expansion hardware  and Memory Machine X software,  that's our product to enable this scaling up of  a memory on the server within  budget and without impacting the application performance. So that's a flow of this.

Let me go into this. Sorry, I just see a question here. I saw the question from Lewis about the math of expansion. I actually have a slide to go into it,  so I'll come back to this. So for CXL, just a brief refresher. I think some of our previous friends  has gone through that before from,  I think, Smart Modular. But in general, what you see originally is that you have  these DDR DIMM slots that's attached to the motherboard,  attached to each of these CPU socket. That's traditionally the only available way for memory. With CXL technology,  there's two more methods to expand memory now. One of them is using these E3.S modules. They're easier because they're front-loading,  so you can swap in and swap out just like you do with a hard drive,  without opening up the box. However, usually they have a fixed capacity. So we have seen 64 gigabyte, 128, 256,  and then I think there's  an announced product within 512 gigabyte as well. The memory bandwidth is usually at eight PCIe lanes here. The other option is something you can plug into the back  with the PCIe adding card, the AICs. These are PCIe card with  a CXL controller and their own DIMM slots. It's more flexible in a sense that you can  populate it with different DIMM configurations. They're in generally higher capacity,  you can go up to two terabyte per card. In terms of memory bandwidth,  they can go up to x16 PCIe lanes,  which is functionally equivalent to about one DDR5 channel  for these socket or motherboard-based DIMMs. These are the different configurations. Because we're talking about extra large capacity,  server out focus on the AIC card.

This is literally like the money slide here out of this deck. What I'm trying to compare here is different configurations. You have a baseline configuration of all the DRAMs on the motherboard,  compare it with other configurations that has CXL expansion. These three columns is just the total system memory spec. Where you have the capacity,  the total size, the total cost,  and then the per gigabyte cost. These columns are here are the memory,  the DIMM modules on the motherboard,  what density we're using,  how many, and the subtotal and cost. Here is the CXL AIC card with the type of expansion capacity and the cost. That's a general format of the table,  but I'd like to just call out a few comparisons here. Now, I'm assuming this is using Intel. I have the figures for AMD as well. If you're interested, feel free to reach out to me. I have my e-mail at the end of the presentation. Let's first look at if you are to build a four terabyte server here. The only way to do it by using everything on the motherboard,  all the memory on the motherboard is using these 128 gigabyte DIMM modules. There are 32 slots on there for a dual socket server. Then this is the cost. It's about $46,000 with $11 per gigabyte. With CXL, there are several ways we can do it. We can move it back on the motherboard itself. We can reduce the density from 128 to the 64 gigabyte DIMMs. That significantly reduces the cost there. Then we make up for the capacity by using CXL expansion card. Here, I'm using these expansion card with eight DIMMs each. I think our friends at Smart Modular have a product with this configuration. You can see here with four of these cards,  we can bring back the total capacity to four terabyte,  but at less than half of the memory cost than  our previous motherboard or socket only DRAM configuration. You can even further optimize that by  using DDR4 at the expansion. By the way, I should mention that I'm assuming all these DIMMs are using DDR5. But on the expansion slot,  you have the option of using DDR4. In this case, if we use DDR4,  128 gigabyte DIMM modules,  we only need two AIC cards,  and we can even reduce their memory cost even further. Both the per gigabyte cost and the net total cost. That's one configuration. The other configuration here, this is a four terabyte. I also run another comparison for eight terabyte. For eight terabyte, if you're to again use a socket memory only,  you essentially have to go for four CPU or quad server configuration. Then each one of these memory slot populate with 128 gigabyte module. Then this is just a purity,  the memory cost for eight terabyte. But in addition to that,  because you are using more custom quad CPU system,  the entire system becomes more expensive. I have some example of that. Alternatively, again, using CXL expansion,  what we can do is we can bring back to a dual socket server using  these 64 DIMM modules and making up the capacity using AIC expansion card. This is what you get. You have a lower cost dual CPU system  and a lower cost memory as well. So about half of the memory cost. I ran a few more configurations here just to show if you want to go out,  build 11 terabyte system using dual socket CPU and CXL expansion. This is what it'll look like in terms of memory cost. Then if you want to go all the way out,  you could essentially build 32 terabyte single machine. This is what previously would have been a supercomputer arranged. You can do that. It's going to be more costly,  but if you have a high value application for it,  then this is something that's possible now.  There's a huge cliff between 96 gigabyte, 128 gigabyte DIMM cost. How much of CXL liabilities based on this difference? Presumably, the cost difference will drop over time. So you're absolutely right. I think that CXL introduces a few more flexibilities. One of them is based on this per gigabyte cost from the different density. The other one is from the different range DDR5 to DDR4. Both of these could play a role. Then in addition, I think it's also the flexibility using dual socket system,  quad socket system, and as well as if you wanted to hit certain size. Normally, when you populate these DIMMs,  you want to keep them in a certain. All the DIMM modules have to be the same size. When you especially go up to bigger memory machines,  it's like a step, one jump to the next up. Then with CXL, it gives you more flexibility on  what kind of total memory you want to configure.

I think there's a question from Louis before. If you have a 64 gigabyte DIMM,  assuming each CPU support 12 channels,  we could get up to four terabyte in a cluster of five. Wonder about the math of 164 gigabyte nodes. Can you expand on that?

I think for Louis,  I could probably reach out to you and we can have a huddle on the math of 164 gigabyte nodes. In the example I had cited,  they were actually using AWS for this. We can look at not just the capex cost,  but we can also look at this from a opex perspective. Any other questions? I hope I answered those two questions. Okay, great. Moving on.
 
Next, I wanted to show just some of the servers that would be able to support this. These are actual models from Supermicro. You can look at this 2U unit that has a dual AMD CPU. If you look at the back,  you could actually fit four of these AIC card in there. Even though there's eight PCIe slot,  but each one of these cards are two,  they have a dual width,  so they take up two slot width-wise. You could actually fit in four of these AIC card. If you wanted a higher capacity like that eight terabyte,  there are 4U systems that can support eight of  these large AIC expansion card using  the PCIe slots that people can use with the GPU as well. There's quite a range of servers,  both from Supermicro and  Lenovo that can support this expansion.

Just a more view of architecturally when you expand these memory,  this is what they will look like architecturally. You have two CPUs here that's connected over UPI in case of Intel,  and Infinity Fabric in case of AMD. Each CPU have its own socket memory. That's one NUMA node, and then it's connected over PCIe to these AIC card. Physically, of course, the link is CXL over PCIe. Each one of these AIC card is its own NUMA node. For this first CPU as well as for the second CPU. Now we have expanded the capacity without breaking the budget.

There are some trade-offs and  potential application performance challenge. Then this is because of this memory hierarchy now  that all these NUMA nodes introduces  a more complex heterogeneous memory hierarchy. Here's a table showing that if you're assuming a process that's running on NUMA 0,  CPU 0, this is what you see. You essentially see six NUMA node. Each one of these NUMA node have its own capacity,  latency, and bandwidth characteristics. The application performance could be  impacted due to their cross NUMA memory access. For instance, if your process is running on the first CPU,  and then for whatever reason,  your page, especially a frequently accessed hot page,  is allocated on NUMA 5. What you'll see is number 1,  is that the latency will be higher. CXL is much faster than PCIe itself,  but still is about 100 nanosecond of extra latency similar  to your previous cross CPU socket latency. You would see a longer latency on the access. Then the other effect is that there are different bandwidth limit.

For instance, again, on this from NUMA 0 to NUMA 5,  what you may see is bandwidth bottleneck between these two CPUs,  because there could be traffic across these CPUs,  as well as the bandwidth is shared between all these three NUMA node.

 The issue is that if you saturate this bandwidth,  then the latency gets significantly higher. This is actually from a measured dataset that we have,  is that normally the latency is as advertised,  100 to 200 nanosecond extra. But as soon as you get close to  the bandwidth limit when you saturate the bandwidth,  the latency goes up exponentially to,  it could be 1,000 nanosecond or even more. Then that could lead to problems not just in terms of performance,  but also a machine check if the latency gets too low. These are the trade-offs.

This is where we come in. Our Memory Machine X, I would say middleware,  runs in the background,  transparent to the application. What we do is that we continuously monitor  the application's memory access pattern,  and we optimize where these pages are being  allocated relative to where the process is running. We would continuously optimize all these memory placements  such that they'll minimize the cross-NUMA traffic,  particularly the frequently accessed hot pages. Here is just to show an example of MySQL and TPC-C benchmark. I have two figures here. One is for the transactions per second,  and the other one is for the P95 latency. These bars are color-coded. The red is the baseline with 64 gigabyte of memory,  and then the rest is I/O,  like you have your local SSD. When we expand that 64 gigabyte memory  with another 64 gigabyte of CXL memory,  we see a significant jump in  terms of transactions per second already. Now, if we add memory machine to the mix,  we see another jump in terms of transaction per second. Then just for the reference,  we compared it with the full 128 gigabyte on the socket. There's no NUMA hop. This is transaction per second. You see benefit from CXL memory expansion,  benefit of memory machine,  our software to optimize all these memory placement for transactions,  for throughput, and then this is for the P95 latency as well.

The last slide here is now,  we're looking for trial proof of concept. Users, and then we have partners. Some of them you met previously in these sessions. We have partners,  Astera Lab, or I should say our software is released,  and then we have partners in  this ecosystem that provides these AIC card. Astera Lab has this A1000 card that's available now. You can see this is actual card. It has four DDR5 DIMMs. It's a full height, full length,  double width, just like a GPU card. Then it's about 75 watt. Smart Modular, they have a smaller client. They have this single width,  full height, half length card that takes four DIMMs,  and then it's here. This is very versatile. It practically can go into any servers. Again, they have this large eight DIMMs expansion card,  along with other vendor who have a DDR4 base expansion card as well. Here's the availability in terms of engineering samples for  PoC as well as production customer samples. I think that's pretty much my presentation.

Feel free to reach out to me,  young.tian@memverge.com for any questions or feedback. I love to hear both,  and then some links to our content. Please connect with us on the website,  YouTube, LinkedIn, Slide, and Discord. That's pretty much my presentation. Thank you very much for joining. Any questions?
