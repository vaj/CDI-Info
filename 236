
Hi everyone and welcome to this session  of the Compute Memory and Storage Summit. We'll be presenting on VMware Memory Vision  for real-world applications. My name is Arvind Jagannath and I represent  VC of Product Management at VMware by Broadcom. With me is Sudhir Balasubramanian,  who is a Senior Staff Solution Architect  at VMware by Broadcom too. Today we'll cover some of the solutions and what we have been  doing in terms of memory related innovations at VMware.

I want to point out to some key trends with respect to our customers. The top graph actually shows that VM densities are increasing. If you can see the red and the green bars here,  you will see that they are showing a constant increase,  which means that more and more customers  are deploying more VMs on their host. This relates to the bottom trend where we are seeing  that customers are deploying more and more memory on their hosts. For example, between 2022 and '23,  we see an increase or a larger increase in  the higher memory buckets of memory, host memory. We will continue seeing this trend and that's why  memory tiering is so important. Because we are bringing  capacity and cost reductions with memory tiering. The other key point here that I want to highlight that we have  seen with our customers is there is always spare CPU cores. That is driving another reason why  memory sizes are increasing is customers are trying to balance their memory.

If you look at this on the left-hand side,  you see that customers typically size their memory in such a way that  they can over-provision some of the memory. But not all over-provision DRAM can be used for workloads because  it's mainly used as insurance to support peak demands. Now, what that results in is that CPU ends up as being spare. A lot of customers, like I mentioned,  have told me that they do end up with a lot of CPU cores,  then memory is always the bottleneck. The other point is that customers also tend to over-commit their memory. Essentially, with memory tiering,  which is on the right-hand side,  what we aim to do is provide this insured capacity from the second tier. Basically, what that means is that customers can fully utilize and optimize on  their DRAM resources which is their most expensive resource,  and also the most performing resource,  and then utilize a slightly lower performing tier,  which is a secondary tier,  and the solutions for which we'll be discussing next. The insured memory also comes from here. Also, the fact that a lot of our customers' active memory usage is around 50 percent. Which is active memory is different from consumed memory. Because active memory  denotes the range of memory accesses across the board,  across the range of the entire range of host memory. That actually is typically 50 percent or even less. What that says is that as long as we do intelligent tiering and  move hot pages back to DRAM,  which is the most expensive tier and performing tier,  we actually end up getting better performance and we actually do well.

Here, let's look at memory tiering and how it's layered. At the bottom of it is the variety of hardware we are planning on supporting,  which is DDR, CXL-based accelerator,  CXL expansion devices, NVMe, and pooled NVMe. The advantage of ESX or vSphere is that all this is completely  abstracted away and customer gets a uniform single memory address space. The workloads actually can now allocate all their memory  from the available full range of memory address space. Which not only includes DRAM,  but also includes some of  these other devices that can be supported as a secondary tier. Now, what are the customer benefits? As you can see, CAPEX is reduced because now customers don't have to  deploy expensive DRAM and they can use some of  the lower cost solutions or lower cost hardware. They also improve on their CPU core utilization. I talked about the CPU memory imbalance. Now, customers can look to balance their resources better. It also reduces their OpEx as we'll see in one of the solutions,  memory can be deployed easily on demand. With the solution such as an accelerator solution,  we can enable not just on memory-related solutions,  memory tiering, and lower TCO,  but we can go above and beyond some of  the traditional memory-based use cases and allow for things like  vMotion acceleration or better page tracking and memory resiliency. And also other future use cases such as things  like offloading of some of the processing for encryption,  dedupe compression, or doing performing analytics, etc.

Now, let's look at the cost comparison. On the extreme left is the two terabyte DRAM only cost comparison. Basically, what it shows is that very high cost bomb for the server. And a lot of the bomb is coming from the DRAM. Lot of the expense is coming from the DRAM allocation. Now, if we can reduce the DRAM and substitute it with  lower cost memory such as in this case, it's NVMe. NVMe can result in almost a 19 percent reduction,  and then if we go further and use  a CXL-based accelerator which provides up to 1 is to 1 ratio,  we can aim to get about 35-36 percent cost reduction on the server bomb. These are costs with a real server OEM,  and customers stand to gain a lot of TCO benefits.

Now, let's look at the first solution which is NVMe-based tiering. One of the advantages is with NVMe,  you can look at free available slots on your server chassis,  and you can plug in an E3.S,  an EDSFF form factor into such a slot and suddenly get expanded memory.

Now, with NVMe tiering,  VMware will be doing software-based tiering. The software that we talk about is fully integrated into the ESX kernel. What that means is customers don't have to  bother worrying about which tiers are being used and how they should be allocated. They will still get a single memory address range,  and software tiering will be done behind  the scenes by the kernel and the hypervisor.

Let's look at the CXL accelerator-based solution. This is a special accelerator that  VMware is building as a hardware software co-design. It's a Type 2 accelerator and will be available from  Intel Granite Rapids CPU launch onwards. The previous NVMe-based solution is available for  the previous CPU generations of Intel and AMD as well. Now, with this accelerator,  the key thing is it's able to support a wide variety of workloads,  because there is intelligence built into the device. Instead of just being on the ESX host itself,  it can do a better job of doing tiering,  and it can also do better performance. It can support a wider variety of workloads like VDI and also databases,  somewhat performance-sensitive workloads too. The other key thing about this accelerator is it doesn't  require any special configuration or deployment steps. It's fully integrated into the vSphere deployment. As vSphere is installed and upgrade,  this device components within this device are also installed and upgraded. Also, four terabytes of additional memory capacity are available. If you imagine a customer deploying it in a one-to-one ratio,  so four terabytes coming from DRAM on the host and four terabytes from  the device can give up to eight terabytes of  total host memory capacity for the server.

Like I mentioned,  VMware has been working on CXL. We are going to support memory expanders with CXL. We'll also be supporting memory tiering solutions. Then we'll move on to support CXL-based accelerators,  which will not only provide a better enhanced memory tiering solution,  but also provide for a pooling solution and disaggregation. If you look at the diagram on the right-most side,  you can see the blue shaded ones are the accelerator. You can imagine a configuration where hosts can access  multiple devices or multiple hosts can act single device. Such configurations can provide to more efficient use of memory across  the cluster and better efficient management that is provided by VMware. There is also other use cases that can be enabled by accelerator. Like I was mentioning,  we can start doing offloads because there is intelligence we can do in the device. We can also support things like analytics. We can finally evolve it into a fully integrated hardware virtualization device  doing all kinds of storage, networking, and memory. Next, I'll pass this on to Sudhir.

Thank you, Arvind. Arvind spoke about the NVMe tiering and the CXL use cases,  the advantage of both the approaches, and so on and so forth. But even before that,  VMware targeted the memory tiering architecture and  the framework by using tier-2 memory. Let's take a brief look at that. Well, in this setup as shown in the illustration here,  we have two ESXi servers. Server 1 with X amount of DRAM and server 2,  the same amount of DRAM,  X, and a little bit of tier-2 memory. Which means let's just call it Y. The intention here was to prove that yes,  we are able to run more workloads using the memory tiering architecture. We essentially use a DRAM and tier-2 memory,  and then provides a single cohesive address space. The performance is very close or it's  acceptable as compared to DRAM-based workloads. What's the use case here? The use case here is increase in memory density for non-latency sensitive,  non-critical workloads, your non-production workloads. Your dev, your test, your train,  and so on and so forth. Back to the illustration here,  we were able to run two such virtual machines on  the memory tiering server and one virtual machine on the DRAM server,  all virtual machines configuration. Which means the number of VCPU,  the amount of memory is exactly the same. What we did was we ran several runs of the test,  and the test here is the workload generator, SLOB. SLOB stands for Single-Cell Oracle Benchmark. It's an Oracle benchmarking tool. The test was a read-only test,  so that's why you see update percentage set to zero. The runtime are 20 minutes,  and as I already said before,  we had a couple of runs. In all of those runs, we looked at two metrics,  the execute SQL per second and the logical read blocks per second. Now, looking at the execute SQL per second,  we can see that the aggregate virtual machine metric,  which essentially is VM1 and VM2,  that was close to around 69,000 per second as compared  to the DRAM-only virtual machine,  which was 41,000 per second. The execute SQL per second,  that is the metric that any database uses  to indicate its throughput or to indicate its performance,  the workload performance, which means how many SQLs  are executed during that workload time period. That's what exec SQL, the metric actually means. Now, looking at another metric,  which is the logical reads blocks per second,  the aggregate virtual machine one and virtual machine two,  run was a lot more than the DRAM virtual machine run as well. Putting these two metrics together and looking at all the runs,  in a sense, what we can see is memory tiering  helps us achieve high virtual machine density  without having to burn a hole in a pocket to go to  the market to procure expensive DRAM.

We run a plethora of workloads with a couple of tiering technologies,  but let's just quickly go through that from a summary perspective. Looking at NVMe tiering,  we ran what's known as the LoginVSI,  which is the VDI benchmark. In this case, we had the DRAM to NVMe ratio as 1-1. When we compare the runs to a pure DRAM baseline,  what we saw was we saw 100 percent increase  in the virtual machine density. We were able to run close to 128 to 256 virtual machines  with 1-2 percent performance drop. Essentially, we just use five to six additional core usage. That was the first workload benchmark we ran. The second workload benchmark that we ran was VM Mark,  which essentially is a VMware performance benchmark. Typically, it's our partners and OEMs, they use that. What we did was we ran a couple of workloads. We ran it for mixed databases,  in-memory databases, for MySQL,  web services with varied virtual machine sizes  ranging from two all the way to 256 gigabyte,  from VCPUs ranging from two to eight. In this case, what we did was we set  the DRAM to the NVMe ratio as a four is to one. Then when we compared the NVMe tiering,  that metrics to the DRAM baseline metrics,  what we saw was we saw a 33 percent increase in the tiles. We are planning to run more and more workloads, so stay tuned. Keep one thing in mind,  and I think I've mentioned this before,  that with NVMe tiering,  the use case here is virtual machine density,  which means we are targeting that towards  non-critical, non-production workloads. TCO is the name of the game here,  TCO is the use case here. Now, flipping to onto the other side,  looking at the CXL accelerator,  what we did was we ran on HammerDB,  the benchmark with Oracle TPCH profile,  which is an end-to-end solution with the NVMe usage. Essentially, we ran that against  a 48 gigabyte virtual machine. In this case, we set the DRAM to a CXL accelerator,  the ratio was a one to one. With this CXL accelerator-based tiering,  what we saw that we were able to get as close to  91 percent of the DRAM virtual machine performance. Which is acceptable to most workloads. Again, more workloads are being  planned and performance optimization in pipeline. Remember, this use case,  the CXL accelerator use case,  it's targeted towards business-critical,  low-latency production workloads,  so performance is the use case here.

From a summary perspective, essentially,  VMware is bringing multiple memory solutions,  be that your CXL expansion,  be that NVMe tiering,  CXL-based accelerator solution,  and that provides large TCO benefits  without impacting performance. Definitely, memory tiering brings  scale without adding any operational complexity. Tiering is ready for future technologies,  that basically brings terminologies like  pooling and disaggregation, those framework,  into this equation here.

From a call-to-action perspective,  please contact Arvind Jagannath to  collaborate and partner on any of the solutions. Again, if you have any inquiries on POCs, VMware,  we are ready for the POC on the solution,  so please reach out to Arvind on that. If you guys have any questions on workloads,  performance from an Oracle perspective,  please reach out to me at sudhirbhalasurman@broadcom.com. But with that, thank you very  much for taking the time and listening to us. Please take a moment to rate this session  and the feedback is very important to us. Thank you.
