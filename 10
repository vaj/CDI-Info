
Hi, I'm Mats Ã–berg.I'm an Associate Vice President at Marvell.Today, I'm going to talk about near-memory compute, or as I like to call it, thinking memory.

First, a note from our lawyers.

There's a big push for making data centers more effective.The concept of compute where the data is reduces data movement.It can also offload the main CPU from some operations and make the whole system run more efficiently.So today, I'll talk about how adding compute capability to CXL memory can improve the overall efficiency in terms of power, performance, and cost.

Let's first take a look at a few of the current memory challenges in the data center.Today, memory is directly connected to processing elements and is tethered to them.It's not easily shareable, which makes it challenging for balancing memory needs.As this graph from Meta shows, as CPU core counts are increased, memory channel bandwidth per core is decreasing.This is due to processing elements being constrained from adding memory channels.This degrades the efficiency.Another problem with DIMMs directly connected is that they do not easily lend themselves for offloading data computations on host memory.And this limits the performance.And CXL is poised to address these issues.

Performance does not always scale with capacity.

If we look at the diagram here, today, if we increase the memory capacity by going from one DIMM per channel to two DIMMs per channel, the overall performance does not scale since both DIMMs on a channel share the same bandwidth.We kind of cap the bandwidth.

In the future, instead of adding more DIMMs, you can add CXL memory modules to the system.And this provides a scalable performance using PCIe lanes to open up memory bandwidth.

Sharing resources with CXL.So if we look at today's platforms on provision with memory to support their most intensive workloads, and as workloads vary, stranded memory can not be rebalanced, which results in inefficient memory utilization.

CXL memory pooling allows memory to be shared across CPUs or platforms.This enables the pooled memory to be distributed to meet varying workloads, resulting lower memory requirements as well as saving costs.And from a recent paper published by Microsoft and others at the recent Aspos conference, they claim in Azure up to 25% of memory stranded.Memory desegregation can achieve 7% reduction in overall DRAM cost, which represents hundreds of millions of dollars in cost savings for large cloud provider.And this is, of course, a significant amount of savings that can be had.

If we look further, one benefit of CXL is CXL switching.CXL switch extends the sharing capability for other resources such as I/O devices, accelerators, memory expanders, additional memory, and to increase the utilization and efficiency and scalability and provide service ability to minimize downtime.

Relatively low latency makes memory expansion as well as pooling via CXL practical.Although longer than direct attached, it's more like one new hop additional latency, it allows for larger scalable capacity.The expression endless memory has been used.As discussed before, it can also be used to increase bandwidth in a scalable fashion.One important aspect is that it's possible to utilize different memory types.So for example, the direct attached memory may be DDR5, which is attached to the CPU.But CXL memory may have, for example, DDR4 or other types of memory to find the right mix, the sweet spot of cost versus performance and may be able to reduce costs further that way.

Although this additional latency is small, we note that any additional latency may cause the slowdown of the CPU.We have, in effect, a tiered memory structure with different amounts of latency.Basically, the direct attached memory has short latency and the CXL attached memory has a longer latency.That is, as mentioned, about one new hop when you use the CXL attached memory instead of direct attached.In a paper from Meta, they describe a solution to identify and place hot and cold pages to appropriate memory tiers in a way that is transparent to the user.And they call this transparent page placement.And you can find more information in the paper in the link I show below.

TPP: Transparent Page Placement for CXL-Enabled Tierd-Memory

A few observations from the paper is that workloads have meaningful portions of warm and cold memory.And warm and cold is something we can offload to slower memory tier.Also, the page access patterns tends to remain relatively stable for minutes up to hours.That means you can observe the behavior and make changes to memory tiering in the kernel space.You can say, well, this is cold memory.We can move it.And it usually stays that way for some time.A large fraction of anonymous pages tends to be hotter, while file-based pages tends to be colder.So this can help in deciding what to do.And also, different workloads have different sensitivity levels to anonymous as well and file pages.But the main operation they're suggesting is to demote cold pages or warm pages to the CXL.This can be done with a least recently used algorithm to see that these are cold, and then promote hot pages to the local direct attached memory.

In the paper, they outline how to do the hot and cold page detection in the Linux kernel.But we know that with potentially a very large amount of memory in CXL, the kernel may have to keep track of a large amount of memory and potentially using up a non-trivial portion of the CPU processing power.So the utilization for the CPU to do the compute, it's reduced.And also, if it's pooling, we have other users and other processor.The CPU does not have information about usage from other CPUs.It's kind of no visibility to that.And here's where the CXL device can provide, for example, top memory accesses on the CXL device and also do precise event sampling for physical addresses, like track ranges of addresses.And we know there is a proposal in the works and the OCP for this.It's in the Composable Memory Systems workstream.So you can take a look at that if you're interested.

So hotness tracking provides a simple compute offload.Although it's a simple use case, we can view this as near-memory compute or computational memory.It helps the operating system reduce stalls due to page misses and thereby improve overall CPU utilization.It also requires less CPU power for hotness tracking.It is completely transparent to the user.So there's no need to update any applications that are already running.Also note another relatively transparent usage is CXL memory compression.It can be done inside CXL memory device and compress pages to a memory swap space.In a memory pooling scenario, compression of pages can free up more memory to share.So there's less need to reclaim memory when resources are running thin.To accelerate the deployment of computational memory, I do believe that it needs to be standardized.

And if we standardize, we can take a deeper look at a more general purpose computational memory and how we can leverage standardization there.

So what can we do with computational memory?We mentioned hotness tracking.And also compression, mentioned on the previous slide.We can also offload, for example, encryption and hashing and filtering operations, transforms, RegExp.And in particular, if we do the operations on data that is already in CXL memory, we can really reduce the data movement.And we also know that with hardware acceleration, the operations can be done faster in the CXL device than in the CPU itself.So in addition to free up the CPU to do other operations, we can also do some of the operations faster.So in the next few slides, we'll show some simple examples of computational memory.

And note these examples are more for illustrative purposes.So assume that the user has stored sales, for example, for last year in a database on a storage device.And now the user wants to calculate a total sales in May of last year.

So it basically instructs the CPU to do this.And the CPU reads the data.And then it calculates the total sales.And then it instructs the CPU to do this.And the CPU reads the data from the SSD into the CXL memory.

And then the CPU instructs the CXL device, well, please decrypt this data in the CXL memory.

And then decompress the data so that we can actually make some sense of the data.

And then the next is to, OK, filter the data related to May 2022 and place these records.And now we place records in the direct attached DRAM.That's not necessary to do.But we do it in this example.It works as fine if we leave it in the CXL memory.

And now the CPU can calculate the total sales with only this limited amount of data that is needed for this computation.

And then return the results to the user.So this is a very simple example.

And now I'm going to look at an example where we do a similar thing, but where we share the data.So now CPU 0 is still going to calculate the total sales in May 2022.But CPU 1 is going to calculate the average age of the buyers in May 2022.So now CPU 0 reads the data from SSD and place in CXL memory.In this example, I skip the steps of decrypt and decompress, assuming data is stored uncompressed in the storage device.But the CXL device now filters the records with the date May 2022 and now place the records in the shared CXL memory.And then CPU 0 can calculate total sales and return results to the user, whereas CPU 1 can calculate the average age of the buyers and return results to the user.That's a rather straightforward example on how things can be shared.And of course, there are many more examples.And of course, typically more users may be involved.

OK, so isn't this flow kind of similar to the flow of computational storage?Here I show an example that I shared at the Flash Memory Summit in 2022 for NVMe-based computational storage or doing the duplication inside an SSD drive.I'm not going to go into too much detail on the flow itself here.But basically, we are moving data from the Flash Memory on the NVMe namespace and places it in a subsystem local memory.Basically, we read data from SSD Flash into a local memory.OK, this happens to be an SSD controller.And then we have a compute namespace that does the compute.But otherwise, the flow is very similar on how we move the data.So I suggest that we consider there has been a significant amount of work in standardizing computational storage within NVMe, as shown here, and also within SNIA.And I suggest that we develop a standard for near-memory compute based on the SNIA computational storage architecture and API.

So let us take a look at that.So the SNIA computational storage is basically move data from storage to memory, compute on data in memory.The SNIA computational storage have three types of computational storage devices.One is basically a computational storage processor.Another is a computational storage array.And the third is computational storage drive.What I showed on the previous page was basically computational storage drive.But with near-memory compute in CXL, well, data is already in memory.But it is more like the SNIA version of computational storage processor, with the differences that it is attached to the host via CXL.And it has a significant amount of DRAM attached to it.And I think it's rather straightforward to adapt this architecture and API to work for computational memory for near-memory compute.

So compute in CXL memory device is a tool to reduce data movement, and of course, also offload the host processor.We can process shared data in memory, which provides value for all users of this shared memory data without having to move data from the device to be processed and then move back to this shared memory.Afterwards, it reduces data movement.And my call to action here is to adapt the SNIA computational storage to near-memory compute applications.I want to thank you for your attention.And please take a moment to rate this session.Thank you.
