
Hello, everyone. I'm Sanketh Srinivas. I'm the product manager for Microchip CXL memory controller solutions. And today we have Tam with me, who is the product manager for our switching solution.

So today we wanted to just keep it at a high level and talk about the compute memory bottleneck and how CXL is enabling to address the bottleneck and some of the use cases that CXL enables.

To start off with, with the shrink and process node technology, the CPU vendors are able to cram in more cores onto a single SoC. However, the SoC package size is limited and we're limited by I/Os rather than the die size. So essentially cramming in more memory to enable the compute resources is a challenge. So how do we solve this? So one way to address this would be with CXL.

So also along with the memory bottleneck created by a ton of CPUs on the cores, we also have heterogeneous computing, which is known to work more performant with the AI and ML workloads. So what is heteroarchitecture? So essentially you have the CPUs, GPUs, and accelerators now competing for the same memory resources that was once only used by a CPU.

So in order to fix this or help with addressing the bottleneck, CXL comes into the picture. So CXL essentially allows memory disaggregation. So it's essentially simple. What it's doing is taking the DDR memory channel and moving it behind the I/O bus, which works on PCI electricals. So we are taking the DDR memory channels there, which is in orange, and moving it behind the CXL. So it takes a DDR DIMM, which is a pin count of around 200 plus, lowers it down to around 20 pins, and now you can scale the amount of memory that you can connect to a single SoC.

So that's one of the use cases. There are multiple ways, like when spoke about the fabrics. So there are multiple system architectures that can be implemented with CXL. As the CXL spec progresses, we're seeing new system architectures that can be implemented. The first case is essentially the CXL memory expander case, which essentially adds capacity and bandwidth and also enables cheaper expansion of memory. So why leave it at that? So you can make the memory controller smarter. So you can enable the memory controller to connect with multiple hosts. So the memory controller can also have DRAMs connected behind it. So based on the host requirement, the memory can be dynamically allocated. So this enables with TCO, essentially there's a Microsoft paper out there talking about how much memory resources are stranded. So this enables to reduce the memory standing problem and improves the TCO by dynamically allocating based on what each individual host requires at any point of time. This next use case is, again, advancing the pooling a bit, sharing. So sharing will enable us to reduce host-to-host communication. So within the memory pool now, you can reserve spaces as shared memory spaces. So multiple hosts can share that memory space and access to it and own that memory space. So a host here can be a CPU or a GPU or accelerator. So if an accelerator traditionally had to access memory that's owned by CPU, it has to talk to the CPU. And it's really inefficient because you're just talking to fetch data from the CPU. So this enables to reduce that overhead and enable efficiency with system architectures. So extending sharing is global fabric, which is attached memory or a fabric memory. So with a single memory controller, you'll be limited to how many hosts you can connect to. So what fabrics enables is to increase the number of hosts. So you're going to connect a host to a switch fabric, as shown in this picture. And to the fabric itself, you can attach memory, which will enable you to efficiently use memory across the fabric.

So this essentially changes the traditional memory model or the pyramid, as we all know. So traditionally, we had the on-package memories, like HBMs, L1, L2, L3s, DRAM, where hot data was stored. Then we had the storage class memory and a mix of DRAM and stored class memory. So now what this pyramid translates into is you have the on-package memory. Then you have the direct attach memory. Then you have the CXL direct attach memory. Then the next tier would be switch attached. And the last tier would be global fabrics attached memory.

So along with these system architectures, what CXL enables us to do is also provides a unified interface, irrespective of what memory is behind the controller. So one of the challenges with, for example, NVDIMM is enabling the support for it in ecosystem. What CXL does is it gives a unified interface for the ecosystem vendors to integrate in their development flows. And behind the CXL controller, you can have DDR4 DIMMs, DDR5 DIMMs, or even have Flash for, for example, the Optane replacement. However, so one thing to note here, each of these, even though it feels the same, the performance is going to be different. As long as the software or the application layer recognizes the performance and understands which tier it needs to place the data in, you can make this work much more efficiently and with a lot lower cost. So this is an example of how a fabric would look like. So in the picture you can see there are multiple hosts. And each host will have its own type three CXL memory expansion device. And also the host will be connected to a CXL fabric. The CXL fabric will have its own GFDs. So the GFD is essentially global fabric device. So where each GFD can have different memory types. So since now it's a pool based on where you want to place the data and how much capacity you want to enable to each host, you can very efficiently design a fabric and take it into and then reduce your TCO.

So from a performance perspective, right, so what do the latencies of these different tiers look like? For a traditional DRAM, the latency is around 100 nanoseconds, right? So the DDR controller on the CPU itself is estimated to have a latency of around 70 nanoseconds. And the DRAM component is around 30 nanoseconds. So for a direct attached CXL, let's assume the memory controller adds a latency of 40 nanoseconds. The DRAM will add another 30 nanoseconds. The CXL stack itself on the CPU can assume adds around 100 nanoseconds. So the total direct CXL latency will be around 170 nanoseconds. So now for a pooling and sharing solution, because there's a lot more that the memory controller has to do, the latency will increase a bit. It's going to be around 70 nanoseconds. And that part will have a total latency of around 200 nanoseconds. Further, for a fabric attached memory, again, based on what memory type you use, it's around 320 nanoseconds.

So at Microchip, in order to enable this CXL implementation, we're currently sampling our first-gen CXL 2.0 devices. So our controllers are one of the-- or the lowest latency memory controller out in the market with very high performance. We have enterprise-class RAS features, essentially enabling advanced ECC, data integrity, and also enabling functions like diagnostics, scrub, and BIST to ensure you manage the DDR memory appropriately. We have leading edge security with secure boot, firmware attestation to enable secure implementation of it in the data centers. We also interoperate with the ecosystems. So essentially, we talk to all the CPU vendors out there and also memory vendors to ensure a product is really stable and reliable and can be taken into deployment. We provide media independence. So one of our key things is we support both DDR4 and DDR5 today, enabling either a performance solution or a TCO-based solution.

So with that being said, I'll hand it over to Tam to talk about switching a bit. 

Thank you, Sanketh. So as Sanketh has mentioned, we belong to part of what we call the data center solution team. And one of Sanket's products is mainly focused on CXL memory controller. So that's not the only product that we have. So we have another product called the PCIe switching right now. And based on the current generation, which is Gen 5, we've been shipping Gen 3, Gen 4 for many, many years. And Gen 5 is now in production. And going forward to our next generation, which is PCIe Gen 6, and it is essentially going to be not just PCIe, but also going to be able to handle CXL switching. And at that time, I think we're going to be going back to CXL 3.1, even though right now our spec is 3.0. And it's going to be a whole new ballgame because taking forward from our current Gen 5 PCIe switch, we're going to have integrated many CXL functions that basically allow customers to have the flexibility to either select PCIe or CXLs based on your requirement. So it is going to be programmable and allow you to connect to multiple hosts and memory devices depending on your use cases. And the key things for a CXL switching is they provide low latency. And we try to either beat the specs of CXL for latency for .cache and .mem functionality. We also allow you to interface to many devices such as CPU, GPU, and memory modules based on whatever is in the market at the moment. So currently, it allows you to basically share memory data, memory pooling, and memory sharing based on the CXL specs. And whatever the CXL specs that's required, we plan to implement all the functionality into our PCIe and CXL switch. By that time, I believe it's going to be 3.1. But currently, we're still going to mention it's a 3.0 switch because 3.1 is not officially out yet. So I cannot mention it's going to be a 3.1 compliance. And so it will support both upstream and downstream, similar to our current PCIe switch does. One thing is that it will support many types, like type 2 and type 3 devices based on the CXL requirement. And you're allowed to cascade different switches. And it's basically you can connect to either a CXL endpoint or a PCIe endpoint. So it is quite flexible for the end user to configure the switch, depending on whether your data are actually PCIe or CXL. Now one thing I want to mention that I think the previous presenters say that it's going to be a highly complex system for PCIe Gen 6 because the encoding is PAM4 and not NRZ. So all the previous generation from Gen 5, Gen 4, Gen 3 has different encoding. And now PAM4 adds one more complexity to the service itself. But since we have already done many service generations, we feel confident that we're going to have enabled this next generation service coming up there. And our CXL and PCIe Gen 6 is going to be taken out next year, around July, June time frame. And hopefully we'll have the sampling at the end of '24, early '25, based on how fast the market is moving. But the ecosystem from SoC, CPU, it's going to be around that time frame. So if you look at all the CPU vendors out there, they're going to have their Gen 6, PCIe, and CXL devices available at around late '24 and mid '25. So we believe that we're going to meet all the schedules requirement for ecosystem that's out there.

So if you look at interconnection for CXL compared to PCIe, so PCIe is a top-down topology. It's more like a tree. Even though our current Gen 5 PCIe switch, we allow customers to build a complex fabric through what we call the advanced fabric switches. And that gives you more flexibility to connect host to host, SR-IOV and multi-host interfaces. That means you can connect one domain of host to the other domain endpoint or the other domain of host. But given that CXL is coming out, our next generation is going to be moved from a proprietary fabrics switch into a CXL fabric switch. Because it's going to be open standard and not based on a microchip proprietary advanced fabric interface. So currently Gen 4 and Gen 5, we have the advanced fabric switch, which is proprietary. But moving forward to CXL and PCIe Gen 6, we're going to adopt the CXL fabrics interfacing so you can connect to either multi-host, peer-to-peer, memory pooling, memory sharing. So based on CXL standard, that's going to be available by that time, either 3.0 or 3.1, depending on when the final spec is going to be released for 3.1. So this is going to give you a very flexible interconnect through our switch.

And here's some of the typical comparison between CXL 2.0 switch and 3.0. So 2.0, you can see that it's a single layer switching. You connect from host to endpoint, very simple. You cannot do a whole lot through a switch. But once CXL 3.0 is out, then you can actually cascade multiple switches and do high-complex fabrics interconnections based on how you want to interconnect your host to multi-host applications. And looking at the fabrics of CXL 3.0, we can see that the key advantages allow customers to configure how complex you want to. So the interconnection is up to the end user's imagination. So it's a flexible switch that we're going to be providing. And with that, I think this is the last slide I want to say is that we have carried forward all the technology from a previous generation, from Gen 3, Gen 4, Gen 5, and now moving forward to Gen 6 and CXL, which is a new territory for many chip vendors at the moment, where CXL is new. But we think that CXL is going to be the next generation of interface protocol besides PCIe. So PCIe will not go away. I think that PCIe and CXL will coexist side by side in the next generation moving forward. With that, I will end the call. Thank you for participation. Any questions?

How do you enable CXL memory? Is it just a plug and play or do you make any modifications and settings in firmware?

OK, so first of all, both PCIe and CXL, they share the same physical layer. And in our switch, we have a process to allow you to program. Either it can recognize the packet, whether your protocol is PCIe or CXL. And depending on if you CXL, you want to maybe put a sharing, then you can configure the switch to do that.

OK, thank you.

Hi, this is Nilesh. I just had a question in your topologies. I noticed you didn't show re-timers. So are you seeing like in the typical deployments at hyperscale or enterprise, there's going to be a need for re-timers?

Yes, good question. So re-timers depend on how long your trace line is. If your system is short, then you may or may not need re-timers. But if you have a large system, yeah, you do need re-timers, especially when you go past Gen 5 and running at 32 to 64 gig. And if your circuit trace or cable is long, you definitely need a re-timer for that.

Just a follow-up question. So in the deployments that you're seeing or you're anticipating or hearing from customers, is that like 100% of the case or like just 50% or is it just like 1%?

Yeah, re-timer is a necessary eval, but it's an expense that nobody wants to put onto it. But again, currently I see that in a large system, they do require a re-timer for even to Gen 5. And if your system is small enough that you are meeting the channel reach of your product, then you may not need re-timer. But still, you may have to use high product, what you call high-quality PCB traces in order to satisfy the signal integrity. And overall, I see that currently in Gen 5, especially in the AI/ML system, they do need re-requirement because those systems are huge. They run long traces and big backplane and everything else, so they do need a re-timer.

Okay, thanks.

In a GFAM system environment, do you expect GFAM is implemented in a PCIe cable-based or Ethernet-based?

More PCIe-based.

You're talking about hundreds and thousands of computing nodes is connected, right? Do you think it's feasible to connect through PCIe cable?

Right now, we believe that PCIe cable handles GFAM. Yes.

Another question is your latency target for host size 100 nanoseconds, memory control 40 nanoseconds, DRAM 30 nanoseconds. Is it your current target? I think the current system, the state-of-the-art system does not satisfy that latency target.

You're talking about the latency of the host to endpoint?

Yeah, so from host to the end DRAM size, including everything. So my measurement is exceeding all those numbers.

Yeah, I mean, we cannot define the law of physics. There's going to be, once you go through a silicon chip, there's always some delay latency right there. But we try to design the way that minimizes the amount of latency through the switches or memory controller. So that's the key factor that everybody needs to deal with. We can't find a law of physics.

What's the current number of your memory controller? CXL controller.

So that's a secret sauce, right? We'll have to talk under NDA on that. But that's where we see the industry going. And we're very close to those numbers that we showed. And we believe we can achieve this target.

All right.
