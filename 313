
I'm going to go ahead and get started a few minutes early so that Charles can start right on time at 2 o'clock because we're going to be doing some speed dating. Every 20 minutes, you're going to have a new date. So, my name is Frank Berry, and I'm the Vice President of Marketing at MemVerge. We manage this industry initiative called the Memory Fabric Forum, which is a series of events, a website, and a YouTube channel. It's all about memory fabric technology. Participating is free, and the aim is to accelerate collaboration on the marketing side, project side, and technology development side. This is our 12th Memory Fabric Forum event, and it's the third time we've hosted this event at the OCP Summit.

And we're going to be doing some speed dating. And at the end of this year, we're going to start a new branch of this initiative, which we call AI InfraForum. Presenting at these forums is free, so if you're a thought leader, an individual thought leader, or if you're a vendor out here that would like to do a presentation at one of these events, my email is up there. Just contact me, and we'll get you involved.

So with that, I'd like to introduce our agenda. We've got 12 presentations during the next 5 hours—or next 4 hours—ending at 5 o'clock. Our first speaker is Charles Fan, the CEO and co-founder of MemVerge.

Good afternoon. I guess I'll be the appetizer—ha-ha—for the event. It's our pleasure to host. This is really for the ecosystem partners to come together, share information with each other, and see how we can accelerate the advancement of technology in the area of memory fabric, especially as AI is becoming increasingly important in driving the transformation of the data center architecture.

So, what's the transformation? Here is a simplistic view of it: We were in the x86 era for the last 40 years, and there are three pillars to a data center—compute, networking, and storage. The compute is centered around x86 processors and DDR DRAM, and networking is typically connected by a TCP/IP network, with storage attached to that particular network fabric. These three pillars really formed the data center that we've gotten used to over the last 40 years.

And in the last 10 years, we are seeing the beginning of a transition into AI infrastructure. These x86 TCP/IP network storage systems continue to exist, but a new, complete computing core has emerged, centered around GPUs and other AI processors. High-bandwidth memory is located very close to these processors, providing high-bandwidth, low-latency access. We believe a new AI fabric has emerged, and that’s the central topic of this session. Numerous protocols, standards, and proprietary technologies have developed to interconnect these AI processors—both between computing units and between computing units and memory units. These include NVLink, InfiniBand, CXL, UA Link, Ultra Ethernet, etc., and we will dive into each of them with our partners. This is likely to be the main thread of this memory fabric forum. We also believe a new data layer will attach to this fabric. This will be memory-centric, serving as a new data platform where data can be stored, shared, and transported between computing units. Combining these three elements—the new processor, new fabric, and new storage—will define the new AI computer.

And so now, let me just give you a quick overview of each of these new fabrics, much of which you're probably already familiar with. You know, NVLink is really the leader in the space, and this is proprietary NVLink NVIDIA technology that provides very high bandwidth interconnects between different processors. It started as an intranode technology, allowing up to 8 GPUs to connect with each other in a full mesh, providing very high bandwidth—900 gigabytes per second—between each GPU. With the new Blackwell technology, this has evolved into a rack-scale technology. As you see here with NVL72, you can connect a superpod with 36 ARM processors and 72 Blackwell GPUs, providing an aggregate bandwidth of 130 terabytes per second. That's an incredible amount of bandwidth, interconnecting what is essentially a supercomputer within a rack. I think this is on the leading edge today in terms of AI fabric, interconnecting various processors.

And then, there is a kind of back-end network that goes beyond the rack scale, which can connect to thousands of nodes to allow large-scale distributed training and other activities to happen. InfiniBand is a leading technology today, serving as the back-end fabric that connects a larger scale of computing nodes. This is a very fast-growing market, and NVIDIA, with its acquisition of Mellanox, is a leading supplier of InfiniBand technology. It is predicted that this market will double, reaching almost $80 billion in the next five years. Additionally, the bandwidth will continue to increase as well.

Now, to compete with InfiniBand, we are also seeing high growth in Ethernet-based technologies, in particular, UltraEthernet. There has been the formation of an UltraEthernet consortium, and the bandwidth is increasing rapidly—from 800 gigabits per second now, with 1.6 terabits per second UltraEthernet connections. In recent months, we have also seen the formation of a new standard called UALink. This is a collaboration with a large number of industry leaders, aiming to create a more open system alternative to NVLink, allowing the non-NVIDIA ecosystem to have a high-bandwidth, multipathing way for processors to communicate with each other. So, there are some very exciting emerging developments, and I think we will have a session to delve into each of these.

And last but not least is CXL, and we have been working on CXL for a while. This is a new memory fabric that's built on top of PCIe Gen 5 and Gen 6, allowing memory to be disaggregated, composable, and shareable between different processing units. Slowly but surely, there has been steady advancement in CXL technology, along with the productization of such technologies. We are starting to see the initial memory expansion devices based on the CXL protocol, and, as you’ll hear from the presenters, we are also beginning to see a new CXL switch become available. This switch allows multiple servers to be interconnected on the same CXL fabric, accessing the same memory pool, which opens up exciting possibilities for data sharing not only in the AI fabric space but potentially in a fabric-attached memory space, providing a new data platform. This list includes various new fabric hardware that is growing rapidly, and over the next five years, it will be fascinating to see how this market evolves and which technologies will become standards. As I will introduce towards the end of this presentation, I believe there could be more than one winner in this space, with a coexistence of multiple fabrics serving different purposes.

Okay, so those are the kind of overview of the hardware technologies. Let me go into the software a little bit. And MemVerge, the company I work for, is a software company developing software on top of AI fabric and on top of memory. I'm going to give you a couple of examples of the software we're working on that runs on top of this fabric, providing interesting capabilities to AI applications as well as other applications.

The first is CXL-based memory sharing. And this is a technology we have been developing for more than two years. It came in the form of a project we call GISMO. It's a global, IO-free shared memory. It has come in the form of a memory appliance, where we run the software on top of this appliance. It allows multiple servers, as you see on the right-hand side—which could have multiple worker processes—accessing a shared memory through an object interface. These are memory-mapped objects, so they can be accessed. The data paths can be accessed with memory semantics to allow very high bandwidth and low latency. These physical nodes can be connected through that CXL fabric to the same memory pool. This essentially allows the same region of memory to be read from and written to by multiple processes on different nodes. Conceptually, what we can do with shared memory on a single node today can now be extended through this memory fabric to multiple nodes, allowing some very exciting new application possibilities.

And one of which is that we have integrated various layers on top of GISMO, including an Alluxio caching layer, as well as Spark, which we can run on top of it. And then we ran a number of benchmarks, including TPC-DS, which is an analytical database benchmark. We can demonstrate higher performance from those benchmarks compared to the classical networking approach. In this way, this is really a replacement of I/O. Instead of using networking to exchange or reduce data between different computing nodes, we can now use shared memory to accomplish the same by having multiple nodes access the same memory region. So that's the first example of an interesting application on top of a memory fabric connecting to a common memory pool.

We are also working on composable memory expansion. This means that, on top of the same memory fabric, you could have a pool of memory available that dynamically extends the memory capacity of different computing nodes. Today, it's primarily applied to CPU memory, but it can also be extended to GPU memory as well. We have developed different policies to manage such memory expansion by applying various tiering strategies. These could be aimed at optimizing for access latency, or they could focus on optimizing for bandwidth, either to minimize latency or to maximize bandwidth, ensuring the overall best system performance from this cluster.

And here is an example of running the memory expansion software on top of a CXL memory module, where we run Weaviate, one of the vector databases popular in RAG implementations in AI systems. We can demonstrate that by adding 10% or 20% more memory, we can increase performance by 7% or 8%. This shows that purely by extending the bandwidth the memory system can provide, we can proportionally increase the performance of vector database applications. These are just a couple of interesting software examples; there are many others that could be possible as well, and we are actively working on them.

OK, now let me complete my presentation by providing a few predictions—three predictions, really—looking at the next five years. A few observations, really, about what I think is going to happen based on the interactions we will be having with customers as well as with our partners. Some of them may be obvious to everyone. Some of them may be controversial.

So, the first point is that, as we talk to more and more customers, we have observed that the GPU is still a very valuable, expensive, and scarce resource that people are trying to obtain. Its availability tends to be unpredictable on the public cloud, and it’s quite expensive as well. As a result, many larger enterprises over the last 10 years have initiated cloud strategies, trying to move as much workload to the cloud as possible. Now, we are seeing a bit of a shift — a hybridization of that strategy, particularly in relation to GPUs. We are noticing that enterprises are revamping their data centers so that they are equipped with the power and cooling needed for GPUs, and they are starting to build on-prem GPU clusters to balance out their presence in the public cloud space. Even in the public cloud space, aside from the hyperscalers that we are all familiar with, there is a new breed of GPU cloud providers that focus specifically on GPUs. So, we foresee that, over the next five years, it’s going to be a hybrid world where enterprises build their own GPU clusters as strategic assets. GPU cloud service providers will continue to grow, and hyperscalers will remain significant players. It’s going to be a mixture of these GPU resources. The management of these hybrid resources will be an interesting problem that the ecosystem will need to address, particularly in how workloads can intelligently select the right computing resources — whether it’s for training, fine-tuning, or inferences — to find the optimal resources for completion.

All right, so that's number one. So, number two, as I mentioned, we believe AI Fabric is going to be a very active space where there are going to be a lot of smart people inventing new technologies, pushing the envelope. And we believe that, certainly, NVLink will continue to be an important player in the space. NVIDIA is going to continue to be a dominant force in the space. But there will be additional industry-standard AI fabrics that will emerge and adopt it. And it's going to be a more complex world in terms of this interconnection between the processors. So, it'll be interesting to see which protocol will win out, and which will fade away. And then, there are new software and application technologies built on top of the emerging standards.

And the third prediction is more closely aligned with what we are focusing on at MemVerge. We predict that there will be a new technology: Fabric-attached memory. In particular, for AI applications, this could be a very useful new layer in the infrastructure. It's going to form the lowest tier in the memory hierarchy, from the SRAM cache to high-bandwidth memory, to the main CPU memory, and then to this Fabric-attached memory. All of them can be accessed through memory semantics, and they can be dynamically resized or reattached to improve the overall performance of an AI application. This could become inter-node shared memory for things like a KV cache that can be shared, and it could play an interesting role in handling the fault tolerance of various computing units in the whole cluster. Additionally, this could replace some of the performance tier of storage today, where data loading will happen much faster, and where checkpointing, a critical task in AI, can be further accelerated. Storage would then serve more for capacity needs, while performance-driven I/O could be handled in a memory-centric manner. So, we do believe a new data layer, Fabric-attached memory, will emerge. OK, that's the end of my section. I hope I'm not over time. Can I ask if there are any questions? Yes, any questions?

Yeah, you touched on, in one of the slides, the last year of memory. The Fabric slide, where you have multiple Fabrics emerging. Now, these Fabrics, do you think any of them are trying to merge GPU and CPU memory? Because a server normally pays heavily for both of them. And what is churning in the industry in terms of any undercurrents to merge these memories or make these two big entities share memory?

Yes. So today, CPU memory and GPU memory are difficult to share in general. Now, there are some interesting developments. NVIDIA has developed the Grace Hopper and Grace Blackwell architecture, where there is a chip-to-chip NVLink between the CPU and GPUs. And then, they create a cache-coherent access between the GPU memory and CPU memory. So, you could have 96 GB of GPU memory with 480 GB of Grace CPU memory. And they can access each other in a cache-coherent way. But that does require the new Grace Hopper architecture.For general x86-attached GPUs, like a DGX system, it is difficult to share the memory in a transparent way. So, this is typically done at a higher level, say, if you're using VLLM or other PyTorch frameworks, where you need to more explicitly perform what's called memory offloading. For instance, if the memory doesn't fit into the GPU anymore, there are typically two approaches. One is adding more GPUs, allowing for scale-out, which has its own challenges. The other is memory offloading, where you move less frequently used data, like the KV cache, to the CPU memory to relieve the memory capacity bottleneck.So, there are various techniques being used, but none are perfect. However, I do believe there is an opportunity for innovation between hardware and software to create a more scalable, continuous memory tier that can enable better processing of AI workloads.

One more follow-up question. You touched on UALink. I'd like to request your opinion or comment on how UALink is transpiring. Initially, the first version is apparently to network only GPUs. Do you feel that there is a need to allow other disparate accelerators to come and network over UALink? Thank you.

Yeah. So, do we have a session on UALink? Yeah, so for sure, I think there will be a session focusing on UALink, which can probably provide more authoritative answers to this question. I believe, my belief is, it is within the scope of the objective for UALink. Yes?

Do you think co-packaged optics will have any effect on the future of memory fabrics?

Yes, I definitely believe so. I think there are some exciting startups working on those technologies. And that's going to provide really lower latency and higher bandwidth. And I believe that's going to really enable this vision of where you can't have a pool of memory that's at the fingertip of all the processors there. So, it's definitely going to be an enabling technology.

And what time frame are you expecting that?

I hope ASAP, but from what I know, I think it's more like three years, two or three years. OK, thank you very much.

Thanks, Charles.
