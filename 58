
So yeah, thanks for coming today. Like I know, probably, except for those maybe in Eastern time or slightly east of that, this is not the most awesome time. And I myself, I have three-year-old twins who just got up really angry. So they may in fact run in if mom can't wrangle them. And so since it's recorded, I'd like to thank my wife and the mother of my kids for taking care of them while I am doing this, because she also has a full-time job. Okay. 

So the talk, when I submitted, we were really excited about the development we were doing for our Linux driver using QEMU as the base platform. And as time has gone on, I think we've really tempered our excitement a little bit because it's not been easy to get any real traction with that.

And so the very last slide is going to go right here. And ultimately, what I'm looking for in terms of feedback is what the heck should we do with all the QEMU work we've done? And I'll give a background on all this shortly, but try and keep that in mind through the presentation.

So just a quick intro here. I mean, I already said what the point of the talk was about myself. I'm one of these people that Bianca mentioned. I haven't done Linux kernel development since 2016. I haven't done kernel development since 2016, and I came back and it's been really great. The last plumbers I went to was 2017, so it's been a while for me. The talk's going to go over what Compute Express Link is and what it's for. And we're going to talk about the driver work that we've been doing, which originally landed in the 5.12 kernel. And then we're going to talk about how it's implemented in QEMU, what we've done so far, and then wrap it up with what should we do next.

I thought about this in the shower this morning. I did leave out a couple of links I wanted to put in. So I don't know. I guess I can go afterwards and update this slide and add those links. But in general, if you want to reach us, the top two bullets there are good for you. If you want to look at our CXL tree and so on. I'll point out one thing, and I have a good joke later. This bottom link is a non-work-funded project that I've been doing or trying to find time to do, which is user space REST bindings to our CXL interfaces. And for me, this is what I've been using primarily to do testing on the driver development I've been doing. I guess in the intro I should have said I'm working on Linux CXL drivers for Intel. The one above that, this ndctl branch, is the de facto and plan-to-record tool for using our CXL driver to drive CXL hardware.

Oh, and if people have questions, feel free to stop me at any time. Although I don't promise to get through all the slides in that case.

So let's talk about the background here briefly. I don't want to read through these all, but I will scan them and talk about a few. In short, these three here are the 'we need more memory, it needs to be faster, and our data centers need to be scalable, we need to be able to hot add and remove new memory nodes, and it has to all just work.' I don't think these challenges...this wasn't my slide, this is a CXL consortium slide. The challenge is these top three are not new by any stretch. And if we go to the right side here, it talks about what CXL does to address that. And probably the most interesting one there is this top bit. So Compute Express Link is built on top of PCI Express, which makes it nice in a few ways. The first is it works in a backward compatible fashion. So if you have software that's entirely unaware of Compute Express Link, your hardware will still function at least as well as it did when it was just PCI. There's also, from a hardware designer standpoint, what's called here three mix and match protocols, which I'll go into in a little bit, that allow you to either increase or reduce the complexity based on what you need for your design without having to implement all the nitty-gritty parts of the spec. And so the rest is all just, you know, boilerplate, 'Hey, we did a great thing.' The asymmetric complexity is a very fancy way of talking, I think, is talking about a bias control. And I don't think I've got the notes in here, but I put a link, and I'll do another version of the slides where I make sure I have the link that talks about what it is. But essentially--well, actually, I'm going to hold off on that. Oops, my keyboard doesn't work. 

And so here's, like, my fake animation. So to me, what makes this the most interesting, though, is that one right there. I think what CXL--and not that it was novel in its attempt, but I think what CXL--what makes it really special and a chance to be special is that it is striving to be an open industry standard, and there are a lot of people who are interested in using it.

This is another nice slide from the consortium of all the people who are interested, and it's interesting here now that I'm looking at it. Two of these companies might go away, right, and get merged into another two companies. But it's an impressive list of people. If I had more time, I would try and figure out the market cap, the total market cap of this. It's probably pretty insane. So there's a lot of momentum is the point of this. And I think if you're going to put your money on a cash-coherent interconnect, this is a good one to put your money on.

There's three usage models in Compute Express Link, at least currently. The first two are not really part of this talk. However, I'll briefly point a few things out. So in the gray box here are what was earlier referred to as the mix and match protocols. The CXL.io is sort of a fancy, like we can do PCI Express. It's a sort of restricted -- well, from a hardware implementation point of view, it's different. But for us software folks, it's PCI Express stuff that we all know and love. And so I kind of laugh at that last slide where it says mix and match because you really can't do anything without CXL.io. That's sort of like the requirement at the very least. There's two device types, like I said, that we're not really going to talk too much about, but I'll introduce briefly. There's a caching, what they call caching device accelerator. And as you can see on the bottom, generally this is referring to NICs. And so the CXL cache protocol is what that asymmetric complexity thing was referring to earlier. So defined by the spec, there's a way to talk about basically which owns the memory, I'll say, and which has to do snoops. And the spec, by the way, if you want to go in and look, and that's going to be one of the links I need to add, it talks about this in better detail. But you can talk -- or you can specify what sort of owns the memory and which agent has to do snoops as a result. So, for instance, if you have a network device that needs to do a lot of packet processing, it makes sense for that device to primarily own the cache because it's doing most of it and the CPU only needs to come in occasionally and do whatever it does. I don't know. I'm not a driver developer for network cards, but it must be something. The accelerator with memory, what's known as the type 2 device, is sort of an interesting one that I think has a lot of undefined and yet to be realized potential. So as you can see, it's the one that uses all three protocols, and so you could, in theory, easily implement the type 3 functionality with a type 2 device. I don't really see a benefit in doing that, but if people have questions on that or ideas with how to use it, it would be good. The graphics track is going on at the same time right now, but probably the devices you're going to see are going to be graphics devices, and they might have some ideas.

The one I want to talk about mostly today is a type 3 device, and so a type 3 device is a memory expander, and so unsurprisingly, it uses the CXL.mem protocol, which is sort of a fancy catch-all for my device can claim memory traffic at speeds that are acceptable to you. So we'll discuss this in more detail, but just from an architectural point of view, you have a processor with its own memory, it goes through CXL bus, and now you have a bunch of other memory devices that can give your system more faster, better memory.

Just quickly, this is mostly for reference. When you're talking about what these things are, in case you're like me and you have a hard time conceptualizing, these are the industry-defined form factors that you might see these devices in, and so probably if you're like me and you don't have a background in data center and enterprise, it's kind of interesting to see the ability to kind of hot-plug memory like this. I mean, I know it's been around for a long time, but there's all these form factors that you can go and expand your devices with.

From a topology perspective, the CXL 2.0 looks a lot like PCI, unsurprisingly, and we're going to kind of ignore the bullets on the right for now. In this example, and we're going to come back to this later, there's two host bridges, and RP is root port, and so you have several root ports. Some of them are older 1.1--well, they're not root ports in 1.1, but it effectively is a root port. And you have an empty slot that can be hot-plugged, and here you have something that's not CXL-capable, and then right here is your actual CXL topology. And so it looks like PCI. You have a switch in this case and some endpoint devices, and additionally you have a non-CXL endpoint device. So just the takeaway at this point in the talk is like, cool, you can really mix and match these devices and hopefully achieve whatever functionality you were hoping to achieve.

The complexity and excitement—and I'm going to talk more about this later. This is a big slide, and this is from the driver writer's guide, which I'll add to the links, by the way. Unfortunately for us, we were writing a lot of the driver before the driver writer's guide was available, so it is what it is. But the CXL protocol—or CXL spec—defines a way of creating interleaved sets of devices, and this is not new. NVDIMM did this. I mean, they've been doing this for a long time, but what's different—and I'll talk more about it—is it's, again, spec-defined and programmable. So, unlike if you want to have your dual-channel DDR and it has to go in this slot and that slot in order for it to work, in theory, you can sort of—a systems administrator or person or whatever can configure their system however they like, and the driver is able to make it all work. I'll mention that I left out earlier when I was talking about the memory expander aspect. This applies primarily to—well, this applies to both persistent memory as well as volatile memory. The driver that we're currently focused on is specifically tied to persistent memory, and I'll get into that slightly more, too.

So this is a quote that I've been using for quite a while, except I added a new bit about atomics. So my background isn't in storage and memory, so I don't know how exciting this may or may not be to folks, but when I come in from my view, this is how I saw it. It's like, wow, you have this NVMe, which is sort of cool because it's PCIe and it can do this hot plug, but it's a little bit difficult to work with if you want it to just pretend to be other memory. And then you have NVDIMM come along and it solves a lot of those challenges, but it's constrained a lot to how the platform wants it to work, and so it might not have been a great fit for folks in the data center.

So as I was saying, primarily our focus is on persistent memory. The persistent memory aspect, the word there, system software, wasn't my word, but it relies on system software for provisioning and management, and what that just means is that OS has to deal with it. So there's -- and I'll probably repeat this, but BIOS -- so in NVDIMM timeframe, BIOS did a lot of this setup and it passed along, like, here's what I did as an ACPI table called NFIT, and so the Linux driver would read the NFIT and say, oh, okay, this is what you meant to do, and these are the things that I need to do to handle it. The model changes with CXL, and BIOS is not expected to do a whole lot, and the driver needs to go and take over a lot of that, and that might sound exciting for you people who are using it, but if you're programming it, it's kind of a pain in the butt. So there's a standard register interface to do that. One of the cool things about this, and probably folks with, like, USB background or HCI background are used to this, there's a generic class code that's defined in the spec that allows us to bind our driver to, and so if you have one of these CXL devices and it's made in the standard way, our driver will just pick it up and provide all the capabilities that are spec defined. We'll get out of the vendor-defined stuff for now. And so that's a really nice model to work with, and it should really ease hardware vendors that are making these devices and want things to just work on the day they release them. Now, if you deviate from that, you obviously are on your own. And from the architectural point of view, there's this capabilities mechanism that's built in that you can kind of enumerate and figure out what your device can actually do, and so you have to implement at least--if you're a hardware vendor, you have to at least implement the thing that exposes what your capabilities are. Mailbox interface to talk to the devices, and vendor specificity is built into the spec, which is not my favorite, but we can talk about that later.

So in terms of our Linux driver, this is another picture from the driver writer's guide that I stole. And it's funny because when I took it--and thanks, Douglas, for your awesome diagram. When I took it, I started--I never really looked at the boxes and what colors they were, and I don't know what you guys think, but as I'm reading this, it seems like the OS's responsibility here is really only these two blue boxes. And so I unfortunately didn't have the ability to edit this, so I just had to draw a line around it. But in fact, the OS has to control all of these entities in one way or another. And again, this is like--feel free to look at it offline. I won't spend too much time on this slide. But it overwhelmingly has more responsibility than any other entities in the system, the driver, that is.

In terms of the enumerable components, there are many. The primary--one of the interesting things, I think, is there's this spec-defined ACPI device. And the reason I think this is interesting is because when you have an open spec, it was sort of surprising to me that it would be tied so closely to ACPI. I guess enough systems are ACPI-compliant or enabled at this point that maybe that's not an issue. So there's this thing floating off in the ether that is an ACPI table that describes some of the properties of the system. I'll discuss that a little bit more, too. But generically, you have your system that I call host, and there's this new thing called an ACPI-16 device, which is representative of a host bridge. And so this is on top of what was there previously, which is like a PMP0A08, I think. And so that provides some mechanisms to both enumerate as well as you can have a DSM on those devices for everything underneath. One of the things that we found when we were enabling this before the spec was released is this ACPI-17 device wasn't a thing. And the goal of the spec writers was, hey, you can just enumerate these ACPI-16 devices, and if you find any of those, you know you have a CXL-capable system. And that's cool, except if you want to write something that is backward compatible, a module that's backward compatible, there's already drivers in Linux that are looking for those host bridges as a PMP0A08. And so we don't have an opportunity to come in and be like, oh, actually, it's a CXL host bridge, and we need to start enumerating things. So the ACPI-17 device is actually a construct, again, that we found doing the Linux enabling that helps us create a module that can potentially be backward compatible on future kernels because we don't have to intercept any of the core ACPI enumeration that's currently happening. And similarly, when I was talking about the class code earlier, so obviously you have your normal PCI enumeration on that, and we don't need to try and intercept anything. It's just a new class code and our driver combined to that. And we're going to talk a little bit about how that all works.

I need to speed up a little bit. Here's the Linux drivers as they currently exist and sort of what I was describing earlier. The two drivers that are enumerable via conventional mechanisms are cxl_pci and cxl_acpi. This was the ACPI-0017, and this is the -- it's been a while. I don't remember the class code. After these things come up, we're able to bring up a few of our other drivers. And I'm not going to get too into the driver model. The way that Linux does its driver and device model was pretty new to me. The drivers I had worked on before this was all well established, so it's kind of fancy. Dan Williams is the mastermind behind it all, although I think he was claiming to copy USB or one of the others. But in general, we have these sets of drivers, and they have devices that are backing them, which is not new. But the way in which we bind to them is kind of cool. Anyway, so we have these drivers, and I'm going to go over all these. And these are found via normal PCI mechanisms, so we're able through MMIO to figure out what capabilities are there. The port driver is responsible for utilizing that and creating Dakota resources, which, again, I'll talk a little bit about. And just as a nature of the CXL topology, we're able to bring up our endpoint devices once we determine it's more than just a PCIe device. And finally, there's this thing called CXL regions, which are an abstract description of the interleave sets that I was talking about earlier, which is multiple devices comprising one large region of mappable memory. And that, if you're not creating it, is determined on a device in something called the label storage area. So when a system comes up, it finds this label storage area, and it's like, "Oh, you had a x8 interleave, and these are the devices in the interleave." And each one of the devices participating in the interleave set has to have that same region information, and then our driver can go in and be happy with that.

So the cxl_core, which was that big red thing, the main responsibilities of that are to maintain the infrastructure for everything else, integrate with the NVDIMM. It provides the ioctl interface, which we're not going to talk about in too much detail.

CXL PCI device is the thing that binds to the actual class code, and those are defined away, so I don't remember where it was. The CXL PCI device is going to provide device manageability. So if you have a device that's not currently participating in an interleave set, or even if it is and you want to, say, do a firmware update, the CXL PCI device is the one that's ultimately responsible for that. And it's essentially the first stage in the bootstrapping process of getting our devices enumerated. Additionally, it'll likely be the driver that's responsible for security.

So in the diagram I showed earlier, these are the boxes that that driver's responsible for.

The ACPI driver, again, it finds that ACPI 17 device. It does the platform-specific enumeration, and so if you are building hardware that doesn't have ACPI, you would have an equivalent driver like this that kind of kicks off the orchestration of walking down the topology and making sure that -- or not making sure, but going through all the components that you find that are CXL-capable and making the driver aware of them. Next, I'll skip these bullets and show them in the slide.

So it finds this thing, and then that driver starts to walk down these guys and get all their properties and, again, get them registered.

We've been using the term 'port' for something that is able to route memory traffic. So a root port is a port, a switch is a port, and an endpoint device is a port. A root port is the downstream part of a host bridge. The port driver, which isn't -- this code isn't yet upstream. I'm still working on it. But its primary goal is to find all these decoder resources, which are the things that are spec-defined way of routing memory traffic, and making them available for use in a region or an interleave set.

And so here, again, these are the components that a port driver would take ownership of. What's sort of interesting that you might see at this point is you have multiple drivers that are actually controlling different parts of the same software entity. And so that was a little challenging in and of itself. So like endpoint device, the cxl_pci driver also wants to use that. And the ACPI-16 device is also referenced by the cxl_acpi driver. And this is where I'm going to gloss over how our driver model, it all works together quite well.

And a CXL memory device is the actual CXL.mem specific parts of the endpoint devices.

And so you can see here.

So it's not super important at this point in time to define the difference between cxl_mem and cxl_pci responsibilities. From enumeration standpoint, it's, again, because we wanted to have this backport ability and we didn't want to intercept any of the ACPI, the core ACPI or core PCI existing infrastructure. So we have sort of a weird dance that I'm going to leave this slide as sort of mostly reference to. But essentially what it comes down to is when cxl_acpi is done, it needs to tell all the cxl_pci entities to go try again. And when cxl_pci is done walking up, so cxl_pci walks from bottom up and cxl_acpi walks from top down. When cxl_pci is done, it basically needs to say, hey, maybe I found a switch and I need you to go rescan everything. So there's a little bit of a challenge there, but we have a pretty decent solution in place.

So I mentioned earlier volatile and persistent memory. So for the volatile case, in an ideal world, the BIOS just sets it up and there's some memory there, and it works and it's working optimally how you wanted it to. The BIOS, again, in an ideal world, doesn't do anything with persistent memory. That's fairly false, but we're just going to, for a summary, it's fine. And OS managed, so again, persistent entirely. It can't get away from the volatile case because the CXL spec does talk about hot plug. And so if you hot plug a device, the OS is expected, a volatile memory device or interleave set, the OS is expected to handle that and set it up. But even aside from that, well, I won't go into the nuances, but error handling is potentially also up to the OS. To use either of them, you need the CXL.mem capability. And I said ha-ha on the persistent BIOS configuration because aside from my bullet there where the BIOS potentially, and I don't know if any of them will actually do it, but it might want to boot off a persistent memory device. And so if you want to boot off that device, you have to first configure the devices. And if they're in the interleave set, you have to program that. So aside from that, though, there is some like error checking and so on, things that the BIOS does in the beginning of time, but it avoids programming the regions.

And so I've been saying regions a lot, and what does that mean? And this is the same picture as earlier, so don't squint. You can go back to that. But a region is simply a set of devices that are accessed in some interleave pattern. There's parameters that are associated with the region. IG is the interleave granularity, the stride, or I guess stride divided by the number of devices. You can specify host physical addresses, and I've glossed over why that's interesting. It doesn't matter too much at this point. There's the number of devices and so on. And so these are all stored on a device if it's a persistent memory device, and when the device comes up, the OS will read it, the driver will read it, and program it and make it available. There's a creation ABI, so if you want to create a region and not do it in an offline fashion, we're working on a sysfs way of doing that. But additionally, you can just program in the bytes and store it in your device's LSA if that's what you want to do. And our driver's responsibilities, validating that the region was programmable in the current configuration, and I have another slide for that. And then programming the actual software-visible entities on each of these levels of the topology, that route traffic. And if you zoom in, or if you go to the other slide, I guess, you can see all those parameters for each of the HDM decoders. And this is sort of a -- you know, it's an 8-way interleaved across two host bridges. Each host bridge has two root ports, and each of those root ports has a switch with two downstream ports. So you end up with eight devices.

This is also from the driver writer's guide in terms of validating a region, and I'm going to leave it just to look at offline. But essentially, especially as the topologies get more complex, it's fairly complicated to actually figure out if you can program it. While the spec allows you to pick the interleaved order underneath, let's say, a switch, there are certain platform constraints that might not allow you to switch ordering, or your ordering might just be totally broken. And so the driver needs to come in and figure out if you can actually make that region work.

This is just for looking offline, but it was, you know, how -- an example of programming decoders.

So I'm going to skip through that. So ultimately, the interfaces you get, there's sysfs interfaces, again, that create regions and infer information about your devices. There's an ioctl interface, which is for the PCI management side of things. So there's a query command, which lets you find out what devices -- excuse me, what commands your device supports. And then there's a send command, where you actually are exchanging information with the device. Our driver manages which commands are allowed and which commands aren't allowed. There was some disagreement, I guess, over this early on, but it seems to be fairly stable at this point. There is a kernel config option you can do to get out of all that and just let user space send whatever commands it wants.

I think I see a question. The question is that the CXL.mem volatile device won't require any OS driver. If I said that, I misspoke. That was not the case. I think the spec writers wanted to believe that that might be the case, but error conditions and, again, hot plug will need to be handled by the driver. Although, actually, error conditions, there's kind of a toggle to what they call firmware first, where BIOS would handle it. In the future, in terms of interfaces, we're going to have this reach and creation ABI that I was talking about.

Okay, so I got ten minutes. That should be good. I think I caught up.

So let's review the goals of what we were trying to do. We were trying to get our Linux driver upstream, which the best way to do that -- we couldn't post code before the spec was released because it's under NDA, but to at least get a driver out in the environment -- in the mailing list, I should say -- on the day the spec was released and get it merged at the first available kernel, which was 5.12. We wanted something that could be backportable to older operating systems or older versions of Linux, I guess I should say. And we wanted to be able to provide to -- well, primarily Intel cared about Intel hardware devices and validation, but we wanted to be able to have a way that we can validate and build our driver as well as provide an opportunity for other folks to be able to test what they were building. It would have been nice if we had something that was reusable after the driver was already brought up, so for regression testing and possibly in virtualization environments. And we wanted something scalable, which I've been an open source developer my whole career and before that, and I think it's clear that an open source project was the right way to go and do this.

So those were the goals. And the existing state of the art, there were no actual hardware vehicles for us to use. There were no FPGAs for us to use. This is going back now a year already, or actually more, I guess, when we started it. Intel unsurprisingly has internal environments for this, but there were delays and priority changes that we wanted to make sure that we had -- we were able to -- excuse me. We wanted to make sure that we were able to deliver on our own schedule. And more important to me was there was no way to get any community involvement, obviously, if you're using an internal project. In terms of the prior art, there is something called NFIT test, and so as I was describing earlier, the way that NVDIMM works, one of the ways in which you can relatively easily test your driver is you create a fake NFIT. So even if the BIOS doesn't create one for you, you can say, "Oh, I have an NFIT that looks like this," and you can mock some stuff up and see how your driver responds, and so that's what Dan Williams and company did, and they got a lot of mileage out of it. And I'll give a shout out to Jonathan Cameron here. The thing that I think really kind of made me believe that I can actually do anything in QEMU was he had posted some CCIX patches for QEMU that I stumbled upon, and it got me thinking, like, "I think this is a tractable thing." So QEMU was the solution that we went with at the time to try and meet all our goals.

So real quick, just the architectural review here. We have a host bridge, right? We have two host bridges. They have root ports and switches and endpoints, and so you can kind of think of topology as three main components, host bridge, switch, endpoint.

Now, PCIe and QEMU is, for the most part, fairly basic. There's a Q35 host bridge, which I think coincides roughly with, like, the same month I started at Intel in terms of when it was released. So I'm not that old, but I'm somewhat old, so it's pretty old. There's a single root complex, and there are endpoints and root ports and switches. The QEMU community has extracted this stuff, and it actually works really well. It's not tied to a specific platform. There are generic implementations.

But all the traffic is funneled through this single host bridge thing, and so if you kind of recall what the topology looked like, that wasn't a good fit. And roughly in 2014, Intel and others started delivering platforms that actually had multiple bridges, and so you had a common bus that would route traffic to the different host bridges. But from a PCIe standpoint, everything looked like one tree. So this is what it looks like in hardware, but if you looked at it in PCIe, it would still look like what you would normally expect with a single root node.

And so our options at the time, or I guess my options, were we could try and make Q35 be CXL2.0 capable, or we can replace Q35 with a newer host bridge. So the latter, I think, was right out. There wasn't much good that would come out of it for us, and the former was fine. I really wanted to have a stable platform and trying to touch your main host bridge and modify it so it did CXL stuff seemed like a little bit too much risk.

And so QEMU, it turns out, has this thing called a PCI Expander Bridge, which is basically a generic way of jamming in a host bridge-like thing into the main host bridge. And so you have an expander bridge here, and it can add root ports and endpoints.

And so that's really cool because if you just rename PCI to CXL, now you have something that looks like a CXL host bridge. And so you can create arbitrarily many CXL host bridges that sort of live under your main Q35 host bridge. And so this allows you to sort of keep your base system stable. And I won't go back to the topology slide, but if you recall there, there was the concept of, hey, I have a mix and match of PCIe only and CXL only. So this actually isn't a bad fit for us.

And so when you actually implement it, I just wanted to show off, unsurprisingly, it's a memory device, it's a CXL device, and it's a PCIe device. So it inherits -- and I won't go into the QEMU implementation details, but it really ends up, at least the way that I've implemented it, ends up being like a mix and match between -- oh, that should say NVMe. Whoopsie doodle.

A mix and match between NVMe and NVDIMM. And so here's an example run using that ndctl utility I mentioned earlier. There's a library called libcxl which ends up calling into our CXL PCI driver, which ends up calling into CXL mailbox handling code. And you can see -- BW is indeed my initials. But you can see you're able to find your device and get some various information about it.

And ultimately, again, recalling the goals, the spec was released on November 10th, and we did, in fact, send patches out for Linux and QEMU on that day that people were able to use. And that was a pretty cool win.

The current state, there are QEMU patches. I've stopped sending them out, and we'll discuss that in a little bit. The phase one of our driver was merged into 5.12. We're still currently working on phase two, which is actually the region programming and configurations, probably the more interesting part. We're hoping to hit the 5.16 merge window for that. I don't know. It's going to be tight. But the coolest thing and the thing that, like, I want people to take away, even though maybe the takeaway is we killed this, but the coolest thing was we did, in fact, find spec issues. And so I wasn't aware of other projects that had sort of done this early development with their driver enabling and simulation before a spec was released. So unfortunately the spec was under NDA, and we couldn't do it in the public, but we were able to address issues with the spec before it was actually released. And so, like, per Jonathan's talk, and I just saw -- that's bad. I should stop reading the chat. But DOE also, like, he used that as an example of, like, the ship sailed and we have to support this thing. And I'm not saying CXL spec was perfect, but we were able to catch some issues.

So what's in the future for us? What do you guys think? This is my joke slide, my obligatory joke slide. I'm sure you're all laughing. No, I love Rust. I don't see a good fit for Rust in the driver in the near future, but it seems like you can get a lot of talks accepted at Plumbers if you talk about Rust.

Okay. So I did pretty good. I have two minutes left to talk about the situation. So the community at large didn't really seem to care about the CXL patches for QEMU, and there was feedback, I would say -- well, Jonathan Cameron gave a lot of feedback. But there was fairly minimal feedback from the community. There's a huge gap in terms of what I did. So, you know, again, my goal was primarily just to get a driver up along and running, and so I didn't really understand QEMU code base very well. The way that CXL operates is pretty novel. It's a single-memory device but can have both volatile and persistent capacities. And there's no real way in QEMU to model something like that today. The interleaving is also something that's not really done. So there's a lot of work, I think, that would need to be done to make a full CXL emulation in QEMU, but the base is there, and a lot of the library functions I did can be reused regardless. And then Dan Williams has been working on CXL tests, which is sort of the equivalent of MFIT test, and this makes the QEMU work a lot less interesting from a regression testing standpoint because the hope is that CXL tests will be able to do a lot of the same stuff. And I'll point out now, Dan tells me anecdotally when he first did MFIT test, a lot of the feedback -- or maybe not a lot, but the feedback from the community was why didn't you use QEMU? And every time around we use QEMU, and I think he has a pretty compelling argument to say there's why. We did get external contributions, mostly from Jonathan -- or no, sorry, I guess in QEMU it was from both -- a few people implementing these acronyms that it looks like people are discussing. Surprisingly to me, there was commercial adoption, so people are, as far as I know, using this in some of their simulation tools that they are being paid for.

And at 45 on the dock, you guys, we get to the final slide. So I would love feedback, and it doesn't have to come in this forum, but I would love feedback on how folks think we should proceed. I don't have a ton of time personally to invest in the serious reworks needed to add interleave support, but I think we can very easily incrementally add to the existing QEMU implementation and get a fairly robust and complete emulation for the other aspects. And I think there's probably people in the QEMU community who could get the interleave parts working relatively quickly.

And last but not least, we are hiring. My team is hiring, and so if you're interested in this talk and it seems like something you wanted to work on, there's a link. And if you want to reach out to me personally and ask any questions, I'm happy to answer them.

And so that was it. I had a backup slide if people wanted to look at later. This is kind of an old list, but it still applies of where we need to do work on this stuff. So if you want to apply for the job, here's some of the things -- well, probably not working on this stuff, but here's some of the things you might be working on. Okay. So, any questions?

Very well done, Ben. We have about 60 people here if there's a question already. Do you want to do a quick poll here to your question? We can make that happen.

Yeah, sure. Go ahead. Should we just drop all the QEMU work and let people -- let it bit rot, or do people think that there's value in trying to keep this alive and continuing to rebase against upstream? Do I have to make the poll?

I got it. I'm on it. All right.

Well, the answers are pretty clear already, but do you want to take Shadrach's question? I think they wanted to say something. I don't know where the question is, but I'll let the poll run in the meantime, maybe in the chat.

What's the question?

Yeah. So, hi, Ben. Question. Do we plan to do volatile memory changes?

Is it a QEMU question or a Linux question? Which one are you targeting? I don't -- well, maybe the polls will tell me otherwise. I'm not -- I mean, I guess to put it bluntly, I'm not paid to work on the QEMU stuff. I put it at Intel.com, but a lot of it was done in my free time. If you wanted to simply say, 'I don't want persistent. I want it to be volatile,' you can easily hack that into QEMU. The biggest problem is if you go back to the thought on, 'Hey, BIOS is doing all this,' or 'It's configuring it all,' with the QEMU emulation, BIOS isn't really involved very much at all. So you wouldn't -- I don't think you'd be -- I don't know what your goal is, but you wouldn't be testing what you wanted to test, I think, if you were just -- like, it's not PMEM, it's volatile. Now, maybe in some -- Go ahead..

Just to jump in there. There are definitely -- say you were working on EDK2 or one of the other open BIOSes, there are definitely ways you could use QEMU with the absolute intent that you were doing BIOS development.

Yeah, yes. We actually tried -- I tried to get our BIOS developers to get some interest in this, and it failed. But absolutely, this is a pretty -- that's a really good point, that if you have experience there, you could totally -- I mean, the QEMU emulation is just using OVMF, Open Virtual Machine Framework, I think, which is one of the packages in EDK2. So if you have that ability, or someone does, that would certainly be a good option.

Yeah, on ARM, we're just using standard EDK2 stuff. So, yeah, it's dead easy to bolt that in. Perhaps a little harder on x86.

Thank you, Jonathan. Thank you, Chandra. The poll is still open. We got 27 answers already, Ben, so that's a pretty good sample, but I'm just going to give a few of the other people a chance to do that. It's on the lower right-hand corner of your screen, just hovering over the slides. I will publish the results in a second. Does anybody else have a question for Ben at this time?

In case you're wondering, I did get a post-COVID haircut.

Looks good. Looks sharp. It's a Twins-compatible haircut, I see.

Well, I didn't get a haircut through almost the whole thing, and then my wife looked at me, and she's like, "You need to get a haircut before you present." 

I didn't do that. I could come on camera and show. I haven't had a chance to do that. All right. There were a few more comments in the chat, Cameron. And I will publish the results. We have 25 and 4. They're not anonymous here, so if you want, I'll share the people that said, "Let it go," Ben, so you know who that is. But there are very, very few of them. So, the results are the following. I guess you got work to do.

Yeah, I'm a little disappointed.

Yeah, it was very, very overwhelming, "Keep it alive" versus "Let it go." So, I think you had a positive audience. This is a bit polarized, right? You have the people here that would care.

Yeah, so my feedback to the 25 of you is talk to all your QEMU upstream people and get them interested, because I tried and failed. And if anybody has a person they know who has some sway in that community, I am not it. So, that would be very helpful.

I don't think we have many QEMU folks here today, unfortunately. So, it might be worth us trying to organize a discussion with them at some point.

Yeah, I tried early on before I had much in the way of implementation. And so, maybe it wasn't so interesting because I didn't actually have anything. It probably would be worth trying again. But again, I don't know, other than going on the mailing list and in their IRC channel, I don't know many QEMU devs.

Thank you so very much, Ben. Really appreciate your presentation and taking the time. Please thank your entire family on our behalf as well. We really appreciate them making it possible for you to present. I'm sure everybody is joining me in thanking you for your great work. And thank you to Dan and Jonathan for contributing on the chat as well.

Thanks, guys.
