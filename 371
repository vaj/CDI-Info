
So, welcome everybody, and I just want to say thank you for attending the last session of the last day. You know, I appreciate all of you, you know, attending. So, let me get started with a quick introduction. So, my name is Eriko Nurvitadhi. I'm a co-founder and chief product officer for MangoBoost. I also run MangoBoost North America, and today I'll be co-presenting with, you know, my collaborator, Craig Carlson here, who is a networking and storage architect from AMD. And today, we'll be talking about accelerating GPU server access to network-attached, this aggregated storage using data processing units.

Okay, so here's the agenda, right? So, first, I'll talk about the trends in AI and implications on storage systems, and then I'll hand it off to Craig. He'll talk about, you know, the AMD GPU ecosystem for AI, and then I'll go over the opportunity for data processing units—you modify DPUs—and then how we could help AI. And finally, we present a concrete case study. This is on LM training on, you know, MI300 AMD GPU servers equipped with their MangoBoost DPUs for you know, storage acceleration with NVMe TCP. All right, so.

Let's start with trends in AI. So I think this is not controversial. I hear a lot of people talking about AI, and then you know, very notable example is the ChatGPT. One could say that it could be the first real killer app for AI, right? And as shown there in the figure, it not only called technology but it also got adopted, right? 100 million users in two months. So, it's not record-breaking at the time, so that's kind of how people get their real use, and people paid real money for AI services. And similarly, there are many reports out there. One of the reports I just happened to highlight there is on the market size for AI. Right now, it's a quite big market, even in 2024 already 15 billion, and then it's projected to be 250 billion into in in 10 years or so. And uh, more importantly, right, it's not like necessarily just a one app kind of thing. So after ChatGPT, there are so many other offerings. I'm not including everything here, but you see, as an example, you know, video generation, right? AI capability. I've seen you know, uh, people providing service for you know, music generation and image generation, multi-model stuff. There's so many offerings uh proliferating in the marketplace, and people are adopting and using it. So uh, you know, uh, the it looks like it's going to continue to stay hot in the in the foreseeable future. It's a very exciting time.

And then there's many branches of AI, right? But uh, for now, I think we're focusing on uh, just what arguably the most popular one is, which is a large language model. Um, and here's just one uh, picture to show kind of the trends, and there's uh, you know, multiple sources of tracking the the what's happening with LLMs, and this is a snapshot from June 2024, right? And then you can see a different uh, circle that represents the size, uh, you know, of the model, but uh, LLMs these days, the biggest ones uh, go over trillions of models, right, 1.5, 2 trillion, and so on. And more importantly, there's multiple of them available, right, and in the trillions, and not only that, there's also a family of these uh, kind of models, not only the super big ones, trillions, and they continue to grow, but also the extra large, large, which is hundreds of billions to medium, and uh, small, which is tens of billions to smaller models, which is you know, a few billions, right? So there's a variety of models available. It continues to proliferate, it moves fast, and they get bigger, but also the small, medium, and large is still continue to be used and then continue to be offered. So, as systems, uh, people here, right, we ask ourselves, you know, how to build efficient systems for LLMs, you know, especially taking account that this model sizes continue to grow.

So let's first look at the hardware prints, right? One of the observations I want to highlight here—and I think others have also mentioned things like this in other talks—is that you know, AI systems are increasingly more challenged by data-oriented tasks, right? There's a compute and sort of roughly, there's also data movement in a store and all that. And one observation right, there's multiple figures like this outside. I pick one example; uh, I guess the legend is a bit small, but basically, it's the x-axis is showing trends over the last 20 years or so, and then the the y-axis is showing improvement over time, and then the gray line that's showing the compute, the amount of compute on chip, right, from flops to more recently is the tensor ops, right, the whatever variety of precision you have, but there's more and more compute, and more recently, obviously, AI compute put on chip, so it's a 60,000x improvement in the last 20 years, right? It moved very fast, and you can see the chips you have these days obviously GPU is a very popular, and it has more and more tensor computation in it, but you can also see CPU now have tensor instruction, right? AMX, for example, it has a you know, AI engine going to you know, CPU client laptop, right? Even FPGA has a tensor DSP has the engine as well. Your smartphone has a a compute for AI, so computers becoming more available in hardware and continue to uh to to grow fast. However, if you look at the trends in you know, off-chip memory interconnect bandwidth, those are the other uh lines, uh, blue and green, right, the the the improvement is much lower. What other way to look at this is in terms of the capacity, the GPU memory capacity available in the device, right? The GPU has HBMs with high bandwidth, but the capacity itself, even though it's continuing to grow, but it's uh also not keeping up pace with the growth in the LLM size, right? So similar kind of the graph, right, the x-axis is showing the growth is trans over time, and then there's two lines again, uh, don't worry about specific data points, but there is the green line that is showing the growth in the amount of capacity in GPU device memory, and then there is a red line that's showing the size of the the LM models, right? As you can see, the the red line surpasses and it moves uh growth much quicker. So what does that mean, right? So it means that today, right, LLMs a lot of them don't really fit in a one GPU card anymore, if you just rely on the GPU uh memory, uh, then it won't fit right because just a much bigger model uh going on, so and then we attempt to estimate here the number of GPUs needed to fit an LLM model, right? And so we take for every year, uh x is uh assists again, uh years, and the y-axis is the number of GPUs if we take a you know, state of the GPU and state of the art LLM model for that year, uh, we make some projection as to how many GPUs as you can tell, number of GPUs needed just to fit the model in terms of the memory capacity. I think it takes a, it takes more and more GPUs clearly, uh, you know we would need to think about you know, larger storage options right.

and uh, so, that brings me to the next point, right? So, uh, in the industry, and uh, and then uh, you know, our technologies, right? Uh, people already been uh, pushing in the recent years towards optimizing more AI storage systems, right? There is a software framework that's now being more and more optimized for AI, for for storage systems as well as there's benchmarks tailored so that we can evaluate the AI storage subsystem. So, I'm going to highlight for example, the AI framework. A lot of the innovations that we've seen uh, has been in terms of being able to spill the data out of GPU memory to use a system memory as well as a disk, right? So, that going to other places where you get bigger capacity, obviously at the trade-off of bandwidth and latency, but smartly managing that. So, I think there's already other talk that talks about the different options. I'm just going to highlight one here, which is Microsoft DeepSpeed, and this is uh, you know, very popular software, uh, open source, uh, you know, run by Microsoft, right? Um, and again, this is the picture from the uh, from their paper. Essentially, what they do is uh, they're able to offload model states, right? And this could be you know, not only the weights or the parameters, but gradients as well as the optimizer states, spill out of the GPU and then to you know CPU memory, they call it slow memory, with CPU memory as well as uh, SSDs, and then by doing this, right? Even if you don't have enough GPU to fit the model uh, into the the memory capacity of GPU, you can still run, run it, and then you know, depending on your configuration, on how you tune it, you could also get quite good performance, at least try to maximize the GPU that you do have, right? And then even though the model go in and out of the GPU memory, and the next one I want to highlight this, there's also uh, benchmarks now that is targeted towards um, you know, evaluating uh, AI uh, storage, um, one I particularly want to highlight is Mlperf storage. Right? You might be familiar with Mlperf consortium, and the traditionally, what's the most popular is Mlperf inference and training has been going for a while, right? Everybody kind of submit their latest numbers there, but more recently, there's also Mlperf storage, uh, you know, some of you might already know this or might already already participate, but those of you who haven't, I encourage this community to check out Mlperf storage. So, we've been MangoBoost, we've been participating uh, um, you know, since earlier this year, uh, there's already uh, one public published results last year, and then the version 1.0 is actually the deadline just passed, you know, we submitted, and there's other submitters as well, and it's being reviewed, so the result is going to come out next week, and here the setup is shown there on the figure. Essentially, the metric is really the throughput coming out of memory storage subsystem to feed the accelerators, and this is emulating modeling the case for training, so where the data set comes from disk and then it's being fed to a number of GPUs, right, or accelerators. So, what's interesting here is that the accelerators, is um, you know, you don't require they don't require to you to have the AI hardware in there, so you because you know, not everybody have many GPUs, so the the GPU computation uh, or AI computation is modeled using just sleeping, but there's a methodical approach to estimate that, and Mlperf storage workload is capturing that kind of modeling and then, but the storage subsystem using PyTorch going to access uh, the disk that's real, that's real, and that's measured throughput, and then the other metric is to show how many accelerators can be supported because obviously, the more throughput you have from your storage, the more accelerators you can support, right? So, uh, that's kind of an upper storage, and you know, we actually have very good results, so if you're interested, check out the next week it should be a press release and all publication, and you know, check out other results as well, and again, I encourage you guys to check out the first storage, it is very relevant to this community, and there are also other benchmarks I'm not showing here, there's also spike storage, TPCXAI, and so on, so forth that also again evaluate the storage subsystem right so, again, this is very interesting for storage Community because you know, in the in the previous case, Mlperf inference for example, is not touching storage, Mlperf training even though there is storage in there, but it's not isolated to know what is the metric for performance for storage, right? It's only end-to-end kind of thing, so it's very exciting.

So next, in terms of systems and hardware, right, there's also an increase, increasingly, more need for disaggregated storage in and this AI systems, right? Obviously, disaggregation and disaggregated storage is not necessarily a new concept, but it's even more important I think, the now that we're looking at the trends in AI. So, this is obviously a simplified, very simplified cartoon, but you have a, you know, storage server on one side, right? You can have a lot of SSDs, and then you can pull the SSD and make it accessible through a high-speed network. On the other side, you can have many GPU servers, right? That accesses to storage, and obviously for AI, some of the benefit would be first is a high throughput, right, and capacity, right? Obviously, you have multiple SSDs in the box, so you have full capacity, but you also have throughput because multiple SSD bandwidth can be aggregated and then accessed through a super high-speed network, right? For example, a 100-gig network link, you can put you know, three or four SSDs together, and you can saturate the 100 gig, you know, assuming your network can keep up, and you can pull also multiple network ports together. Um, and in this case, it's important for providing the data to the GPU server because you know, we don't want the GPU to be idle, right? There we want to max out the the compute utilization of the GPU, and also for large models, uh, large, you know, states, right, that we need the capacity as well. And in terms of, you know, disaggregation, obviously the known benefit is eliminating over provisioning, right? We can flexibly configure and assign resources of, you know, storage to the right GPU server, and it's important because as I mentioned, there's a variety of our workloads, even the model there's a big model has multiple variations, super big model for different reasons, some are smaller, a bit smaller, some bit bigger, so you want to assign and the right resources for these different instances. And on the GPU server side, right, this GPU server trend is toward more and more dense servers, so you want to pack a lot of GPUs, you have to deal with thermal and all that stuff, you know, so you ideally want to, you know, utilize the most of your, you know, PCIe slot, this is a link that connect to your GPU right, because you want to make sure it's fully utilized, and you might want to make sure it can, uh, you know, get the data through right, uh, if you have one PCIe car, you know, slot and you connecting it to GPU, if you have an SSD, one SSD may not be enough to saturate the bandwidth, right? But if you have a NIC instead, one NIC you can, uh, get high-speed network right, or one DPU in this case, and then you can basically uh group multiple uh, you know, remote storage right, uh resources to really saturate each GPU, get the most data from the available PCIe link in this case, so that way GPU can still be dense, and you don't have to put too many cards, just put enough of this uh NIC to match the the GPU link, right, NIC or DPU to match the GPU link, and this would help with the form factor and also improve the bandwidth utilization as I mentioned.

Okay, with that said, let me hand it off to Craig.

So, talk a little bit about the ecosystem that these guys have been using, um, so AMD GPUs. Uh, if you've not already, we're coming to classes. You have the Radeon, which is your, uh, your uh gaming GPUs, um, that you know, you can go out and buy and put in your gaming box and get good performance, and then you have your Instinct GPUs, which is our, which are the big guys, those are the ones are the data centers, and, um, they're the ones that come with, um, HBM uh memory and, um, InfiniBand, uh, interconnect, um, to connect the GPUs in a very high-speed interconnect. But you also have to have when you have the hardware platforms, you also have this HIP software platform as well to take advantage of it, and that's the ROCm development platform, which is something that AMD has developed open source, and it supports, actually supports both types of GPUs, both the, uh, the Instinct and Radeon. Now, of course, if you go back 20 years and find your old Radeon and GPU sitting in a drawer, you probably won't support that, but the new ones, um, new ones, it supports.

So, ROCm is uh uh, is a very full-featured uh open-source stack. It's really really designed to work in the same ways as the uh CUDA software stack, and, um, the—of course, as I said, the real strength of ROCm is that it's open source. So, you can go out and you can get to get the source for it; you can make contributions, you can go and compile it, you can do you know, whatever you want with it because it's open source. And I have a very very hard-to-read figure over here, but you can see there are a lot of different components to it.

Um, one component to it is the HIP environment, which is a lot of the runtime and kernel extensions. Um, so that includes some of the compilers. Now, there's a whole bunch of different tools, so I'm just—when I say this is one of the components, this is one of the components—there's a lot of different tools that that I'm not mentioning, but HIP is one of the bigger bigger tools. 'CC' which is a front end to C++ and Perl, uh, 'FLANG' is actually a Fortran front end as well. 'Hipify,' um, which converts the uh, which can convert CUDA to ROCm, and um, of course your your your um, your 'make,' 'cmake' as well for building building your applications, and um, in a debugger, and there's also recently added some profiling tools as well.

Um, these are the ROCm libraries. Uh, I won't go through them though. I do think the ROCm games are a little bit cooler than the other side, but um, the uh, the uh, it you know, it's a very full-featured set of libraries there.

Um, very the development has been continuous since uh 2016. In 2016, the first uh version 1.0 was released, and that was really focused on HPC. Uh, in 2019, 3.0 was released, and that was more focused on AI. Uh, and then in subsequent generations, have been uh, have been released to support uh, different GPU models. Uh, in 2022, 5.0 was released to support MI200, and in 2024, the full support for the MI300 was released in version 6.1. 6.2 is the current release, and that added some additional profiling tools so you can see see what's going on while you're running your code.

Some references, um, like I said, it's completely open source, so if you want to go out and take a look at this, we've got a whole set of documentation. I've got to get there's a GitHub where you can go get the source files, and there's also a community where you can ask questions, and and and share experiences with other users. That's what I have.

Okay, so let's talk about the opportunity for a data processing unit in AI.

Okay, so to motivate this, right, uh, you know, one kind of known trend is that the traditional you-know server architecture that is more CPU-centric has been getting more and more burdened in the recent past, right? And that's because the increased complexity—not only software and hardware—and there's this terminology called "data center tax" that's been you know introduced since I guess 10 years ago, uh, it's got 2015. But uh, intended, but uh, basically the idea is that you know, as we go through the cloudification and everything like that, right, there's more and more infrastructure stuff being done, and these are data-oriented related stuff, and then this could include, you know, virtual machine managing, virtual machine, uh, dealing with disaggregated resources, right, like for storage or sending me over fabric, and then on the hardware side, right, the server these days, there's more and more devices that are in the server, right? And this would be higher speed uh, uh, NIC, smartNIC, DPU, and all that right, the uh, compute, data power compute type of accelerator like GPU, right, or AI computer NPU, there is also better and better discs, right? So the speed and feed of these devices are you know, improving rapidly, and CPU getting more and more burdened, so there's many data out there, and I just, I'm just quoting what I thought more notable one, which is a study by Google and Facebook; it's got a thousand five fifteen paper and s plus 2020 right, this is kind of they're studying the fleet of servers they have and showing that really these data center tax of running infrastructure, uh, you know, are real right, then it looks like over time, the the the the burden has increased.

So, you know, what's happening in the market is that, uh, this so-called data processing unit, or DPU, right? That's been proposed so that we can, you know, offload and accelerate these uh infrastructure data processing oriented tasks, right? From the CPU to the so-called DPU, right? And then a cartoon here, obviously, but the DPU will handle all the different infrastructure functions, at virtualization, networking, storage, and so on and so forth. Um, and the intent is, uh, you know, to be able to do all—you know, if you do DPU properly then obviously, you can improve efficiency because you free up CPU, and you can also better keep up with the improving speed and feeds of the devices, right? And by the way, obviously, DPU is a kind of marketing term, some other people call IPU, infrastructure processing unit, right? SmartNIC, super SmartNIC, and so on, but in this stock, basically, I'm talking about DPU is anything that is used to, you know, offload and accelerate the infrastructure data processing task.

And there are multiple products already entering the market, right? Um, you know, and then I guess a typical analogy I would give to people in the old days, you know, we have a general purpose CPU that does everything, right? And then at some point graphics becoming more popular and it could occupy a lot of CPU, and then hey, then GPU was made, right? Then obviously, for AI, it also turns out to be you can add I compute in there, so that becomes more mainstream, right? And similarly, for DPU with infrastructure, and are you processing more and more, you know, people are looking at offloading this? Right, there's multiple, uh, you know, offerings out there, right, including from MangoBoost, uh, and uh, notable approaches for this the DPU are, it could be a FPGA base or a ASIC base, but usually the card has some uh, uh, number of CPU in the card, so you can run software, you're gonna run your full infrastructure software there, and uh, you know, there's some form of hardware acceleration uh, in the card as well, that's assisting the CPU, right? And um, there's multiple FPGA based options, you know, for MangoBoost, you know, we focus on DPU solutions, so we have more hardware acceleration IP that we can put on FPGA. We can compose them together. We can even harden it in chiplet or SOC. But for now, we focus on putting it on FPGA. In fact, what we're showing in the case study is our solution on top of AMD FPGA.

Okay, so now let's talk about opportunities for DPU and AI systems. Obviously, AI system is very complicated, so it's kind of like a bit high-level cartoon picture to kind of get the point across. But basically, the typical setup is there's a fleet of GPU servers, not shown in there, but inside a GPU server box picture. There's also multiple GPUs in there, so eight GPUs in a server, for example, becoming more common. And then there is sort of networks that go across. There is sort of the inter-GPU network or network across a different GPU server. These are for things like collective communication for the communicating data across GPUs, from one GPU to another across servers. And there's also... GPU communication that is intra-node or within a server, and this would be using peer-to-peer communication or GPU-to-GPU fabric, things like that. And finally, there's also the access to outside, right, to get storage, and that would be storage network, right? And so the inter-GPU network, people could use RDMA, RoCEv2, InfiniBand, and so on, is, again, collective communication type. And for within the node, it could be peer-to-peer communication, and for storage network, it could be NVMe or for fabric, so it could be either over-RoCE or over-TCP, right? And the point here is that, you know, as you can see, there's all the orange part, right? There's a lot of places where DPU can help, because whatever data moves, right, in and out, going through a network, and from the storage to the GPUs, right, and then a GPU to another, then it can potentially get help from the DPU in there, as data move, improve the processing of the data. Okay. And then for this upcoming case study, right, that I'm going to talk about next, we're going to focus on, you know, storage, right? So this will showcase, you know, how DPU can help in both NVMe over fabric, especially over TCP, as well as the peer-to-peer communication, so you can directly put data on the GPU by bypassing the CPU. So we're not going to cover the inter-GPU network, you know, although, you know, we do have solutions both for RDMA, inter-GPU, and then storage, as well as inference. But the focus today is for the storage.

So let's talk about the case study. So here, you know, we're targeting an LM training case study with DPU-accelerated storage.

And for that, let me introduce, you know, our solution, MangoBoost GPU Storage Boost, as well as the baseline system, right? So let me start with the baseline system. So the baseline here is shown there, right? Essentially, we can have a normal NIC. We use ConnectX in this case. And then we... We can run software, and we want to run, essentially, NVMe over TCP, you know, to access the target, you know, storage server at the bottom there, depicted at the bottom. And in this case, you know, one of the possible challenges would be, you know, we have to run a bunch of software on the CPU, right? Obviously, we need the ROCm, because we need the ROCm to access the AMD GPU. But, you know, for NVMe TCP, right, Linux has standard NVMe TCP stack. You can run it, and that's what we use for our baseline. But for that, we run NVMe TCP and the TCP. And then the driver, right? And in this case, the host sees the device as an NVMe TCP device. And moreover, the data also have to go through the bounce buffer in the host memory, and then go to the GPU. So that's kind of the baseline. So there are two kinds of red dots there we're highlighting. And the first part of our solution is this NVMe TCP full hardware acceleration, okay? So in this case, the picture is illustrated on the next of the baseline, right? Here, when we put the NVMe TCP hardware acceleration on DPU, what we do is that all the NVMe TCP software stack gets hardened and moves into the hardware. And also, the hardware has some CPU for the control software. So what we can do is the DPU is seen as the host CPU as a local NVMe device. So you see as an NVMe PCIe device there. And then what it used to run on the CPU host now goes to the DPU. And if we go a bit closer inside the DPU, here in our solution that we're showing, we have a FPGA and an embedded ARM CPU core. So what we want to highlight here is we actually accelerate everything, all the hardware data path for the full NVMe TCP, including stateful TCP under the hood, on the FPGA. So this is different than running it on the ARM core on the CPU, right? The ARM core on the CPU is really for control and for lightweight software stuff, right? There's SPDK and DPU driver. So that's kind of how we first achieved improved efficiency. And we'll show the results, you know, in a bit.

And the second part, right, even the solution that we have before is not yet complete because we still need to move the data from the DPU to the GPU, right? If you don't do anything, it will show almost, you know, on my right there, it will show basically the data have to go to the host memory first, bound to a buffer, right, and then go to the GPU. But here, what we add on top of our previous solution to complete the GPU storage boost is the peer-to-peer communication. So DPU will receive the data, and then it goes peer-to-peer to the GPU, bypassing the CPU. So with this, right, we can have truly more efficient data flow.

And then next, let me talk a bit about our software enabling, right, just to kind of get a sense of how we make this work. So basically, a lot of user applications typically access the data through the file POSIX API, right? So in the baseline, on my right, right? Over the right, it shows basically a POSIX file going to a standard NVMe TCP Linux stack software, right? And in our case here, what we do is we make a Mango, what we call a MangoFile, and I'll talk a bit more in the next slide. So this essentially has a similar set of APIs as the POSIX file, but it allows the buffer coming out to directly put stuff on the GPU instead of on the CPU. And it also obviously works out with our DPU in the back end.

Okay, and then it's a bit detailed slide. I'm not going to go through all the detail, but I keep it there in case anybody is interested. And obviously, you can feel free to talk to us offline or contact us. But what I want to highlight among some of this is that first, you know, we essentially provide this, you know, API, right? The kind of key API mentioned on the shown on the on the most right corner of my right, the orange boxes, right? First is just to kind of set up things, right? Set up the file. And then second is really to read and write file, right? And the bottom is to clean up, right? And then the code that we make is we make a MangoFile kernel module, right? And then for that, the module, we have to talk to the GPU side, right? For that, we basically have to modify and add an API to ROCm so we can call ROCm. And, you know, this is actually one of the things we like about ROCm because the open source nature is open source. So we basically just take ROCm open source and add our own API and it can talk to our Mango kernel module. And then on the other side, you know, on the NVMe side, right, we go over the standard virtual file system and then the NVMe driver, we use the standard NVMe driver, just slightly modified, you know, so that we can connect to our kernel module, right? And again, this is related to getting the GPU address and GPU buffer because you have to poke the data directly to the GPU, right? Instead of typically you go to the host CPU first. And that's it. So I guess the orange part highlights the part that we add or modify. But this thing works on top of the standard ROCm as well as the Linux, you know, NVMe TCP stack.

Okay. So next, let me talk about the system testbed. You know, the table kind of shows a bit more detail. Again, I won't go line by line. But the main part is that basically, and it's shown in the figure, we have essentially populated our server with four AMD MI 300X GPUs, right? And then we also put four DPUs. So basically, the same testbed. But we use it for both baseline and the DPU-enabled one. In the baseline, we use, you know, standard ConnectX 6 NICs, right? And then in the DPU one, we basically just take them out and we put our DPU solution there. Again, this is our solution on top of AMD, you know, U45N FPGA card. And then we also configure it so that each DPU or NIC is assigned one-to-one to a GPU. Okay. So it's going under the same PCIe switch. That way when you do peer-to-peer, you know, you go to NIC or DPU, it just goes directly to the GPU because one-to-one. You don't have to go all the way up to the other, you know, PCIe switch. And each of the NIC or DPU is equipped with two times 100 gig. So 200 gig per card. And because there's four cards in the system, so it's an aggregate of 800 gigabit per second of Ethernet coming out of the system.

Okay. And then let's talk about, you know, kind of the evaluation. So we have two types of evaluation. We start with a micro-benchmark, and then after this, we'll show the, you know, workload with DeepSpeed. So our micro-benchmark, we just use popular, you know, FIO. Everybody knows FIO, right? And then basically, to enable FIO, we do a little bit of, you know, setup and software here. The baseline, you know, we couldn't find a ROCm that works with FIO out of the box. So we just make our own, like, a little bit backend of the FIO. Keep most of the FIO software intact. We just, you know, modify a little bit of the IO engine. We add to it so that they can call, you know, the appropriate ROCm API to move data around, right? And that's what we call the libhip. That's our own, you know, creation that we add on the backend of FIO. And then for the DPU-enabled system, we have what's called a libgsb, similar approach. You know, we keep FIO and just modify the backend IO engine so that it can talk to MangoFile, right? And then from the MangoFile, everything works with our DPU. So that's how we enable the FIO benchmark here.

And then here are the results. So on the most right, on my right, I guess your left, there's the data movement bandwidth. So this is we measure the achieved bandwidth out of the, you know, storage system. And then we vary a little bit. We vary on the x-axis different block sizes, just to kind of see how things perform over different block sizes. And then on the y-axis, the y-axis, right, is go up to 100 gigabyte per second, right, 800 gigabit per second, which is our aggregate network bandwidth. And then we have the line rate, dotted line. And then the two lines, right, the orange is from using DPU, right, our Mango GPU storage boost. And then the darker line is for the baseline, right, using the standard NVMe TCP software. And you can see, you know, essentially we got the 1.7 to 2.6x higher bandwidth depending on the block size. And also notice that in the baseline, right, the CPU seems to get saturated after a certain block size. So it kind of plateaus from that perspective. And again, this is the CPU is actually host CPU in the baseline runs the full NVMe TCP software. And then in terms of latency, you know, the middle part, right, the movement latency, we also show the latency improvement compared to baseline. Again, because we're not having the data go to host memory and then go to GPU, right, there's no bounce buffer. We go direct using peer-to-peer. So in that case, we got .25% lower latency on average. And then finally, on my left, you're right, the CPU core use, right? And again, here, the CPU basically needs to run some threads for the FIO itself for the application. So that one we cannot get rid of because you have to run FIO. And then in the baseline, in addition to that, it also runs NVMe TCP. But in the DPU case, all the NVMe TCP is handled by the DPU. So we save a lot of cores, like up to 36 cores, 22 to 36. And we basically, mostly, we free up all the CPU that's used for NVMe TCP software. Obviously, there's remaining 16 cores also. That's for the FIO stuff. And then a little bit for the NVMe PCIe driver because the host will still see the DPU card as a local NVMe PCIe, right?

Okay. And then for the next evaluation, beyond micro-benchmarking, right, we actually bring out also evaluate a workload. And this is the Microsoft DeepSpeed we talked about during the introduction. Right? And here, the setup is we use Llama 3.1, you know, as the model, right? This is a very well-known latest model from ETA. And then we also essentially have to modify a little bit on the back end. So, similar approach to the FIO. We only modify the back end part of the DeepSpeed. So everything else is not touched, standard, right? And then there's a swapper module, which is the back end that handles the data movement. When you spill data from the, you know, the AI states from the GPU memory to storage. And we modify that so you can call MangoFile, right? And that's how we get it to work with our solution. And essentially, we compare again. The only difference is just the NIC versus DPU.

Okay. And then the result is shown here. So we showed the same type of metrics as the micro-benchmark. So the latency one in the middle basically is not that easy to get the latency because now there is also the DeepSpeed workload itself running, as well as latency of accessing storage. So it was not easy to distinguish it. But we have data movement bandwidth still reported and CPU cores used. And as you can see, basically, the improvement is quite noticeable. And this is kind of within the range of what we saw in micro-benchmark FIO, but on the lower range because obviously the DeepSpeed itself has to run stuff, right, in terms of just the AI FIO. So we're on the lower range of the FIO benchmark that we saw, but still quite significant at 1.7 and then saving 25 cores. And I guess one other note, you know, to show that the solution is general. I mean, we work with ROCm, right? So ROCm supports multiple AMD GPUs. So we also have AMD MI210 GPUs. So we're not reporting it here, but if anybody is interested, you can contact us for details. But we run a similar set of experiments with MI210. And then we also show similar kind of type of improvements as well, as expected. So, you know, not here, but we can talk about the result if anybody is interested.

Okay. So to sum up, right, you know, an efficient storage system is becoming, you know, really a key factor in AI systems. And this is because there's a lot of compute already, right? We need to feed the beast in a way, bring data to the compute. And the model is so big that, including data, the other states, right, there's optimizer states, not just the model, that doesn't fit in the GPU device memory anymore. So you have to spill, right? And that's where a smarter storage subsystem really is critical. Obviously, Craig has gone over the AMD GPU ecosystem, which is quite state-of-the-art. And then we also talk about DPU and how it can improve, you know, storage system efficiency and infrastructure in general. In particular, we highlighted one of our solutions, which is the GPU storage boost from MangoBoost. And then our case study, you know, we have concrete numbers, and we show, you know, improvement in both, you know, storage throughput, as well as a reduction in the number of CPUs that got freed up, right, because things are accelerated, you know, fully in the DPU. And last but not least, you know, if anybody is interested, contact us, contact at MangoBoost.io, and we can provide a demo. You know, with that said, quick disclaimer, thank everyone for staying and open for questions.

So I guess the pricing, oh, I have to repeat the question. What is the pricing of the MangoBoost solution? So I would say the pricing is competitive. And you might have an impression that FPGA pricing is super, super high, but, you know, if you talk to us in detail, and then we have partnered with AMD also, you might be surprised that it's actually quite lower than you think. So it's competitive.

Competitive to other offerings. Again, I can talk the detail offline, but maybe not in the public forum. It's competitive, yeah.

You had a picture where you had a combination of ConnectX, and you forced the ratio of how many ConnectX companies you're using in that.

Oh, you mean in our setup?

Yeah. Let me see.

Oh, so basically the intent is the same number, because the intent is the baseline does not use DPU, right? It has four network cards, and we use four ConnectX. And we simply replace one-to-one, right, instead of using ConnectX. So let's just put a DPU.

So, these are impressive bandwidth numbers. What network were you using?

We have the 2x100 gig per card. And then there's basically four of them in the server.

It says, "gigabytes per second". How do you transport 80 gigabytes per second over a 100-gig network? Ah, so I think I made it very clear.

I guess the question was, I forgot to repeat the question, sorry. So the question is, what was the network setup? And then, you know, to explain the result. So basically, it's 800 gigabit per second aggregate. And then, so I just convert it to bytes. So that number is 100 gigabyte. Yes. So the top line is basically 800 gigabit or 100 gigabyte. And, you know, we're close, but, you know, it's not 100%. Obviously, you can't get 100%.

So for the red line, no ConnectX, it's just our DPU. And then, for the baseline, we use the ConnectX 6. And then, but the NVMe TCP stack is run in software, because ConnectX doesn't have NVMe TCP acceleration.

The Mellanox doesn't have NVMe TCP acceleration. OK. Yeah. Yeah. So, so, by the way, we also have RDMA, but we thought the NVMe TCP is more unique. So obviously, RDMA will be more standard. So you get a point. If you use RDMA, there's an RDMA hardware. But there's, in TCP, there's no hardware.

Ah, you mean the driver we have here? I guess. Yeah, obviously, we want other people to use it. I think that the specific plan, we're still finalizing it. But if nothing else, you know, if you contact us, if you have interest, you know, we can talk and make it real. But the intent is, we want to make it real. And for us, to enable other people to use it. So, you know, that's kind of the plan. Oh, repeat the question. Are we going to contribute to Linux? So, the driver to Linux. That was the question. Sorry.

So, what is the MangoFile? Is that the driver? Or can you explain a bit more, what is the MangoFile?

Yeah, so, so the...

Opening that up as well.

So the MangoFile, I think... Is it proprietary?

Ah, yeah, yeah. So, so the MangoFile is... Basically, the software library we make, so that it can work with our DPU and enable the peer-to-peer. So it's, in a way, it's tailored to our DPU. But the intent is to create a file abstraction, right? Similar to POSIX. But this is allowing the peer-to-peer communication. So the target buffer, instead of CPU, it will go to the GPU memory. And I guess one other thing is, if anybody is interested in talking more in detail, actually, from MangoBoost, we have two of our lead engineers here, Hitek and Changsu, somewhere here. That can... They would be happy to talk to you guys more in detail for this low-level stuff.

Sort of, really quick. Do you have... do you do IPsec uploading? Is there support for that?

Ah, in this case, we're not using IPsec, but, you know, we have different IP. I think we have security IP as well.

Do you have a block on the card? I'm just curious. It's kind of hard to see.

Ah, okay, okay. So the question is, do we also support IPsec? I guess one other question is, the answer is that... So we use FPGA, right, as our deployment. So any hardware IP, technically speaking, you know, we can offload there. So we have different mix of IP. Obviously, for this talk, we're not focusing on IPsec, but contact us, and we can mention, we have security IP as well that we can put in there. And we can compose different IP together. In this case, we showcase our full TCP accelerator and NVMe IP, right, and peer-to-peer IP combined together to make this solution. And we can mix other things and put it onto FPGA. And we also have a software layer on top, just like what I showed here, to make it so that it's seamless to deploy. People doesn't have to know there is an FPGA in it, easy to deploy. So it's easy to deploy as other NIC card, right? It's hiding under the same standard kind of driver type of thing.

So we can support multiple options from the known FPGA vendors. There's data center ready. There's different scales for this one. We use U45N from AMD. You can look at the spec. But we can go up higher. You can go smaller. And we usually look at the use case. And we obviously don't want to be overkill. So we can go with smaller FPGA. So we have tools. And basically, our secret is a hardware generator, a set of IP and tools. So we can go up, go down. We can even chipletize things, and harden things, and so see if people want that. Yeah.

Okay. If there is nothing else, again, thank you very much, everybody, for staying until the end. Thank you.
