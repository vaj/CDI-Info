
Okay, hello everyone. My name is Nathan Hanford. I'm a computer scientist at Lawrence Livermore National Lab, and I'm here to talk about some of the work that we're collaborating with the OFA on with Sunfish, and how this can tie in with a lot of work that's already going on with SNIA Swordfish.

So quickly, we'll go over the agenda. So I'm gonna do a quick Sunfish overview, discuss Sunfish and agent communications, discuss how Sunfish can work with multiple agents, then we'll talk about a little bit of the gory details of boundary component merging and multiple agents, and how Sunfish is planning on handling that, and then status, and then summary, and wrap-up.

So, probably most people here are familiar with Sunfish, but we'll go over a quick overview of Sunfish and why it's a good thing, somewhat for myself because I'm very new to the project personally. But, so composable disaggregated infrastructure, as previous talks have mentioned, really great for energy efficiency in most enterprise data centers. You know, we start with disaggregated storage. We're moving towards having disaggregated memory and even accelerators, which I'll get into a bit, and assigned resources may be private or shared amongst consumers, which is another thing I'll get into is something that we're considering. And CDI requires one or more interconnecting fabrics, and in an HPC center, we actually tend to have multiple fabrics because different clusters are going to naturally use, at least at Livermore, we just use whatever is out there depending on the needs of the system. Disaggregated storage is already supported on several fabrics, so obviously there's InfiniBand, Ethernet, PCI Express, Fiber Channel, Slingshot, and disaggregated memory, available.

And the issue is that we need to avoid disaggregated management stacks. As has been discussed in previous talks, part of the issue is that we have a multitude of independent management tools, and the goal of Sunfish is to put them all behind a single consistent standards-based API.

So, really quick, what is Sunfish? Composable disaggregated infrastructure, as I mentioned, provides a lot of great opportunities for reducing energy consumption and providing rich services to users. We want to solve the management problems of this in an open, standards-based way. Sunfish is a project of collaboration between DMTF, SNIA, the Open Fabrics Alliance, and the CXL Consortium. And we want to provide elements of the overall solution and be Redfish and Swordfish compatible. It's designed to configure fabric interconnects and manage composable disaggregated resources and dynamic HPC infrastructures using client-friendly abstractions. And so we're going to talk about how Sunfish does this in this presentation.

So HPC, it's always interesting to hear other folks talk, and it's actually kind of fun to be here as a sort of homecoming. My first job out of college was helping administer a fiber channel SAN. That kind of got me interested in the fiber channel SAN. I was interested in high-performance networking because I realized that there's a bunch of things that were missing from my undergraduate networking courses with regards to other protocols that exist out there. And so it's cool to come back to this. HPC, obviously, we use a lot of fabrics that are quite similar to fiber channel. And we need to be able to manage all of that. So mostly batch workflows. We do some online transaction processing style workflows. We do some containers. But mostly what we're doing is running jobs using schedulers on huge supercomputing systems. In fact, we are actually going to be citing, or currently in the process of testing our latest system, which is El Capitan. It's going to be an exascale system on the Cray EX platform. It's going to have, for the first time for us at Livermore, in a major system like this, near-node local storage, which is composed of NVMEs, which are going to be located closer to the actual compute nodes. As it turns out, in exaflop systems, you do so much, you have the capability of doing so much storage transactions that writing out your results could rapidly overwhelm a traditional Lustre file system over a fabric. So we actually do need the near-node capacity. We're also exploring fabric-attached memory and provisioning those resources. And we also have two AI ML accelerators, one from a vendor, Samba Nova, and another one from Cerebrus. And interestingly, we would actually like to allocate and share those between clusters if possible. So different clusters with completely different fabrics, with completely different schedulers, should be able to allocate the same accelerator between different jobs. We do currently have our accelerator is hooked up across multiple systems. But right now, we're actually at the stage of people saying, 'Hey, I'm going to use the accelerator, please don't,' and then provisioning things in a more manual fashion. So we'd like to automate that so we could get the accelerators deployed to more clusters. In order to do all of this, rather than using Slurm, which is the most popular HPC job manager out there, we've actually developed our own job manager called the Flux Framework. And actually, the near-node local storage on this new system is one of the reasons for Flux being a thing, because we needed a lot of capability of scheduling. So we've already been through kind of approaching composable disaggregated infrastructures, maybe being dragged, kicking and screaming towards this conclusion that we do need to actually start worrying about these problems. And so that's where a lot of development work has gone in at Lawrence Livermore. And one of the things that I'm really kind of trying to champion at Livermore is that when users run a job on a certain subset of a machine, it would be great if they could get statistics back after their job ran with information such as power consumption, how fast did you run, how did you hit the fabric, did you use resources efficiently? We do occasionally come up with issues, in fact, this kind of came up in testing, where due to, within the server, due to like core affinities for example, um, you know, you might be S or even in networks you might have everything set to go on one core um by accident or you might have everything going on one network link or something by accident, so like, you know, these things can happen when you have, you know, thousands of nodes on a system and then you realize you can actually go way faster than you previously thought, which is always great for users, so we'd like to be able to do more of that.

So, really quickly, I'm going to do the quick animated run-through, um, of the Sunfish objective in visual form. So, we have specific hardware. The hardware is going to come with its own hardware managers or agent providers that are going to allow us to provision that hardware, or at least things like BMCs that know what's going on in the hardware.

Sunfish agents want to hide hardware specifics by creating appropriate Redfish models of resources, so that we don't end up in a proprietary nightmare of tons of different manual code development to try to make sure that we do this for every single new generation of hardware we get. So, we have hardware-specific agents in Sunfish.

Then, Sunfish services manages the Redfish models of all the resources from the hardware agents. 

And then, through a RESTful API, Sunfish will provide a composability layer that allows clients to see abstracted fabric-attached resource objects. And so that's where we come in with not only Flux, but also things like Kubernetes operators and things of that nature. So we can use Kubernetes, OpenShift, whatever we want to do provisioning, both for traditional HPC and also for containers. So Sunfish defines all the policies that the agents follow when creating resource models, so the clients know how to interpret and manipulate them. So that's kind of the overall goal.

So, as an example of what this could look like, maybe you have CXL hardware with its own fabric manager, you have PCIe fabric manager, and then you have some Swordfish appliance API manager. All of them have their own respective hardware,

And each of them are going to have their respective agents. They're going to have events that are going to be pushed through the management layer for Sunfish services, but also more importantly, and for us at least, provisioning these resources effectively is going to be done through this management layer by Sunfish.

And that presents the API, which then goes to the composability layer, which allows the clients to interact with it, and so this composability layer is what the clients are actually going to interact with when they need to provision resources.

So, Sunfish hardware agents, we've, you know, started work on these. We have many different hardware manufacturers who can easily create agents for Sunfish simply by being Swordfish or Redfish compatible, and then perhaps some modifications would be necessary to be, you know, absolutely Sunfish compatible. The beauty of this is that for us, when purchasing hardware, we don't have to put vendors and ourselves through a nightmare of conflict. We're creating completely custom agents that are going to work for HPC land, when in fact that might not be even the vast majority of market share for a particular hardware provider, hardware vendor, so that way, you know, it saves everyone work. So, and we have an alpha release of the agent emulator, which is very closely based on, I think, CXL's agent emulator, which is available online.

We've also started exploring the Swordfish API emulator. So, we've gone ahead and taken our CXL agent and modified it to be more compatible with the Swordfish emulator API, which, as previous slides have mentioned, is available for free on GitHub. So, this is imminent. We're just working out the last bugs with this right now, but we'll be able to have, you know, some proof-of-concept code very shortly on the Swordfish API that we're developing.

So, as Jeff's presentation had mentioned, one of the major issues that we need to resolve in Sunfish is merging multiple agent resource trees, and this is something I kind of alluded to before. Because we are allowing the integration of heterogeneous fabrics, we have two critical problems that occur: First, Sunfish needs to detect and resolve URI namespace conflicts, and second, we need to detect and resolve boundary component duplicates.

So, kind of a quick example of that is that you have, you know, some CXL fabric-attached memory appliance. It has an enclosure manager, okay? So, it goes ahead and registers an agent with Sunfish. Everybody's happy. It has its own Redfish URI namespace, and so everything looks great. It has some Redfish V1.1 fabric CXL. So, that's how this enclosure manager sees the fabric that it's connected to, and that's great, until you start adding another enclosure.

So, now you have the same actual CXL fabric that is represented by two different enclosure managers, using possibly the same URI, because they both think that this is just the V1 fabric CXL URI. So, Sunfish needs to actually have a management system for resolving this. So, it will actually go, Sunfish will go ahead and aggregate the resources. Obviously, hosts have BMCs and OS that see what the host can see from its own point of view. You know, we have, I don't want to go through all the gory details, but basically the Sunfish service is going to eliminate the duplicate URIs based on some detection algorithm, and that will make sure... They can, even though both use the same Redfish URIs, the way Sunfish will understand this and forward it to the actual allocation, the, you know, Flux allocator, it will actually say, okay, these are actually different, these are different references to the same fabric. The other thing that happens is we have these boundary links, and so this is kind of the example that's going on here. Because these boundary links also need to be mapped in a canonical way, so that Sunfish can have a global view. So, a scheduler like Flux will be able to correctly allocate resources.

So, extending this example, we have a merged model that's available here, where basically what has happened is Sunfish, because it has a global view of the fabric, can actually see not only the metadata on the fabric, or the URI for the namespaces, but also has access to actual hardware logical addresses. And so, it'll be able to, hopefully, if everything goes right, match and sort of deduplicate these different URIs, make them unique. And then that way, we have one single view of the same fabric that Sunfish can then reason about and send to the client layer.

So, that's not all we're working on. We actually have more storage management capabilities in progress. We want to manage NVMe native devices along with appliances. We are working on aggregation of multiple Swordfish targets. We would really like to learn, and that's one of the things that I want to learn about here, is adaptation of the SNIA Swordfish compliance testing for our purposes, so ideally we would be able to utilize the SNIA Swordfish compliance testing API with Sunfish, so that Sunfish would be able to go ahead and mock provision storage appliances and storage devices.

And so, we do have a couple of new releases that are imminent. We have more things going on that are outside of storage, I guess. We're going to do a customization plugin. Sandia is also working on a Redis storage backend, and we are currently soliciting new use cases in hardware. We've also at Livermore started working on the Flux client side, along with Sandia, and then we're working on client side resource blocks and managers, and creating a reference CXL agent is still something we're working to finalize.

We will be at Supercomputing this year in multiple different places. So, we will have a Birds of a Feather on Thursday, unfortunately, it's kind of during lunch, and then we will be in the Open Standards Pavilion, along with other folks here, at booth 1815 at Supercomputing this year.

Okay. And so, to summarize, Sunfish allows allocating multiple different Redfish-derived hardware agents from different vendors in the same Sunfish instance. We can merge different fabrics via the Sunfish service layer while paying attention to boundary links and making sure that they have all canonical names. We are currently targeting Kubernetes via several operators and the Flux scheduling framework for HPC, and we'll have a booth in several demos during Supercomputing. In fact, actually, I forgot to mention, we're also going to be doing a couple of Sunfish demos over at the Department of Energy booth as well.

So, let me know if you have any questions. Okay. Any questions? We have documentation available via GitHub, and if you want to join development, there's always room for more help, of course, to help steer development over in the Open Fabrics Alliance. Any questions? All right. Thank you very much.
