
Hello everyone. I'm Clarete Crasta.I work with Hewlett Packard Enterprise as a Master Technologist, and my experience is in the area of storage, virtualization, and high-performance computing.Over the last few years, my efforts are focused on building a software stack for fabric-attached memory systems.

In this presentation, I'm going to speak about the fabric-attached memory architecture that HPE has built.We will look at what is the motivation for HPE to build the fabric-attached memory architecture, the hardware components that have contributed to the fabric-attached memory architecture.We will explore some of the components of the software stack, which HPE has built to access fabric-attached memory from the compute nodes.We will have a brief look at some of the basic results with access latencies and throughput that are possible with the fabric-attached memory.We will look into some of the possible use cases in high-performance computing and high-performance data analytics space that can benefit from fabric-attached memory.We will end the presentation with a short summary and a peek into the future work.

We all know that there is a data explosion all around us.The amount of data that is being generated is doubling almost every two years, and it is expected that by 2025, we will have close to a couple of hundreds of zettabytes of data generated from billions of connected devices and people.It is not just the large amount of data that is generated that is the problem but how this data can be analyzed in reasonable time to arrive at meaningful insights.We can see on the graph on the right that the value of data analyzed reduces as the time taken to arrive at the results increases.Also, we have seen that the technology advances to the computer architecture that we have witnessed in the last several years are starting to flatten out.There is a growing capability gap between the amount of data that is being generated versus the amount of advances to the technology for the computer architecture.In response, HP is arriving and building an architecture with fabric-attached memory, which provides the flexibility of getting various components together and scaling them independent of each other.

HP is taking advantage of the technology advances to the hardware components in the spaces of memory, interconnects, and computer.In the past, the storage or memory was primarily consisting of two hierarchies.One was the primary storage that was coming from the volatile memory present on the systems.The second was the secondary storage, which was through the hard disk drives and the solid-state drives.While the primary storage came with much lesser access latencies, they also had limitations with capacity.The secondary storage, while they support much larger capacities, they came with the overhead of larger access latencies as well.In the recent past, we have seen multiple entries into the segment with high-bandwidth memory, storage class memory, memory semantic devices, which come in larger capacities and also in much lower access latencies compared to that of secondary storage, bridging the gap between the access latencies between the primary and the secondary storage.There have also been multiple advances to the interconnects.We have seen with advances to photonics that much larger bandwidths and throughputs are being made possible.HP has its own interconnect, the Slingshot interconnect, which primarily focuses on catering to HPC workloads.We have also seen the advances to CXL and the way CXL plans to support memory pooling and memory sharing with the version 2.0 and 3.0 specifications.We have also seen multiple advances to the compute in terms of GPUs, FPGAs, accelerators, and how all of these are coming together to solve complex high-performance data analytic problems.

HP is taking advantage of these components and building an architecture where we have multiple compute nodes or what we call system on chips which have local DRAM connected to each of them.Connected over a large or a fast memory fabric to a large pool of memory.The compute nodes could be in the order of tens of thousands and the memory could be in the order of petabytes.All of this memory is accessible to all of the compute nodes through the fast fabric.This architecture allows for independent scaling of memory and compute.Earlier, if the workload required more memory, the only way to do that was to add additional compute along with memory to the configuration.But with this architecture, we allow for independent scaling of compute and memory.Also, because of the separation of the compute and memory, there is decoupling of the failure domains.The architecture also provides direct unmediated access from the compute nodes to fabric attached memory.The access could be through RDMA or it could be direct load stores, depending on the capabilities of the underlying interconnect.Some of the challenges that we see with this architecture are that the coherence domain is limited to individual nodes.When we have a distributed application running across multiple nodes, it is the software's responsibility to handle the coherence, especially on RDMA-based interconnects where the interconnects do not support coherence.Also, we have to note that there is a latency difference in accessing the local DRAM that is present on the nodes versus accessing the fabric attached memory.Applications trying to take benefit of fabric attached memory need to refactor the data layout and the communication pattern in order to account for the latency difference in accessing DRAM and FAM.

Based on this architecture, HPA has built a hardware prototype.It consists of a few compute nodes, 32 compute nodes, 10 FAM A partitions and 10 FAM B partitions.The difference between FAM A and FAM B is that FAM A contributes volatile memory to the fabric attached memory pool, while FAM B contributes volatile memory and persistent memory to the fabric attached memory pool.In this case, with this hardware prototype, we have used Optane, but the architecture supports any other advances to persistent memory that would come along in the future.The architecture is adaptable to including such components.We also plan to introduce support for CXL, both by supporting CXL memory modules on the memory server side, as well as supporting the CXL interconnect with slingshot fabric.Here in this hardware prototype, we have all of the computes, the FAM A and the FAM B partitions, connected to each other through slingshot, each node having about 400 Gbps bandwidth.There is a total of 82 terabytes of DRAM coming from the volatile memory, from the compute nodes, the FAM A partition, the FAM B partition, and a total of 80 terabytes of persistent memory.We are using this hardware architecture for evaluating some of the proxy applications, which we think will benefit from the fabric attached memory architecture, and also for performance benchmark and performance optimizations to the stack.

We will look at some of the components of the software stack that we are building to support the fabric attached memory architecture.One of the main components is OpenFAM.OpenFAM is an API and a reference implementation to access fabric attached memory from the compute nodes.The application is linked with the OpenFAM library, and can talk to the memory nodes through the OpenFAM APIs.OpenFAM API is generic in the sense that it is supported both on scale-up and scale-out systems.It currently supports both volatile and persistent memory, and it also supports RDMA-based accesses and direct access depending on the capabilities of the interconnect that is used for the configuration.

We will look at some of the other components that go along with OpenFAM in the software stack.As you can see here, applications written in multiple languages at a C/C++ chapel, can access fabric attached memory through OpenFAM APIs.We also support the HDFS file format on fabric attached memory.The HDFS file interfaces support reading and writing into HDFS file files on fabric attached memory through OpenFAM APIs.We are also currently evaluating support for fabric attached memory through OpenSHMEM APIs.The OpenFAM library depends on a few other software components.It uses gRPC and there are support for Mercury that is in progress.It uses Slurm for workload management, Slurm and Mpirun for workload management and PMIx for process management, and uses Libfabric for RDMA.Through the Libfabric, we support interconnects such as Slingshot, InfiniBand, Omnipath, and in future, we would include support for CXL as well.

We'll now look at some of the basic results that we have with respect to accessing fabric attached memory from the compute nodes.We have carried out latency tests and throughput tests.For the latency tests, we have used short messages of 256 byte sizes.We have seen that the short transfer latency is in the order of three to four microseconds when the compute nodes access the short data items in fabric attached memory through Slingshot interconnect.For the throughput operations, we had 16 PEs accessing data that is stored on fabric attached memory, 100 operations per PE with a transfer size of 64 megabytes.With this, we have seen that when we increase the number of memory servers, that is the number of memory nodes that are contributing fabric attached memory, there is a linear scaling which is close to the available bandwidth when we access the data that is spread across the different memory servers through get blocking and put blocking calls.There is more on the performance analysis and the performance numbers that we have shared in the paper, submitted to the OpenSHMEM conference, and the link to that is available here.

We will now look at what fabric attached memory enables for some of the high-performance computing applications.Fabric attached memory enables applications with large datasets that do not fit into the DRAM of the available nodes.Fabric attached memory, as we saw, allows for independent scaling of memory and compute, and also for dynamic scaling of application workloads in terms of addition of components to existing applications.Fabric attached memory also enables heterogeneous configurations with support for GPUs and accelerators as and when we make that available.OpenFAM also could be used along with other libraries such as MPI and OpenSHMEM to augment some of the capabilities with these existing partition global address space libraries.What fabric attached memory provides is that the memory is large, the memory is shared, and the memory could optionally be persistent if we are using the persistent memory modules.With large memory, we see that in-memory indexing, in-memory databases are possible, and with the memory being shared, large amounts of in-memory communication can be made possible.If the memory is persistent, we could also have faster checkpointing of data that the applications would normally do with secondary storage.We see that applications such as large-scale graph analytics in the security social network advertising space can benefit from a fabric attached memory, and also HPC workflows and pipelines in the genomics and AI space, where there is large amount of data involved and there is data transformation with data sharing across the multiple applications in a workflow.Fabric attached memory could be used as a staging area for such use cases.FAM could also be viewed as a distributed high-performance cache and made available through OpenFAM APIs.An idealized workflow would probably have these components as shown on the right-hand side.We here have ingest threads, which are ingesting streaming data that is coming from the generated data.This data could be stored in the compute's node local memory in terms of raw data buffers.There could be analysis threads analyzing this data and making the results available either in fabric attached memory or in archival storage.When the analyzed data is made available in fabric attached memory through the in-memory data store, it enables faster access to the query threads, to query that data, and make the results available.Also, we could have incremental updates to the analyzed data available in fabric attached memory, giving a history of data or results that is available in FAM.

In summary, HPE has built an architecture for fabric attached memory both by defining the hardware components as well as by building a software stack that enables accessing the fabric attached memory from the compute nodes.We do have performance results which showcase the access latencies and throughput that is possible with the different interconnects that we support.We have also looked at some of the use cases that can benefit from FAM, and we are evaluating our proxy applications on the hardware prototype to arrive at use cases where FAM can be beneficial.In future, we plan to support CXL memory, CXL interconnect, and any new memory technologies that come along to be supported on the memory nodes.We will continue to work on more ecosystem enablement in terms of making FAM available through OpenSHMEM, Chapel, and CUDA, and also by supporting the different data formats that are required for HPC workloads.

This brings me to the end of the presentation.In case fabric attached memory has a use case for any of your applications or workloads, please do get in touch with us and we would like to hear from that.Thank you.
