
Hello, everyone. I'm glad to meet you all virtually today  to talk about an important topic of our time right now. What is driving AI? And what are the hardware boundaries  to the infrastructure? Spoiler alert-- memory is a key factor,  and that's what I'm going to focus on today.

So before that, let me give a bit of context  from a meta perspective. As you know, Meta has a mission to help people  to build community and bring the world closer together. This is done through a family of apps Meta has--  Facebook, Instagram, Messenger, WhatsApp. And today, Meta family of apps is  used by mind-blogging 3.19 billion users on a daily basis  and almost 4 billion users on a monthly basis. So we have a responsibility to enable this mission  with very efficient infrastructure.

Let us connect that goal to AI use cases  and eventually to the infrastructure demands. So Meta has two major AI use cases. First is to serve personalized recommendation  to the users that make their social experience rich. So the deep learning models that we have  provide that personalized experience  by training the models and providing the recommendations  as an outcome of that inference. Gen AI is creating richer experience  for coding and enabling industry overall for other research  and commercial use. And all these use cases are growing very fast  and changing the infrastructure requirements  at a very fast pace.

Now let's dig in to see how the infrastructure needs are  differing from AI. For AI, as compared to our traditional general compute  infrastructure that we have been using in a hyperscale data  centers for a long time.

There are fundamental differences  that AI brings to infrastructure as a use case. Now while general compute applications typically  run as a millions of small stateless applications. Any of them can fail and the application can survive there. That's how it will work for the traditional use cases. And that's what we optimize for. Whereas the now large AI applications  as compared to that run across cluster of GPUs or accelerators  as a single job. So failure has a significant penalty for an AI job. For example, training job that runs for weeks or months,  if it needs to restart, even if single GPU fails in the job,  then you have a significant penalty because the whole job  gets impacted. Performance for AI jobs depends on all the components  in the cluster. And needs closer hardware software co-design  to make sure that we have optimal ratio. One more thing to keep in mind is  that many component technologies are a platform cadence. That is they run on a general compute  that are driven by general CPU roadmaps. Whereas the AI needs are growing much, much more rapidly. And we foresee that very fast pace of demands  is being placed on the components that enable AI systems. So we are talking about all the system components,  all the network components, all the data center components. So overall the center of gravity for AI systems  is now towards GPU accelerators instead of CPU  in a traditional compute system. And the exponential growth of AI demand  has shifted the data center infrastructure design  challenges in a significant way by these GPU centric designs.

Now, ideally, one would like to have one largest accelerator  system to run the whole AI job. Because we talked about saying that AI job runs as a single. Why not just get one largest accelerator? How that's not possible because as the model sizes  are increasing, that accelerators  need to continue to increase in a clustered fashion. So that cluster size needs to be required for the training jobs. Now, there are multiple methods how  we can achieve this higher performance by cluster. One could parallelize the tensors or context  to run on multiple GPUs. Tightly coupled clusters allow such tensors and context  to be run parallel. And then one layers loosely coupled clusters  on the top of the scale up clusters  for parallelizing the data and pipelines. So this allows a large AI job to be run on AI cluster  by optimizing various parts of executions  and make sure that we can achieve  the scalability of the performance as we need.

Now, let's look at what does it take from the network  perspective to enable such kind of a scale up and scale out. Tightly coupled jobs that use scale up networks,  they demand very high bandwidth and lowest latency. There are examples of interconnects today  that are load store based interconnects. And you know those. There are NVswitch. There is an infinity fabric being discussed. Word based or message based interconnects  like RoCE at a very high bandwidth and low latency. And then loosely coupled jobs like data parallel  use scale out networks. And they need also higher bandwidth. They need much higher bandwidth than the typical front end  networks, but little more lenient as compared  to the scale up networks. These are typically already main networks like RoCE  or InfiniBand. Higher bandwidth requirements for scale up networks  is pushing the physical limits of distances  and driving tightly connected cluster topologies. For example, how do we make sure that what is the distance that  we can drive with a service like 200 gigabits per second  per lane or 400 gigabits per second per lane. So this is going to continue to bring the accelerators tighter  and tighter together based on the limitation  that we run into the distances. Scale up interconnects carry a lot of memory accesses also. Because that's what the tightly coupled systems will do. These are the memory accesses that will run across the fabric  and they drive very high bandwidth and low latency  requirements. This also weighs in on the overall system design. Now for a scale out network that we talked about,  there are additional challenges as we spread the systems  to the larger topologies. How do we keep the effective bandwidth for the applications  higher by maintaining P99 or the tail latency in check  by avoiding all hot spots and the congestion? And this all depends on the workloads and applications  also. So let's look at that, what those applications are  and how they impact the overall system design.

There are multiple use cases. There is a significant diversity in the AI system requirements  based on these use cases. Broadly, they are classified into two. You have training and then you have a recommendation and GeneAI. And within the recommendation, you have training and inference  and recommendation training, required training  and req inference. And then for GeneAI, you can have training  and then you also have inference. And then inference, you can further granularize  by saying pre-fill, which basically  is getting the first token in the LM world,  and then decode, which is getting the subsequent tokens. And these use cases have significantly different  requirements. So we look at all these use cases. You can see that the system resource requirements  for each of these use cases and layering them  on top of each other, you can see  that various models place very different demands  on resources that makes the overall system design  optimization for TCU a very challenging job. AI use cases push this envelope most for most of these use  cases. The need for building larger accelerators,  larger tightly coupled systems, as well as growing model size  and performance needs are pushing  the AI systems and components beyond their traditional growth  trajectories. Now let's focus more on the memory part of the requirements.

Now, why is memory important? And what is exactly driving that?

Ideally, we talked about saying that to run a single job,  it will be great to have a single accelerator. So I know we said that this is not feasible  and we need to run into the accelerators. However, as we looked at the scale up demands,  you know that actually as we continue to grow the models,  we want to get each accelerator also  to get larger and larger to make sure  that the scale up cluster remains in a feasible domain. So there is a strong reason to make sure  that we continue to increase the accelerator sizes. So as we increase the accelerator sizes,  this will continue to drive the memory capacity, as well  as the performance for the memory that  is integrated in the DAG. To achieve the kind of bandwidth and bandwidth  that we need for the GPUs and accelerators,  typically we have memory that is integrated into the package  itself, which is the high bandwidth  memory or HBM that is very much known. One also has to keep in mind that this increase  in size of accelerator comes with additional burden  of increasing reliability. Cost of job failure is very high,  although we'll talk about that a little bit later. So bottom line for memory capacity and bandwidth need  for the accelerator is that the first priority  is to make sure that integrated memory technology like HBM  need to continue to grow for capacity, performance,  and reliability.

Now, as we know, even though we talked about this primary  memory that sits inside the accelerator,  that we have talked about the memory wall a lot. Model sizes, as well as the accelerator performance,  are driving memory needs beyond the embedded HBM capacities,  as well as the bandwidth. As you can see in the charts, that's what happens. We are continuing to drive the performance of the GPUs,  but the HBM capacities that are coming out,  as well as the interconnect bandwidth that are coming out,  are definitely not able to keep up together. So the requirements for this accelerator performance  are going way beyond what we are seeing in these charts also. So they're going up dramatically,  which will make it even more challenging for the network  and memory capacities to keep up with this. So as we're making this large accelerator,  the tightly coupled scale-up clusters demand  are actually growing exponentially, not linearly. And a higher bandwidth also need to keep up. And we want to make sure that we are  maintaining low latency. So this is pushing hyperscale data center to its limits. Because as you keep on growing larger accelerators,  that comes at the point of basically increasing the power,  how much you can land it in the power,  how many accelerators that you can have it in a single rack  to make sure that you can achieve the scale-up clusters. So you're looking at accelerator performance. You're looking at memory capacity  that it is connected to. What's the performance that we provide for the memory  to the accelerators? And then you're looking at the bandwidth  of the memory to the accelerators. How many accelerators we land,  which basically is going to push the rack power,  the network bandwidth to connect such high performance  net accelerators in the scale-up networks,  as well as the scale-out clusters. The network latency that we can achieve  and how we manage that. Types of networks that we support. How do we make sure that the cluster reliability  is maintained? That means the cluster reliability of all the components  in that cluster that participates in the job. All of these are pushing the data centers to its limit. However, let me hold my excitement there  and keep focus back onto the memory.

Now, what are the components of that increase  the total memory need per accelerator for the AI job? So at broad categories level,  one has activations that are computed  during the forward and backward passes. If this exceeds the memory capacity, internal capacity,  then typical technique is to recompute  during backward pass that results into accelerator compute  and memory trade-off. Model parameters and embeddings are increasing also  to a large value and cannot typically fit  into GPU's internal HBM. However, the access patterns are such  that one could use locality and tiered memory accesses  with appropriate prefetching techniques  that allow right placement across two tiers of memory. And that there are additional accelerator memory users  like gradients, weights, optimizer states, et cetera. And these are all contribute  to the local memory needs of the accelerator. And as a completion of the job is important  in face of various failures,  checkpointing also allows restarting the jobs  and after failure. And so without losing all the time on the job  that was already executed,  you can achieve significant improvement  by having the checkpoints. Of course, this has memory requirements. And as we start looking into this,  there are innovative explorations going on,  basically how do we make sure  that we can do in-memory checkpointing, et cetera. So these are going to drive the memory requirements  even faster than what we have seen in the past. So all of these add up,  activations, smart embeddings, gradients,  all of this basically add up  to the overall memory requirement  that is being addressed by the HBM  that we already saw was falling behind. So you're looking at capacity  that already is falling outside,  it's falling behind the accelerator,  and we want to make sure  that how we provide the tiered memory for that.

So as overall memory requirement is not being satisfied,  that's what we have inside, which is the HBM. We need to start thinking about memory expansion  and the ways we can achieve that.

So the next option is basically  we start using some kind of tiering. So use HBM as a tier one memory,  and you have tier two memory  that is accelerator has access to,  which will allow higher capacity  at a relatively reduced bandwidth  as compared to the aggregate HBM bandwidth  that we have inside. But AI geopartners potentially will allow us maybe  to do this tiering,  which uses HBM for its bandwidth needs  and then has the capacity  attached to the tier two memory  to be used as an overall solution,  bring that in an appropriate fashion  for the AI jobs to use  to satisfy the overall requirement  that we have for the AI clusters.

Now, how do we do this? So the easiest and first preferred option would be  you already have a CPU attached  to the accelerators in the system,  so expand the memory through the CPU. So accelerator can access the CPU attached memory  as the tier two expansion capacity. You already have the high bandwidth interconnect  between GPU and CPU,  which is required to match the desired memory bandwidth. So as you put more memory behind it,  yes, obviously you need higher bandwidth  to make sure that you access that. You have PCIe today, CXL potentially,  that can be used there. NVLink C2C has been out there now  as a new proposed example  for how to use such a expansion through memory. Infinity Fabric, all well-known examples of this. There have been well-documented examples  that if you use the memory attached to the CPU  through the attached interconnect  between the accelerator and a CPU,  one could benefit that  and how to do that with embedding  is something that we have been out there  and we have put the information out there  and demonstrated that also.

Now one could also, if it is within the node,  when I say node-native, it is within the node  which has the CPU and a GPU,  one could imagine of expanding that memory within that node  by adding the memory directly to the accelerator also  using any expansion interconnect like CXL  or any other interconnect  that allows the memory connectivity.

Now let's dig into what happens to the node-native memory  that is fabric attached. In this case, node-native memory  that is attached to the CPU or the GPU  directly inside the system itself. To achieve this, one needs to make sure  that the bandwidth and capacities are addressed  at each point of component interaction. Now the interconnect that is being between the accelerator  and a GPU, accelerator and a CPU  has to have a high bandwidth  without burdening already shoreline limited accelerators. In the sense, you have to provide that bandwidth  to make sure that I have smaller number of services  that allow me to achieve the higher bandwidth. This means the service has to achieve the desired bandwidth  without really putting too much burden on the accelerators. Let's take an example. If one needs to achieve the tiering,  which has let's say 1/10 of the bandwidth  of your tier one memory, which is HBM bandwidth. And if that HBM bandwidth is let's say  eight to 10 terabytes per second,  the tier two memory has to have 1/10 of that,  which is maybe one terabytes per second or more. So if you do the math, basically you want to make sure  if you're using CXL or PCIe,  you want to make sure or any other fabric  used as less number of services as possible  because this is going to continue to increase  as we go forward. CXL is, or PCIe is challenged here. Higher speeds like PCIe Gen 7 will be required  to make sure that we can keep up  with the accelerator requirements in next few years. And this bandwidth needs, once you go across to the CPU,  has to be matched by the number of memory channels  that we have on the CPU to match the interconnect bandwidth. So you need the memory channels  as well as the speeds on the memory modules  that you're pacing. So higher speed of the DIMMs  and higher speed of capacities are also required  to match the bandwidth and the capacity requirements  of this accelerator. So this is what will be required to make sure  that we can enable that higher capacity,  tier two capacity and the bandwidth for the accelerators.

Now one could think of fabric attached. So we discussed the scale of fabric. This has a very high bandwidth. So today between accelerator to accelerator,  you have in a tightly coupled systems,  you are actually using fabric attached memory,  which basically means one accelerator can access  the memory that is connected to the other accelerator. One could also consider that, you know,  you can have a memory attached to the fabric itself. Oh, by the way, before we go there,  the fabrics today that I talked about  that allow accelerator to accelerator memory access,  the examples are well known out there. There are loads to interconnects well known  like NVLink, Infinity Fabric,  and also there are message passing interconnects  like RDMA or RoCE. CXL can play a role here. However, as you can see,  you need to match the performance and latency needs  that the scale up networks demand.

Now if to enable that,  now let's look at what that would mean  if you're going to enable the fabric attached memories  or the memory interconnect. Now, if you look at here,  there are vendor specific technologies today,  as we said, is NVLink, there is Infinity Fabric,  these are available, these enable load store accesses. CXL is an open fabric now available  that can enable load store accesses  to accelerator to accelerator,  as well as the accelerator to memory connectivity. However, CXL lacks these available technologies. We talked about NVLink and Infinity Fabric,  as well as Ethernet by at least one or two generations. And it is also very slow to respond  to the fast changing AI demands  because it has a large established base of PCI ecosystem. However, this is the open option available today. So if it can address the challenges,  then it has a potential  of becoming a very interesting option here. Accelerator to accelerator memory accesses  are already established for scale up networks  that we talked about today. One could imagine in future,  memory, adding memory to these fabrics  or to the scale of fabrics  because they already have high bandwidth and a low latency. And then can potentially enable future novel use cases  like what happens if you can do near memory compute,  can you offload any kind of compute from the GPUs  to scale it across to the memories,  potentially a novel use case. In-memory checkpointing that we talked about. These are all potentially allowing you  to use increase the reliability  as well as the performance of the AI jobs in the future.

So to summarize,  AI is pushing the memory requirements across HBM,  across all the DRAM technologies,  across all the interconnects,  for the capacities as well as the interconnects. This is going to require a lot of innovation. We are going to use this technologies,  we are going to use these products  and the use cases are going to,  AI use cases are continuing  going to push the envelope on this. This is going to require new standards, system designs,  industry collaboration for such requirements,  use cases, benchmarks and shared knowledge. Various standards bodies are working on this. And a plug for this work being done in OCP  is Composable Memory Systems,  a large community jointly working on this. So the appeal for all of you is to join hands  to address this exciting and daunting challenges  by working with the SNIA and JEDEC and CXL and OCP  and make sure that we'll be ready for all the challenges  that AI is facing for our infrastructure. Thank you very much.
