
Welcome to the real-world challenges  between data and AI. Hello, everyone. This is Steven Yuan, CEO of StorageX.ai. Welcome to SNIA Compute Memory in the Storage Summit 2024.

In the past century, we went through a lot  of technology cycles, all the way from mainframe to PC  to networking to desktop internet, so-called Web 1.0,  and mobile internet as well, Web 2.0, and cloud and SaaS. Today, we're seeing the significant cycle  that is coming, so-called AGI. We call it AI 2.0. During this cycle, we are going to see a lot of changes  into the business models, technology developments,  and things that could change our life for the next decade.

So in the past 20 years, the CPU and GPU  growth basically followed Moore's law  and started to slow down recently  because of the scaling issues. And the data itself, however, is growing much faster  in a rapid case. Basically, it's followed the exponential growing pace  that because of all the things that I mentioned above,  because of IoT, because of mobile internet,  because of cloud business and the new applications coming  from AGI or tools that are needed to train large language  models, et cetera.

So by looking at these, we're seeing that per data volume,  the compute resource is really decreasing,  despite the piling of resources to be used for training  the large language models, for example, GPUs and other stuff  that needs a lot of silicons and devices to be used. But the data has been generated by the lifetime  and by the devices that we used and by the smart factories  and by the vehicles, smart vehicles that are now  on the road is more and more. So we're really seeing that bottleneck being pushed back  to the hardware stuff, for example, compute  now from storage itself.

So the ideal hardware or compute resource  is able to grow as fast as the data itself.

So this is the architecture being used for decades  because of separation of the compute and the storage nodes  left on the GPU, left for the GPU and the CPUs,  right for the storage. And the data being moved around these nodes. With data science being growing really fast,  it creates really heavy traffic and a lot of latency  itself for applications. The network becomes more important  because it's more important right now.

And narrowed down to the compute efficiency,  it's really proportional for the compute power  and the data acceleration, IO bandwidth,  and the negative impact comes from data volume and distance. So with that being said, moving compute closer to data  and the return results become more efficient to lower  the traffic and lower the latency. So data-centric computing is becoming more important.

So this is an example from the process of training  or a ChatGPT and such for a large language model. Step number one, supervised learning. So we need to collect a lot of demonstration data  and train a supervised policy from the beginning point. This data being used to fine tune the models  with the supervised learning. And then use that data for the reinforcement learning  that collect comparison data and train a rewarding model. That rewarding model being used and the labeler  can rank the outputs from the best to the worst. So that process is used to train the rewarding model. Step number three, proximal policy optimization. With that outputs, new property is sampled from the data set. And that policy against the rewarding model  use the PPO reinforcement learning algorithm. That policy generates an output and the reward model  will calculate output from a reward for the outputs. And then reward will be used to update the policy itself. So this is a very iterative process  from beginning to the end.

This requires a lot of data movements  and the compute resources as well  because the data is getting bigger and bigger  and the process is getting more complicated.

Another example is the smart factory. That's a real world case that intelligence  is needed for the efficiency. Back to the old days, automation was very important  for the factory itself because of the cost reduction,  because of the improvement requirement  for the production life cycles and factory capacity itself. And that being used to reduce the labor cost. And later, that digitization has been used for these factories. Tools like ERP, CIM, MES, MES systems,  and hardware like IoT devices and edge clouds. That those use the data to achieve the intended product  life goals, production goals, or operation efficiency  for the better business decision making. However, recently there's a development  with smart factories applying the artificial intelligence,  use the intelligence for self-monitoring,  automated problem solving, and identification  and solving, and management, et cetera. So this has been used for the lighthouse factories  to achieve higher capacities, higher yield, faster problem  solving, shorter life cycle, and more flexibility, et cetera.

So next slide, I'm going to talk about real world  challenges for data and AI. In terms of these machineries, automation tools,  in particular for AI, it's automated optical inspection  tools. It's going to generate a lot of data, especially images  that are being passed through the network  and saved on the storage nodes or data nodes on the SSDs. And these data will be used and transferred  through the network and be used for the analytics processed  by CPU and GPUs. So the real world requirement is it creates a big data set. There's a lot of large amount of data  being generated from those machineries,  and millions of files per day, and a cumulative petabytes  data per week. So each file could be a gigabyte size,  and less than a second for the decision making. So that challenge due to the infrastructure  will be storage in terms of write time, IOPS capacity  in network, in terms of latency, throughput, and distance. And a computer is process time. It all impacts the decision making stuff. Because of the short cycle time, some of the decisions  need to be made by seconds. So that's why that's a big challenge for the real world  hardware stuff.

So similar solution is proposed to do that  is transferring the AI and automated tools data  to a data and network processing devices all combined  that are able to generate some results before being  saved on the SSDs. And then can be processed through the time  when it's idle or when it's sitting around  for the further data mining and data processing. So it creates a opportunity that able to process the data  real time that's being transferred in. And also, the opportunity is to process that data afterwards. So without transferring data back and forth  from one server to another, especially between data nodes  and compute nodes. So the end result is it creates a faster decision making  process, storage, faster write time, shorter data paths,  network, lower latency, less data movements,  and compute, faster processing time.

So this is a benefit to the smart factory. And these real time data analytics  will be required for the key factors,  including low latency data fetching and pattern analysis  and faster common energy study to find the results. Because of the data feeds from the machineries  are very big and fast and important. So at the end of the day, faster data driven decisions  will result a quicker failure identification  and problem solving. And to the process itself, sometimes we  see a lot of hundreds of processes  during the whole manufacturing chain. And there's a lot of machineries involved. So finding the issues will be very important. And to result in higher yield and faster wrap up time. So quicker failure identification  is very important. So with that being said, each capacity, each factory  calculates the margins based on capacity times  yield minus cost. With this type of process being improved,  improved, implemented, we're going  to see a very high yield and faster yield wrap up time. So that creates a lot of spaces for the margin  for the factory itself during the product production cycle.

This is a holistic architecture from infrastructure-wise  all the way up to the SaaS platform  that they're able to learn to run this data collection,  process monitoring, retrieval, yield analysis,  and processing movement based on the hardware that  built for faster data processing itself. So we call it a smart data lake API. The infrastructure itself will collect the data  and save the data on the servers or data lake  that are able to be analyzed in the real time. That factories could be across the different industries,  LCD panels, foundry, solar, and battery,  3C products, et cetera. So this is very important in terms  of the industrial data mining analytics platform.

Next step, with that being said, all data  is saved on the smart factories. That creates a big data set. And combining a vertical large model,  which is next step for those domain-specific applications. That even better for the better intelligence  for those factories to create a better business intelligence. Smarter business decisions can be made through the process.

So the conclusion, data-centric computing  is really important for the data analytics workload. Today, we're living all the data with all the data. All those mobile devices, vehicles, and the machineries  generate a lot of data. So this data can be used to create better productivity  for smart and intelligent factories. Data can be very big and fast. We need a better architecture to handle this. One of the big challenges to the infrastructure,  because of the data-heavy workloads,  compute network and storage will be challenged. So compute and data can be used and can be very important  to solve those issues. Lower latency leads to faster cycle time  and leads to faster business decisions and better efficiency. Higher yield, lower DPPM, et cetera. So vertical large models can be very important. The next step for the domain-specific areas,  data can be accumulated, analyzed, but not sitting alone. AI-powered commonality studies represents  a significant advancement in terms of identifying  the patterns and similarities within large data sets  versus scientists or engineers doing that. So the fast processing becomes more important than ever. So we need to look into the new architecture  and opportunities to improve those processes. All right, so this is a wrap up for today's talk. Please take a moment to rate this session. Your feedback is very important to us. Thanks, everyone. See you next time.
