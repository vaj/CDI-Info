
My name's Andy Banta. I am a storage janitor. I'm currently working with Magnition, which is working on hierarchical caches and memory systems. This is Kin-Yip Liu, who's with AMD. I've spent a long time bouncing around the storage industry. I've been a storage developer for longer than this. It's been a conference and amazingly enough, this is my first time here at this conference. I've worked at a variety of different places, mostly on storage protocol work, but in some cases on storage things itself.

Good morning, everyone. My name is Kin-Yip Liu. Currently I'm with AMD. I've been working on a lot of the storage applications from a CPU vendor perspective, working on system architecture and performance analysis of storage workloads, wired and wireless applications, security applications workload. And previously I led architecture work on persistent memory modules, as well as new memory hierarchies and other kind of server networking mobile CPUs.

So this presentation has a lot to do with NUMA. And we're gonna look at different kind of memory, or we would like you to imagine different kind of memory topologies, hierarchies. Basically, there's a lot of innovation in memory hierarchy that will be enabled. So we start with like what NUMA, right? Non-unified or non-uniform memory architecture. In today's world, NUMA is pretty simple, right? In today's world, you have a server, you will usually like do a socket and okay, you go to the next socket, that's NUMA. And as you're gonna see, we're gonna enable a lot more variety of NUMA topology and we will need analysis. And that's kind of what the basis of this presentation is. And one interesting metric is NUMA distance, which Andy is gonna go through in more details.

So, in the previous talk, there was a lot of details about CXL. So I'm just going to do a very high-level summary. CXL is a driver for a lot of the new memory hierarchy innovations and feasibility and possibility, as well as something that we will need to consider more, especially from a system architecture and performance perspective. And I'll highlight some of those. Currently, a lot of the, if you look at what's implemented in existing silicon, so CXL 1.1 typically, for example, the first step as a memory module, I mean, you've seen other talk about, by addressable SSDs or other kinds of memory module based on CXL. So that's enabled by CXL.mem, right? The .mem gives you the better latency as compared to .io. It has the coherent characteristic as well. So that gives it a chance to add other kinds of memory to the system besides DDR. And then the next step with the 2.0, we're gonna see that you can pool memory. So why people want to do that is mostly TCO advantage and utilization of hardware. And there's a switching capability as well. And then when we go to 3.0, there's also sharing of data coherently by multiple servers or multiple compute. So there's a lot more, and a lot more switch fabric capability as well in the 3.0.


So what does that mean to the memory hierarchy, right? I mean, I'm sure a lot of you have seen this kind of, you know, a pyramid diagram. So between the top and the bottom here, you know, typically, you know, the top you see cache hierarchy, right? That's within the CPU, and the bottom storage hierarchy, which, you know, is in the system and different kinds of storage devices. In the middle has been just DRAM, right? And it's pretty simple. And what we wanted to basically give an intro here is that you're gonna see different kind of attachment of DRAM, right? Even if you just put a CXL module inside your server, that CXL module could have very different characteristic, you know, and some, there was a question in the last talk about whether you're gonna do it for capacity or bandwidth. You need to really consider those decisions as well as, you know, whether you want to interleave or not, and there are trade-offs. And then as we get into like pooling, basically, you know, a lot of the motivation behind it is how do we reduce the amount of memory on each server? You know, you can pull it, you know, put it in separate part, desegregate it in the rack, you can get better TCO by sharing, you know, and then there's persistency as well, right? So we need to be able to represent that kind of characteristic and account for it. And whether you see it as a fast storage or kind of a second-tier memory. So all these considerations need to be taken into account.

So what are the kind of consideration, you know, from our application and system architecture tuning perspective? The performance has a very big implications. You know, CXL gives you all these new capabilities, but it also gives you a big abstraction. You know, what is behind this module, right? Is it really an SSD? Is it a flash? Is it DRAM? What generation of DRAM? It has huge implications because today, you really don't need to worry about those things. Memories is the fastest main memory, and typically it's all uniform, right? Because you need to interleave all the memory channels. So with the memory modules behind CXL, first of all, it's going to have different performance characteristics compared to the DDR. And then second level is, you know, this could be very different, right? If it's a flash type device, typically your access granularity is a big consideration, right? Even though people might say it's byte addressable, but you know, even if you have just one byte, you're going to pull some amount of minimum granularity, right? You know, from a DRAM perspective, cache, you know, it's typically a cache line, 64 bytes, but memory and flash, the flash level typically is a page, right? So that you need to account for. And then also the read versus write performance is very different. You know, if it's a flash type of device, you know, read can be faster, but still, you know, much slower than DRAM. Write typically is a lot slower. And then there's also consideration of trade-off between the number of channels that a CPU is directly connected to, as well as, you know, what's considered the NUMA domain, right? So nowadays with CPUs with like more than a hundred cores, you know, like AMD has 12 channels of memory. You can actually partition even a single chip into like a multi-NUMA domain for further optimization, or you handle it by default is like the EPYC's socket is a remote NUMA. But with, you know, with additional memory types, you know, you could define, or you would actually want to define more NUMA domains. And then also how many DIMMs you want to put on a channel also affects your speed, right? So typically you might want to have the DRAM as your performance tier, right? So one DIMM per channel, and CXL become your capacity layer, right? That is one way to add capacity. But if you want to add bandwidth as well, you also want to consider whether you want to interleave these together. Then going to NUMA and how to track this, right? So you can probably imagine that you can have many ways to build different systems, both within a server node or a rack where you might disaggregated memory, maybe even connected multiple racks with switch fabric in the future. How do you put your workload, you know, in which node, where are you going to put the data? So all of that consideration needs to be considered. And you also want to be able to track how these data is used, right? Which one is the hot data, which one is not as much used. So from overall infrastructure perspective, typically you also want to consider collecting a lot of telemetries. You want to be able to do some way to track this data so you can optimize where to put the data. Because in a hierarchy, if it's closest to the memory, the main memory is highest performance, but if it's further away, the latency of accessing is longer, how much you need to minimum access size when an alert is different. So all of these needs to be considered. And this is really, we just want to bring it up as a lot of considerations and innovation is happening in the industry for how do we track these pages? How do we collect telemetries so that we can automate where you want to put the workloads from an infrastructure perspective? So at the end of the day, with CXL also, there's the accelerator perspective. So there's also been a lot of compute in storage talked in this conference. So that's also plays a part, right? Where you put the data, you know, maybe you can take advantage of some acceleration, some compute devices, which is part of storage. So the bottom line is there's going to be a lot more innovation and different kind of hierarchy, different ways of doing NUMA. So Andy's going to go through a lot of, you know, what kind of tools exist today and what can be used to analyze and simulate and model a lot of these infrastructure kind of possibilities. Andy.

Thank you. And as Kenny mentioned very early on, there are lots of different ways to access memory, and not all of them are created equal. And you as developers are going to be the ones who are responsible for paying attention to this and figuring out the best way to make use of the various different tiers that are available to you. At risk here, if you don't, is that you're going to end up with applications that run very inconsistently because they will be accessing memory from different tiers and you won't ever be able to get the same results. You won't ever be able to say this application runs with this type of performance. There are a handful of different ways of dealing with this. The two that I'm going to touch on most here are going to be segregation, where you can actually have applications that are assigned to various different tiers of memory. Much more interestingly, you can actually tier the memory in your system and use the various different tiers as caches. One of the things that you can actually do with the caching model is you can actually dynamically realign the way an application runs. So if you have a handful of applications running on a big system, you can move them around to different tiers of memory as necessary for ones that need better latency versus ones that need less latency.

Segregation, there's a handful of different ways that you can do segregation. Most commonly, you would do it at the system level, where you could tell your VMs to run in a particular NUMA node, or you could even split your VMs across various NUMA nodes. If you're using the other major operating system, Linux, there's a way that you can actually assign processes to a specific NUMA node. There are also ways inside an application where you can say, this application should be accessing different tiers of memory on its own, and it should be able to move things from one tier to another as necessary.

So if you slouch back into your developer chair or admin chair, one of the ways that you can actually do this with VMware is through the advanced configurations for a VM. So all you need to do is go in and say that you want to assign the affinity for this VM to a particular node or multiple nodes. In this case, it only shows one, but you can, through VMware, actually assign multiple NUMA nodes to a particular VM. In this case, the VM will always allocate memory from that NUMA node. If that NUMA node runs out of memory, your VM runs out of memory.

If you have a large VM, you can actually tell VMware to split that VM across several NUMA nodes, and you can tell it how many cores you want, how many virtual cores do you want in each NUMA node. And once you've done this, VMware will automatically allocate memory from those NUMA nodes for those cores.

Similarly, in Linux, there's the numactl command, and the numactl command allows you to do things like telling a particular process to use a CPU or a set of CPUs, as well as the memory associated with those NUMA nodes. If you don't actually assign a CPU, you can tell it to simply allocate memory from the local NUMA node rather than wherever it feels like. Unlike VMware, you can actually tell Linux that you would prefer a NUMA node, but if you exhaust the memory in that NUMA node, go ahead and allocate from a different node. In this case, you don't run into memory failures if you've actually exhausted all the memory on a particular NUMA node. And something that was talked about in the previous session, there is the possibility of actually interleaving between these NUMA nodes to improve your bandwidth.

I did mention that at the application level, you can actually use the SNIA Persistent Memory Development Kit to choose which bank you want to use. And even though it's designed for persistent memory, you can actually use this library to choose different NUMA nodes, even if they aren't persistent. Similarly, VMware has a way of telling a VM to use persistent memory. In that case, VMware actually is aware that it's really persistent memory, so you can't fake it out and tell it that it's just a different NUMA node is persistent and you use it that way.

More interestingly, you can do caching between the multiple different tiers. And there's a handful of different ways that we can do caching. The most common is that you would have a lower latency tier in front of a higher latency tier and everything in the lower latency tier is actually stored in the higher latency tier. Much more efficiently, especially when you're dealing with memory size class applications, is the idea that you can promote the memory from one tier to another or demote it, depending on what the application needs. I'm gonna show you that this actually becomes a huge complex problem when you get into multiple tiers of memory and multiple tiers of CXL.

So once again, when we, okay. First off, I wanna talk about the hit or miss type caching. If you're not familiar with it, typically you would populate the faster, the lower latency level with things that are in use, or like if it's a storage cache, if it's a disk cache, you would potentially predictively fill it. Like if you say, I'm gonna read one block, there's a good chance that the following blocks are gonna be used very soon. So you might pre-fill the cache with that. Keep in mind, there's a whole variety of algorithms for the various different components inside a cache. So you have different algorithms for allocation, for lookup and for eviction. Of course, like the allocation portion is gonna be very different depending on whether you're using a memory cache or an SSD cache. The way that you fill up memory in SSD is very different. An example I'm gonna show you later on is simply showing the differences between using two different eviction algorithms that actually has very big results. And the important thing here is that you, caches actually can be specifically tuned for the workload. You don't just say, this is the right cache layout for everything. You actually wanna know what your workload is and arrange the caches appropriately for this. So here we have a cache, and the cache is basically a decision-making engine. And the whole idea is that it gets an input from somewhere. And if what you're interested in is in the contents of the cache, it goes to a local media and gets it and gives it to whoever requested it. And we call this a hit versus a miss. And typically the miss would go to a much slower media, or we're just going to call it an origin media in this case. So I just wanna draw this out to give you an idea that most of the diagrams I'm gonna show you are going to have this hit or miss topology that you see here.

By contrast, the promotion and demotion method is less frequently used. One of the big reasons for this is it requires data movement, and the data movement can take time. The big advantage of it is that it's much more space efficient. You don't end up with duplicate copies of the data in multiple different tiers of your caches. And one of the things that was talked about several times at this conference is the smart data accelerator interface that SNIA has a working group for. In fact, just before this, I was in a storage field day presentation printed, presented by Shyamkumar Iyer, who was talking about SDXI itself. If you go to techfieldday.com, you can actually see that video and hear the 30-minute summary of SDXI.

So once again, let's slouch back into our developer chair and take a look at the way that we can actually use built-in caching. VMware has a way to say that we have multiple different tiers of memory, and one would be a tier zero and one would be a tier one. In this case, it is a hit or miss type cache, and everything that is in tier one is also going to be in tier zero. By contrast, Linux with the numactl command, again, allows you to do multiple tiers, but they allow you to do many more tiers. It's not just two tiers, and they do use a promotion and demotion caching method. They base this entirely on no distance that Kin-Yip showed you at the beginning, and you can say that you have a handful of different tiers and what your promotion demotion timings are and what you want your no distances to be.

So in everything that I've described here, you can see that there's lots of variables even in a single-level cache. When you get into multi-level caches, you end up with far more variables. These are currently used in content delivery networks, but they're going to be applicable to multi-level CXL tiers as well. This one is specifically going to be useful for a lot of different types of caching. It's specifically going to be useful for AI workloads or high-performance computing workloads like electronic design automation or medical simulation or places where you need lots of memory and lots of compute. You can see in this example that I've just tried to draw out a handful of the variable, the various different ways that you would, the various different pieces that you could have in a multi-level cache. You have a variety of different transports between a variety of different media and these caches might each have different algorithms in them as well. So the number of variables that you end up with in a multi-level cache becomes dizzying.

Let's just take a look at a CXL host. So this is what servers looked like 20 years ago. You had a CPU and you had DRAM attached to it.

About 20 years ago, we went to the idea that you could have multiple CPUs inside a server.

And this is kind of where the idea of NUMA came from, where you would have the two CPUs that talk to each other and memory access to each other's DRAM would not necessarily be consistent. You would end up with different latencies, depending on whether it was DRAM directly attached to your CPU or was DRAM attached to your partner's CPU.

Welcome CXL 1.1, where we have the idea that you can slap a PCI card into your host and you can end up with additional memory.

Or if you run out of PCI slots in your host, you can put an interface card in and attach a CXL chassis to your host as well. That's a lot.

And for that same $19.95, you get CXL 2.0. CXL 2.0 gives you the idea to put a switch out there and have CXL attached via a switch outside of your box through those same interface adapters or through different interface adapters.

And then we get the CXL 3.0, where we're gonna have another host that's potentially sharing that and CXL is going to be responsible for maintaining the coherency of that CXL memory between those various different hosts.

So let's take a look at this diagram real quick. We have one, two, three, four, five, six, seven different NUMA nodes that each processor is going to be able to see and have access to all with very different latencies. I don't know how many people were here for Tuesday morning's keynote, where there was the idea that you could attach SSDs through CXL. So we go anywhere from the idea of tens of nanoseconds of latency to memory to tens of microseconds or hundreds of microseconds of latency to memory, all with that's going to simply be memory addressable to that CPU.

Throw in that we have the contention with another host, at that point you end up with who knows what latency because you might be stuck behind a bunch of other requests. So there's quite a few variables here to think about and this is going to become very difficult to keep track of.

So one of the ways to actually figure out the best way to make use of these various different levels of CXL memory and memory tiering in general are to do modeling and optimization. These are gonna be workload dependent, hopefully because it varies on what type of workload you have and there's the idea that these could be either static or dynamic reallocation where you would actually build out a specific system and have the caches laid out in a particular way and have the caches algorithms laid out in a particular way or you could have a system that actually changes things on the fly based on what the workload is doing.

So I do want to point out that workloads make quite a bit of difference. Rather than trying to run iometer against the system and get an idea of what your cache should be, you actually should be running genuine workloads against it. I mentioned content delivery because this is actually one of the ways that multi-level caches are being used today, not in terms of memory tiering but in terms of storage tiering and access tiering. Things that do present interesting problems for memory tiering are inference workloads, learning workloads, even file sharing is an entirely different workload compared to both of those and at the same time file sharing is going to be potentially a memory-intensive workload.

So one of the things that Magnition, who I'm working for, is working on is the idea that you can actually do simulations of these systems and come up with the best initial configuration for these. The way we do this is that we do behavioral models of each one of the components, behavioral simulators. So each one of these blocks that's in this diagram, we will build a behavioral model for that models the latency that you would find and the contention characteristics you would find and the queuing characteristics you would find. So you can come up with a pretty good idea of how your system is going to work with the multiple tiers. One of the big things we're doing is we're pre-building components. So somebody who actually wants to use a simulation can pick and choose the pieces off the shelf. We have a handful of components that are based on open source projects. There are other things that are homegrown. And if you as a user of this system are interested, you can certainly build your own custom modules if one of the ones that's pre-built isn't ready for your use.

So let's think about engineering for a second. I mean, engineering makes use of simulations, all the nature of engineering. We have electrical engineering where you run months and months of simulations on ASIC layouts and print circuit board layouts and everything else to come up with the optimal way of doing this. Similarly, there's chemical engineering. Chemical engineering uses all sorts of simulations and all sorts of computation, typically in resource extraction, but there's other things that happen in chemical engineering as well. We have mechanical engineering where nobody bends a piece of sheet metal until you run all sorts of design analysis on computers. You've done material analysis. You've done strength analysis. All of this is done in simulations before you do take the first step. You have civil engineering, which is much of the same thing as mechanical engineering, where you, again, are going to do material analysis, strength analysis, those types of things. And then you get into the biotechnical engineering. It's cheaper and faster and more flexible to do the simulations rather than actually build the thing from scratch and say, "Oh, wait, that doesn't work." "I'm gonna scrap this car and start over." "I'm gonna scrap this ASIC "that cost a million dollars to tape out and start over." You need to, you run simulations on all of this stuff. I think it's about time that we start treating software engineering as a genuine engineering discipline and start doing simulations on our software designs and on our system designs and treat them like a genuine engineering effort rather than just somebody who's, you know, had too many Jolts and Coke Zeros and is sitting at their computer saying, "Let's see if this works." Simulations belong in software and system design.

This is what we're doing at Magnetian. We're actually doing simulations of software and system design. And this is an example of a diorama. I realize it's not readable. I'm simply showing it. But it shows the layout of a multi-level cache that we've built a simulator for, and we've modeled each one of the components in it separately.

So if we drill down a little bit, you can see these would be the two levels of caches, and it shows the various different inputs and outputs. These get fed into load balancers and various different media simulators and all the various different pieces. But each piece we've modeled individually, and we also modeled the latency of it. So when we do these simulators, we don't actually need to do the entire workload. All we need to do is simulate the latency that's going to be involved with each component of it.

And at the end of the day, we end up with matrices of how much latency we would see in a particular system. We use these very nice heat maps. So you don't even need to be able to read the numbers. If you just look at the map, you can get an idea of where things are higher latency and where things are lower latency and what the various different components are. In this case, all we're doing is comparing the number of caches. And in this case, we've used a particular caching eviction algorithm. By comparison, there's another matrix that I didn't include in this demonstration that shows a very different characteristic. And you can actually just look at the two heat maps side by side and say, hey, this one actually provides a better algorithm than this one. And it becomes a very easy way to do this comparison when we use our behavioral simulator for this reason.

This is a lot less interesting to show, but I will show it anyway. This is what you can do on a running system. So this is when you've actually have a prebuilt system and you wanna try to optimize your workload for this system. The idea here is that you can change the algorithms that are involved in some of the software and you can change the sizes of your caches. In that case, you can actually optimize the results that you see. In this case, you can't actually go in and change the components that are in your system and you can't change the interconnects in your system. All you can do is the things that are soft.

So when we start talking about things like AMD's newest generation processors that have 100 plus cores and 100 plus PCI lanes, these are not gonna be running single workloads. These are gonna be running multiple workloads and all of these multiple workloads are probably going to be using the system in different ways. They're probably going to be using CXL capabilities differently. Those PCI lanes might have CXL hung off of them. They might have NVMe drives hung off of them. They might have networking hung off of them and each one of those workloads might be using those heavily or might be using the CPU heavily. We don't know, but the way to actually figure out the best way to allocate the resources for those workloads is to be able to simulate what they're doing and figure out the best use of the resources that are available. We're just talking about one system here. I'm talking to people who are talking about building out clusters of eight to 16 of these systems. So you're talking about thousands of CPUs, thousands of PCI lanes and a huge number of terabytes of memory in this single cluster. So these are things that you can't just sit down with a piece of paper and a pencil and say, let's lay it out this way. You actually need to go through, simulate your workloads, figure out the best way to make use of them or else you're gonna end up buying four of these clusters when you could actually only need one.

This is where we're headed. This is just a couple years off. We're gonna have compute systems that have this type of layout, that have this number of NUMA nodes attached to them and we have to make use of those NUMA nodes the best way we can. So let's talk a little bit about what happened, what's been going on today.

CXL does offer a lot of flexibility. It means that we have a whole bunch of different ways of connecting memory to a host. At the same time, it adds complexity. We can't just say, oh, CXL offers us all this flexibility, but of course it will naturally fall into place on how I'm gonna use this. The operating system vendors are on board. The operating system vendors have an idea that this is happening and are offering capabilities in their operating systems to take advantage of this. This flexibility actually requires that you think about system design a little bit more than you have. System design has been a big thing for a long time, but nobody has actually thought about it in terms of tiering memory. Up until now, it's always been, this many CPUs has this much memory to work with and that's just been a fixed thing. Not true anymore. You're gonna be able to have all sorts of system design questions around memory. And the only way that you're ever gonna be able to figure out how to make the best use of this and optimize your system design is to actually go through and simulate what you're doing rather than sitting there and plugging in CXL cards until something works right and figuring out the proper way to segregate things. So there's a handful, there's a bunch of different things that you can do with CXL, but the flexibility and complexity comes at a cost.

So I wanted to mention that Magnician has been working on this problem for a while, even before CXL came along. CXL has worked with a major vendor who is here at this conference today, who I really can't name. And before we started working with them, they had a team of engineers, a sizable team of engineers. And that team of engineers was running about two experiments a day. We went in and worked with them on our ways of running simulations. And it got up to the point where they were running tens of thousands of experiments a day. Over the course of a several month engagement with them, we ran over a million tests. And the end result was their product that shipped had a double-digit performance increase over what they had had before we started working with them. So this is not an idea. This is a working concept that has been used by a major vendor.

Kin-Yip, you wanna talk about your company?

Yeah, so as you've seen from this talk, there's a lot more things the industry can do with CXL and a lot more need for tools. So we just wanted to reiterate that AMD is very strongly behind CXL as an industry standard. We are very active in the industry as well as the standard bodies. We have CXL supporting our products as well as we are pretty actively working with the ecosystem, for example, and this company. Yeah.

Thanks. And yeah, please do take the survey for this session. I know Stena likes it when they get ratings in sessions, and they actually do give us that feedback. So please let us know what you think. If you thought this session sucked, I wanna know so I can make it better. Thanks a lot.

Thank you.

 And we have some time for Q&A.

Yes, absolutely.

Yes?

I'm interested in having a more dynamic algorithm that can handle motion or captioning or LRU or MRU and all those sorts of capabilities that can react in real time. Would it be sufficient to do experiments and lock each of those cache levels or whatever the current workload simulation optimized?

Yes. So the question is, wouldn't it make more sense to do this dynamically rather than in a set configuration? And the answer is yes, that certainly works. And yes, we have demonstrated that we're capable of doing that. At the same time, the question that we as a company get asked more often is I wanna build this thing right from the outset and I don't wanna just slap a bunch of stuff in a box and hope that it's the right configuration for a customer. So we actually are getting asked by particular people, it's like, I'm planning on running this type of workload. Can you help me build the box out to be the right box for this workload?

Those workloads are changing over time. They're not like the status. I mean, you can't train, right? You train, last year was seven billion parameters, large language model, next year is 146 billion parameters, large language model. Those workloads are not static.

True. Go ahead.

Yeah, sorry, maybe I didn't take a cut in. I mean, like the caching, and I mean, this is, I think Andy's talking about like a whole infrastructure with racks of servers, storage systems, and certainly a lot of software is running on top. I don't think we are talking about fixing the cache algorithm. It's not like we're fitting a hardware, it is an entire infrastructure. So certainly these algorithms are flexible and software needs to be managing. And I completely agree that dynamically managing this, taking the heuristics, so that the kind of the management layer of the infrastructure needs to be able to like, okay, if it's this type of workload, we have a lot of telemetries to know it has this kind of characteristics on like compute requirement, data, how much data to take, and how long this data stays active. So that can be fed into having a dynamic environment. And it should be dynamically adjusting to the exact workload, the type of data, and also collecting data. So I think you can even use like AI to manage the infrastructure and apply all these dynamically.

Yes.

Super interesting talk, thanks for the content. Question about the optimization though, it's interesting how you kind of did that with CMS. The results of the simulation. How do you kind of go about choosing what's best for your users? It seems like it's a smaller kind of theory as well, that the numerical value is better than that. Is it more of a one-line interpreting, like subjectively what CMS looks like? Or is there a formula for that?

Yes and no. Actually, the way we approach it is an iterative approach, where we'll have, I'm sorry. The question is, how do we go about figuring out what the best configuration is? Is it done subjectively? Or is there actually like some analysis that goes on? And the way that we actually come up with the best way, the best simulation is we run these iteratively. So we run a handful, and at a given time we typically will run with a baseline, and a set of various different variables associated with it. And out of that, we will typically pick the best run, and then do the same thing with another set of variables. So we do run this iteratively, and that's why we're talking about millions of different experiments. It's not just like you get a million different heat maps. We actually go through and say, "Okay, here are the results we got from this series of runs, and let's pick out the best results, and run it against a different set of runs, and come up with the best one." I use the heat map simply as an example of, if you look at the heat map, you don't even need to read the numbers. And I wish I'd actually put the other one in there, because two of them side by side are like entirely different spreads of the colors. And it's very clear which one gives you better performance. And if anybody's wondering, you want more blue than red. But yes, and it's, I mean, no, you can't sit there and look at a million different heat maps and say, "Here's the best result." But at the same time, you can look at, you can take a look at the two or three heat maps that would be generated from a couple hundred thousand runs, and say, "Okay, out of these couple hundred thousand runs, here are the ones that look the best." So that part is the very small subjective part of it. If you don't want to look at the heat maps, you could just run your set of a hundred thousand runs, and it'll generate like, "Here's the algorithm that was best for this workload." And one of the reasons why you would potentially want to look at the heat maps is if you had like a set of three workloads that you were running against, you might actually take a look at the heat maps from those three different runs and get an idea of where your middle ground might be. More questions? Did you learn anything today? Okay, thanks.

Thank you very much.

