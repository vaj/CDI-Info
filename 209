
Good morning. Thank you all for joining us for the CXL Consortium Memory Challenges and  CXL Solutions webinar. Today's webinar will be led by Chris Peterson, CXL Consortium Director  and Hardware Systems Technologist for Facebook, and Prakash Chauhan, CXL Consortium Director  and Systems Architect for Google. Now I will hand it off to Prakash to share today's agenda.

Good morning, everyone. I will just talk about CXL, but I will start out the talk with the  memory trends and challenges that are facing the industry. And this will be followed by  some discussion on how emerging memory technologies can help with some of these challenges. At  this point, I'll hand over to Chris, and he will discuss how CXL provides a great ecosystem  to solve these memory problems. And he will also introduce the CXL.mem flows that are  required to support memory expansion devices.

So this first slide, I want to highlight a trend that shows that memory costs have basically  stagnated after scaling down nicely for a period of time. And so basically, cost per  bit used to drop year over year. But starting about 2013 or so, the memory prices have flattened,  and this causes an impact to system level cost because memory is an increasing fraction  of system cost. And the demand for memory keeps increasing because the processor vendors  increase the core counts to drive performance up. But the upshot of the memory costs staying  flat and memory demand increasing is that system cost keeps scaling with the increased  performance, and we end up with not so good performance per dollar.

So next, I'll sort of speculate on why memory cost per bit is not dropping anymore. And  for that, I'll bring up the picture of how DRAM works. And basically, it is a charge  storage device, a capacitor. And the capacitor stores charge that indicates whether it's  a one or a zero. In the good old days when planar technology worked, we had – the transistor  was – transistor and capacitor were both on the same plane. But as we scaled the technology  to smaller and smaller dimensions, we still had to retain the capacitor's value to store  the charge. And that meant that the capacitor grew vertically. And today, it's like a tall,  skinny structure. And if you look at the aspect ratios, they are kind of exceeding the 50s  to 1 aspect ratio. To contrast that, the tallest building in the world has an aspect ratio  of less than 10s to 1. So there are several problems with this current situation. One is that fabrication of this  kind of tall structure is difficult and costly. Also, in order to keep the capacitor's value  stable, we have to increase the dielectric constants and materials that are used for  dielectrics. And that makes the capacitors more leaky. So the net result of all this  is that your transistors are leaky here, your capacitors are leaky here. Things are crammed  together a lot, so there's interference from cell to cell, from wires to cell, and so on. And that leads to variable retention time on these capacitors. And this is now resolved  by using On-die ECC, which then adds even more area overhead. So essentially, DRAM scaling  has become pretty difficult.

So the next trend is that the memory bandwidth also needs to scale up as the CPU core counts  increase. But over in the last eight years or so, what we've seen is core counts have  gone up by a factor of three. However, the memory bandwidth for these systems has gone  up only by 2x. What that means, and it's shown in the red curve, is that the memory bandwidth  per core is actually diminishing. And at the same time, we have applications and workloads  that require more memory bandwidth per core. So we have a shortfall here.

And the challenge is that to add memory bandwidth to a system, you have a couple of ways to  do so. One is you add more channels to the CPU. What that does is, because each channel  of DRAM requires hundreds of pins, it basically bloats the socket size. So now we have sockets  in like 5,000 pins, 6,000 pins. And these kind of sockets are not only expensive to  make, the contact reliability and all that also becomes quite hard to solve. Second issue with adding channels is that you have to route those channels. And that  needs additional PCB layers, which is also a cost adder. The third thing is to accommodate  all these DIMMs that you're adding because of the channel add, you have to figure out  how many DIMMs you can place while still being able to fit in a standard rack width. The second way of increasing bandwidth is to increase the data rates. And we've seen  that happen with DDR3, 4, 5. Each generation brings a faster speed loss. But this is not  trivial. I mean, this is pretty hard. You have to do advanced PCB technology items,  such as back-tooling vias using surface-mount and low-pin capacitance connectors and exotic  via technologies in order to make sure that the signal is not distorted significantly. The other side of this increased data rate is that in a multi-socket system, you also  have to scale the bandwidth between sockets so that the remote socket can get reasonable  bandwidth to the remote memory. And that requires additional layers and low-loss materials to  support. Also, in order to get data rates higher, more digital signal processing is  required, which adds cost and complexity to both ends of the device and the controller. And finally, as data rates keep going higher, two DIMMs per channel is becoming increasingly  difficult to support. And that brings its own issues in terms of how to support different  size DIMMs to get the system capacity that you need. You cannot have mixed DIMMs in a  channel anymore because you only have one DIMM. So then the only way to get capacity  is to go with exotic DIMM types like LR DIMMs or 3D stack DIMMs.

So this – we'll kind of switch into emerging memories and what role they have to play in  reducing the cost per bit aspect that we discussed in the earlier slides. So because DRAM is  not scaling well, if we could substitute it with a different, cheaper memory, that  could help us solve some of the system-level cost problems. So this pyramid might be familiar  to most of you. It shows faster and more expensive memory types towards the top and cheaper but  higher capacity and more persistent type of memories at the bottom of the pyramid. But  the section that we're more interested in is the newly emerging memory types, which  are slower than DRAM but much more cost effective. And they are fast enough to be used in a load  store model. And I've highlighted two tiers here because the nano RAM and magnetic RAMs  are generally lower latency compared to the storage class memories that are resistive  and phase-change material based. So the other thing that's different about them is that their access times are more symmetric  whereas the ReRAM and phase-change memory are more asymmetric read and write times. So we are trying to figure out how to fit these type of memories into our system memory  hierarchy and lower the system cost.

So one thing I wanted to kind of visit before we get into the integration into the system  is why are emerging memories better positioned to scale beyond today? And to show that I  show like on the left there is a memory array and around it you have peripheral circuits  like column and row decoders and sense amplifiers. And when we move to emerging memories, the  way the memory is implemented is not on silicon but based on the metal technology that we  have today. So you essentially have word lines and bit lines that are orthogonal to each  other and the memory element is sandwiched between them. So they sit above the silicon  layer and the silicon can be used to implement the peripheral circuits. So that's step one. Step two is that you can, because these things are not the aspect ratio of DRAM, you can  actually stack them on top of each other easily. So you get density increased by putting logic  under the memory and then you can stack multiple levels of memory. And finally, these technologies  scale well with feature size. So you can go to a new lithographic technology and scale  density that way. So they can keep growing in density for a while to come.

Now the question that I asked before was how do you actually put this into our systems? And today one approach is to have the emerging memory sit on a DRAM channel as a DIMM. But  there are certain problems with this approach. One is that you might have to share that channel  with a normal DRAM-based DIMM. And the other option is to put it on a channel by itself. But as we saw before, we want to use all the bandwidth possible for DRAM, high-performance  DRAM. So the general conclusion there is that you have to share this with another DRAM. But these new memories, especially like the phase change and resistive memory, are often  transactional in nature in that they have non-deterministic timings because you might  be doing some kind of wear leveling operation. Or also they don't have the same read and  write timings. So having the memory controller to support these two different media types  becomes very problematic.

So here we're talking about the kind of problems that occur if you try to solve this, to put  different memory types on a DIMM-type solution. So I already mentioned that we have transactional  DIMMs that have problems coexisting with the regular DDR DIMMs because A, they lower the  bus frequency in a 2D PC config, or you have to lose the channel bandwidth. Secondly, you  have to arbitrate, the memory controller has to arbitrate between requests to DRAM and  to the slow memory. And that impacts bus efficiency. And finally, you have interference between  the DRAM and this new memory type, which causes the new memory type to steal bandwidth from  your high-performance memory and increase latency. Also in the current DRAM technology types, even if it was not transactional, we would  have a problem because the memory controllers work best when you have the same generation  of DDR. I mean, this is a file limitation actually because you cannot put a DDR4 and  a DDR5 device on the same channel. Also, the speed and speed rates and timing have to be  exactly the same for controller implementations and simplification. And the devices have to  be the same geometry, otherwise you cannot interleave them, interleave data across all  channels uniformly, which is kind of important to get high bandwidth and low access times. Also, the DIMM form factor limits us in terms of what power and thermal headroom we have  in putting different memory types. So at this point, I'll hand over to Chris to show how these problems are kind of helped  by CXL. And take it from here, Chris.

Thank you, Prakash. So overall, CXL provides us with a number of opportunities to help  address some of these challenges. So CXL is a memory agnostic and coherent interface. And in this context, when I say coherent, what I mean specifically is that we're able  to map the underlying memory devices into the system address space so that we can do  loads and stores directly to that memory device. The other area that this helps us address is really address some of these specific system  challenges that Prakash walked us through earlier. And I'll get into more detail on  how we can do that in just a minute. And then finally, CXL allows us to explore new and different compute and memory architectures  because we now have the opportunity to separate out the memory controller or memory controllers  from the CPUs themselves. So we can scale them independently. We can also build different  types of heterogeneous compute platforms.

So overall, CXL actually spans a wide range of possible use cases. For those of you who  are perhaps less familiar with CXL, at this point, we have three different device types  defined, type 1, type 2, and type 3. Specifically for this presentation today, we're going to  focus primarily on what we call type 3 devices. And these are basically memory buffers or  memory expansion devices that allow us to provide memory bandwidth, memory capacity  expansion, or access to other types of memories for system media and so on.

So CXL provides us basically with a common standardized interface that allows us to put  any number of different types of media or memory behind that interface. So now we have  the opportunity to have a common platform interface with CXL, but then build CXL memory  expander devices that have media-specific controllers. So for example, this provides  us with the flexibility to support a variety of different memory devices. Those could be  DDR3, DDR4, or DDR5 DRAMs. It could be low-power DRAMs, or it could be persistent memory components  like some of the ones that Prakash walked us through earlier. Or it could be some new  media type that has yet to be defined or released into production. The benefit here is that now that you have a media-specific controller, you can address  some of the specific challenges of those media types directly. So now you can have a specific  controller for each type. So if there are asymmetric timings or non-deterministic timing,  media-specific error handling mechanisms, or other needs that a particular type of memory  requires, you can do that with these media-specific controllers. With CXL, on the system side of the interface, we now have this common standardized interoperable  interface. And you can design your platforms such that you can basically provide common  slots. And you can install any of these types of devices without having to worry about some  of the details of the specific media, as it's a common interface. And then you can basically  mix and match different device types from there. So overall, it provides us with a great degree of flexibility here so that for different  use cases, we can apply the different media characteristics appropriately. So whether  that's the persistence characteristics, different bandwidth latency characteristics, or even  different endurance profiles, for example.

So Prakash talked a little bit about some of the challenges with attaching different  memory types in the same fashion on a DDR bus, for example. So ultimately, DIMMs themselves  are not necessarily the ideal form factor for all of these combinations. And the way  CXL helps us address some of this is that now we have the opportunity, basically, to  isolate different tiers of memory from each other. So we don't have to share a bus anymore,  and we can put the different memory types on a CXL interface, for example, which allows  us to keep them completely separate from the direct attached DDR DIMMs that we typically  use. The second benefit here is that this really allows us to scale up the memory bandwidth  on a particular platform very efficiently, because we can simply scale memory bandwidth  by scaling the number of CXL links that we provide on the platform. And this is a much  simpler approach than trying to continue to scale DDR channels. It also provides a secondary benefit, because now we can do things like have one platform  support direct attached DDR5, but also have DDR4 on the same platform for bandwidth expansion  for specific use cases. And then finally, CXL really allows us here to continue to scale capacity as well. So  similarly to bandwidth expansion, we can scale the CXL links up, and we can provide a large  amount of additional memory capacity if the application demands it, just by scaling up  the CXL links and the CXL memory devices attached to those links. So we're not going to be limited  by the number of DIMMs per channel or even the number of ranks per channel, because we  can scale those independently of the standard DDR channels. Ultimately, we also would expect that this should be a more cost-effective approach to  scaling, because we can focus on optimized buffering solutions for CXL that don't have  to rely on large parallel high-speed buttons.

So in terms of some of the other system challenges – so Prakash alluded to some of the challenges  with the power and thermal limitations of DIMM envelopes. So typically, DIMMs have to  be placed very close to the CPU. They're very tightly spaced, because the electrical requirements  are very stringent, especially as we go to higher speeds. And so that, as a side effect,  really limits the power and thermal envelopes that we can do with a specific DIMM slot. And typically, that's somewhere around 15 to 18 watts, and often even less than that  in more dense platforms. And that works okay for many DRAM applications, but in many cases,  some of the emergent technologies really want to see a higher power envelope, especially  to address some of the capacity challenges. And so we need a way to separate that out,  and CXL enables us to do that, because now we can build basically a platform that has  a DIMM slot that is optimized for DDR DRAMs, for example, and then CXL memory slots, which  perhaps could have a higher power envelope – you know, it's 25 watts or more in many  cases. And we can also optimize the cooling solutions for those particular types of devices,  versus trying to create a one-size-fits-all solution. And then the second challenge here that Prakash was talking about was just in general in terms  of scaling the memory channels themselves. So a parallel DDR interface requires a little  over 200 pins just for one channel, basically, whereas with CXL, we only need about a quarter  of that. So ultimately, that means we either end up with a lot less packages and less pins  and signals per package or per socket, and we can build a smaller socket and smaller  packages, or it allows us to pack a lot more potential channels into the same footprint  as we have in the past. So if I look at that more closely, that definitely has PCB layer count implications. For example,  just to give you a rough idea, a six-channel motherboard today is on the order of 12 layers. As you move to eight channels and up, you're a 16-layer motherboard, and from there, you  know, 10-plus channels, you're easily over 20 layers, which not only starts to become  expensive, it also becomes a rather thick and challenging board to deal with. And that  drives things like the back-drilling and some of the other PCB technology improvements that  have to be implemented that Prakash mentioned at the beginning. And then finally, because we're now talking about a serial interface with a much longer  reach potential, you can also place the CXL memory in different locations. It no longer  has to be sitting right next to the processor. You can put them in more optimal locations,  which allow you to also optimize things like the airflow across these CXL memory devices. And that by itself will also drive the ability to support higher power, for example.

So let's take a look at what a potential example solution could look like. So in this example,  let's assume that we have a couple of different application requirements. In one case, we  have the need for a relatively small amount of compute, and therefore, a relatively small  amount of memory bandwidth, or a relatively small amount of capacity footprint. On the  other hand, we have an application that has very high core count requirements because  it's very compute intensive, and therefore, it also drives higher bandwidth requirements  and higher capacity requirements. But in this example, what I'm showing here is that we have the potential now to create  a common motherboard between these two applications. So what we can do on the left here is basically  route out four standard DDR5 channels, for example, and that would be sufficient bandwidth  and likely sufficient capacity for the lower compute use case. And then on the right, because  there is higher core count, we can now expand the capacity and the memory bandwidth by adding  six additional CXL links here, and those can connect to the additional either DRAM-based  or other memory-based memory modules. And that allows us to scale up that capacity and  bandwidth that I mentioned earlier. The benefit here, of course, is with this common motherboard, it'll be a lot lower  layer count when compared to trying to route this with 10 DDR5 channels, for example. We're  talking well over 800 less pins and signals that need to be routed, not only through the  CPU package out through the socket, but also on the motherboard, of course. And so it should  allow us to create a much more optimized motherboard and system solution here. And then finally,  I can place those CXL memory modules in a more optimal location. And we can also dial  in the memory bandwidth per gigabyte that makes sense for the application with those  CXL memory modules, because now we can install not just DRAM components here, but also some  of the persistent memory options, for example. And we can really mix and match as the application  requires.
 
So I'd like to spend the next couple of slides walking you through a little bit on  some of the basics in terms of how the CXL Type 3 devices actually interface with the  CPUs and some of the flows involved. So first, let's start real quick with a couple of basic  pieces of terminology. In the spec, when we talk about these flows, we talk about a master  and a subordinate. And in this context, the master is basically the CPU that sends the  CXL commands, and then the subordinate is the Type 3 device or the CXL memory device. So let's start with a basic memory write flow. So the host or the CPU will send a memwrite  command. There are a couple of variants of this, but basically, this is issued as a request  with the data. And in some cases, this may include some metadata as well. This is a little  bit less relevant in most cases for Type 3 devices, but applies to some of the other  device types. And that metadata could be used for tracking coherency state, for example. So when the memory device receives that, the memory controller will send that out to the  media itself as a write operation, typically with some ECC as well, and then include the  metadata if it's available and if it's supported. Then the media device, if needed, will send  back a completion to the memory controller. And then the CXL memory device will send the  completion with no data response back to the host, saying that the write has completed.
 
All right. So the next flow we will go through is the memory read flow. So it's similar here  as well. So we send a request with no data in this case as a memory read. Typically,  the metadata here is sent as a NOP, assuming it's supported. The CXL device memory controller  will then send the read to the media. The media will respond with the data and ECC if  applicable in addition to the metadata. And then the response is sent back to the CPU  as a response with data. And the metadata here is basically just any state, as it's  not as applicable for the read flow. 

And then finally, there's also the case where we may  have a memory invalidate that we need to issue. This is typically used in a couple of different  cases, but one relevant example here is that it could be used to tell the device to flush  its caches out to persistent media, for example. So the way this works is the CPU will send  a memory validate command. Generally, that includes some metadata in this context that  uses the value zero. The memory controller goes out and reads the media itself to determine  what the current state is. The media will respond back with the data plus the ECC and  the current value of that metadata. The CXL memory device will send a completion with  no data back to the host. And this is just a completion. There's no additional data component  here. And this will include the old value of the metadata. And then the CXL memory device  will issue a write with its associated ECC and the new metadata value. And then finally,  if needed, the memory device will send a completion back.

So now that we've walked through the basics of the Type 3 memory flows, I just want to  come back to the storage pyramid that Prakash brought up at the beginning. So what I've  explained here is that CXL offers us the opportunity to help provide an interface and a new attached  solution that covers a big portion of this pyramid here. So everything from DRAM through  the NRAM/MRAM, 3D cross points and phase change memories, most of the emerging technologies  we expect we should be able to attach with CXL.

Of course, this is just the beginning. There's quite a lot of work that has to happen here  as with any new interface. And there are a couple of specific areas that I think are  worth highlighting here. Ultimately, we're going to need a lot of help from the industry  and from a lot of the consortium members to make sure that what we build is truly a strong  ecosystem and we address a lot of these challenges. So on the hardware side, for example, not only do we need to continue to grow the ecosystem  itself, we need to go off and build a number of compatible devices. We need to work through  the interoperability here. We have a pretty long, good track record of developing interoperability  for PCI Express. And so we need to continue that here with CXL as well. So we've talked a fair amount today about form factors or some of the challenges with  form factors. So there's clearly a lot of development and alignment work that has to  happen there to help us work through some of the power, thermal, mechanical and management  interface pieces there so that we can build form factors that are suitable for some of  these emerging applications. Then we also need to spend a lot more time evaluating and characterizing the memory latency  characteristics here. We are going to be adding some amount of latency as you add a CXL interface. And so developing benchmarks and tooling around this to help us really dial in what  the right characteristics and the right designs are is going to be very important. Finally on the software side, we need to work through a number of challenges there as well. Because of the latency difference that I mentioned, we're going to have to think about non-uniform  accesses to that memory. So we need to think about how do we schedule jobs that may take  into account latency, bandwidth or capacity characteristics. We need to think carefully  about interleaving approaches to ensure we get the maximum parallelism we can do, but  without ultimately impacting the aggregate performance. RAS here is going to be also an important topic. We need to think about how do we do  unified error reporting and handling for these different heterogeneous memory types. How  does that get exposed to the system? How do we log it? How do we respond to them and so  forth? Security is an important topic as well. We need to not only think about the security  of firmware running on these devices, but also media security. So your data at rest  solutions because we may potentially have pluggable devices, for example. So we need  to be concerned with some of the security challenges associated with that. And then different media devices will have different integrity and isolation challenges  as well. And that also translates into quality of service effects that we need to look at. And we need to look at this at both the link and media level and understand the potential  interactions there at the system level.
 
So in summary, CXL allows us to build various memory tiering solutions with a variety of  media types, including the different emerging technologies. And it allows us to do so in  a media independent fashion while addressing some of the scaling challenges both from the  media side as well as from the system side. And finally, it allows us to build a future memory roadmap here by providing us with new  opportunities and new potential architectures so that we can continue to scale the memory  capacity and bandwidth in the most efficient fashion for our future server generations. And with that, thank you. I will turn it over back to Elsa.

Thank you, Chris and Prakash for sharing your expertise. We would like to encourage our  viewers who's interested to learn more about CXL to join the CXL Consortium, download the  evaluation copy of the CXL 1.1 specification, and engage with us on Twitter, LinkedIn, and  YouTube. We will now begin the Q&A portion of the webinar. If any of our viewers have  any questions, please submit via the BrightTalk chat window. We will start with the first question. Will CXL latency be similar as DIMM connected directly  to CPU?

 Chris, do you want to take that?

 Sure. I'll take a first shot at that and then feel free to jump in. So, as I alluded to  earlier, there is going to be some amount of added latency here. You are moving from  a CXL interface and having to translate that to some other media-specific interface. So,  there will be some amount of added latency. Having said that, CXL was designed from the  ground up to focus on minimizing that latency as much as possible. So, all of the layers  in the CXL stack were designed for low latency. So, the focus will be to minimize that as  much as possible.

Yeah. I think the goal was to have it look like a remote socket memory latency if it  is actually DDR that you are trying to attach to the CXL device. So, it could be like a  NUMA node that you typically see on a dual socket system.

 Another question is, is it possible to interleave multiple CXL channels to improve CXL bandwidth  usage?

So, I'll take a first shot at this as well. So, the interleaving mechanisms are really  application-specific and implementation-specific. Having said that, there is nothing in the  spec that precludes you from enabling interleaves across devices or even within the devices  themselves. So, it is really an implementation choice.

 What is the purpose of metavalue? Is it for cache coherency?

Yes, that's correct. So, that's the typical usage. So, you can share the state of the  line that way. So, whether it's exclusive, shared, or any, for example, or depending  on what your coherency protocol is, that's the intent is to provide the state of that  line.

Do you see a value for small capacity, low latency, NVDIMM-P as a complement to CXL expanded  memory, which will have a higher latency than DDR? This could be lower latency persistent  memory such as MRAM, NRAM, or DRAM buffered small amount of PCM, ReRAM on NVDIMM-P.

Okay, I'll take this one. So, yeah, I mean, the point of CXL is to provide options. So,  you can still continue to do innovative stuff on your DDR channels, but you have to be careful  that this other memory that you're attaching does not affect the bandwidth and latency  aspects of the DRAM that you have, because then your system starts seeing hiccups. And  also, you don't want to lose bandwidth for your DRAM. So, you have to basically see how  – whether your DRAM controller can handle, like, different timings and still be able  to meet the high performance, high efficiency, high bandwidth, low latency character.

 What is the expected relative latency to CXL attached memory compared to DDR attached memory?

 I think we've already covered that. Go ahead, sorry.

 Right. Yeah, exactly. So, I think Prakash answered this really well earlier. I mean,  obviously, the exact number will vary based on specific implementations, right? But as  Prakash alluded to, it could be on the order of a second socket NUMA node latency.

 What is the granularity of the mread, mwrite number of bytes cache line OS page?

Yeah, so it's – the CXL.mem portion is cache line based, right? So, all of these are reads  and writes of your cache line granularity.

Does CXL support atomics? Can a processor issue compare and exchange type of common  atomics ops over CXL?

Chris, I'm not sure if I know the exact answer to this. But I assume that you should be able  to do these kind of operations.

Yeah, I mean, you just have to think of it as loads and stores to memory, right? So,  it's not anything special outside of that. You don't need any special operations like  you might have to do with IO atomics.

Are we enforcing app developers to change the applications to support CXL-based memory?

So CXL memory is just added to the system address space. So, depending on what you want  to implement, it could be fully transparent to applications. Of course, that is an implementation  or a system or a platform choice. You could also make the choice to expose it as different  regions, for example, different NUMA regions, similar to the way we do things with multiple  CPU sockets today.

 I think if you want to get the most out of it, you might – sorry. If you want to get  the most out of your system, you would have to probably consider the different tiers and  do things accordingly. But if you're not really worried about – if you just want additional  capacity and bandwidth, and you can deal with different latency and characteristics for  different memory addresses, then you can make it completely transparent.

Why do we need the meta value and the meta data for type 3 devices?

So those are optional fields, so you don't necessarily have to implement them. It's a  choice. There might be some use cases that would still want to use those if you want  some degree of coherency engine still within the device, but they're not required.

Do you expect memory device directly interface the CXL?

 I mean, it's not forbidden. Go ahead, Prakash.

I think that's an implementation choice, right? But I mean, the general understanding I have is that you want to keep the memory and the  media controller separate, but one could integrate everything in one device.
 
Will latency time crossing the PCIe bus be reduced? Thinking about low latency requirements  and HPC or HFT.

I'm not sure if the question is about PCIe or CXL. For CXL, by making the flits and having  different channels to send those flits over, you can achieve high – sorry, low latency. But PCIe is not the topic that we're discussing here, so I'm not sure whether the question  meant CXL or something else.
 
Does a CXL write command specify a column row address, or does the media controller  assign the location in the media?
 
Yeah, so columns and row addresses are media-specific definitions. So that is handled typically  by the media controller or the memory controller itself.
 
A compromised endpoint shipped with a root kick will mean access to the entire or most  of the SAM. How do we mitigate these challenges?

So I think in general, security of platforms, regardless of the interface, is a very large  and deeply discussed topic in the industry today. And there are a number of solutions  that are in flight to help address this, and CXL will follow industry best practices, as  well as some of the other improvements being implemented in other standards.

Is there any difference of meta value meaning between Type 2 and Type 3 devices?

 No, I mean the CXL.mem commands are the same, whether it's a Type 2 or Type 3 device. It's  just a different implementation, and in some cases, a different response, if necessary.

So in slide 6, it shows an independent memory channel on CXL. Is that an expected channel  count choice among platform vendors? What is the expected size of each memory module  over CXL?

I think we will probably not be talking about specific product and platform offerings. You  can basically have to talk to your processor vendors to find out what they're planning  to offer.

Any thought on CXL link efficiency? For example, actual data rate versus physical link bandwidth?

I've seen some estimates. It obviously depends on how well your traffic packs into the various  CXL FLIT types, but if you were to compare with a PCIe DMA kind of application, your  efficiency is lower than what you get on PCIe, because this is cache line size transfers.So for bulk transfers, your efficiencies can be in the 70, 80 percent range. And that's  okay, because the data rates start at 32 giga transfers per second. Chris, do you have additional insight?
 
Yeah, I think that's a good estimate at this point. To Prakash's point, though, it is going  to be workload dependent, right? And also, to some degree, implementation dependent in  terms of what specific optimizations might be implemented. So I think we will know better  as devices start coming out on the market.

I assume that the system mixes and matches, say, DDR5 and DDR4 over CXL, for example. That would create non-uniform latency and non-uniform bandwidth, right? Total bandwidth  scales, but if it would create software work to account for difference in bandwidth and  latency between different memory types.

Yeah, so if you want uniform interleaving across uniform memory types, then that is  indeed true. And you have to have – you have to do the work.

Yeah, I mean, I would just add that I think that's the case even today, right? If you  have a two-socket server, you already have non-uniform latency, non-uniform bandwidth  access anyway, because unless you pin your software threads to a specific socket, you  already have to solve this problem.
 
For a Type III device, can they ignore the metadata since they are meant for cache-lined  space?
 
Yeah, I think I answered this earlier. It's an optional for Type III devices.

 How does the host initially determine the latency and bandwidth of given CXL-attached  memory?
 
I believe this is through – this is provided through ACPI tables, if it's exposed in  the NUMA node.

 Yeah, that's correct.

 What are interleaving options accounted for in CXL memory for FLIT rate or RAS consideration?

For example, modules becoming unavailable due to uncorrectable. Does the entire CXL  channel become unavailable?

Yeah, I mean, the exact choices around how to do interleaving at the system level are  really going to be processor and implementation specific in terms of your goals around RAS  or bandwidth and so forth. The spec doesn't actually make any – provide any limitations  here. It really depends on what makes sense for your specific platform.
 
Do you see CXL as a first step towards moving second-tier memory out of the box entirely?

So that's an implementation-specific question. While it is possible, and as Chris mentioned,  due to power and thermal considerations, it might actually be beneficial. It's not mandatory. I mean, moving outside the box has its own challenges, like cabling and keeping synchronization  between – of power and other stuff between the two boxes. So, I would not necessarily  say moving second-tier memory out of the box, but it definitely opens the possibility if  your system architecture can deal with it.

 In the context of keeping memory costs low for the system, how much memory does a CXL  module need to accommodate to make up for the added cost of the controller that will  be required between the CPU and media?

I think that's a good question. You have to do the math and figure out how it works  out. Basically, what percentage of your memory can be slow, and if that percentage is replaced  by this additional controller plus the new memory type, what cost does it work out to?

Is ECC required to be transported over CXL.mem?

So I think it depends on what ECC is meant in this context. But if we're talking about  media-specific ECC here, no, it does not get transferred. The cache lines themselves don't  – it's just a data cache line. We're not sending ECC with it. So, if it's media-specific  ECC, the memory controller handles that, and that's just a transaction between the media  controller and the media itself. CXL as a link has multiple layers of protection built  into the link itself, similar to PCI Express, so that the link itself is protected as well.

So this question's about the CXL Type 2 device. How can an accelerator on the CXL endpoint  comply with the host memory consistency model?

I was just going to say it's not directly relevant to this particular presentation,  and I would direct them to the CXL spec.

Okay, this will be the last question. Can two sockets which are not connected via a  coherent interconnect use the CXL metadata meta value to manage coherence for CXL-connected  memory?

All right, so that's not really the intent here. CXL supports asymmetric coherence protocols,  and you need to still have a coherency agent on the CPU itself. So the memory device itself  is not intended to be a coherency agent.

So being mindful of the time, we will wrap up today's webinar. The presentation recording  will be available on CXL Consortium's YouTube channel, and we will upload the slides on  CXL's website. Unfortunately, we couldn't address all the questions we received today,  so we will be addressing them in a future blog, so please follow us on social media  for updates. Once again, we would like to thank all of you for attending CXL Consortium's Memory  Challenges and CXL Solutions webinar. Thank you.
