
And I guess how many people have either developed a silicon chiplet and delivered it in known good die?Anybody have experience with that?Okay, I'm not getting a lot of answers there.How about receiving known good die and integrating that into either an organic substrate or some kind of 2.5D kind of substrate?Okay, had a couple of hands on that one.So we think at Credo we're qualified to kind of talk about this because as an industry we keep going back and forth between homogeneous solutions and heterogeneous solutions.And I'll talk about the benefits based on where we are in process technology.But ultimately, especially as we talk about high performance AI and machine learning, if we want to break apart the compute, the switching, the storage elements of those big chips and create more die size so they can actually have bigger switching tables, bigger routing tables, bigger compute tables, you need to move the high speed I/O off.And over the, I'd say in the last five years, we had two major developments at Credo and they were both ended up being 3.2 terabit per second chiplets.One was based on 56 gig lanes on the, what I'll call the line side or the transmit side off the big chip.And the other was 112 gig lanes.So at those performance levels, most people are used to taking that die, doing some level of probe, and then putting it into a package part and doing some level of system level testing to be able to deliver that to an end customer to build up a system.But in the known good die scenario, it creates a lot of challenges with custom probe cards, with how you go about achieving 99% yield so that when you deliver a reasonably, I'd say less complex re-timer, SerDes type of chip, which I'll talk about, relative to the big ASIC that could be doing AI or high performance computing or switching, there's a big cost paradigm there that you have to make sure you get not just your yields right, but then they get the integration yields right.So we want to be able to talk about that.So the fact that we're actually shipping both of those, they're on our website at Credo.So again, one, so as far as interfaces, I'll talk about three different die interfaces today.The first one kind of as an industry we were dealing with was a bunch of wires, which ended up folding into OCP as a standard.It's the one that's been pushed through this summit in this forum.And that's a wide parallel bus.So it requires a silicon interposer to be able to be really, really close and to be able to operate at anywhere from one and a half to three gigahertz of performance across those wide wires with a lot of redundancy built in.And then IEEE and OIF worked hard on the XSR standard.So extremely short reach.And the benefit of that, in lieu of die area or beachfront, it allowed you to go a couple inches, so 50 millimeters, and somewhere around 8 to 10 dB, depending on how good your channel was in the package.So that allowed a lot more flexibility to use an organic packaging substrate and push the chiplets out a little further.And when you take a look at what that means to the overall package or even the insertion loss as you're breaking off that big package to be able to drive long reach channels, that's an attractive approach.And I'll show an example of that here in a minute.So those are the two that we built on.So our first one was a bunch of wires.I'll talk to that one once I get the slide going.Second one was XSR, again, both in production.And what the industry has been talking about-- has everyone heard about UCIe?So UCIe is probably one of the fastest growing consortiums that I've seen.There's a lot of MSAs I've seen that have grown fast.But as a consortium, just because maybe the power and influence of Intel, the fact that it's leveraging off of all the work of PCIe and now using it in a different kind of form factor of interface.But there's a lot of momentum.And then the real potential is-- when we talk about chiplets, it's all about picojoules per bit per area.So we talk about beachfront.And we talk about what's the power distribution across that.So when I gave that bunch of wires versus XSR example, the bunch of wires is like 0.1 picojoules per bit for a 2 gigahertz interface.The XSR, with all the flexibility it gave, but it's a true SerDes.And that's 1 picojoules per bit, or 1.1 picojoules per bit.So there is a trade-off there.But the trade-off is you get more flexibility on how you construct different dies together.And so see how we're doing on IT.

But I guess the cool thing about heterogeneous solutions in general is you get to break them apart.So we can use LEGO analogies.And it allows for different trade-offs.When I first started looking at these about 10 years ago, some of the trade-offs could be if you're building, again, a big switch, a big high-performance computing, a big AI die, you get to max reticle pretty quick.And those are expensive, expensive to yield.You die per wafer.So there's one element, say, if I could break off the high-speed I/Os, then I could actually shrink my die size and actually get better die per wafer and a better yield ratio.So you get better cost structure.But like anything, as soon as you pull something off, they just want to fill it back up because they can have bigger routing tables.They can have bigger switch tables, bigger compute files.So there's a lot of trade-offs you can make once you start going heterogeneous.But generally, at least my experience is, as soon as you make that decision, it gives you more opportunity to create a better mousetrap of what you're trying to go achieve.Is, again, usually those big ASICs, I'll call them, those big SOCs want to go to the most advanced process node.But especially when you talk about SerDes's technology, we can mature those in more mature nodes and prove out all the channels.So in the case where we built this chiplet called Blue Jay-- I'll talk about it in a minute-- but our partner had to necessarily go to 7 nanometer at the time to be able to do what they needed to go do for the switch ASIC.And we were in 12 nanometer very comfortably with a good power distribution.So I could operate in an N minus 1 technology, in some cases N minus 2 for my chiplet, while the main ASIC had to go down-- or had the opportunity to go down and take advantage of the advanced process node where the SerDes hadn't been hardened yet.And a lot of times, that high performance SerDes's, whether it's now 112, going to 224, is the long pole in the tent for that ASIC to actually tape out with the confidence that you're going to tape out a $30 million, $50 million device, and you have confidence in your I/O.So it allowed for that kind of bifurcation to have confidence in the I/O when they could focus on a digital hierarchy of their architecture.So that's why we thought chiplets would be cool, since we've got a lot of experience.And they're kind of hard, but kind of fun at the same time.

So my name is Jeff Twombly.I'm the vice president of business development at Credo.And so I've been with Credo for 10 years.Again, was responsible for kind of the product definition of these two.But I thought we should step back and say, what's driving this demand?Again, I'll pick it up like we've got a tailwind.I tend to look at-- it's all about the hyperscale or data center.Now we can just say AI, AI, AI.But whether you look at geographically where they're being built, the number of data centers being built, or you look at the sheer double digit year over year growth, it just says not only is the bandwidth expanding, but the need for us to be able to deliver solutions as an ecosystem that can scale to the volume, yield at the volume, scale at the volume, provide the power at the volume.

So if we dig deeper in, and you look at the sustained growth is there, but what's driving it?We all have to get our AI in.But it really is AI, machine learning, a little bit of high performance computing, but really the AI element.And the key takeaway here is not only is the bandwidth requirements and the compute requirements going, but the amount of data flow that has to happen is matching.So now we're server speeds and NIC speeds are matching the switch speeds and actually pushing the switches to go faster.So that's a paradigm shift on data mobility.So the connectivity angle matching the switching and the compute requirements.So great opportunity.

I thought I'd use Dojo as an example.I don't know if everyone's familiar with the Tesla Dojo.Peter Bannon presented this at the TSMC conference a year ago, June.And so it's a tremendous accomplishment.But what I like about it, he talked about in the middle of that picture on the right, I guess left on that side, there's 25 max reticle chiplets, I like to call them.So they surrounded their big die with XSR.That's the technology they chose, and they chose Credo.And we partnered to deliver this.But the cool part is that they use that because in order to create an array of 25 devices, you couldn't do that with a 2.5D technology.So you needed 50 millimeter, a couple inches of reach to be able to make all your connectivity in your crossbar.So kind of cool.When generally I think of a chiplet, I think of like a chicklet.It's small.It's the I/O. But literally, if constructed correctly at the system level, your big die will be chiplets as well.It's a matter of how do we connect the big die together.Then eventually, you've got to get off into the super highway world and get panel to panel or box to box.So what he talked about also is they had 576 lanes around each one of those D1 ASICs.But then in order to get from this panel, high compute panel, to another panel, then they used more of a traditional I/O chiplet, which allowed you to take the XSR and catch it and then go more with an MRLR class technology that would get you panel to panel.So it's a good combination of how somebody took a look at their system and said, here's how I'm going to solve it.A bunch of wires wouldn't do it.I don't think UCIe would solve this problem just because of the scale of it.But I think UCIe, when I talk about it later on, has its place.And it has really good, again, on that picojoules per bit per area score, it's got some exciting attributes to it.

OK, so I kind of hit this earlier.So I won't hit on-- this was kind of talking about why chiplets, why go heterogeneous.It's a system level decision you have to go through.A lot of times, if you can get homogeneous and you can get those hardened I/Os and you can kind of fit that all into one nice yielding one chip, it solves your problems.That's a good solution.Other times, it makes sense to break it.And again, break it could be just to get more reach.If I can push the chiplets out to the periphery of the package, then my insertion loss in the package might only be 1 or 2 dB and not 5 dB.If I'm trying to do an LR channel backplane, trying to get through some big system constraint, then that gives me some margin I can build into my system.So there's a lot of different-- depending on who you are, where you play in the network, what you might balance.But whether it's stand mature technology for the chip, for the I/O, go to advanced technology for the core, they all kind of play out.And again, I think we have three good alternatives.A bunch of wires here, part of OCP.XSR, part of really OIF.And now we have UCIe, which is a big industry consortium that we can pick on, but we have to make some decisions.

I presented this originally at the TSMC conference, a good partner of ours.And they have some fantastic tools that have really matured since I went down this path about eight years ago, but allow you to do some front end work on how you actually look at different ways of partitioning into your heterogeneous pieces.And then when you actually get to the other side, really focusing on how do you actually deliver those in an info or a co-auss package.So if you're not familiar with those, just look them up at TSMC's website.

So real quick commercial and credo, because it kind of starts to make sense as to how we think about the market.So literally, our mission is to address every connection in the hyperscale data center, period.And if you look at just at a product level, we can go after things with our chiplets and our IP and our standard line card products.At the big box level, we have DSPs that go inside optical modules.I'll hit on that.And then we have, if you go on the show floor, you're seeing a lot of purple cables out there.And those are our active electrical cables.And they allow for switch rack and NIC to torque connectivity inside the data center.So a good portfolio.

So my-- Don Barteson, who you'll see out in the Innovation Center, he runs that group for me.He talks a lot about-- it's the SerDes.And having a purpose-built approach to SerDes when you're building I/O chiplets is important.So that's why I like this diagram.So I can just show that PAM4 eye, and everything goes from there.So for every reach, we look at it, whether it's an optically-oriented environment, whether it's a pure copper going across a backplane, or if it's that XSR I just showed.Those are purpose-built architectures that allow us to build out line card, optical, AEC.And the one that I'll focus in on now is really driving into the chiplets that we built.

It was just one more.So in order to build that up, I talked about this N minus 1 approach.So we have an IP portfolio on the left-hand side, where we pick and choose to build our products.Again, Credo builds re-timers, gearboxes, DSPs, build cables.So we use that IP internally.But then I move it to advanced nodes for licensing, so that the big chip guys can-- well, thank you very much.So we took the bingo card on the left, and then we created the two chiplets that I'm going to talk to you about, the Blue Jay and the Nutcracker.Again, both production-released, both shipping at high volume.And we'll tell you about the nuance of that.

So again, this is that 64 lane.It says 68.That was some redundancy.But really, 64 lanes of 56 gig gets you the 3.2 terabit per second performance.And again, if you look at-- this was announced with the barefoot Intel, where they had their Torfino 2, these chiplets, were the ones that powered their I/O. So it was a decision that was made prior to the Intel acquisition.Good partnership.

Went to the Nutcracker, kind of same scenario, but we moved to 100 gig lanes.So it was 100 gig on the XSR side, again, extremely short reach.And then MR, MR plus on the line side to be able to push cables, push optics, and the like to build the system up.

So the way that kind of looks as an example is you kind of look at somebody building a 25.6 terabit switching example.They would license the XSR from Credo.They'd put that in their big die.And we would provide the chiplet, push them out to the periphery.And it makes for a very quick time to market.Again, if that ASIC wanted to be in 3 nanometer today, we can still deliver this chiplet in 12 nanometer as a very fine-tuned product offering.

So one of the things we learned when you go down the path of anything is things get-- sorry, didn't mean to cheat you guys on this side-- super challenging.When we went down this path, and chiplets have been done.I think Altera might have-- no, the Intel might have some of the best experiences with their AIB and EMIB technology, because that pushes out to high-performance I/O or modular I/O.But being able to kind of probe at speed, custom probe cards, how do you achieve your 99% DC scan?How do you actually start looking at actual traffic functional testing at the probe level?Super hard challenges.So we went through this with our partners.And then we built it up so we could get to that high-yielding.Effectively, at least in this OSAT world, you're marking your wafer.And you're just passing it along.And they dice up the wafer.So you've got to be super accurate.It's super challenging.But what you do is you work back and forth with your partners on what's the most important test to run that would actually be system-level tests on the big chip side.Then the challenges are equally as hard.If I go back to the Dojo example, they have a 500-76 lane max reticle ASIC that they've got to test at a known good die level before they put that in there.So it's not just at the I/O level from a credo perspective.Like 3.2 terabits sounds like a lot.But they've got like 4 terabytes per side on that chip.So just a lot of nuance.I won't kill on that one.

So I mentioned the emergence of UCIe.It's probably in 30 years one of the fastest consortiums I've seen.We joined it.We're a contributing member.I think it's got a lot of promise.But at the same time, it's got a lot of questions.

So very similar to the slide I presented at the front, what are all the benefits of going heterogeneous and having your Lincoln log or your Lego approach to building up the device?So nothing new there.I think everyone gets that.

But the nuance of UCIe was they've got a unique way of looking at two different advanced structures that we call it.And that's where the 2.5D or 3D silicon interposer type approaches, so super tight spacing.Then they have a standard that's kind of similar to an XSR approach, but it can only go-- it's spec to go 20 millimeters, not 50 millimeters.So it's got a little bit of limitation on the reach there.But the beauty is if we can define those and the big chips lead the march.And that's my message to the industry is for guys like Credo to-- and we have competitors that could do similar things.But to be able to build the reasonable I/O chiplets in a mature technology, we can get there fast.I mean, literally, I could-- if we agreed on which one of these UCIe variants that we wanted to have, it's not as simple as, but just take the XSR out and put the UCIe in.So we've already got the framework for how to build it.Now it's more of a layout structure and proof.And these SerDes are 32-bit-- 32-gig NRZ.They're not advanced 112 PAM4.So when I talk to partners and customers and competitors, we're all in.Just like XSR is so clean, since it's got-- it's one industry standard.We just are ratifying XSR+, which kind of gives you a little bit more reach to get outside the package.Great extension.But if we're going to go down this UCIe panorama, we have to agree on which one of those interfaces or which two of those interfaces makes the most sense to-- because it's a huge financial commitment for the big AI chip, and then for us to go try to match them with something that's reasonable as an I/O.

One last Credo commercial.It's like the chiplets are on our website.Again, we engage selectively, just because there's a lot of support.But if you're interested in learning more about them, please come on board.Like I said, we have e-mail boards.We have evaluation systems, test data, and the like.So with that, open for questions.And if you want questions, there's two mics on either side.I'm open.

Hello, Jeff.What's your thoughts on chiplets versus CPO?Or sorry, co-packaged optics.

Yeah.I mean, the co-packaged optics, it's had a lot of promise and a lot of demonstration for a few years now, say five or six years.So I still think there's some work to be done there.And there's concerns just more on the thermal and what are we doing with the laser concentration and the failure rate.So I think some of those quality reliability still need to be wrung out on the CPO front.I think chiplets can be part of that, like in the near package optics, in a sense, or getting it as close as you can.So I think that might have some meaning.But I think both of them need to be looked at in parallel.

Well, thanks, everybody.Like I say, I think heterogeneous and homogeneous will continue to fight each other.But chiplets do have their place.But we just have to agree on the standards.And like I say, UCIe has some-- there's not just the electrical side and the physical side.There's the protocol side.There's the streaming versus running like CXL on top of UCIe.So there's just a lot of fundamental decisions on where do you partition, where do you put the control logic.So there's just a lot of nuance that if we go into it eyes wide open, with the right attitude and the right level of taking a capability to a possibility, it's there.It's just a matter of the ecosystem communicating.And that's system vendors, partners, and competitors.We all have to understand that we're all in this together.So thank you.
