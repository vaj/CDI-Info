
So I'm Jonathan. This session, I give the Meta CXL journey and learnings. I have 20 plus slides. So I try to go through as quickly as I can. And if you have any questions, just ask any time. I don't think we will be able to flush all the questions in this session. This is merely to spark the conversation. And actually, tomorrow afternoon, there's an MC on CXL specifically.

So the work is not possible without a village. So we have all those reviewers from different companies. Thanks for the work.

So for the agenda, we will go through the CXL memory technology quickly because some of you may have not-- I guess may not have heard about CXL. And then I'll talk about the Meta status and our plan. Then specifically, it goes down into the memory management and the at-scale device management. Then finally, we have discussions.

OK, so if we go to this link, this is a public link, you can download the current spec. It's 3.0, like 1,500 pages. So all the CXL devices, they are-- CXL protocol are built on top of PCIe protocol. All of them has to implement a CXL.io. But depends on your usage, there are three different types. One type, the typical usage is like for NIC card, that smart NIC, that it implements CXL.io. So you have cache coherency. You have cache inside the CXL card that it cache coherency with the system memory. Or you can have like a GPGPU or dense computation for AI that you implement both CXL.cache and CXL.memory. That means the device has memory that the host can access, not just the device. The memory buffers implement just the CXL.main. And the usage could be bandwidth and capacity expansion, and also a storage across memories.

And those are just examples. So this is a diagram from the CXL spec. It shows you that in parallel to the PCIe protocol, it has defined the CXL.cache protocol and the CXL.mem protocol for link layer, transaction layer, and there's arbitration and mux layer.

The current status. So CXL 1.1 was released, I believe, is 2019. And then 2.0 is 2020. 3.0 was released last month. And this PDF has this white paper. So if you don't want to read 1,500 pages, read these three pages. The processor, the CXL root port, right now we have 1.1 will be product launched soon, in a few months. I guess, I hope. And CXL 2.0 support right now is being worked out. The silicon companies are working hard on it. And the CXL memory devices, the 2.0 support will be product launched soon. And this table shows the differences between those three protocols, how it evolves.

So this is the basic configuration. You can see that the processor has processor-type DIMMs. And the CXL core has a processor-type DIMM. And the CXL core has a CXL-type DIMM. So now, as a kernel developer, you say, what do I see from the kernel viewpoint? You run numactl. You see that this is a dual-socket system. So you have two NUMA nodes, 0 and 1, both have CPU and memory. And you have a node 2, which does not have CPU, but has memory. So it's a CPU-less memory NUMA node.

So why is the industry excited about CXL? Because it actually decouples the compute and the memory. Now, it's not tightly coupled. So that frees a lot of things, changes quite some assumptions. Like, it increases the bandwidth and the capacity. And it gives you a tiered memory hierarchy. If you look at the right bottom, this pyramid, you see that CXL memory actually fits in nicely a void area for the latency. It allows you to have media diversities. Your CXL memory could be any type of memory. So your processor-attached memory and CXL memory could be different. And CXL memory, you can do anything you want. It provides flexible and fungible memory. So this gives you something very hot-turned, called software-defined memory. Now, you can do hot-plug. You can have several hosts put in the same memory space. And you can share memory dynamically as well.

So there are a number of published use cases from Meta, from Microsoft, from Kaist. So links are all there.

So Meta's plan. So we finished the system-level prototyping, including software and hardware. We met the latency and the power budget. And latency impact studied, and we posted the kernel patches. Thanks for our kernel team. And we worked through multiple generations of system configuration, multiple generations of devices and processors. And we are working on the at-scale management.

So this is our initial target. We start from simple use cases, where CXL memory is added as system memory. It's static configuration. And for the kernel OS, by collaborating with the CXL kernel and OS community, we did the memory management part. And together, we are working on the device management part.

So memory management. What's our goal? So CXL provides more bandwidth to you. But in the meantime, the latency is longer. So we want to minimize the performance impact to the workload, despite the memory latency. But in the meantime, we don't want the kernel to have to know the prior knowledge of the application behavior. And we don't want to do in-depth application tuning. But the kernel is not equipped to meet this goal. The paging mechanism is latency intensive. So paging from the hot memory to the warm memory. NUMA balancing is not able to move the pages to the CXL node, CPU-less NUMA node. And when there's a memory pressure, it behaves not so good.

So to really study the workload and also study the impact of our patch to understand our design, we developed a user space profiling tool that you don't need to change the kernel. And this gives you some examples of our data that we have a web cache 1, cache 2 warehouse workload. And we find out the access patterns. And then we have this diagram shows up how we designed this user space profiling tool. We will have it posted upstream. The details are in our paper.

We developed a patch set on transparent page placement. John is sitting there. He is the author. So our design is we focus on the migration for the lightweight reclamation. We demote the code pages from the processor-attached memory to the CXL memory and promote vice versa. We decoupled the allocation and reclamation. And we also added bits so that we can observe the behavior. The effectiveness is that we have two configurations. One configuration is all 96 gigs of memory are attached to the processor. Another configuration is 64 gig attached to processor, and 32 gig is a CXL memory. In this case, we can find that with the TPP patch, the performance impact to the workload is less than 1%. So that's good enough for us in exchange for other benefits.

Now, so what I said about that patch set is for latency-intensive workload. Now, what about a bandwidth-intensive workload? What about the bandwidth is also at its bottleneck if it is a processor-attached memory? So we have this interleaved improvement that you can manually define what's your interleaved ratio. So if it is the default, the current kernel default is 1 to 1 ratio. In this case, we see 40% drop. But if we change the ratio to 5 to 1, we get 8% increase in terms of performance. So CXL memory actually improves the workload because now you have data flow not just go through the memory controller on the processor. It also goes through the PCIe subsystem going into the course. And we have an interesting finding is that the optimal ratio, it actually depends mostly on the hardware. The shape of the latency and the bandwidth curves, it does not have too much correlations with the workload itself as long as the workload is bandwidth-intensive. That's our observation. So any questions so far?

Now we go to the at-scale management. So now, it all looks good if you work on a rack or a single system. But what happens if you have a lot of things, a data center full of CXL device? How do you manage it? So for the memory to work, as long as the firmware works, it will just work automatically. So CXL memory is either added as system memory when the Linux boots up, starts to boot up, or you can access it through spatial devices, such as DAX device, direct access device. You can do it either way. However, now, the CXL device is actually a PCIe device. The memory is not managed by the core directly. So we need a CXL driver. The CXL driver can communicate with the device through mailbox. Mailbox is defined in the spec. And the driver can discover and enumerate the hierarchy. It can find out, well, how many CXL root port I have down there, how many switches are there, each switch, how many devices are there, to find out all the hierarchies. And the CXL driver is also designed to a number of other things, like firmware upgrade, like event management. Because CXL devices generate events, like hardware error event. Oh, I have a warning. I'm too hot, things like that. Now, there is a complication here, is that you have host, you have device. Host and device could support different CXL versions. So a clean case, you have a 2.0 host and 2.0 device. Now, as I said, we will have 1.1 host in the market soon. But all the device vendors actually will roll out devices, this is 2.0. Now, 2.0, you have two modes. One is called RCRB mode. Another is called non-RCRB mode. So the CXL driver needs to support both of them so that the production system could be supported by the CXL driver. So that's something the kernel community is working on right now. For example, you can see that the registry mappings are different. And also, yeah, the registry mappings are different. And then RCRB is not exposed to the kernel at this moment. It's not config space access regular.

Now, talk about event management. So the CXL device could speed up different types of events. Could it be CXL memory error events? That's green. And you could have the red one, the other spec-defined events. Let's say, oh, I need-- I generated some performance data you can grab. Or device vendor-specific events. And this is a design that's currently on the paper. There's no code for it. So I hope to work with the industry to really get this done together and refined. And then it goes through the kernel. You have the CXL driver that's pulling, that's interrupt, that's command handling, and that's the event handling framework. And then the data flows to the trace buffer. Then all the agent, including the user space, could register for it and get new events. And once with the new event, it needs to go through the dispatcher. Some of the data, the green data, the raw data, goes through the raw statement. So that from a user's viewpoint, all the DIMM errors, processor DIMM error or the CXL DIMM error, they all go through raw statement, show up there nicely. You just say, oh, this memory attached to this processor memory controller had an error. That memory is a CXL memory. It's represented by this PCIe BDF. Our device function had an error. And then flows to the device vendors, specific event handler. So we are thinking this design so that device vendor could add their extensions. And the customer, like us, like Meta, we could have our own handlers to suit for our formats and policy needs. So this is on the paper. We hope to work with the industry to get it improved.

Another complication brings forth by CXL is a reset. So PCIe, we all know that for PCIe device, there is hot reset, warm reset, and cold reset. So that's cool. That's almost the same for CXL. Now, there is added something called a CXL reset. So CXL reset means when reset, it resets the entire CXL.io and CXL.main protocol for all layers. Now, the complication is for the entire device. Now, the complication is you might have multiple hosts sharing a device. This does not exist today, but it exists on the paper. And the device vendors and the silicon vendors will make it happen. Now, it's up to kernel community to deal with it. Well, HDM region. So HDM is a term you might not be familiar with. This is a CXL spec defined term. It's called host-managed device memory. So in the device, you could have some memory just managed by the device, like today's GPU device, for example. And you could have some of the memory is actually CXL memory that hosts can access each load store command directly to access them. So that region might migrate from one host to another host. So when you do reset, you need to consider this. And that's from the device viewpoint. Now, from the host side, the host, you have a piece of memory region. The piece of memory region now points to one CXL device. Next time, it may point to another CXL device. So those are the things that need to be considered.

The reset management is a CXL driver flow to support the reset. Before the reset, you need to offline the HDM ranges and make sure the host stops initiating any new requests and clear randomize the content from the security viewpoint. Because if you don't be careful, your memory now is exposed to another host or to another application. Then you issue the CXL. You tell the device to do a CXL reset. And after that, we initialize all the CXL functions and we initialize the HDM ranges.

Switching to another topic is that today, how do you know as a customer, as a user, how do you know the information about the processor attached memory? You run dmidecode. Now, the problem with the current dmidecode is that it only shows you the processor attached memory data. Today, if you connect a CXL device, you don't see the memory from the CXL device. So if your firmware supports it, then it would work. So get it to work or add it. We basically have CXL, have host firmware in Linux, by the way. Now, dmidecode is filled in by the firmware. So that's a static information. But going forward, actually, the hot memory plug becomes common, especially with CXL memory, because now it's just a PCIe card. And also, you can switch between different hosts with a CXL switch. Then you actually don't need to do anything physical to plug and unplug memory. And the dmidecode only shows the system memory. It does not show memories accessible through devices. Here, devices means Linux devices. This is DAX devices. So the proposal-- what about something like lsmem? It's similar to lspci, but it's actually more complicated than lspci. I'm not going through lspci today, because there are patches posted for lspci, because lspci today only understands PCIe language. Does not understand the CXL dialect. So there is a price tag for that already. Now, the memory side-- if we focus on the memory side, how to show the kernel data? How to show the memory media information? How to show the interleave information? You could interleave between the CXL devices, or interleave between CXL and the processor memories. That's all possible. And the memory segment information-- what memory segment right now is provided by system memory? What are represented accessible through the kernel memory devices? And what are the CXL hierarchies? With today's CXL memory, CXL driver-- kudos to the CXL driver community. In the sysfs, the hierarchy is there. We just need to show to the user nicely.

So call to action. So yeah, there are a lot of things going on. If you go to the linux-cxl mailing list, you'll see all the work about the at-scale management. Now, if you-- memory-- existing memory mailing list, memory management, they have a lot of discussion. Yesterday, there was a session-- BOF session from Google. And yeah, so-- and also join the CXL forum. And there's a system software workgroup and a memory system workgroup.

Tomorrow, 3 to 6.30 in Herbert over there, our Samsung friends and the Intel friends will lead a CXL microconference. Yeah, it's only 3 and 1/2 hours. But next year, it could be like eight hours. Get it? Grab your coffee.

So yeah, kernel memory. So here are some-- so even though our patch set meets our kernel needs, but we think that in longer term, something needs to be done, especially from the tuning point of view. For interleaving, I said that we have to run some experiments to find the best interleave ratio. So how to tune the interleave ratio with less tuning effort? 
Second, the latency sensitive workload and the bandwidth sensitive workload. Right now, we improve their performance by different method. Now, they are exclusive technologies. On one device, you might have both workloads running. What do you do? For us, today is not a problem, but for the industry, right? 
Tiered memory hierarchy. This kernel team, kernel community has been discussing this for some years. We need some solutions. I think it will get more and more complicated. In our case, we have only two configurations. One is a processor-attached memory, and one is CXL memory. In the future, you have a lot more. You could have a lot more. Like with switches, you add the latencies, some latencies. And you could have different memory, CXL memory media, that have different memory latency characteristics. And today's memory hierarchy, memory management, does not really consider the actual latency and bandwidth. It does not really consider that. It's not smart enough. Hotplug. Yeah. 
Hotplug will be the foundation for the software-defined memory. So today, we have nothing for that. Hotplug, you need to consider the traffic management, device management, memory management, security. A lot of things need to be figured out. 
Memory sharing. Once you have memory sharing, how to maximize its impact? Now you don't need to move data from one host to

I just have a question. What are, I guess, the limitations of-- you have the switches, like the distances that you can have this CXL shared memory between two hosts?

So if we go back here-- this is a theoretical data. Looks very nice on the paper. On the actual system, we did achieve this. But it's a direct connection from the host to the device. Now, if you add a switch in between, switch will add latency. How much? We don't know. Switch will add latency. And so this number, 170 to 250 nanoseconds, is about the same as if you are accessing the memory across the processor. The processor 0's core access processor 1's memory, about the same. Yeah.

Sorry, me again. So let's say that you were running virtualized loads. The expected level of steel is pretty much-- you're anticipating the expected level of still on virtual VMs to be about the same as if this was just a regular NUMA node with CPU and memory attached?

Yeah, it should be the same. If I understand your question correctly, it is the same, because now it's just a piece of memory. Anybody, including VMs, can access it freely. And you just need to keep in mind that now, in terms of zeroing the memory, you switch from VM to another VM. In terms of zeroing, you need to be more careful. Yeah, the processor-attached memory, my host firmware already set it up. But now device is different. You may just have to tell the device to zero the memory or randomize it. Yeah.

My question is similar to him. So from a user space perspective, you just load and store into a memory position, and it goes to CXL memory?

Yeah.

Or do you need to have a page fault to access CXL? So for instance, there is a TLB entry for the CXL memory, so you can convert from virtual to physical. Is this similar to regular memory, or is it a different path?

It's the same path. So if the host firmware is configured to add the CXL memory as system memory, you just access it, load and store. It's no difference at all. No difference at all. If it is added as DAX device memory, then you need to bind it to it.

Yeah.

Yeah.
So that if you are CXL memory-aware application, you can do that way.

Sorry, me again. The actual memory on the back end, is there any way to-- I guess what I'm trying to ask is, is the memory in any way virtualized? So for example, you had a failure, and you could replace in runtime your back end memory, and whatever P2V would just be remapped.

Today, it doesn't. Today, it is not. So this is a stability problem that needs to be managed very well. Is that today, you know that if system memory has uncorrectable error, and it happened in the kernel critical region, then the kernel simply cannot run. If it is not critical region, you can do a lot of things just to quarantine it. Now, CXL memory is in the same category as system memory now. So today, if some error happened on PCIe device, so what? Then this kernel is still happy, even though the app is not happy. VM is not happy. But the kernel is happy. This is that-- this with CXL device, if there is a device error or CXL memory error, it impacts the kernel, has a potential. So the stability, it becomes very important. The device vendor needs to do a good job.

And a follow-up question. The backing devices, are they dynamic at runtime, or is this something that has to be after post?

It is dynamic in terms of how you map the memories. You can change it at runtime. However, you cannot dynamically provision a device. You cannot say, let's say, two hosts connect to a switch to the same device. You have to shut-- at this moment, you have to shut down both server, and then you do a switch through the firmware. Kernel is not able to do that today.

All right.

Yeah. Good questions. Hope in a few years, the answer is different.Yeah.

Just a little follow-up for that. Would you envision it becoming a sort of that where you would not want the kernel to run with the CXL device, and just have the CXL devices used for user space allocations, and you would keep the kernel in DRAM? You could configure the kernel in a way that critical regions just go through the processor test memory. You could do that. Yeah. Now, the impact to the application, right, is still there for the VMs. Yeah, that's a good point.

So if not, we go to the last page. Oh, I'm too fast. Performance tuning tools. Could some of the memory performance tuning be done in user space? How do you change the latency and the bandwidth profile of CXL memory regions to simulate memory hierarchy? This is basically about how to make the kernel developers' job easier. You don't want to have-- I mean, we have-- Meta has the luxury of having the devices. We have racks and racks of CXL device we can work with. But memory management developers, how do they simulate that? How to change the behavior of the benchmarking apps, such as the page type ratio, memory access pattern, to simulate workloads? So those are all about the tools of helping memory management work, of simulating the device characteristics, the system device characteristics, and workload characteristics, how to do that. So we don't have an answer. Some of you are smarter, and we'll have answers. We look forward to that. Any questions on this? Actually, there's a last page. Or any other questions on any other aspects regarding CXL?

Ankaj Gupta asks, does CXL DAX device back end-- is it different from the traditional NVDIMM device DAX back end?

It is not exactly the same. But they are built upon the NVDIMM device driver. So instead of it's a PMEM device. Right now, it's called HMEM device. I forgot the term. So if you go to the driver, Linux kernel code, you see driver PMEM. And then you have a PMEM directory and you have an HMEM directory. So they share some infrastructure but are different device driver. So the DAX device. But you can use daxctl command to change the CXL memory from system memory to back to the DAX controled memory or vice versa. That's all working today. Any other questions? Thank you.

