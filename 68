
Well, we'll go ahead and get started. Thanks, everyone. And welcome this morning. We're going to be talking about CXL. And everyone may be going, what does CXL have to do with storage? That's exactly what we're going to talk to you about today. Woo-hoo! All right. So I'm your moderator, Richelle. You may have seen me around here. I'm the vice chair of SNIA. And I do have a couple of other things here. I work predominantly in a lot of manageability aspects and crossover. And we're working -- one of the groups I work with is the Open Fabrics Alliance, which you may have heard a presentation talking about CXL manageability and CXL Fabrics. And so my other esteemed panelists today, we have three folks here that are going to talk about -- we're going to kind of give a little bit of a primer before we get started into Q&A. So you'll hear a little bit about -- learn a little bit about CXL before we jump in and give everybody a chance to answer some questions as well.

First panelist today is Sandeep. Sandeep's a senior product manager at Astera Labs. He's been there for about 15 years. Well, he hasn't been there 15 years. He has over 15 years of experience with semiconductors, software diagnostic tools, developing security strategies, and firmware development in SOC product lines, including CXL, SAS, RAID, controllers, SAS expanders, and PCIe switches. So a lot of experience in this area. At Astera, he focuses on driving product strategies for new market segments by translating data center bottlenecks into profitable and competitive hardware and software product roadmaps using CXL technologies.

Our next panelist, moving this way, in the middle here, is Gerry Fan. So Gerry Fan is the founder of Xconn Technologies, which develops CXL and PCIe switch silicon. And so we'll be talking quite a bit today about those switch technologies. So Gerry spent 30 years in ASIC product development. And so prior to founding Xconn, Gerry has been at Broadcom, Marvell, Cisco, and some other startups. He has a MS in EE from Boston University.

And last but not least, Steve Scargall. Switching a little bit from those hardware-centric perspectives, we have Steve coming in on the software side. So Steve works as the senior product manager and software architect at MemVerge. He works on software-defined memory solutions using CXL devices. And Steve works across the board with industry leaders in the CXL hardware vendor, OEM, CSP, and enterprise and system integrator spaces to architect cutting-edge solutions for software-based memory management. He also has a very impressive pedigree here with a bachelor's degree in BSC, Applied Computer Science and Cybernetics from the University of Reading, and has also been working on contributions to SNIA's own NVM programming technical work group, PMDK. And he's going to kill me with all the acronyms here. ndctl, ipmctl, and some other memory-centric open source projects.

All right. Let's jump in a little bit. First, we're going to kick it over to Sandeep.

Thanks, Richelle. Hello, everyone. First of all, thanks for being here. Really excited to talk about CXL. First and foremost, let's quickly dive into why CXL? What are the challenges that actually call for a new interconnect? If you look at the data centers today, the kind of applications it's targeting, there's a massive growth in applications such as AI, machine learning, cloud-centric applications. And to meet the requirements of these applications, the computational complexity of processors has to be significantly higher, and the industry is keeping up with it. We have processors that can keep up with these computational needs by adding more and more cores. But what's missing is an I/O interconnect which can provide memory expansion capabilities, higher performance, lower latency, and also provide heterogeneous computing. The last part of what I said, which is heterogeneous computing, becomes extremely critical to scale the operations as we move forward, because you want to be able to intelligently and efficiently manage the resources as the scale of the data center increases. And we do have some solutions today for heterogeneous or desegregated computing. However, it can't keep up with the kind of scale that we're talking about for AI and ML kind of applications. So there is a need for memory expansion, capacity, and bandwidth expansion, as well as an open industry standard that can address all these requirements. And that's where CXL comes into play. It's a cache-coherent interconnect for processors, memory expanders, as well as accelerators. And more than anything, it leverages the existing PCIe standard for the physical layer aspect of it, which is what I'm going to explain in the next few slides. And on top of that, it adds several other layers to meet the memory coherency aspect of it. It provides low latency, targeted at near CPU cache-coherent latency limits, and the asymmetric complexity of CXL really eases the burden of cache-coherent interface design.

Digging one step, one layer deeper into how CXL is layered, as I mentioned, looking at this block diagram here, it's built on top of PCIe physical and electrical interface. What that means is all the existing PCIe devices can seamlessly operate with CXL through the CXL.io protocol. The left-hand side of this block diagram, which I'm pointing to, is the PCIe logic, the CXL.io logic. And you can see at the bottom that the logical and physical layer of CXL is based on PCIe. And that means that all the discovery, configuration, interrupt mechanism, the DMA engines that you're used to for traditional PCIe devices, including NVMe storage devices, it would seamlessly work with CXL. On top of that, there are two additional protocols that CXL adds, CXL.cache and .mem. .cache is primarily for devices to access the processor memory, and I'll explain a little more about this as I go to the next slide. And then .mem is for the processor to expand the memory from its local memory beyond what it has. So you can see from the left side, if you have a CXL device, there are three different protocols it supports. .io, as I said, is PCIe, cache, and then memory.

Let's look at the representative use cases. CXL defines three types of devices. Type one is what we call caching devices. These are accelerators or NICs that traditionally don't have memory attached to it. And so what it can utilize is actually processor memory. So you can actually cache the memory attached to the processor. Type two devices support all three protocols that I explained earlier, which is .io, .cache, and .memory. These are devices such as GPUs or accelerators, which do have memory such as high-bandwidth memory or HBM attached to it. The advantage with type two is you can not only cache -- the host can not only cache the device memory, but the device can also cache the host memory. Type three, which are basically memory buffers, is where you have a host with traditional local attached memory, or say DRAM, for example, but the amount of memory available to the processor is insufficient. And expansion of that memory within the enclosure is cumbersome, either due to the space constraints or due to the thermal constraints. The amount of capacity you can expand is limited. So which means that you need to expand it out of the local attached memory, and that's where the memory buffer comes into play. And .io and .mem protocol is what we would use for expanding that memory, and that provides your not only capacity expansion, but also bandwidth expansion.

So from the protocol standpoint, we've had three major releases, CXL 1.0 and a revision of that is 1.1, CXL 2.0, and CXL 3.0. CXL 1.1 and 2.0 have a max link rate of 32 gigatransfers per second, which means that if you have a x16 link, you can go all the way to 128 gigabytes per second. CXL 3.0 doubles that. It goes to 64 gigatransfers per second. All three releases do support type 1, type 2, and type 3. FLIT is basically flow control units. If you are used to the PCIe terminology, you're used to the TLPs and DLLPs. So the packet transaction is what the FLIT encompasses, similar to TLP and DLLP. The distinction between 1.1 and 2.0 is primarily that 2.0 supports single-level switching, which means that memory pooling capabilities can be supported with CXL 2.0 devices. It also includes global persistent flush for nonvolatile memory and CXL IDE or integrity and data encryption, similar to PCIe IDE for security purposes. Going from 2.0 to 3.0, we've added more capabilities for switch architecture, in that you have multi-level switching capabilities in 3.0. It's more targeted for fabric architectures. We do get direct memory access for peer-to-peer. And in addition to the memory pooling capability, you also have memory sharing capability. So overall, 3.0 encompasses a whole lot of fabric management capabilities, and the traditional multi-level switching is what you can expand to with CXL 3.0.

Looking at some of the actual end applications and use cases with all these protocols, this slide represents direct attached use cases. The left-hand side shows how you can expand the memory beyond what you have locally attached. The top half shows all the different hosts, and each of the hosts can attach to multiple memory expansion devices. That obviously provides the bandwidth expansion capabilities along with capacity and lower latency as well. It also reduces the cost because you have the capability to pool the devices as well. The right-hand side actually shows that use case, where you're pooling the devices amongst many hosts. What that means is your address spaces can be allocated and deallocated between the different hosts, so the memory over-provisioning or the stranding of memory can be averted, thereby leading to lower total cost of ownership.

Now coming to the heterogeneous systems. CXL 2.0, as I said, enables single-level switching. What that means is all the three types of devices that I explained earlier, which is type 3 memory expansion, type 2, as well as type 1 devices, accelerators and NICs, can be attached within the fabric, which means that the host can utilize all these different devices. On top of that, you can have the traditional storage devices such as NVMe SSDs. As I said, that's because the .io semantics supports the tiered memory structure or topology. The fabric manager capability is what's most attractive about the CXL 2.0 switches. PCIe switches does perform fabric management, but typically it's through proprietary ways, which means that there is added overhead of having to maintain proprietary drivers for NVMe devices. Namespace management becomes cumbersome. All this can be managed much more easily with CXL 2.0 switches. The other key feature of CXL, as I mentioned earlier, is cache coherency. That significantly improves the computational efficiency, higher bandwidth and lower latency. Last but not least, PCIe switches are known for supporting the I/O virtualization capabilities, whether it's MRIOV or SRIOV. This can still be supported with CXL switches or CXL devices in general. That makes this very attractive for the storage devices as well. To expand a lot more on CXL 3.0 capabilities as well as the software side of things, on how it extends towards storage devices, I'll request Gerry to take over. Thank you.

Thanks for Sandeep's go over the basic semantics for the CXL. Now the question for the industry is how do we make best use of CXL. One of the key components, the important for the CXL is that CXL can introduce much shorter latency compared with a PCIe. In SSD, traditionally, the latency is much higher. Definitely SSD is used as storage, as a persistent storage, which can offer much larger capacity. How do we make the SSD to be able to interface with CXL so that we can take advantage of the larger capacity and show the latency which CXL, the .mem, the protocol is able to provide.

If you look at the traditional memory and the storage hierarchy, and I borrowed one of the slides from the hot chips I attended several weeks ago. On the top of this pyramid is obviously the register. It's very short latency followed by the cache. The memory is on chip inside CPU. Then you have this memory which is directly attached with CPU. Typically, they can offer the wrong chip from 80 to 140 nanoseconds. Below the level, that is what the industry is doing right now. They have this CXL-based memory. This type of memory module has a CXL interface to either directly connecting with the CPU or it can connect behind the CXL switch. This type of CXL-based memory module, the latency obviously will be larger than the directly attached with the CPU. However, in general, it still offer you very attractive in terms of the wrong chip latency is between 170 to 250 nanoseconds. We call that one NUMA node hop, this kind of latency. One hop NUMA node is basically from one CPU to access another CPU. That's how much the wrong chip time is. The advantage of this CXL memory is compared with the main memory, the capacity is much bigger. If this memory is behind the CXL switch, typically we can build the memory expander. This memory expander can be tens of terabytes or hundreds of terabytes. As the migration from CXL 2.0 to 3.0, which is enabled the more fabric mode, and you can interconnect many CXL switch together so that you can build a much larger memory pool. However, this still provide this memory type is volatile. Behind that, what the industry is going to working on or has already working on is to have this, like marry the memory and the SSD together. That's build another type of more like a memory semantic, this type of SSD. I will talk about that in the next slide. The traditional SSD obviously has much longer latency and they will serve as a storage device to hold all these large file system. Of course, the bottom is going to be hard disk drive and hard disk drive today is more like used as a cold storage. Obviously, the more you move down this pyramid, the capacity will increase phenomenally. However, the access speed is much longer. The higher you move to the top of the pyramid, you're going to have much lower latency. However, the cost is going to be getting higher. Also, the storage is not persistent.

How does SSD industry take advantage of the CXL with a low latency and combined with high capacity of the SSD? This picture is one of the typical use case model. Today in the CXL switch market, people are talking about the memory pooling and memory sharing. Which means that you could have several hosts to connecting with the CXL switch. The CXL switch is able to connecting to either the CXL memory module, which I just mentioned earlier. Another device is more like memory semantic SSD, which has CXL interface connecting with CXL switch. Right below the CXL interface on that device is a DRAM. We can use the DRAM access speed obviously is much faster than SSD. We can use DRAM as a cache. For any of the operations, for a cache here, then the data can be retrieved directly from the DRAM. Only for those data which is not available in the DRAM cache, that goes out to the NAND SSD. Obviously, the NAND SSD capacity is much bigger. It could be several terabytes. For the DRAM, the size could be hundreds of gigabytes. So, this device will be served as another layer, another level of this memory device behind the CXL memory module. So, typically CXL memory module today on the market like 256 gigabytes to 512 gigabytes. Maybe some next year people will start building, getting into one terabytes with several channels. However, with the memory semantic SSD, you can build primarily based on the SSD's capacity and combine with the fast access speed to the DRAM and with connection through the CXL interface. So, this will give us another layer on this pyramid to give us a good latency. Of course, if you carefully managing the cache hit operation and also can give us a very good capacity and to improve the overall performance, that can be done through this memory tiering and this type of memory management software along with a fabric manager software. So, by saying that, I will hand it over to Steve and he can talk about on the fabric manager and the memory tiering type of software.

Thank you very much, Gerry.

So, as my introduction said, I spent most of my time in the software layer. So, how do we take all the devices, the different versions of the CXL fabric specifications and build it into something that we can use from the operating system from an application layer. So, I spend a bit of my time in the kernel, we do QEMU work in the emulation layer and that helps us build the development platforms that we need going forward. But the focus of my piece today is really just talking about fabric managers. So, again, the different CXL specifications, the different fabric topologies that we've talked about, the different device types, how do we manage, orchestrate, get some telemetry out of that level there. So, the specification talks about the fabric manager and really you can kind of think of this very similar to how you would manage a SAN or a network, right? There are little bits all over, whether they be in a switch, they could be an endpoint device, they could be in the host themselves. So, it's really a conceptual term, it's not an actual thing because it lives everywhere, right? So, that's the software piece where we can do the composability, we can allocate resources on demand either statically or elastically. And like I said, this stuff can live anywhere, right? And any piece of hardware can move in some part of the fabric management specification. So, we can also do, depending on the command and the requirements, we can also do in-band and out-of-band management. So, obviously, the administrating piece would probably be out-of-band, but there are some pieces like dynamic capacity devices, for example, that can do in-band requests of saying, "Hey, I need some more memory. Do you have it? Yes, okay, give me more," right?

The fabric manager itself is flexible by design, right? There are very few requirements needed to be implemented by anybody for the fabric management. And that gives us the building blocks required to go build bigger and bigger pieces throughout the network. Again, a switch doesn't need to implement everything that a fabric manager needs to do. It only needs to implement whatever a switch needs to implement. You know, what's connected to my port? What's the speed link? What protocol or protocols should I be passing through? Am I in a mesh network? Am I just a single-level topology, etc.? So, depending on who you are, whether you're an enterprise guy, an embedded, automotive, hyperscaler, you can start to build out what you need for your infrastructure and requirements. So, this gives us the huge flexibility that is required for modern-day data centers. Now, there are advanced features of fabric management. And again, that kind of leads into multi-logical devices. So, very similar to how you would partition a hard drive, we can do that with some of the endpoint devices that implement that. Some are single port, some are dual port, multi-port devices, you know, which port is being used. The DCD, dynamic capacity device, like I say, is really just allowing us to elastically allocate memory on the fly from a device without having to pre-partition it ahead of time. And again, just configuring the switches, the fabric, how do we want this thing laid out? Obviously, things change over time. So, that's a lot of what we do at MemVerge is that orchestration piece. And there's other operations like security and firmware updates that are needed for day-to-day operations and management.

So, bringing this up, there are low-level APIs. Each vendor can implement their own thing, so long as it implements some part of the fabric management specification, that's fine. But then we have communities like DMTF Redfish that have built a CXL specification, so that's now available. So, that'll go into your servers and other devices that implement Redfish. Again, at MemVerge, we're building the software tools, both proprietary, contributing to open-source communities like the kernel, like QEMU, open-source tools as well. And the Linux community, where I spend most of my time, also has additional tools that are baked into the operating system. Now, you can just download them from your distro's package repo. Predominantly, CXL and daxctl are the two tools that we have now. These lead on from the Intel Optane days, where we had ndctl and ipmctl as well. So again, building on what we've just described, we can build in modules, plug-ins, whatever it is, for your data center infrastructure management software. So, that was what I had.

I mean, there is a little thing at the end which abstracts a little bit more of how do we get this physical media through the operating system and really into the hands of the application. And that's what this busy diagram is trying to describe. Each one of these vertical columns is showing you a different way of managing your memory. In this case, it could be persistent memory, it could be volatile memory, it could be NVMe storage or memory semantic SSDs that we just described. So, there's a lot going on in the kernel. There's a lot already there right now. So, if you take a fairly modern Linux distro with a fairly modern kernel, this stuff just works out of the box with hardware devices. And again, the gray bit in the middle, the middleware, the SDKs, numactl, etc. That's kind of where we're spending a lot of our time right now, just making sure that we can allow unmodified applications to take advantage of all of the hardware that's coming down the pipeline over the next few years. So, with that, I will hand you back to Richelle. Thank you very much.

Hi, thanks, Steve. All right, thanks. Thank you, gentlemen. So, now we're going to switch over into a little bit of Q&A. So, if folks have questions, we're going to have to repeat them for the recording. If anybody has questions, please let me know. I do have a couple to get us kicked off here. One of the ones to get us started with is you guys have mentioned a lot about PCIe and CXL, you know, in CXL 2.0 and then moving into CXL 3.0 as we start to see the switches become more and more full-featured, pooling. Clearly, CXL has been working on memory first, and that's a big need in the systems. But as we start to move out to other types of devices, SNIA is doing a lot with accelerators. And SNIA has been expanding our scope to -- as anyone can tell, I just stuck my storage hat on, by the way. As SNIA is expanding our scope from storage to data, we're doing a lot with data and data accelerators. And also has the PCIe and CXL behaviors on the system. So, as the system vendors are putting PCIe devices and accelerators and the CXL memory, what's the behavior of the system going to be? So, for you guys, what's in the spec and what's up to the switch vendors? So, what can these guys building the storage devices expect to see and what do they need to do working in these environments and putting those systems together? What can they expect for -- as they put these devices, put their SSDs in, put the storage devices in, working with the data acceleration behaviors, that starts to cross over between SDXI or CXL, what can they expect to see working with your devices, with the CXL network switches?

Yeah, Richelle. A really long question, sorry, by the way. So, I'll be happy to give a shot to this question. The question itself is pretty abroad. So, basically, as Richelle mentioned, that CXL has like waves. So, today is more focused on the memory. So, that's primarily based on the CXL 2.0. So, for the CXL 3.0 and 3.1, that is more towards building much larger fabric to have more AI applications. So, to go back to today's, the 2.0 CXL, the protocol-wise and the people -- what people are doing right now is primarily building these composable memory systems. And to enable these large in-memory computing, these type of applications. So, for the SSD vendors, obviously, the one I just mentioned earlier, that's one of the typical applications which we see people are working on. And that can fully take advantage of the CXL 2.0 .mem capabilities. And also, in the current data center for the AI computings, obviously, you could have the SSD to connect to, for example, like Xconn, we're building the CXL switch combined with PCIe. So, today, the SSD mainly is based on the PCIe switch. So, that SSD drive can be seamlessly working with the CXL switch as well. So, that will provide much larger storage capacity. Yeah.

Okay. And just to clarify there, that's not in the standard. SSD vendors don't have to worry about building the CXL.io. It's just you just have to implement the PCIe. The switch vendors will take care of routing the traffic for you. You'll have to understand what that specific switch is doing in terms of load balancing and everything. So, work with the switch vendors to understand the behavior. But just build the SSD, plug it in. If it can handle PCIe and CXL, just build PCIe unless you need the CXL functionality.

Right. Absolutely.

One of the things that's inhibited the adoption of CXL by the accelerators is bandwidth and latency. So, look at what's happening in the GPU space. These are typically proprietary interconnects or moving to the Ethernet or InfiniBand. So, I understand that tying CXL to PCI Express is expedient to get it off the ground. But at some point, do you feel that you have to make a break with trying to write on the back of a PCIe physical layer and switch to something that's fast?

So, just to recap, the question is concern from the accelerator and GPU perspective has been, is there a concern from the CXL perspective that bandwidth is going to inhibit the ability for accelerators and GPU to adopt CXL?

Yeah, I can take that. So, from the protocol perspective, obviously, CXL 3.0 is increasing the speed of transfers to come closer to the needs of the GPUs and the accelerators. There are two ways of looking at it. Yes, if you traditionally look at GPUs, the kind of memory it attaches to, HBMs, that's extremely high bandwidth. And that meets the requirements of the applications that GPU targets. However, there are workloads which don't require the same level of bandwidth and latency, which can tolerate a little bit of latency. And for that, you can actually use CXL attached devices, memory to be specific. And the advantage there is, if you have an intelligent fabric manager that can determine what the workload needs are in terms of bandwidth and latency, you can target the right workloads for HBM and the ones that can slightly tolerate the low latency to CXL attached devices. So, there are two aspects to this. One is the protocol itself is evolving to match up to the speed requirements, and it will catch up. And secondly, targeting the applications appropriately based on the needs of the workloads. That's my take on it.

You caught the question. Can you repeat the question for the recording?

Yeah, I can take a shot for that. So, the question was, what do we think about the bandwidth expansion by using the CXL technology in AI accelerated computing? So, you are correct. So, the answer is, we are able to add much more additional channels and to enable those AI accelerators to communicate with each other directly and without going through the CPU. So, that is in one of the regards to mitigate the bandwidth consumption through the CPU, through the direct communication of these accelerators through the CXL switches. And so, that's one of the primary advantages to have these switching type of structure to enable those AI accelerator communication. And of course, the CXL also we are adding much more capacity by going through the expansion, like type three memory expansions. And more and more AI accelerators are going to support the CXL interface. So, that will fully take advantage of this type of much higher throughput. And today, we are on the Gen 5 and the CXL3 is going to be on the Gen 6. And the bandwidth is going to be double compared with a Gen 5 version. So, that's mine.

If I may add one more to that. The protocol defines what's called as interleaving between channels. And interleaving is an interesting concept where if you have multiple channels of memory attached, you can actually interleave the traffic between those channels. And that makes a huge difference when it comes to bandwidth expansion. And that's another aspect one can leverage as they design their systems. You don't no longer have to restrict yourself to local attached memory. You can actually interleave the traffic between different channels. And that makes a huge difference.

I'm coming from a software perspective as well. Because as we mentioned in the memory storage hierarchy, right, the really good high-performance stuff is very expensive and probably not very high capacity. So, one of the things that we're looking at is how do you tier data? How do you get tier up to the faster tiers and feed them as quickly as possible? So, GPUs is a great example. Today, NVIDIA has GPU direct, for example. That's great. You can go and access the data directly from an NVMe drive. But it's far faster to go access it from some other memory device, even if it's not inside of your system. Now, we've got access to switches. And we can provision the capacity and bandwidth according to the requirements of the application. So, again, one of the things that we do is do this elastically. So, on-demand composition of memory. So, we can take heterogeneous devices of whatever characteristics they are and start provisioning them ahead of time, load the data in, and make them available for the next tier above us so that when the data is requested, it's immediately available. Those are good questions.

Okay.

So, just for the recording, the question is a question about resiliency. Does using CXL affect how you manage your fault domains?

I can take that. So, interesting you bring that up. You correctly pointed out that resiliency is extremely important, especially in data centers. The protocol is very unique in that it has RAS capabilities built into it, the reliability, availability, and serviceability. And that includes being able to detect faults, especially when we talk about memory devices that are extremely critical. Because these are volatile memory devices we're talking about, whether it's switch-attached or direct-attached. Anytime you run into an error, there will be catastrophic consequences. So, the RAS capabilities include detection of these errors, reporting it in a timely manner, and also prevention of errors in many ways. It has error handling schemes, what's called event records, which all the way to the bank, rank level, rows and column level, it can detect the errors and report it in a very timely manner. So, that's also very unique to CXL. And this is in 2.0 spec, and it's even more enhanced in 3.0 as you go to the fabric-attached architecture.

Okay.
One last comment. We are just about out of time, so I'll let Steve respond, and I think that'll be it for us.

Yeah, sorry. Again, from a software perspective, right, so we can receive these messages coming from the hardware layer and then do clever things in software. So, again, assuming we detect that a device is failing, we can provision a new device in, copy the data over, unprovision the failing device, and mark it as replacement. So, from an uptime perspective for infrastructure guys, we're trying to improve that, right, so that you don't have to wait for a failure. You don't have to implement RAID 5 or some kind of erasure encoding, which would obviously increase latency further, right? So, if we can do it from a software and a hardware perspective, I think the applications win out and get better TCO value for that hardware.

All right. Thank you, everyone. I just would like to thank again our panelists.

