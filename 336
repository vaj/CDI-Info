
My name is Steve Scargall. I'm the Director of Product Management at MemVerge for AI and CXL. So, I'm going to talk to you this morning about the work that we have done and are doing in AI/ML using CXL devices. So, I'm not going to talk about the spec; I kind of assume you at least have the fundamentals about CXL.

So, I just want to talk about, again, what we're doing and why CXL is useful and where it's proving its worth—at least in the research that we're doing. So, I've got a fairly packed agenda because I have the full 50 minutes. So, you'll be sticking with me for the next hour.

So, let's talk about the ecosystem. So, I just want to bring you up to speed before I get into the work that we're doing.

I want to bring you up to speed with what's happening in the ecosystem around CXL, and specifically around the kernel and QEMU, which is under active development. And there's tons of work going in every single release. So, what I've tried to display on this screen is the CXL specs at the top. Again, there are no timelines in this. And then I've lined up the kernel revisions underneath each of the CXL specifications. So, the CXL spec has a little bit of a blur in front of it, just because, you know, there's no big bang moment for if you get to kernel 6.x, you're going to get all of the features of, you know, 3.0 or 3.1. Right? This is an ongoing development. So you usually start off with getting the fundamentals in there first. And then, as we go through the iterations of kernel releases, we can then build upon that towards the next release. So that's what that's depicting. I don't put in everything that is going into the kernel because it's just way too much. But I highlighted the key features that I care about, which are the TPP, the time frame page placement, meaning the latency-based policy that's now in there. The kernel where you can detect hot memory and move it to DRAM if it's in CXL, and move colder memory into the CXL devices as needed. We also recently introduced the weighted interleaving. Weighted interleaving is our bandwidth policy, meaning if you've got lots of DRAM bandwidth and lots of CXL devices available to you, you can now tune the system dependent on what the ratio you require for your applications, but also dependent on the device. So that's one of those characteristics as well. And I'll talk about the quality of service feature in a moment here. Now, below that, you know, I don't want to forget about QEMU. There's going to be a great conversation later on today about QEMU. So I don't want to, you know, burst anybody's bubble or talk too much about that. But again, we as software developers don't always have access to hardware out of the box, right? So QEMU is a great way for us to set up an environment, emulate a CXL device or CXL devices, or various CXL devices, or various types. And again, now with the latest 9.1 release, we can go all the way up to kind of CXL 3.0, CXL 3.1 devices. And then we can write our software on top of that, right? We don't really necessarily look at the performance of that right now just because the emulation stack is quite deep. But it's a really good way for us to get going with this stuff. Plus, it allows us to play with all the kernel features and everything else and really learn CXL before we get our hands on the hardware. So when we do get the hardware, we're ahead of the game and we're just utilizing the hardware features here. So this is one way that we do it. At MemVerge, we build the QEMU plus kernel features because some of this stuff is bleeding edge, so it isn't quite upstreamed yet. So we take a lot of the upstream patches, branches, merge it all into one, and then we build Docker images and push them up to Docker Hub. So if you're interested, I've got a link to that. But again, this is just an easy one-button way of emulating. So we do it with expansion, we do it with pooling, and we do it with sharing. DCD is in there as well. So that's all coming down the pipeline for hardware. But as a framework to get us going, this is a phenomenal way of doing it.

Now, I won't go through all of this stuff, but this is just a snapshot of all the tools that I use on a fairly daily basis. Here, everywhere from the administration side through to fabric management. So again, there's Jackrabbit Labs that have got an open source project. You can go and emulate an entire fabric if you want to, and start playing with that. You know, if you're into the fabric side and you want to add CXL fabric management into your product, this is one way of doing it. The memory tiering stuff I just talked about with TPP, memory bandwidth interleaving, our own memory machine from MemVerge does this as well. We also do the visualization so you can kind of see what the system's doing, see what the CXL is doing. And then you've got the BIOS implementations as well. So if you're on legacy operating systems, legacy kernels that don't yet have the feature that you need, quite likely, the BIOS provides at least some mechanism for managing CXL for you, so that even the operating system doesn't have to understand what's going on under the covers. On the telemetry side, again, we've got a memory machine that gives you the heuristics and metrics into DRAM, CXL. We've added GPU in our recent release as well since that's all part of the AI/ML stack. And then, of course, you've got the CPU vendor tools like uProf, micro prof from AMD, and PCM from Intel. And we just talked about the emulation piece for QEMU.

So then, how will applications use this, right? Again, I'm not going to talk about the spec and everything else, but this is kind of a fundamental way of how we're setting up our environments. We're playing around with different CXL environments for different applications. The left-hand side is on the expansion piece. So again, we can have multiple CXL devices, all providing memory capacity, memory bandwidth, in addition to our DRAM that's in the system. We layer on top our memory machine, or if you prefer, the TPP and weighted interleave from the kernel. And then you have an unmodified application that sits on top of that, and you can manage where that memory gets placed underneath the application itself. In the middle, we have the memory pooling. So now, we've pulled the CXL out of the system predominantly, although you can still have local CXL memory if you desire. But now it's like a storage array, right? So, it's sitting outside there. You can connect it to multiple hosts. You can provision it how you want. You can have the capacity that you need, and have lots of different systems using it. So this solves some of the stranded and frigid memory problems that Cloud Hyperscale has talked about several years ago, actually. But where we're spending quite a bit of our time actually is in this right-hand side, the memory-sharing piece. So again, the memory sits outside of the servers, but it is now physically shared. So the same physical addresses are mapped to multiple hosts. So we can have multi-reader, multi-writer. Ideally, at the moment, anyway, without back invalidate, we've really just got the single-writer multi-reader, just because of the coherency challenges involved in this. But fundamentally, this is where we're spending a lot of our time. We have what we call Gizmo, a global IO-less shared memory object store. So if you need an object interface, you can use our Gizmo. And I'll talk about FAMFS, which is a file interface later on. It gives you a similar interface to this. But your applications typically need to understand now that if I write to memory offset X, it's going to be immediately available to other instances of my application. But there's a lot of flushing that goes on and coherency that needs to be managed.

So let's look a little bit more now, coming up the stack from the operating system up to the application, right? I've highlighted here some of the ways that we can use memory either with our unmodified application or by modifying our application. To start off simply, again, we've just talked about the hardware piece here at the lower level. We've got some CXL devices. And then, as we build the stack up here, we can get up to exposing our CXL memory as either a device DAX. So, if you're a developer and you know exactly how your memory is laid out, your data structures, and you want to really control where that gets placed, just memory map the device DAX, and then you have full control over all of the reads and the writes. If you're not quite there yet, then you can just convert that device DAX into a NUMA node so it looks like another piece of memory. The kernel understands it. Applications that are NUMA aware understand it. If your application doesn't quite understand NUMA atomicity, then you can use NUMA control to manage where that memory gets placed at some higher granularity. And then, in the middle here, we can start to build up our application. So, we can put a middleware in front of this, right? This might be an interposer library that you build, or you borrow one that's already out there in open source, and there are many of those as well. Or you might have something like MemVerge's memory machine, right, that manages the memory on behalf of the application. All of the memory system calls work. So again, if you malloc or mmap into your application, all of this stuff works today. We can start to create bigger, what we call regions at the operating system level. So we can aggregate many different devices and make them look like a giant device to us. It's not RAID, so there is some fault tolerance that you need to build into your application, but we can aggregate the capacity of multiple devices in there. And then as we build out this slide, so we've got dynamic capacity devices, right? These are the ones that are elastic, meaning if I start off with 10 gig of memory mapped to my host, and later on I need 20, 40, 100, well, now I don't have to mess around with anything. I just go to the appliance, say, "Please give me more memory." If it has the memory available, it'll map it, and this device changes capacity so you can keep going, right? So, if your applications are elastic in the memory capacity that they need, or that you're using something like a VM or a container-based environment where those containers and VMs start and stop quite frequently so your memory capacity needs to change, DCDs are a great way to do this. And we have this running in our lab right now. It works quite well. Then we talked about shared memory and how that gets mapped, and then in the future we'll have, hopefully, persistent memory coming back, right? Which is pretty cool.

So, I won't go through all of this, but if you're interested, there's a whole bunch of references here. The one I'd call out to you, if you're a kernel guy, is the maturity map that just got placed into the documentation piece now for the source code. So now, the kernel community is at the point where they're confident enough that all of the features that they want to deliver are up there, and this maturity map will keep going, right? So, as we build up new kernel versions, new features into the kernel, this maturity map is a great way to say, in kernel 6.x or 7.y, this is the features that you're going to get.

So let's talk about benchmarking now. One of the favorite things. So, this is a very simple diagram, but what I've tried to do is show you what tools I use on a fairly daily basis here for my CPU to CXL, CPU to DRAM. These are the CXL benches and the MLCs, and then whatever application benchmark that you're typically used to. And then, if I want to look the other way, I want to go CPU to GPU, then I can use the NVIDIA nvbandwidth, or there are some other tools out there, the CUDA examples as well. So, that's the benchmark layer. And then underneath, the purple ones are all of the tools that you can use to observe what's going on at different levels, right? Between my DRAM, my CXL, the CPU, the GPU. So, you can start to introspect what's going on and where the data movements are. And then, on the right-hand side, it's just a very simple test matrix, right? Typically, on a multi-socket system, you want to be able to figure out what's the performance if I keep everything local to the socket. In theory, it should be faster than if I have to go to remote. But quite often, going remote is better than going external to the system itself.

So... And then, here are some screenshots of the nvbandwidth and the bandwidth check itself. So, I was just... It's an older A10 NVIDIA graphics chip, but I was able to run these benchmarks and get consistent results between the two. In fact, I had to put a patch in for this bandwidth check to make it work correctly. But, you know, fundamentally, now I can see how much bandwidth that GPU is actually going to need. Or GPUs, I should say. I only had one, unfortunately. But now that you know how much bandwidth your GPU is going to consume getting data into and out of itself, you can start to architect, you know, the DRAM side, the CXL side. Is it good to have shared memory, memory capacity on the expansion side? Is it pooling? You know, all that type of stuff can be derived from these results.

And then, the CXL bench, which I talked about, this is something that I started putting together about a year ago. It's got a growing list of benchmarks. We started off, obviously, with the CPU-based ones, but I have, in a couple of branches, I've got some AI/ML ones that are based off the work that I'm about to talk about. So, if you're interested, this is all open source. Feel free to contribute. Use it in whatever... day-to-day operations that you guys have. If there's anything that's missing you think would be useful, feel free to reach out. We're happy to kind of work with you and figure out if there's anything that we want to add to this.

And there's the references for anybody that's interested in getting this stuff. Again, it's all open source; nothing proprietary here.

So let's talk about memory placement and movement strategies, right?

I already talked about the two big ones, the latency optimized and the bandwidth optimized.Again, just briefly, the latency optimization is all about detecting of hot working set size, the hot memory, and when you fill up your DRAM, now we've got a decision to make as to what memory do we have to move out of DRAM into CXL, and when data in CXL becomes hot, how do we move it back into DRAM where it's got its lower latency?On the right-hand side, again, the bandwidth optimized is all about strategizing how I can extract the best bandwidth from all of the devices that I have attached to me, including DRAM, right?So this includes, are you populating DRAM in one DIMPA channel, two DIMPA channel?Do I need a 50% ratio of bandwidth from my CXL to DRAM, or am I okay with maybe a 10%?And there are pros and cons with all of this, but this is some interesting research that we're currently investigating and doing.

So, the way this bit works is to take a pretty average graph of latency on the y-axis and throughput on the x, and at some point, you're going to get to this hockey-puck piece where you're not going to get any more throughput, but your latency is just going to skyrocket, right? And at this point, this is when you want to start adding more bandwidth. So, this is at the point where CXL makes an awful lot more sense, assuming you've run out of DRAM slots on your motherboard and things like that. So, on the bottom side here, the way latency—sorry, the bandwidth optimization works—is if I have a single DRAM NUMA node and a single CXL NUMA node, based on the bandwidth that I've just measured using the tools that we just talked about, I would probably want to strategize maybe a 67:33, so a two-to-one ratio here.

However, if I add more CXL devices—so let's say I add now five CXL devices and expose them all, each as NUMA nodes—then maybe I want to strategize where DRAM is less prevalent, so it's only 50% of my bandwidth. And now I can shard all of the data across the other five CXL devices at 10% each. And that's assuming they're all the same, of course.

So, that gets us onto the quality of service. How do I know what that CXL device or devices have? You know, what are their read and write characteristics? Now, in the spec, there is a part of that which determines from the device vendor that they can program this into the device firmware and advertise that out to the host. And then, the kernel advertises this to us, as the user space, through the sysfs path, and now I can go and read what my read bandwidth, read latency, write bandwidth, and write latency are for that device. So now, it might be that I can have mixed devices available to my system, dependent on what applications I know are going to be part of the runtime environment here. So this thing gives us the next level; so in our memory machine, you might look at this type of thing and go, "All right, well, device A is better than device B." Now, do I strategize for latency? And if so, I probably want to look at device A over device B, but if I'm looking at my bandwidth policy, then maybe doing it the other way around is fine as well.

So that's quality of service. So now I can get more information from user space and the kernel about the devices that are out there. And again, this is just references. I'm interested to go learn more in the documentation.

So, why is this important for AI/ML? Well, it's very important because most of the work that we're looking at right now is GPU-based, high bandwidth memory—very good, very fast, but in small quantities, right? But, at some point, you've got to get that data from storage into the CPU, process it, get it to the GPU for further processing, and then get some result back out, right? So, there's an awful lot of data movement in the system with GPUs in it, which means with larger language models, or even multiple small language models, I need an awful lot of memory available to me, right? These models are not small in terms of memory capacity for high bandwidth memory or main system memory. So, when I'm either doing the training or I'm doing inference, I'm going to need a fair amount of memory, and a fair amount of memory might be multiple terabytes, right? So, in addition to that, again, bandwidth. I need to be able to move data into the GPU. The GPU has got far higher bandwidth than we do on the CPU side. So, I need to get it there as quickly as possible, and I need to get the results back as quickly as possible. And, if DRAM is my limiting factor, both in capacity and bandwidth, CXL can definitely help us in this area.

So, that's what we're, again, looking at here: how CXL addresses the memory challenge for AI/ML workloads. And again, we talked about the capacity, the bandwidth, dynamic capacity, because the workloads do change, right?

If I'm loading part of a model, or a full model, and then shared memory actually helps us—instead of having to move data around over a network or over storage, SAN—I can do this now over the memory fabric itself. So now, I'm getting memory speeds for data transfers.

So, let's look at the workloads themselves. We showed this earlier this year at NVIDIA's GTC conference, and this is a memory-tiering inference engine called FlexGen. It's a university project, but the intent was, "What can I do in an environment where I don't have tens of thousands of GPUs and I don't have the budget to go buy A100s or H100s? I'm relatively low-cost. I need an RTX or a desktop or workstation kind of GPU-level system. So, what can I do?" And the answer is, "Well, you can do quite a bit, actually." So, the memory that is on the GPU is pretty restricted in this case. This is only 16 gig of RAM, but we have plenty of memory on the system. And in our model, depending on which model we choose, this is the OPT 66 billion parameter model, so it's pretty big. What happens when I exceed GPU memory and main system memory? Well, in a traditional environment, I can only go back down to storage now, which FlexGen does support. And that's what this green line is showing us here. So, this is us starting the workload. The GPU utilization is on the Y-axis. Time is on... Sorry, on the Y-axis is GPU utilization. X is time. So initially, both solutions are able to keep the GPU fairly busy, you know, 80%, 90% busy. But then the green line tails off as we exceed the memory capacity. It has to go to storage, which is not performant, right? So now my time to result is 600 seconds. But if I replace the NVMe drive with CXL, I can keep the GPU at least 90% busy, and my time to result is 50% faster. It's under 300 seconds. And that was the intent of this workload, right? We scaled this up. We put in two CXL devices, and we saw double the performance. So now, we can get our result four times faster just by replacing the storage with CXL.

So, a lot of people are focused on RAG, as are we. And I think the magic for us is in the retrieval part, right?

So, this was some work that we did recently with Micron. On the left-hand side here, we have a traditional, naive RAG where the data in our dataset, and we just took some Wikipedia, open-source Wikipedia articles, embedded those, put them into a vector database, then we ran our RAG pipeline using LlamaIndex. The retrieval was fairly straightforward: it just went to the database, got that piece, passed the query, plus the data that it retrieved, onto the generation piece, which is the LLM side, and then we got our answer back, right? So, we said, well, if the magic's in the retrieval piece, how can we improve that? And the answer that we came up with? Well, now, with LlamaIndex, we can use its simple, composable memory feature, which allows us to have multiple data sources, and each data source, we can now manage its performance on DRAM and CXL. So that's what we did on the right-hand side. We effectively kept everything else the same: same dataset, same embedding models, and we just replaced the single vector database with our composable memory solution, meaning that we could keep the answers that we had generated previously in its own database, and then the embeddings, we could segregate if we wanted to, across multiple databases as well.

So, now the database itself can start to use tiering and bandwidth optimizations. And what we saw from that was at least a 30% improvement in the database performance, just by doing this. So, what that had was a better effect on the generation piece, because now you're able to get the data to the LLM faster, and subsequently, you can get the result back faster as well. So, the next piece that we did, that was the result at the top. The piece that we did on the bottom is, say, "Well, since we have this copious amounts of memory now, now I can start to scale up. I can either have very big databases, or I can have lots of little databases if that's the preferred method, right?" So again, this is kind of looking at scale-up, and this is scale out in the same system, but this is more of a, you know, a VM or a container-based solution, or again, maybe I shard my data better so that I have better redundancy and resiliency and I'm not choking on a single database.

So then, the question became, "Well, how much RAM could a vector database use if a vector database could use RAM?" And the answer is—a lot. So, this is just a summary table, and the way to calculate how much memory you're going to need for your database is, if I calculate the raw memory. So, this is just looking at the vectors itself. If I take the vector length, multiply it by the number of vectors I need, plus the data type size, I'm going to get this result back. And then, obviously, there's a database behind there, so there's some overhead, and that's what this metadata ratio has on here. So, for this example, we chose Quadrant, a very popular open-source vector database. It has a guesstimated ratio of about 50% overhead dependent on the vector size. So in this table, I just summarize, you know, the vector lengths. Some of these are pretty common: 768, 1536 are pretty common for text generation models these days. You can look at the data types. Again, quantization is a popular way of reducing the memory footprint, you know, trading off the quality of the results that you're going to get back. And then, what happens if I look at 1 million vectors up to 100 million vectors? How much memory am I going to need in all of this, right?

So, I blogged about this, and I actually broke down a little bit more. So, for each data type, like float32, which is the default for most of them, I'm kind of looking at maybe a terabyte or more just to store my vectors in here. And this is for dense vectors, I should say. Dense vectors meaning I filled up the entire vector with data. You can get a sparse dataset of data vectors with, you know, zero filled, and then you choose: do you want a left or right pad? But this is a dense data calculation. And then, if you want to quantize and go down to a floating 0.16, you know, this is how much memory I'm going to need. So, still half a terabyte at least for some of the popular vector lengths. And then, if I quantize further and get down to integer 8s, then, you know, considerably less memory.

But what does that mean for me when I'm looking at my data, right? How much memory am I going to need to embed my data, assuming you're an enterprise and you don't want to push it to the cloud and use all the open source stuff? Then you can do some napkin math again. So, following on from what we just discussed, I just took the CXL 3.1 specification document. It's on the large side, I would say, for a spec. But, you know, 11.68 meg, almost 1,200 pages. Number of tables is 735; figures, right? So, using the assumptions below here of I'm just doing a naive embedding model, so I'm just using a sliding window effectively across this with an overlap, then this is the output that you can calculate pretty easily. So, my 11, almost 12 megabyte PDF file, with overhead, comes up to almost 20 megabytes. So it doesn't sound like a lot, but enterprises typically have years of data. You know, it might be structured, unstructured data. So you can do some quick napkin math and figure out how much memory you're going to need for a lot of this stuff when you're embedding it.

And then, how much is this going to cost me, right? So, Quadrant has an online calculator. You can punch in the numbers that I've just been talking about here. So, if I go back to that original summarization table and, instead of telling you how much capacity it's going to need but how much it's going to cost you in dollars, then, you know, this 2K vector with 100 million is $14,000 a month running on the cloud. Now, these are public cloud numbers, right? You may get your own numbers, but that's quite a chunk of change for running a simple-ish vector database to run a large language model for embedding my enterprise-class data.

I know a lot of people are moving to on-prem, so a lot of this gets washed out, but we can show you now what does that look like if I go and buy a server, right? So, a bit of an eye chart, but I'll walk you through it real quick. So what we do is we group the memory capacity. The top three lines are looking at 4 gig, then 8 gig for the next two, around about 12 gig, and then finally 32 gig. So let me walk you through the first one, and the others are all the same. The first one is if I did nothing else other than go out and buy a two-socket system and I populated it with a number, I would need 32 128 gig DDR modules, right, to get there. So that's going to cost me, based off memory prices earlier this year, about $46,000. So what happens if we substitute some of that and say, well, let's choose the smaller-capacity DRAM, the cheaper stuff, at 64 gig? We'll still keep the 32 count, so we're still fully populating that system, which means I've got 2 gig of RAM, and I've got a little bit of RAM that I'm going to need to get there. Which means I've got 2 gig of DRAM, and then let's offset the other 2 gig with 2 gig of CXL memory. Now, this is using the add-in card-based modeling, so I can now choose, do I want DDR4 or DDR5 modules, and how many of those? So that's what we did here, the DDR5 and the DDR4 calculations, what capacity do I need, number of DIMMs, number of add-in cards to get me to the 2 terabytes, and that system cost estimate is 50% cheaper than if I just went out and bought DRAM alone. And I haven't lost, really, any performance. Yeah, question?

Can you elaborate on how you get the per-gigabyte cost?

The per-gigabyte cost?

Yeah.

I think it's just 46,000 divided by...

11.2 for DRAM versus 5.6 for CXL.

Yeah, so this is, well, for this line here, this is just 46,000, 46,000 divided by 4096.

What was the question?

I'm sorry, the question was, how do we... Can I elaborate on the cost calculations for cost per gigabyte? So effectively, it's 4,096 times 11.25 to get you to 46,000, right? And then for DRAM, because I only need half of that, I only need 2 gig of that, it's now 22,000... Sorry. 550 times 2 terabytes, effectively, gets you to 6,000 here. And then if I went and bought the CXL device, this is how much it's going to cost me, 16,000 here. So they're pretty equivalent here. Obviously, CXL costs a bit more, but I'm offsetting that by what it would cost me to go buy the 128-gigabyte DIMMs.

And the total cost is, like, market prices or...?

Yeah, yeah. It's just MSRP prices, basically, right? So, you would take your customer discount, go talk to your OEM and figure this one out, right? This is just for illustration purposes only, but, you know, you take your customer discount in here. I'm sure it would be a lot less, but the takeaway here is that if I offset... Even though CXL is more expensive than DRAM, the total system cost for my memory is actually significantly less. So... Because, unfortunately, DRAM prices are not linear, right? They're kind of a hockey stick. So, if you want to go 128s or 256s, you know, this sort of stuff's going to cost you quite a bit of money. And that's, again, what we did with the next one down, right? So, let's say you want to go to 8 terabytes. Well, now I'm really stuck with 64, so now I'm having to go up in terms of socket count, right? So, now my server cost and my CPUs are going to cost considerably more. But if you want to stick with the two-socket systems, the cheaper ones, I can do that, and I can still attain the 8 terabyte of memory by combining 2 terabytes of DRAM plus the capacity in CXL, right? And, again, this is adding cars. You know, if you bought an appliance, this was a pool-based system, you know, you would calculate accordingly for that as well. And, of course, if you want to get down to 32 terabytes, well, now you're really stuck with 4 or 8 sockets, which is terribly expensive, but you can still offset that with a boatload of CXL. So... Does that help?

So, I mentioned at the top of the talk about data stores, right? So, where could I store my data? Obviously, everybody here is focused on storage, which is great, and that's the persistent way of doing it, but there are two projects currently in open... Well, one open source, one is ours, proprietary, that store data in memory using CXL shared memory. So, very much like your appliances that are hooked up to all different servers, I can do the same thing with a memory appliance that's connected through a switch to many different servers. So, Gizmo here is using that CXL 3.0 sharing capability of the appliance switch. We're an object store, right? So, if your application is object-based, Gizmo is the preferred choice. On the other side, the right-hand side, this is still in development. It's open source, being driven by Micron, and it's a FAMFS. It's a file system-based solution. So, if your application is file-based, meaning you do the open, the close, the reads, the writes, FAMFS would be the preferred method for accessing data in a shared memory object store here, in a shared memory file system, I should say. However, neither are currently persistent. There are plans to tear the data out into persistent storage, but since this is all active development, we haven't got there yet. But, you know, you can start to see that the ecosystem is driving towards this ability for us to keep data in memory so that when those GPUs need it, they can go direct to memory. You don't have to do all the loading and paging and everything else. I know the GPU drivers have the GPU storage direct, right? So, you can do DMA operations, and effectively, this is doing the same sort of thing, but at a memory operation there. So, it's all byte-addressable. I don't have to worry about blocks and all of that stuff.

So last slide here, I would say, you know, don't wait, right? This stuff's pretty real. Talk to your OEMs. Talk to your server guys. Figure out, you know, what capacities work for you, what devices are available, have been validated. Look at your applications. If you're in AI/ML or not, that's okay. It still works. And then if you're not quite there yet, go play with QEMU. Again, QEMU delivers all of the features and functionality that I need. It doesn't have necessarily the performance that hardware devices have, but it's a great way to get your feet wet, to understand the technology, figure out if it's right for you, and figure out what features of CXL you would like for your applications or solutions. And then join some communities. There are plenty of them out there. MemVerge runs the CXL Fabric Forum. It has almost all of the CXL vendors in there. We have Core, quarterly webinars. We all get together and talk about what's new in our little corner of the ecosphere. And then attend a lot of the CXL talks that are coming up at this conference. There's a track in here, and there are many other tracks as well that I saw all talking about CXL and memory capacity. So with that, I thank you very much for your time. I'll open it up for questions. Thank you.

Go ahead.

On your reg slide, you had a feedback path for the chat history. I'm just curious about what kind of heuristics you're able to develop from that.

Yeah. So the question was, on this feedback loop here—from the answer back into the chat history—what heuristics can we attain from that? And this is more what LLM generations call the "memory," right? So, a lot of the big models, your ChatGPTs and Geminis and things like that, they're all talking about my memory. Now, of course, "memory" becomes an overloaded term, right? In that situation, what they're talking about is: how far back in my conversation with you, or me with the chatbot, can I remember? And the memory is getting pretty big these days. And that's effectively what we modeled here; now we can have an almost infinite memory, that we can go back through our chat. You know, if you and I have this conversation on a Monday and I get to Friday and go, "Well, I kind of remember talking to Bill on Monday, but I don't quite remember what it was all about," then I can ask a question and go, "Hey, I spoke to Bill on Monday. What was it about?" And the response will come back through this query and feedback loop of going, "Now I'm going to retrieve that conversation, pass that into the LLM, and now I can remember what was going on and maybe even start a conversation or start a new conversation from that." So that's what this feedback, specifically for the chat history, was all about anyway. But obviously, this takes up more memory, right? The more you chat with it, the bigger it gets. Cool. Yeah, another question?

One of your evaluation slides said memory tiering does not work weighted interleave does.

Yes.

Can you elaborate on that?

Yes. Let me see if I can find that. So, the question was, the evaluation slide said memory tiering doesn't work. Yeah, let me see if I can find it real quick. Anyway, the question was, memory tiering doesn't work in this type of environment. And the reason is, it's because of the driver optimization. So, with NVIDIA, they have this concept of memory allocations at runtime, right? Just like any other application. But in order for the GPU to access that memory, the optimized path was you should pin that memory in place so that it doesn't move around, so the GPU knows when it gets a page fault, now I go directly to this DMA address, grab the data. If I don't pin the memory and I let it float around like any other kind of piece of memory that a typical kernel VM subsystem would do, then the kernel has to manage the memory on behalf of the GPU. Meaning, now the GPU has to go out there, now the GPU has to go back to the kernel and say, "Hey, can you give me the physical address based on this virtual address, do all of that?" and it just becomes slower. It's entirely possible to do, so if you definitely want latency-based tiering for GPUs, you just don't pin the memory effectively. But that's not the optimized path. The optimized path is: pin the memory, let the GPU do the DMA. Now because it's pinned, nothing we can do, right? I can't move it, and the kernel's not going to move it, so now it becomes stationary. So, hotness tiering, that makes no sense in that type of environment. But I can still do bandwidth optimization because the GPU doesn't care where the memory is. It can sit on DRAM, it can sit on CXL, and just DMA to wherever it might be, right?

And so the statement only applies to like an accelerator GPU setup?

In this example, yes. It's an accelerator-only statement, yes. Yes.

In your cost analysis table, what was your assumption for the cost of the extension card, and how many DIMMs per expansion card were you assuming?

DIMMs per expansion card were eight in this model.

What was the question?

So the question was, what expansion card was I inferring, and how many DIMMs on that expansion card do we model? And the answer was: eight DIMMs per add-in card.

And the cost per add-in card relative to the memory inside it?

I won't give you that number, but it's... Yes, we did, yes. So, we bought the card and we bought the DIMMs, yeah. Now, one of the other benefits of an add-in card over, say, an E3—and I'm not knocking the E3s because they are very good—is that I can reuse my memory, right? So, we model DDR5 and DDR4. So, if I'm replacing my fleet of servers and they're all DDR4 and I'm going to DDR5-based servers, well, don't throw away that memory because you've already paid for it. Throw it into an add-in card. It doesn't cost you anything. Now, you're just paying for the add-in card cost, and this number comes way down.

I just want to make sure that you counted for...

Yes, we did. Yeah, yeah, yeah, yeah. So, there were some more questions over there, I think. Yes?

So I'm curious about the price of the cost of the CXL in this table?

So the question was, do we consider the cost of the CXL in this table? Yes, we do. So this is the cost of the card plus the cost of the memory at retail prices that we are aware of, right? And, you know, the DRAM prices change every day, so it's hard to predict, but it's a good approximation for the purposes of the exercise of this discussion, meaning, again, the takeaway is I can save at least 50% in this model of memory costs if I go buy the cheaper DRAM plus go buy CXL, which, again, is more expensive, right? I mean, it's $6,464 for DRAM, two terabytes of DRAM, and then $16,000 for two terabytes of CXL. But the combined system price is still 50% cheaper than if I have to go up to the next DRAM capacity, right? I don't consider MRDIMMs or anything like that in this model. You know, that's all coming down the pipeline, but, you know, that would be another technology that we could definitely look at. But fundamentally, everything that I've said works with MRDIMM and everything, from an application perspective anyway, so... Yeah, another question?

Should we include the cost of the first lock within the server? You know, like, most server slots are valuable.

Yes, yeah. So the question was, should we include the cost per slot? Yes, I mean, we don't include the system price in here, but, yes, I mean, potentially, you could add the system price and do that calculation of, you know, dollar per slot, that type of stuff. Because, yeah, I'm using PCIe slots instead of DDR slots, and that's taken away from maybe some of the GPU slots that I need, right? So there are some disadvantages with this approach, but if you don't need eight GPUs and you don't need all your slots, fill it full of memory and let the GPUs sing, right? So... Yeah. Good question. Thank you. Yes.

How do you view fabric management orchestration? Do you feel it's getting settled, or is it still...? What are your thoughts in that space?

So the question was, how do I feel about fabric management of CXL devices? It's settling down, right? The CXL consortium is... building out, or has built out, the CXL FM APIs. That has been taken and implemented directly by Jackrabbit Labs, that open-source project called Jack, and Grant, I think, will probably talk about it later today in his talk. So from that perspective, yes, I think it's definitely getting there. I think the missing piece, though, is the spec came after when the Switch guys started development. So the Switch APIs don't match the FM APIs, so now we have a little disconnect, which I'm sure will, you know, over time be resolved to implement that. So at the moment, we're using the Switch management commands and APIs that the vendors give us, not necessarily the open-source stuff. But again, you know, it's fairly trivial for the open-source to say, well, if I detect Switch vendor A or Switch vendor B, I'm going to use these APIs and still support the FM APIs that are available. Otherwise, we just currently just use the commands and the APIs directly right now, so... Good question, Jan. Any other questions? Yeah?

In that FlexGen study you showed, are you preloading the data from this?

Yes, yes. Yeah, so it's preloaded, and it's warmed up, yes. So that's why there's no... Nothing before this huge spike to get me up to close to 100%. The data's already paged in, effectively up to a point. And then, it's only when I exceed what data was already pre-warmed up that my performance drops a little bit, in the case of CXL or BlueLine, or dramatically, in the case of having to go back out to disk to page it back in, so...

What was that question?

So, the question was, was the data pre-warmed before starting this benchmark? And the answer was, "Yes." Okay. I got five minutes, so... All right, well, I'll be at the show all week. So, thank you very much for your time again. Appreciate it. Hope you enjoyed it. And thanks for turning up first thing in the morning.
