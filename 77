
All right. Good afternoon. My name is Charles. I'm co-founder and CEO of MemVerge. I'm going to in the next 15 minutes introduce a new software project called Gismo. It stands for global IO free shared memory objects. And it is a software written and designed for CXL. And it runs on CXL 2.0 fabric. It can allow multiple nodes in a cluster have another way of communicating with each other and share data with each other. All right. So let me get into it. 

The I think you have probably seen different versions of this slide in almost every talk where the growth of AI, the large language model, driving the amount of parameters, amount of data that need to be processed quickly, thereby needing more capacity and more bandwidth from your memory subsystems.

And in the AI workload, aside from memory bandwidth and memory capacity, there's another bottleneck. And that's related to IO. When multiple nodes need to talk to each other or need to have access to the same piece of data, inevitably they need to incur IO. And it's either networking IO where they need to be serializing their data and put them on the wire to send to the next node or share data through some shared storage systems. And this is a bottleneck because it slows things down. Because of serialization, deserialization you need to do in the software. Because of the various copies you need to make at least from A to B, but there could be multiple other copies that happen in the stack. And when it happens to the shared storage, sometimes storage media is much slower than the memory media as well. 

And we believe the emergence of the CXL standards and various products that's being built is a great solution space for this problem and that can alleviate this bottleneck. 

And in particular, with 2.0, as you probably heard from others and there are probably solutions being displayed and we're working with a few partners on the initial prototypes where you can envision a memory appliance that can be connected to multiple hosts through PCIe Gen 5 or Gen 6. And within this appliance there could be a switch controller that allow multiple hosts to access any of the CXL modules that are connected to the switch controller. And then on each of those hosts there could be different workload running and we can have our software running on those workload. And there are typically some management software that are managing this fabric that's either running inside the appliance or running on a management server somewhere outside of the appliance. 

And in this context, essentially that we can potentially enable at least initial prototype that we have a demo that allow a memory appliance to be connected to be multiple nodes, connected to the multiple nodes and each of these nodes can exchange data not going through regular Ethernet network but by accessing the same area of memory. Now this does introduce consistency and cache coherency problems. When you have multiple nodes accessing the same memory there could be CPU cache that are out of date and therefore introducing to incorrect results. And CXL 3.X standards are designed to address this problem to be implemented in the hardware and 2.0 does not support this. And so what our contribution at MemVerge or Project Gismo is to implement a software layer that runs on CXL 2.0 hardware that can process in the consistency and cache coherency between multiple nodes in software to ensure correct results when you are sharing memory this way. And now we do not do that for general purpose any of the access patterns because that will introduce too much overhead. And the initial patterns that we work to support is a single writer, multiple reader type of scenario where you have at any time one of the nodes doing the writing and all the other nodes doing the reading and this is an easier scenario. And the initial API we designed for it is an object API where one node can be writing or appending to the object while the other nodes can be reading from it. And that's the kind of design objective of the Gismo project. 

So if you see the Gismo project you have multiple hosts connecting to the same memory that we have SDK that the software can be linked to and we have a manager which is a daemon that is running somewhere and these SDKs can be communicating to coordinate with each other.

And we present an API that's a pretty simple API similar to object API where you can connect to it and you can essentially create your objects and you can write into your objects and after you write it you can seal your objects. And not available in this version but it's going to be available in the next version. Any time you can unseal your objects and append to it and reseal the objects for everyone to access. And we will also include the synchronization mechanisms to allow the different nodes to coordinate with each other.

And to prove whether this works and how it does to the performance we've been working with a few partners who are creating applications or frameworks that we would integrate with or they would integrate with our SDK and to show number one it works correctly number two it shows improvement over the status quo. And the first of such software that we integrate with is a framework called Ray. It is a open source compute framework for AI. The company that's behind it called AnyScale and it's quite popular. It's growing in popularity among the AI developers. 

And so what we did is replacing the object that's in memory inside of Ray that's using Plasma today with our Gismo. And what Plasma does it's a single node shared memory object store and that Ray already integrated with. Now if the object is not on the same node that Ray has the capability to look that up and then copy the data to the same node before you can access it. With Gismo that you can look it up anywhere and you do not need to do such copying anymore. You can just access it from any of the nodes just directly from the shared memory that's delivered from the memory appliance. 

And so to illustrate this I have a very short one minute or so demo which is essentially showing the comparison running some shuffle benchmark on Ray on the original setup compared to running with Gismo integrated. So let me show this video. Hopefully it will run here. So first we'll show the baseline and you will see there are four nodes here. Each of them have four vCPUs and this is a slow baseline and we are running the bench the shuffle benchmark of about 20 gigabytes and now it's running it and so it's going to take a little bit of time to complete and when complete you will see it takes about a minute or so. 57 seconds or so for this shuffle of 20 gigabytes among these four nodes to complete. So this is 57 seconds and then we're going to run it with our system with a Ray integrated with Gismo.
Again the same four nodes the same 20 gigabytes and now it's no longer using the plasma but it's using Gismo so it's using the CXL shared memory to do so and it's roughly half the speed about 30 seconds when you run this shuffle benchmark.

and you know this is just a quick example with a small set of data. We also have run a number of other benchmarks to show that it works correctly and it is faster. You know so here are some some of the data if you are they are accessing the object on the same node on the baseline 0.4 seconds with Gismo it's also 0.4 seconds which is good because this means accessing local memory and accessing shared memory in this case it's not slower it's the same speed. Now you if you need to access from the next node it's about seven times faster rather than 2.7 seconds it's 0.4 seconds. There's 20 gigabytes is what we just show 57 seconds to 30 seconds and when we shuffle 50 gigabytes it is a more improvement 5 to 515 seconds drop to 185 seconds. So this is just a demonstrating an easy demonstration of using shared memory to share data makes it faster. 

and the reason is that it takes out these serialization deserialization the I/Os the copies and also it's a more efficient use of the memory with less copies you do not have to replicate the memory usage therefore they're less likely for the spilling to happen as well because there are more more memory being accessible. 

and and on this example we are collaborating with the Ray community we submitted a enhancement proposal to creating a pluggable object API so that a new object such as Gismo can be easily plugged in and so can other type of alternative object stores and this is currently being worked on. 

and this is not the only use case a shared memory system powered by Gismo can be applicable to a number of other use cases we are working with various partners and customers on these use cases in the early exploration of these use cases including creating a cache that's powered by shared memory rather than individual local cache for a distributed application for for higher performance you know higher hit ratio and and more efficient use of memory space we are looking to support various entertainment and gaming companies distributed rendering use cases and we're working with a number of database companies on the distributed databases use case as well. 

so if you are interested learn more please contact us gismo@memverge.com that we are looking forward to talking to you if you are developing an application or if you're in the field of AI ML and are you looking to increase the performance of your system so that's the the end of my presentation and open to questions. 
