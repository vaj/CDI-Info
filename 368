
So I'm here to talk about IOMMU, if you are IOMMUFD again, and and the work that's going on for this generic page table thing, and uh, I have a couple of slides about where we are now, and what this even is. I think I probably should still include that, and I'm hoping to have um, I know a couple of people had come and said they want to talk about things, so feel free to bring up what you want to talk about. We can talk about it in the room. So to put some grounding here, IOMMUFD, this is the picture that Kevin made a few years ago. I keep recycling it, but it replaces the uh, the container, kind of VFIO type 1 interface, with a new file descriptor called dev IOMMUFD that can do all the same maps and unmaps, and managing things, and we've been adding more and more features to it to allow the accelerated virtualization features that are present in all the modern IOMMU hardware to be fully utilized and exposed to the guests. So we can have accelerated IOMMU in the guest, and in the industry one of the motivating factors for this is that we want what's called virtual SVA in a guest running at full speed, and virtual SVA is sharing the CPU page table with the IOMMU hardware, so there's really no opportunity to do crazy paravirtualization tricks like everybody does up till now, so you need really full transparent assigned stuff to it.

So uh, for a while we've had the base stuff, this is like the summary I make every year where we're at. Uh, we got PRI fault delivery and nested page tables merged since I was last standing here. Um, and there's a whole bunch of stuff that's in flight right now for these other features. So, passive, we talked about last year. VIOMMU is a relatively newer concept that's encapsulating some of the hardware acceleration features, I talked about. Steve is working on change process and maybe memfd as a way to hitlessly restart QEMU, like you can take a QEMU and you can k exec to a new version of QEMU and all your VFIO DMA continues hitlessly, which is really needed. I guess memfd has become interesting for confidential compute, AMD needs it to back its private memory pages, uh, consolidated page table, generic page table, that I'm going to talk about. And the last two, the SIOV is kind of, VDPA has kind of seemed to have fallen off my radar, at least there hasn't been any postings in quite some time. Um, and then the new topics that have come up is the confidential compute TDISP enablement, which Dan and a few others, hi Dan, are are talking about, that I guess I'm supposed to talk to a lot of people about too, and Nicolin is going to take a run at doing the ARM its direct routing, so this would be to put its physical page directly into the IOMMU page table, so that all the mechanism that's imagined to work on the ARM side works the way it's meant to work, so he's going to make a go at that, we'll see how that looks, he hasn't quite started yet, but um, it seems like that's the right thing.

Uh, this is my driver modernization slide from last year, so again, if you're working on IOMMU drivers, we're trying to get everybody into a kind of a more modern situation. We've made a lot of progress, I think, on these topics. Uh, ARM is in pretty good shape now, Intel's like 90%, and AMD's made quite a bit of progress on some of these topics. And there are patches on the list now to do, like, um, some of the identity domain things and some of the improved passive things. But we see use cases that these unblock that we want to get everybody to the same kind of functional level. Certainly, I don't want to see different functional levels between the different uh, server drivers. The, the, the embedded drivers can do a more basic thing, but the server drivers supporting personalization and VM should have, um, a more complete set of functionality.

So, the new thing that um, I've started working on with um, Jao and um, Alejandro from Oracle is this, this concept of deduplicating all of our page table code. So this came up last year, and I think with Sean Christopher, forgive me, I mentioned in the room that's like, you should do that, you should do that. Earlier from KVM, we felt it was kind of a mistake to let it linger for so long, uh, so we're working on that now on the IOMMU side. I posted an RFC patch series that shows what it looks like in sort of the general design, um, and coverage for whatever was a seven different page table formats that we support, and showed that they could in fact be brought together. The design I came up with has a small help file that, it's kind of like the mm architecture code that defines uh, the logic to do PMDs and on all the page table levels, and then generic code again, much like the mm has, where you could write something like get user pages or map or unmap or Cut in IOMMU language using those helpers using a common API, and then you just replace the the format code, and then recompile the whole thing. That's the same way architecture and mm have a relationship, so uh, it all works out. Um, we're at the point now of, Alejandro has a draft patch to convert AMD's driver to use a new mechanism. It's still not quite working, but we're continuing to make the progress through the system.

So it's um, it turns out that all of the different page table formats are actually quite quiet similar, but every single one of them has its own little special thing that's a little bit different than everybody else. So, this is sort of a summary of if you take everything and cram them all together across all the different things you need to support, like two to six page table levels. AMD has six page table levels. Um, ARM has a neat trick where the top level can be like a bigger table than the lower levels, instead of being like a one page, it can be you know several pages big. That has to be supported. Uh, huge pages is a leaf page at any of the page table levels, so like AMD supports this at every single level, and other architectures have it in different places. The contiguous pages thing just recently got merged into the core mm. This is you can set a bit in a PTE and it says that the next 16 PTEs, for instance, are a kind of a huge page, and they share the same address. That improves um caching in the TLB caches, and and all the IOMMU caches too. At least some of the hardware supports that. Um, the one of the designs that I wanted to get out of this is that you could write this this fully inline thing and get the kind of performance you get out of the mm. So in the mm, you'd write something like you know PGD, PMD, PUD, whatever PTE, and you'd get this sort of function that was fully inline, and all the math was you know, constant propagated, and just like perfect. So we can obtain that too in this new scheme. Um, so one of the the hot paths is to do_iova_to_phys , and when I look at the disassembly of what can be built here, it's like uh, uh, maybe 300 bytes long, fully inlined, no jumps, no memory references, like no stack, and no function calls, just go bang bang bang, exactly like the uh mm would would give you. So it's a little bit difficult to do, but it does seem to turn out fairly naturally, and in a different approach, I've got the next slide on this, and the last thing is that there's a kunit that covers absolutely everything. See all the architectures, um, all the algorithms are kunit tested, uh, at boot time on like x86 or whatever, or whatever you want to do, and that's turned out really quite well, I thought.

Um, so here's kind of a code example of what it looks like. This is the iova to phys, that's fully unround, so it looks not like the mm, but the concepts are similar. So, you load an entry into a state. I kind of ate what Matthew did with X-ray, and you know, depending on what it is, you you make your decision logic here. There's some macro magic in the in the background that does all the unwinding. So, the mm version of this is, you actually have to write six different versions of this function call, calling six different like, different functions, because that's how their API works in this version. What I did is, instead of saying PUD or PMD or PTE, there's a level number, a level constant, and then inside the, the, the format code, it looks at the level number to figure out which level and which thing to do. If the level number is seen to be constant by the compiler, then it generates the same code as if you had called this correct function, like the mm does. So, the trick here is that the macro invokes the function on the screen with a constant level number, and the compiler sees that it's constant, and constant propagates it all throughout all the code, eliminates all the branches, compresses it down, and you get a version of this for like level zero, one, two, three, four, and then that in turn is then inlined into another function that generates the thing that you'd see with the mm. So, it's you end up having to write um, your your function for a single level, and it magically expands to all the levels, and it optimizes and constant propagates everything. So, there's two options: you can write it like this, or the same code with some of the annotation removed will also work as a recursive function, like is typical in the IOMMU today, and we can look at how much code is consumed and how big these things are, and we can make decisions like iova to phys, it's actually smaller. The fully inline version is smaller than a lot of the drivers' is recursive implementation for a single level, so it's, you know, it's like five times less instructions involved here, it's a pretty big improvement, but for things like cut and map on the map, which are really complicated, it's probably not worthwhile to to fully unwind.

Um, so the reason that we're doing this and that it's come up is because there's a lot of improvements we want to make to this API that just would be so painful if we had to go and change seven drivers' implementations of this format. And some of the things on this list are fantastically complicated, like I did a draft of CUT; it's very hard. There's a lot of complex details, and CUT, especially if you have dirty tracking turned on, you want to be able to change, restructure the page table hitlessly while the hardware is writing dirty bits to it at the same time, and then propagate the dirty bits so no dirty is lost. It's really quite difficult, and there's quite a lot of code and supporting things like contiguous pages, and still being fully optimal in all the cases, is it's complex enough. Um, so my implementation of CUT is actually larger than map and unmap combined; it's it's that bad. And then all the other things are also kind of complex in that way. So one of the complaints that I see from quite a lot of people is that it's very slow if you're doing VMs and you have an enormous amount of memory, like terabytes of memory in your VMs because you're, I don't know, AI or databases or something. It can be quite slow to set that up and tear it down because the code today has to go 4K at a time to sense the page boundaries that were stored in the page table. And it's fantastically slow and unnecessary when we can just have the page table code walk the page table and tell you, 'here's the one gig page that I know is there, just there it is.' You don't need to know anything more. So we can get magnitudes of performance improvement on some of these operations here. I've been talking to Christoph Helwig about doing a biovec direct mapping for the DMA API. Leon is working on a new API for the DMA API that would naturally fit that in. I think there's also opportunity to optimize the sg path that we have today. If we lower it down, we don't need to keep re-walking. We don't need to do as many indirect function calls, which are quite slow on x86. And then when we talk about dirty tracking, which is, I think I was talking about it here last year, is a big thing that we got landed. This is the ability to do live migration and keep track of the data that's being dirtied by DMA so that you can live migrate incrementally. For performance on the IOMMU, on these big VM cases, you want all your pages to be mapped with one gig so that you get a high hit rate on your TLBs, your IOTLBs. But when you're doing dirty tracking, one gig granularity is completely useless. So the methodology that people want is you run at one gig until you start to do a live migration event. Then you'll take the one gig and you'll shrink it down maybe to two meg. Then you'll do your live migration, and you'll be done. And if your live migration fails for some reason, then you have to take those two megs and you have to put them back to one gig. So your VM goes back to running at full performance. It's not impeded. This is also very hard. It's like a more general version of code. And you have the same issue where you're hitlessly restructuring the page table while dirty bits are coming in that you can't lose. So again, it's very, very difficult. And I'd also got some ideas about how to address Pasha's sort of note about from last year's session where we have a lot of memory wastage in certain VFIO cases. If you're circling through memory, you end up having empty page table levels just sitting there and no way to free them. But I have some notions of how that could, at least for IOMMUFD, how we could truly fix that.

So in terms of getting this actually merged, I've been posting what I think of as like the maximal implementation. So this is like all of the stuff combined together, all of the formats, all of the weird little features. And so that people can look at it and say, yeah, you know, here's the big picture. This is what it looks like. But what I intend to do is to take that and remove all of it and narrow it down to just what AMD needs. AMD has the simplest hardware for this. They have the simplest flushing logic. They don't have any incoherent DMA issues like ARM and Intel have. So it can be very, very simple. And then we can move on to kind of two forks where we're adding support for more drivers and we're also adding more ops. Like we could then go and add cut. We can add the other things from a previous slide. And I hope to get to the point where we've added enough ops that there's like undeniable value to implementing the new implementations and then we'll finish off the rest of the drivers. So I've done enough that we could, in theory, do all of the current IO page table drivers, but I don't know that we actually would want to. There's a lot of embedded ones that do, but also who would test it, who would see that it's not broken and talk about that in a few years. So that's all the slides that I had prepared. If anyone wants to talk about—Stuff said he was here and wanted to talk about something. Catch box.

I guess I have nothing special about this, but does it make sense to like, extend the IO pagetable library like this?

The IO page table library isn't really, it's more of an API contract than a library. Like, it doesn't actually help the implementations do anything really. Like, it tells you what implementations would opt to call and what you should do, and what kind of quirks you might be able to have, but like, it doesn't give you any help to walking page table levels or dealing with allocation very much, or, you know, kind of these issues. There's another little side library that helps you with the memory allocation.

Yeah, exactly. So, for example, like the second problem you mentioned about the, like I go to this before unmap, I've been looking into that, and I have some patches—not on the list at the moment—where you can just add a walker to the IO page table library, and then you can have your own logic in the walker, and then you can avoid having the IOVA to phys.

Yes, but then you have to go and add a walker to seven different page table formats, and then validate it. And then you're still not as efficient as doing it concurrently with doing it during unmap.

Yeah. Yeah. I think we, we need to just look more into these like benchmarks. We need maybe, to check the prototype, make some more benchmarks, and make a conclusion about them.

So a bit of a different topic, but when I first tried to use IOMMUFD, just with vanilla upstream Linux and QEMU, and just pass through an NVMe device, it fails with 'peer-to-peer DMAs can't be mapped' and like QEMU just fails to start. So I wanna ask: what is the actual status of like IOMMUFD support in QEMU, and why is peer-to-peer DMA such a problem? And is there a plan to fix that?

There is a plan to fix that; there's patches in RFC from it on the list already. Peer-to-peer DMA is such a problem because the existing implementation has a big security issue. And I have been reluctant to copy the security issue and the resulting kind of user-space experience into IOMMUFD. So my intention is to use DMA buff to transfer the physical address of the peer-to-peer into IOMMUFD so that it can have a reference on the MMIO memory and avoid the security problem.

But without this functionality, like, can you actually use IOMMUFD to pass through a PCI device at the moment?

Yeah, I don't think QEMU hard fails. I think you get some.

Exactly. Yeah. Yeah, you'll get warnings and the mappings won't be there. You won't have peer-to-peer in the guests, but it should work.

Yes, that's my understanding. And the people that do want peer-to-peer to work have written themselves a patch to reintroduce the security problem into IOMMUFD. So, if you would like that patch, I can get it.

I don't know, maybe QEMU shouldn't hard fail. Maybe I was using the wrong branch or something. But for me, I ended up having to work around it in QEMU, and say, like, ignore this IOMMUFD error. And this may be one that doesn't actually work out of the box.

So maybe the VFIO logic ignored it, and the IOMMUFD logic fails it or something. I haven't looked into this.

As far as I know, it works like we described earlier. So, I think there were some unique cases about cold versus hot plug, but I think in general, it should always work.

Great.

Yeah, thanks, Jason. I think the goal of this is really good. I really like the idea. You know, you can make higher level changes to the API without having to fix some of those nodes of code. But I think I'm—hopefully understandably—nervous about abstracting away potentially some of the quirks that exist in the low-level page table today because IOMMU is a bit like the Wild West compared to the CPU, right? So one of the things that makes me nervous is the on ARM. You're probably aware, right? We have these break before make rules, but you have to go via invalid entries. And today we kind of just pray that the hardware is going to tolerate us not doing that.

I noticed that. Yeah.

Because, because honestly, if you take a fault on DMA there, you're a huge trouble, worse trouble than just praying that it works. I think when you get to a point that you're doing the kind of pretty aggressive optimizations, things like the dirty bit stuff you were talking about, cracking block entries, contiguous bits in the mix. I really worry that our prayers may no longer work.

Oh, no. So, so, like, I am aware of the BBMLs on our right. So, the the way this works is there is something analogous to what was called quirks in the page table thing. I, I've called them features here. So one of the features is you're you're allowed, you know, your hardware will tolerate a break, right? So if you set the break thing, then you get Cut. If you don't say, your hardware will tolerate it, you don't get Cut, you don't get degrade, you don't get increase, right? So we will close that hole. But this is also why it's a little bit difficult to do some of the stranger drivers, because you have to understand what the hardware doesn't have to sell, set the right features. Right? So, and feature. Features are going to be things like what kind of manipulations can I do without causing my hardware to catch fire? What kind of flushing rules do I need to follow? Do I need to do things like the DMA coherency business? Those are all like what I've called as sort of IOMMU features or like the hardware features that they're separate from the page table. So I can do all of the Cut things. I can show that it all works on like the ARM formats, but maybe your IOMMU driver won't turn it on because your hardware won't support it. So for SMMU v3, I imagine that we will read the IDRs, we'll learn the BBM level and we'll set the correct bits in ARM today. I noticed that it implements something like Cut kind of copied from Intel, but it doesn't follow the BBMLs. That's probably okay because that will never actually be used. It turns out that the only VFIO type zero or 1.0 would ever trigger that behavior, and it turns out no one ever actually uses that. So it's all. Just. Dead code anyway. So my expectation is we'll just cauterize that off. If the driver doesn't say it supports the correct BBM level, you will not get that feature on.

Okay. Makes sense.

And, you know, like to the hardware people that may be listening someday, if you implement dirty tracking on your IOMMU, make sure you support BBM level two and all that other good stuff. It will be very important. Sorry.

Yeah, I also like this project, and so for me, what I'm looking forward is like an ability to have abstracted macros for set PT, clear PT, and basically, every time we update a page table, that it's an abstracted macro, which we could use page table check to also learn when we add and remove any entries to IOMMU page tables to add some sanity and uh, ensure that we don't do uh, like wrong double mapping or inserting a page that is already incorrectly shared with generic page tables and so on. So basically, extend page table check to IOMMU page tables. Another thing is that we have page walk and we have page table dumps. Those are implemented for CPU page tables. It would be very useful to have something very similar for IOMMU page tables as well. And this is an enabler for that.

Intel's driver implements a DebugFS for dumping the page tables, and my plan was to make a generic one of that in DebugFS for this framework as part of replacing the Intel driver. As you go through drivers, you have to add more things. I haven't written that yet, but it's not too... The challenge there is... Because you have to use RCU and Intel has like a technical bug that if you go and use their DebugFS concurrently with map on map, you're going to create use after freeze because they don't have any locking. And the scheme here that I've got, it uses like the new sort of folio approach that Matthew's got where you can make a descriptor subclass kind of like, again, like the PT desk is in mm. And so we get ourselves an RCU head and we get ourselves a list head. So we can RCU free the page table memory, implement an RCU save walker. One of the technical challenges that I haven't quite decided on what we want to do about is sub-order page table configuration. So like, say you're running on 64K arm and you want to use for some reason a 4K page table format, then you have to track... You can't use the PT desk approach like you do because there's too much data to fit in the 64 bytes. So what I'm probably going to do when we go to optimize that is to jump ahead of Matthew a little bit and implement his next idea, which is you just hold the pointer to memory that you allocated so you can allocate the right number of RCU heads and the right number of list heads and say, that's my descriptor for this 64K page. And then you can subdivide it up into these bits will be for each 4K and you can keep all the same algorithms going ahead. Especially on arm hardware, there's some weird reasons why you might want to use a 4K page table on a 64K system because the 64K page table option doesn't support all of the huge page sizes that you might want to use. Like it doesn't support a one gig page size, which is not what the people doing VMMs necessarily want.

Hi. So... all these interesting optimizations kind of look like the stuff when we implemented the VDT emulation in QEMU, we kind of assumed people won't do. So I'm just curious if you're testing inside KVM?

We haven't got like, we haven't got to doing Intel, right?

Yeah. You might need to then maybe test that. If it doesn't work, maybe kind of disable some of these.

So this is going to be...

I don't know.

This is going to be a problem for Intel, right? You know, it's like what Will was saying, if your implementation, be it paravirtualized or physical, can't handle these scenarios, you need to tell them somehow. So we've had a number of issues on AMD and Intel where their paravirtualized environments don't behave correctly. And they... the VM is unaware. And they have missed adding negotiation in their paravirtualized interface to tell the VM that it's operating in a defective environment and you need to do something different. Like AMD has this issue now, I forget what it was about, but they forgot to add a feature negotiation. So they're going to add some new thing and it's not going to be enableable in a VM, which is really bad, but I don't know how they're going to want to solve these problems. Like I'm not the architect for their hardware. But they are going to need to have feature flags, if that's what the situation is. Or ACPI or something. Fortunately, they're not really essential, like unless you're doing dirty tracking. So that's still a forward-looking feature on Intel hardware.

Sorry, but I have to ask you to wrap it up. And I think also because it's time for a break, so I think we can...

Oh.

Yeah. Good. Very quick question, but very quick, it must be.

Okay, one question. Just I see you have the virtio, maybe you can support on the list in the last slide. I want to ask some people implemented or emulated the IOMMU for the guest, but some direction, another direction is to use the virtio IOMMU. So I think that there are some patches about virtio IOMMU implementation on the mailing list. How do you think about that?

I actually don't know the current status of virtio IOMMU. It has the current... What's currently merged in Linux is a hyper call per map and unmap. So it's not going to be able to do VSVA, but it's certainly a nice way to get something like a DMA API security kind of model, or maybe get DPDK to work inside your VM. I know that there are patches around to add nesting support, which is where you could do VSVA, but I haven't seen them reposted in a long time. I'm certainly completely happy if people want to pursue that. And I imagine doing it by having the drivers be a little bit involved in the virtio thing. Because if you're going to emulate a virtio IOMMU, you do need some support for the driver to understand how to translate the virtio kind of language into the hardware language. And that's all very solvable, very, I think, very straightforward to do. We just need to actually... People who are interested in it need to come and do it.

Yeah. I just wanted to say...

I'm sorry. I'm sorry. We need to wrap it up, guys. I'm sorry. We need to keep it online, not before thanking Jason for his presentation. Thank you very much.
