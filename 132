
All right, 2.31, so might as well get started. Welcome everyone to the CXL Microconference. Very glad to see you all. Thank you for our sponsors. And I'm also supposed to say that we are under anti-harassment policy, so please everyone behave. [BLANK_AUDIO] And yeah, we have till 6 o'clock this evening. The way it's structured is kind of a bit how we do the online upstream meetings with a state of the art of QEMU first, and then we'll have presentations upon some of the different things driver-related that are being worked on currently that need proper discussion, and then gradually go on to some memory management topics towards the end. The idea is to discuss, participate, and when everyone has a question, please stand up so that you can be seen. And yeah, let's go ahead and start. First up, QEMU.

Hopefully good enough. Okay, so welcome back to what is effectively the second presentation on the QEMU side of things. Last time, we spent some time introducing why we were doing emulation of CXL in QEMU. This time, we're going to jump straight over that bit and move on to kind of a bunch of stuff on what's new. So Fan Ni is going to be co-presenting on the basis that he's done a lot of work on the DCD emulation, so he's going to handle that part when we get onto it.

But first of all, yeah, a quick agenda. So we're going to talk a little bit about what landed since last year, some stuff on major topics in flight, and the key bit is discussion over what's next. As I say here, jump in at any point with questions. Just feel free to stick your hand up or stand up, and you'll get hit on the head by a thrown mic, and hopefully can, yeah, raise your question. I put a proviso at the bottom here, no, we're only going to talk about stuff in published specifications. I'm usually the militant one on this, so I may just say, nope, can't talk about that. But that's the very nature of CXL, is it's moving fairly quickly, and there's lots of new and exciting stuff coming.

Oh, some very small text. Sorry about that. Anyway, yeah, so before LPC last year, we did sort of basic enablement. Since then, a number of things have landed upstream in the last year, so thanks to those various people who worked on these. I'm not going to call out names because I always forget people. We've got volatile support, multiple HDM decoders, CDAT, RAS error injection, poison injection, all that stuff. Under review, various other things like dynamic capacity devices. The things with stars, hopefully you can see the stars, are the things that we're going to go into in a little bit more depth, so save your questions on those for now. Yep, things like CCI, we work in to support fabric management. That's been a big topic of -- I've been working on recently. Scanning media stuff. We have a bunch of things sat in the staging tree that we operate for QEMU that have not gone upstream for various reasons. I'm quite happy to talk about those. Let's address that a little bit later. And we have a few other things that have been posted for discussion on the mailing list, but no real intent yet to upstream them. And in fact, the next talk is going to involve some Type 2 stuff from Ira. I don't know if he's talking about the QEMU bit, but hopefully.

So, handing over to you.

Hello, everyone. My name is Fan Ni. I'm from Samsung. Today I'll give a brief update about the dynamic capacity device emulation in QEMU. So I'll start with a brief recap about the dynamic capacity device. So before the introduction of dynamic capacity device, if we want to add or release some memory capacities for CXL memory, actually it can be very disruptive. Because the host needs to be reprogrammed to cover the range of the device change. And also, like, all outstanding traffic to the device needs to be -- and also most of the time the system reset is required. So with DCD devices, it's much easier and convenient to do, like, allocation or deallocation memory capacities without the need to reprogram the HDM decoders. So the basic idea is that present the maximum capacity of the device to the host. And then the host can program the whole -- the HDM decoders to cover the whole DPA range. And the device provides a set of commands to control the real, like, allocation and deallocation of the ranges that can be accessed by the host. So this is the basic, like, unit for allocation and deallocation. It's called dynamic capacity extends. Basically, it's a continuous range of blocks.

So what we have now for dynamic capacity device emulation in QEMU. So currently it's quite simple. We augment the type 3 memory devices with dynamic capacity. So before that, we have, like, a volatile and non-volatile static capacity for type 3 devices. Now we have a third one called dynamic capacity. It's backed with 1 to 8 configurable dynamic capacity regions. And also in the device, we maintain our extend list representing the DPA ranges that can be used by the host. And also we support the basic read/write access to the dynamic region. And also provide some mechanism to validation to make sure actually the address range is backed by valid extends. So to control and use the device, we also implement some mailbox command. Like, for example, we can use, like, the get dynamic capacity configuration command from the host to retrieve all the dynamic capacity region information. Like, start address of the region, the length of the region, and so on. And the host can also use, like, a get dynamic capacity extend list to retrieve all the DPA ranges that it can use at this moment. Since this extended list can dynamically change when we add more extends or remove some extends from the host. So whenever we want to get the latest DPA ranges we want to access, we can use this command to do that. And the add dynamic capacity response command is used to respond to a dynamic capacity add event in the event log. So whenever we see a dynamic capacity add event in the event log, and the host can, after the host, like, process the event, and it can send an add dynamic capacity response to the device to show, like, whether it will accept the extend or just reject it. And whenever the host wants to release the dynamic capacity extend, it will send this release dynamic capacity command. This can be a response to the dynamic capacity release, like, event in the event log, or just because the host don't want to use the extend anymore. So based on the specification, like, the FM is the main component to issue the add dynamic capacity extend or release dynamic extend. But at this moment, we don't have a Fabric Manager in QEMU, so we use, like, a QMP interface to simulate it. So we can use this interface to send a dynamic capacity add or release command to the device, and the device will add the extends in the event log. The host can retrieve it from there and process accordingly.

So with the functionalities mentioned above, we can make some very simple tests and also support the kernel side code development for dynamic capacity support. But we are still missing something important. I think that's really something that makes dynamic capacity very, like, meaningful and useful. For example, currently, we only add dynamic capacity to the simple, like, Type-3 MEM device. But for more complicated devices, like multi-headed devices and the FAM or GFD, these devices are supposed to be the main scenarios we'll use dynamic capacity device, but we don't have the support yet. And also, for the DC regions, currently, we hard-coded as non-volatile, but this is easy to fix in the future if we want to support, like, volatile regions as well. And also, at this moment, like, we do not support, like, shared extends. That means each extend is only used by one host. So any, like, tag related to sharing actually is there, but it's not used or tested at all. And also, the extend list, the generation number is there, but it's not used. That one should be easy to fix later if we have the need there. And also, like, all the add or release capacity actually is based on the extend list. So low tag-based operations there. For example, for the release command, actually, based on the specification, we can release based on the tag. But currently, we don't have that support in there since tag is not used actively now. And also, all the, like, dynamic management command set from the FM side is not there because FM is not there.

So we also noticed some issues about the current CXL spec, 3.0 for dynamic capacity support. For example, the first issue we have noticed actually is that the spec mentioned that FM is the main component to add multiple extends in one request. From the definition of the request payload, we see, like, there is an extend list there. But the tags also mentioned that each request actually may at most result in one record in the event log. But based on the dynamic capacity event record, it can hold only one, like, one extend there. So if we want to add or release multiple extends in one request, actually, we cannot do it in one, like, by inserting one record in the event log. So that's something supposed to be, like, fixed in future versions of the specification. Another thing, similar thing here is that when the host, like, responds to a dynamic capacity add event, the tags also mention that the host will send exactly one, like, response there. But the response payload also includes an extend list. So if it's, like, a response to one record, so that means the extend list can hold at most one record there, like, just one extend or no extend at all. This is based on current 3.0 specification. And so this subset, like, mentioned in the text, like, extends accepted by the host can be a subset of what has been offered by the device actually can only be zero or one. So this is something that needs to be fixed in future specifications. There are some other things, but I haven't mentioned in this talk yet. OK, that's all for.

Can you force release from QEMU, like, the QMP interface? Or do you have that support yet?

That command is not added yet. But it should be easy because, like, you only need to change the event type.

OK. And then IRO is covering DCD on the kernel side, right? OK. I'll save that for later.

I had a general question. What was the motivation for nonvolatile support by default?

I think that's just because, like, even for the static capacity, we first support, like, a nonvolatile. And then on the kernel side, when we process nonvolatile and volatile, volatile seems to be more complicated because it needs auto-discovery, right? So when I implement it, actually, I just want to make it simple and usable. It can be tested easily. But that's not saying, like, we cannot support volatile.

That's just a technical fall down. Did the patch from Gregory ever go up for volatile support? No. The patch -- because there's nothing fundamentally different about supporting volatile versus nonvolatile. I think the ghost of PMEM is why the QEMU support started with nonvolatile. But there's nothing --

Yeah.

The reality is it doesn't make any difference. I had to pick one value for the flag. The volatile support would have been landing roughly in parallel with your work. So while it is upstream -- Yeah. It probably wasn't when you started.

Okay.

Fair enough.

Yeah.

There will be plenty of time for more questions about DCD at the end, hopefully. Okay.

Next up, there's a lot of material in this bit. So feel free to ask questions and dive a little bit deeper. But the topic here is fabric management. So debated for a while whether it made sense to emulate this in QEMU, because it isn't something that is directly visible to the host. But things like dynamic capacity are very strong justifications for why we do need at least some of the fabric management, as Fan Ni mentioned a moment ago. I snuck a star in on his slides when he said there wasn't any FM support, because there is one command in one of the staging trees. It's completely useless, but it was just showing how it would work. You can add capacity, but you can't remove it. So it's the same reason for emulation, basically, as we had for doing the QEMU emulation in the first place, which is as a test bench for, in this case, fabric managers, providing us something where we can tweak things, get an idea of how it all fits together. We are very much in the early stages for open source fabric managers. There's some test code. I'm not sure if there's anything yet out there corresponding to a full stack. But if anyone does have one and wants to talk about it, let me know, because I'd love to know what to test with. The other classic here is CXL standards to prove out. I think the exercise of doing the original QEMU emulation and, indeed, all of the kernel work is showing that not everything can be considered in a spec that runs quite so far ahead of where hardware is. So it's very useful from that side of things. Another key thing here is if we are driving some of these complex devices, so some of the switches and the configurability of those, or, indeed, dynamic capacity, it's really handy to have a standard interface to poke it with. And adding the fabric management gives us that option. And we'll come on to exactly how that comes together. I have added a note here that we're not talking about CXL3 fabrics in this case. This is the simpler version of fabric management where it all looks a bit like a PCI topology.

So there'll be a lot of slides here with a lot of lines on them. But hopefully I'll skip through what we're covering. So there are all sorts of different communication channels.

Hi, Tom. So the fundamental question, and it's good to ask, I think, in this kind of context, is, like, we upstream the QEMU support for CXL3 because we need pre-hardware. But we had a production use case for the kernel code. And the devices are coming. You might have seen my comments on the switch CCI stuff. We don't do -- do we? Or, like, the upstream consumer for switch CCI and the kernel side is not as clear to me. Like, what are we building? Like, basically, I'm not super confident about just landing kernel code just for testing.

Yeah. Well, I think the key thing is the blob on the left here is I agree it's not a normal host kernel. I think we will see BMCs making use of this in the same sense that they make use of some of the MCTP stack that, frankly, the kernel supports and doesn't get used from Linux.

So should I be reviewing these patches in the context of when somebody else from a BMC land comes along and says, hey, we want to build fabric manager interfaces, like, this is the ABI that we are recommending for you to use?

Well, I think the last thing we want to do is to end up with a Linux kernel running on a BMC stack that bears no resemblance to the main kernel stack where there are shared components. And particularly for the switch CCI, we've also got the PCI mailbox thing, which is MMPT, which is kicking around, which is going to see some use from hosts because it's much more heavily used for things like firmware update.

Who is going to work on the blob of the BMC part, right? Like, so I see the pieces there for the switch CCI, but I think it would make it very clear for people if you're like, okay, this is exposing it over TCP to another host, right? And then you talk to the other. So the pieces are there, right? Like it's almost there, but it's not real, right.

So there was a lot of discussion, a lot of diagrams drawn on the many, many forms of fabric manager that you can evolve. And to be fair, this one is focusing on the very simplest, which is simply a memory appliance with, and in this case, the BMC is just the controller of that memory appliance. So commands coming into that will come over something else. They won't be over FMAPI. There'll be Redfish or one of the DMTF standards. Some of that stuff is still being defined.

So it's still weird with QEMU because like the current switch is owned by a host, single host, right? So there's no real mechanism to discover ports that are coming from somewhere else, right? Like it doesn't make complete sense, right? Like from a QEMU perspective, right? What do you think about that?

Agreed. You're absolutely right. There have been discussions on how to broaden to multi-host for precisely that reason. So they can be able to run a number of hosts.

Who's working on this? Is this in public domain that people are talking about this?

There have been a few discussions in feedback on some of the patches about it. And I've had a number of other people reach out and say, 'Oh, can we do this slightly crazy thing?' Because they are very much interested in working on the large-scale fabric managers. And if you can poke some data through. Because there are, I mean, people have looked at emulating just that without the hosts involved as well. And the problem is you never prove your full use case. You always prove a little emulator of just the fabric wiring.

One more thing I'll let you get on with. So are these switch CCIs going to be hidden? Like imagine we have QEMU, BMC running. Are you expecting BMC is describing the switch CCIs itself and they're invisible to the host? Or are you in that environment?

We'll get there. Okay. Absolutely, there's nothing in the QEMU configuration that says the switch CCI needs to be on the same topology as the host is. I mean, in reality we only have one topology at the moment. So it would just end up on a different root port. But you can certainly do that sort of configuration. We'll come on to, yeah, a little bit more on combining the BMC and the host for testing purposes. Anyway, this slide is just meant to say there's some in-band stuff. That was kind of it.

And we will build up. Another thing that's crucial for fabric management is MCTP over various transports. There are a lot of them drawn on here. So you can see there's a whole bunch of wires going to effectively every component. This is looking at the outer band type signaling. So this would be something like I2C, I3C, or it could be Ethernet. It could be any number of different things. MCTP is carried over almost anything. This includes stuff like the multi-headed device pull CCIs. There are other paths to get to those, which we'll come back to. Yeah, basically you wire everything to everywhere, and so we need a path to emulate that.

Moving on to the next bit. Now, you'll notice that there's no new stuff in the list of things we can do with this. But this is just introducing the fact that MCTP can also be carried over vendor-defined messages on PCI, which becomes relevant in the next slide.

Yeah, feel free to look at these later. There's a lot of lines. Okay, so the key thing here is that this ability to carry them over PCI VDM enables the tunneling concept. So you can go into, say, a switch, and then you can tunnel down through the downstream ports of that switch to the individual devices below it and configure all of them. You can also tunnel out from where you first hit, once you go to the downstream device, where you hit something that's owned by the fabric manager. You can tunnel out to the individual devices exposed to a host and do things like set the label storage area or pre-configure stuff. You can actually send PCI writes and reads as well, so you can configure anything. Yep. I can't even read the slide. Yes, that's just saying what I said. Okay, yes.

So this is the question Dan was asking about, which is, how are we emulating the switch CCI or indeed the rest of this stuff? Now, under normal circumstances, this would be a different host. That's not something that's easy to do in QEMU today. There are out-of-pre-solutions for this. We're aware of a number of companies who have such solutions. If anyone wants to speak up. Ah, I think he's saying he wanted to speak up. Darn it. Yep, they do exist. So there are multi-host QEMU setups that are useful for exactly this BMC type emulation. But for now, and also useful for testing purposes, what we actually do is we sweep everything in the BMC round and push it on to the host. And in fact, we actually combine multiple hosts as well. This allows us to exercise all of the various parts of things going away, reappearing, all of the DCD flows, all of that stuff can then be poked from a single host, which makes testing a lot simpler, hopefully. Although, as I say, we only have one command at the moment for DCD.

Okay, so skipping on. And I don't propose to talk about anything on this slide. This is here as a prompt list in case anyone doesn't have any questions. But the aim at this point is to address what people are interested in seeing, what they're not seeing at the moment in the CXL emulation.

So there's a question online. Lee's asking, "Is there a plan to enable KVM and CXL emulation with the node problem?" Yes.

As long as you don't like instructions. I mean, without instructions it works perfectly. No, you can use it. Data's fine. Execute. So the fundamental problem here is that for CXL, the granularity of interleave, which we've emulated for a long time, is much finer than page level. Now, in order to do any of the stuff that's done with KVM, et cetera, you tend to do it in page tables. So at best you've got 4K. Combining the two is a bit of a problem. Actually, even in TCG we had a problem for a while because it didn't handle x86 instructions that went across the page boundary. But the reality is the way we're handling it at the moment is every single read or write has to go through the ultimate slow path in TCG, which means there's no caching of instruction translations, any of that stuff. So it's very slow. And also with KVM, in order to make that work, you would have to add emulation of pretty much every instruction to the kernel, and that's just not going to fly. Now, we have had discussions about how to do this. We can put caching layers in between. But the fundamental question is do we actually care? As a general rule, the QEMU emulation is not there for performance cases. We do have a problem at the moment, which is that it's so bad that it tends to stall completely if you're running any programs out of this memory, which does make certain types of testing a little challenging. It leads to some not helpful bug reports along the lines of 'it don't work,' which we're fully aware of. So it's absolutely something that we can look to address, but it's a big and complex bit of work, and it's not clear if it's worth doing. Yeah, I stuck it down here as performance optimization, but, yeah, I guess KVM is a really big performance optimization. So it was a good question. Any other questions at this point? Oh, jump on anything on there. Oh, we've got someone at the back. Throw him a mic.

It says ARM support. Could you use some help at the bottom? Can you go into a little detail on that for me?

Oh, yes.

Excellent.

Thank you. So one of the things on the ARM support is we're trying to add it to ARM virt in QEMU. Now, the problem with ARM virt is it has a fairly strong rule that it must have device tree support. There are one or two things that have slipped through when maintainer didn't notice. So one of those is the PXB stuff, the PCI expander bridges. Now, those are used for a bunch of reasons around numerative policies and things normally, but the CXL device is a descendant of one of those. And the problem with those is they have an enumeration problem, which is that they're actually enumerated by EDK2. And if you're in -- so you go through a double firmware build thing. So you build enough information to pass to the BIOS. The BIOS then enumerates the entire PCI expander and fills in the gaps for all the PXBs. And then you go back into QEMU, which rebuilds all the tables before providing them to the distro or whatever is running on top. And device tree, there's no such path. So the discussion there is what on earth can we do about enumerating these? We could enumerate them in the kernel, but is the kernel going to take a bunch of nasty support for what is basically a QEMU hack? We could do static enumeration. I gave a talk at Lenore Connect on this earlier in the year and got some good feedback from the maintainers there, but we haven't solved it.

I feel like the idea that QEMU is going to need to get arbitrary device trees passed into it -- like if you're running an ARM kernel under QEMU, intrinsically you have to describe the whole platform you're running on. So to me, the idea that --

I was like, "Gotta throw it back." Just make him walk. We're done. "Ah, time. Ah." Anyway, carry on, Laughless. I talked for that one for many hours.
