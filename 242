
Welcome. I'm David McIntyre. I'm the Director of Product Planning  and Business Enablement at Samsung. I'm very privileged to share with you  some very exciting CXL Accelerator technology  in conjunction with VMware. Let's get to it.

First, let me give you a background on  Samsung strategy around CXL platform deployment. We have a portfolio of solutions called CMM,  CXL Memory Module,  and the one that I wanted to reference today was around CMMH. H is for hybrid,  and that means that it's a combination of DRAM and NAND media,  all in one SSD form factor. There's two use cases I also wanted to share with you. The first is the tiered memory use case,  and the second is the persistent memory use case. For tiered memory,  if you can imagine that by combining  the DRAM and NAND resources by having  DRAM cache to read and write to within the SSD,  the same SSD that the NAND media type is in,  we're able to do all fantastic things. For one, the host will see  the total capacity across both the DRAM and the NAND,  as one common resource. Through the cxl.mem, we're able to prefetch and  equip that DRAM cache for  improved data access and performance for the application,  and we're able also to write to that cache  to properly prepare and improve  the overall storage of  the data as it's being created by the applications. In persistent memory, the second use case,  if the lights go out and you're concerned about losing your DRAM data,  you don't have to be any longer because we actually put  an internal energy source within the SSD form factor,  so that during that situation,  the controller will sweep the data from  the DRAM to the NAND resource and you maintain persistence. This is really good for that power outage condition,  as well as for those applications that have  certain policy requirements where  the data needs to continue to persist. For higher capacity points,  we will also have the ability of  an external energy source available. That's the background on CMMH. You can see that it could be really handy and useful for  today's AI applications for large language models and  memory intensive data analytics and databases  where you don't want to have to restart the clock if you lose  the data or you just want to improve the performance. That's what CMMH is about today.

Now, we have a specific use case that we've had  the opportunity to partner with VMware on. It's a version of CMMH,  which we call CMMH VSA for Virtualization Services Accelerator. You can see a picture of it here on the right. It's an add-on card that has an FPGA that is instanced with  the VMware IP system and provides  the same resources that we talked about with  the general CMMH but specific to the VMware solution. The CAPEX advantage of using the CMMH,  because we are also combining the DRAM and  NAND memory in a common form factor for tiering,  we're able to improve  host CPU core and virtual machine utilization. This provides CPU savings because we're able to,  instead of just brute forcing the problem to add more memory,  we're able to actually free up virtual machines  to be better optimized and aligned  with the memory resources through CMMH. The operating OPEX benefits include improved vMotion speeds,  which is basically a management resource  that VMware creates to move and optimize the positioning  of virtual machines  across the infrastructure to support applications. This reduces failure probability and  also reduces host evacuation time for maintenance.

Let's get into the VMware IP key benefits. Because we're able to deploy and improve  the utilization of CPU servers and VMs,  we're able to consolidate workloads  across this underlying infrastructure. By aligning memory resources,  optimizing their deployment with the CMMH VSA,  we're able to reduce the vMotion VM migration time  and improve workload profiling for optimum resourcing. Future use cases that certainly could benefit from  CMMH VSA include memory pooling,  remote memory tiering,  acceleration of specific workloads in AI,  ML, and data analytics,  and then also memory tracking,  scrubbing, and providing proactive maintenance. Those are all key benefits for take away,  improving your performance along with expanding on TCO capability.

Here's an example of this technology in our labs. On the left, you can see the VSA add-in card  integrated into Intel Granite Rapids development platform. This is up and running today,  and we're actually deploying demonstrations  with end customers as well. Some of the key design features,  so the host interface is CXL 2.0,  as well as Gen 5 x16. The FPGA is an Intel Agilex,  and it also has an integrated ARM core within it. This card supports Ethernet as well as very manageable power,  as you can see there. As I mentioned earlier,  there's an external power interface as well. With that, I'm going to pass it over to  Arvind Jagannath to continue our story.

David touched a lot of points about  Samsung specific solutions such as the CMMH. I'm going to cover some of the aspects of what  VMware has been doing with respect to CXL accelerators. I want to start by covering some of the CXL market data. There's been a lot of buzz around CXL in general. We see a lot of memory vendors,  switch vendors, IHVs, etc,  talking about CXL. There are a lot of variety of CXL protocols that are being  supported in the industry such as CXL.mem, CXL.io, etc. As you can see, the ecosystem is growing,  and there are a lot of variety of  different products centered around memory that are being built. From this data, we can see that the market is going to  grow to up to 15 billion by 2028. Out of that, you can see that memory forms the majority of it. The product that we are going to talk about specifically is  about not just memory but also  covers one other use case that we are going to talk about. But what we want to show or what the point is  that this can be extended to a variety of use cases.

VMware is working on a hardware software co-design  based on the Intel Agilex platform,  and we are building a three-fourth length PCI form factor card. Basically, this card is going to be a flexible card that will  support not just memory tiering  but a variety of other use cases as well. The other advantage of this is that it's highly optimized by VMware. Since it's hardware software co-design, it's integrated. The ESX kernel understands this card and works  in concert with the code that runs inside the card. This is also suitable for a variety of different workloads. I mentioned databases, VDI, etc. It can also suit some of  the performance-sensitive workloads as well. The other important aspect that I want to highlight is,  this card does not require  any special BIOS or any special changes to OEM servers,  nor does it require separate lifecycle. It's fully integrated into the vSphere lifecycle. Whenever vSphere is installed or upgraded,  the components within the card are also upgraded or installed. It uses an Intel default BIOS,  which means that an OEM doesn't have to worry  about making any specific changes  or have any requirements on their BIOS. The special thing about this card is,  it supports up to four terabytes of additional memory. Not only does it have processing capability,  it also has memory which comes from  some of the devices that David talked about earlier. It supports up to four terabytes of memory. It can also support 1-1. This is mainly specific to memory tiering. It supports a DRAM to CXL of 1-1 ratio.

Next. This shows the evolution for memory tiering  and where VMware wants to go with respect to  memory and the use of CXL. VMware will start with CXL memory expansion and tiering,  which you see on the extreme left. Then we'll move into doing  specific accelerator-based memory tiering,  which is special devices. We will also use the same device to  do things like pooling and disaggregation. Because this device has ways  to interconnect with other such devices,  you can achieve pooling and disaggregation as shown on  the extreme right where the devices  are shaded blue and they can interact with each other. Hosts can access multiple devices. Multiple hosts can access single devices,  the device, and this forms a complete disaggregation solution. We can also evolve this solution  because this is a programmable solution. It can be evolved to do things like  special encryption, dedupe, compression, like offloads. It can also do analytics, for example. In the end, VMware could also look into  this device as something which can virtualize hardware.

 Finally, let me cover one use case which is not exactly memory  tiering but is another use case. The important point here is to illustrate how,  whenever we talk about CXL,  it doesn't have to specifically be about memory. It can also be about other use cases. In this case, this use case is unique to VMware because VMware  is a pioneer in doing memory migration for VMotion, for example. In this case, as you can see,  when we take a specific workload,  we can see up to 30 percent of  performance improvement because of the use of the CXL accelerator. In terms of the guest,  the performance loss is similar to  what we would see with a normal VMotion. Now I'll pass this on to Sudhir,  who will cover more on the performance side of things. Thank you.

The previous slide, Arvind covered the accelerator use case for  a non-memory tiering based solution. In this setup or in this example,  we'll talk about the accelerator-based memory tiering use case. Let's talk about that and  the results that we got at our lab,  and look at the performance setup  from a very high-level perspective. We have a short demo that's following this illustration that will have  a lot more details into the performance metrics we got  from this accelerator-based memory tiering. But even before we go into the testing,  let's take it a step back and let's look at  the advantages or some of the advantages  of VMware software memory tiering architecture. But some of the advantages that come to my mind is,  from a VMware vSphere perspective,  the platform or VMware vSphere,  that has the most awareness of  memory or the workload memory access pattern. VMware vSphere can control the memory allocation on demand,  and it is able to intelligently  manage the hot and the cold memory pages,  and that's completely transparent to any application workload. Which means the software memory tiering architecture of the framework,  well, that can span across all memory technologies,  be that DRAM, be that tier 2 memory,  NVMe, CXL, accelerators, so on and so forth. From a business critical workload perspective,  be that the Oracles of the world,  be that the SQLs of the world,  all the SAPs of the world,  they can make use of this memory tiering architecture  or framework very, very transparently. Back to the accelerator-based memory tiering test  that we performed in the lab,  and as we can see from this illustration here,  we had an ESXi server,  and that server had 128 gigs of DRAM,  and a CXL accelerator card attached to it. This card was a PCI form factor accelerator card,  and that had 128 gigabytes of memory attached to it as well. From an ESXi server perspective,  both the memory address space,  that is the ESXi DRAM and the ESXi accelerator memory,  which is the 128 gigabyte,  it appears as a single cohesive memory address space. So it's a single cohesive memory address space. And further, what we did was we created two virtual machines. VM1 was a virtual machine with an Oracle database. The specs was 12 VCPUs and 48 gigs of RAM. With the virtual machine memory,  or the entire virtual machine memory,  was coming from ESXi DRAM,  which means there is no memory tiering  associated with virtual machine one. With virtual machine two,  which was exactly the same  or almost a similar virtual machine,  we created again an Oracle database,  same config as VM1, 12 VCPUs, 48 gigabytes of RAM. In this case, the virtual machine memory  was coming from both the ESXi DRAM  and the CXL accelerator-based memory. The ratio was 60/40%, which means 60% from DRAM  and 40% from the CXL accelerator attached memory. What we also did was we had,  if you look at the top of the illustration,  we had two more virtual machines, VM3 and VM4,  and essentially those virtual machines  were client virtual machines. They were running HammerDB Oracle load generator,  and those load generators were being run  against the Oracle database on virtual machine one  and the Oracle database on virtual machine two. What we then did was we captured statistics  based on the runs and the demo,  which we'll be going through in a couple of seconds  that basically goes into all of the details  of the performance metrics. But as we can see from the demo,  the virtual machine one, the DRAM-only VM,  that generated close to 1.4 million transactions per minute  as compared to virtual machine two,  which was close to 1.3. What we saw was a slight reduction of 127,000  or close to that number, transactions per minute,  or let's say a reduction of 8.5%, which is very nominal,  and which is great when we start looking at metrics  over a longer run of time. So from a memory tiering perspective,  the memory tiering virtual machine,  well, that was able to achieve around 91% of the performance  that the DRAM virtual machine had to provide. And let's move over to the next slide  where we'll talk about the demo in question here.

The primary goal of this demo is to illustrate  impact of memory tiering on guest performance is minimal  when using the PBerry Accelerator. But this demo, we're using Oracle as a guest application. And the key performance metric in this demo  is going to be Oracle throughput,  which is measured by the number of transactions  committed by Oracle every minute. So here, let's walk through the various windows  displayed on the screen. Let's start with the window on the top left corner. Here, we're showing you the console of the HammerDB client  responsible for generating load on Oracle instance  running inside the baseline VM. The graph depicts time on X-axis  and transactions on the Y-axis. You can also notice that the HammerDB console  updates the screen every five seconds. And we also see that Oracle throughput  is reaching approximately 1.5 million transactions per minute. This is our baseline performance. And this figure that you're seeing here,  it represents the highest achievable throughput  since this baseline VM is not subject  to any memory tearing. Now, let's shift our focus to the second window. In this window, we are showing you the console  of the HammerDB client generating load  on Oracle instance running inside the memtier VM. Here, we notice a slight decrease in Oracle throughput  compared to the baseline performance. This outcome is expected since this VM  has a memory tearing enabled with a significant portion  of its memory already demoted to the PBerry Tier 1 device. And let's now switch our focus to the third window,  which illustrates Oracle throughput in memtier VM  as a percentage of the baseline throughput. The metric that you're seeing here is calculated  by dividing the throughput of the Oracle instance  running inside the memtier VM  by the baseline Oracle throughput,  and then multiply it by 100  to express this as a percentage. So on average, we see that Oracle throughput  running inside the memtier VM  is approximately 90 to 92% of the baseline throughput. Now, let's switch to the bottom right window  that shows the breakdown of VM memory  across different tiers. We see that about 19 gigabytes of VM memory,  which amounts to like 40% of the VM memory  is currently demoted to the Tier 1 PBerry device. And about 29 gigabytes, which is 60% of the VM memory  continues to reside in the DRAM. Additionally, in this window,  we also see continuous reads and write access  to the NVMe storage that's backing the PBerry device. These continuous NVMe access  are that result of the OLTP workload  that we are using in this demo. As we mentioned in our demo,  we are using the HammerDB workload,  which is based on the TPCC profile,  which exhibits a highly dynamic access pattern,  which results in continuous changes  to the guest working set. So all these NVMe access that you see here  correspond precisely to these shifts  in the guest memory access pattern. Specifically, the NVMe reads  correspond to the guest access to pages  that were previously identified as cold  and were demoted to the NVMe storage. Similarly, all the NVMe writes that you see here  correspond to the previously identified hot pages,  which are now transitioning to a completely cold state  and hence are being demoted to the NVMe storage. Finally, let's also look at the window  which is showing the vCenter console. The vCenter shows that our testbed has two ESX hosts,  six and eight. The PBerry eight host has the PBerry device,  and it is running both the baseline VM and the memtier VM. And the client host is running both client one  and client two Oracle client VMs. And within the vCenter console,  we also have a graph that illustrates the VM memory usage. The green line here signifies that  all the 48 gigabytes of the configured memory  that VM has is fully being utilized.

All right, so from a call for action perspective,  to learn more about VMware CXL plans  and the accelerator solution,  please contact arvindjaganath@broadcom.com. And to learn more about VSA and CMMH,  please contact David McIntyre@samsung.com. And with that, please take a moment to rate the session. Your feedback is very important to us,  but yes, thank you very much for attending the session  and have a nice day.
