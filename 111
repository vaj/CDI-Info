
All right, good morning. Thanks, Reddy. Like Reddy said, I know people are warming up, but this is going to be a very tight day today, so Reddy is going to help us to make sure that we run on time and then I just don't go on talking about for half an hour. So we'll try to stay within our schedule. So good morning, everybody. Samir Rajadnya and I will talk about hyperscale use cases. I'm Manoj Wadekar and I'm from Meta and Samir Rajadnya will be talking about Microsoft and what their use cases are for composable memory.

And then we'll try to get into some more benchmarks like what Reddy said. We don't want to just go away and say these are our requirements, but we want to give more information about how do we really look at various solutions, what benchmarks are used and how it is measured towards the goals of the applications that we have. So we'll leave some time for the benchmarks and then talk about where you can participate and where we really need help and participation from all the community.

Just to set up the context for why this matters to us, and I'll try to connect it to actual hardware and discussions we have. This may be little data because I think the quarterly results come now. This is the first quarter data, but we have more than three billion active users daily on Meta.

And if you look at it, basically all of these go through our hardware infrastructure. And what we are going to talk about is basically how that hardware infrastructure enables these active users to have the experience where they have the feed, they have the recommendations, they have all the AI interaction that they have. And so we'll try to map through that. For composable memory systems, we are grouped into two areas. One is basically what are the AI use cases that we have and where we need the memory to be composable and why we need it. And then there are non-AI, other normal use cases that have grown over time, which is a significant part of it comes from the caching database and data warehouses. They are slightly different use cases in the sense AI use cases are driven because of the way applications are running on the GPUs or accelerators and the performance of those accelerators continues to grow. And also the models that get trained continue to become larger and larger. And this is the thing that drives a need for higher memory for these platforms. On the caching database and data warehouses side, it's different. Caching requires data to be held in the memory for a certain amount of time for it to be useful for the users, which basically means you need to maintain it for what is called as retention time. So needing that data for that long in memory is what drives the need for a higher amount of memory. Databases make more efficient use of storage capacity if they have enough data for driving the SLA for the queries per second they have. So all these three things have slightly different requirements. Data warehouse, most of our storage is disaggregated in the data center where the storage is served by the scale-out file system. And you have the data warehouse running into the compute-heavy nodes. These compute-heavy nodes, again, can benefit from a higher amount of memory by having more capacity of memory that gets served and maintained the SLA for the QPS. So two different areas of use cases. I've talked about caching databases in the past. Today I'll try to focus a lot more on the AI use cases.

So on the AI side, I'm going to build this out. What it means is basically when we talked about what it means for the users to get the experience that what they want, they come onto Facebook or Instagram or Reels or whatever we have. What they need is basically what we have. The AI is basically you're training the models for the data we have collected based on their experiences and what likings people have. And then you do search, which is inference. So you're going to say, OK, based on that, what would I recommend for this user to have? That would be inference. And synthesis is where you are creating new data. This is not just looking, inferencing from the data that it was trained on. But based on the trained data, you create new models. You create new information. Lama would be a good example for that.

ChatGPT, of course, everybody knows that example also. So all this comes into these three categories that they get mapped into the data center services. The first one is ranking and recommendation. This is where deep learning recommendation models really are key for most of our use cases where you have video suggestions, ranking of even your feed or the recommendation for the ads. Computer vision gets into the image classification. Language gets into the translation. And these get mapped into the models on the right-hand side, whether they are deep learning recommendation models or whether they are convolutional neural net or the RNNs and so on and so forth.

Getting to the AI use cases. So if you see ranking and recommendation, this is where we do the personalized recommendation, as I talked about. And then from a hardware perspective, we have two things going on. You're training the models, and you're using inference to make the recommendation as the users basically get the recommendation. Large language model, this is the buzz of the new things now. Lama2 is an open-source one. Here you not only have training and inference, but inference falls into two categories. First is basically to get it to what is called as a prefill, which is access to the first byte when you train, say, hey, yesterday we were talking about there was an example saying that somebody wants to create a story and says, I like peanut butter, but. And that much part is basically when you're doing a prefill. And then your model starts working and says, but I like fries more. So the later part is basically when you're doing decode where you're creating more and more tokens. So that is roughly where we can say that five types of operations or cluster actions that are happening. You're doing training or inference on the ranking and recommendation models, or you're doing training for the GenAI kind of large language models. And then for inference, you're doing prefill, and then you keep on doing decode as you build the stories.

Now let's take those five things and come back to the hardware where the challenges are. So first of all, there are challenges in the system from a memory perspective. At least I'll focus at this point. You may have seen this kind of charts before. As the models keep on getting faster and faster, you really want to make sure that you have enough capacity into the GPUs. So single CPU, GPU memory is going to be limited based on how much capacity of memory I attach to it. These typically tend to be HBMs. And what this chart is showing is basically how are the models growing in the flops that they have, and how is memory capacity growing that is attached to the GPUs or accelerators. So as you can see, at some point of time, the green line is basically falling behind the red line, which basically means that we start running out of memory, which is almost now basically. And then on the other side, you look at basically what is the kind of bandwidth need and how many parameters. I think-- sorry, I misspoke. The first one was basically the parameters. So this is the bandwidth, which is the teraflops per second that we are running. And here you look at basically how many transactions you're going to do to the memory. And based on that, it decides what's the bandwidth expected. This green line what shows is also the same for the many other network interconnect underlying that. But what it means is that actually, in general, your cluster bandwidth requirements are going to go up, and memory cannot keep up with the bandwidth. So we have both problems. We have a capacity problem and bandwidth problem. And this is, again, focusing from the AI use case perspective. This was a slightly different problem when we had talked about non-AI use cases in the past, where you could trade off bandwidth for TCO.

This is a general memory problem, by the way. And this was looking at from DRAM perspective. Generally, there is a challenge as we look into how the cost-- memory costs are basically a significant part of our spend. From server bump perspective, a significant percentage goes into memory and memory capacities and costs going into that. And it's kind of stabilizing from cost perspective. So as we keep on requiring to have more capacity or more bandwidth, essentially, your costs keep going up.

Let me switch the gears and talk about those five use cases that we talked about-- ranking and recommendations for training and inference. And then we had Gen AI or LLAMA requirements for training inference, basically, in pre-fill and decode. All of this, basically, you're trying to do with the infrastructure that is as much common as possible across these use cases, which means you are going to start balancing the components in that system to make sure that we optimize the overall TCO for the data center. This means basically making the choices on the compute. When I talk compute here, this is two parts of compute here-- compute that happens on CPU and compute that happens on a GPU. Memory-- when we talk about memory, we have to talk about how much capacity you need and how much bandwidth, network, and the model sizes that you're going to train on, cluster cell, because those are five different use cases. All of these are something that you decide to-- when you put it in the data center, when you design what hardware it's going to go, you need to know what to optimize for.

But it's very difficult to serve all classes. And this is a problem that we get into, that if you don't have a great choice, we need to decide where we start giving in from the design perspective for what we put into the systems. What this spider chart is trying to tell is basically that you'll see different use cases have different sensitivity points. Some of them may be more sensitive to the memory capacity. Some of them may be for the networking latency. Some of them may be for the networking bandwidth. And this is the challenge, which basically means the more capable solutions we have in our pocket to make sure that we don't really burden the remaining use cases as much, a better chance is going to be. And this is going to be the next frontier for innovation as we define the systems. What are the kind of memory technologies and connectivity? What are the network solutions that we're going to bring to the forefront? And this is where I think we'll make our system choices much more easy to handle relatively, but there are, of course, going to be challenges. And we will end up defining the right solution as we get to, at that point of time, what solutions are available from component side.

Let me bring it up to now up level again a little bit and talk about how this works into the data centers. So traditionally, the networks-- I'll talk about networks and memory, and we'll close on memory. On the network side, traditionally, what you would have had is you have lots of compute in the racks that gets connected to the network hierarchy. And you have most of the developers that are coming through the front end to program these GPUs and GPU systems. And then you have data that gets ingested. We talked about training. You need to ingest the data that is sitting into our data warehouses and et cetera. And you train that. That's your front end network that comes through. As the requirement for accelerators grew, there is a need for going beyond a single GPU, whether it is because of the computing capacity in that GPU or whether it is a memory capacity or any other system resources. You need to take a model and basically spread it across multiple GPU. As you know, if you consider there are three types of parallelism that you achieve in these AI systems, you have model parallelism, you have pipeline parallelism, and you have data parallelism. So what we talk about here is a model parallelism where multiple GPUs are collaborating to run the single model, but they are just distributed across multiple GPUs. This requires you to have a network which is significantly high bandwidth because anything that otherwise the job stops or that doesn't achieve, it takes longer time to train the models. So you need the highest bandwidth and lowest latency for this kind of a scale up network, if you will. Tend to be something inside a system or in a smaller scale, going up to maybe 32 to 256 kind of accelerators kind of a scale. But then some of the models won't even scale even with that kind of capacity. Now you are going to go out and what is called a scale out. This is a very high number of accelerators that you're going to connect. It is a high bandwidth as compared to the traditional front end networks, but it also has a capacity, low latency requirements. And this is where you get into the higher-- this is where RDMA works or anything that allows you to do much more efficient data transfer. This is a scale out networks. So as you can see, these are three different types of networks that get deployed into the AI clusters today.

Let's look at from a memory perspective. As we talked about, we see that as we start running out of memory from the given accelerator, there is a need for more memory. And we need to create some level of tier 2 memory capacity. That is handled because it is going to trade off some bandwidth as compared to the immediately attached HBM, which is very, very high bandwidth but lower capacity. So you do need node memory expansion. This is higher memory as compared to the traditional networks. But this will be a certain percentage of HBM. So you need high capacity and need high bandwidth, a certain percentage of HBM. It's possible that this is something that you want to have access from CPU and accelerator, but these things will evolve. Just to add that, AI use cases continue to dramatically. So sometimes you create solutions, and then AI systems basically end up using that. Or sometimes you just have to create them. So this is evolving as we go. Second part is basically scale-up networks that we talked about for accelerator to accelerator discussion. Again, most of the time, the operations that are going across these accelerators are really memory accesses. So you do need high bandwidth, low latency, scale-up accelerator to accelerator interconnect. And then as we go forward, this is going to become much more manageable as we start desegregating memory. Now we are looking a little bit further into the future. Once we create that high bandwidth, low latency fabric which has that load store interface, that makes it logically possible for us to start desegregating memory and use it into this AI infrastructure.

These use cases could be using pooled or shared memory in both of the cases. So let me pause here and hand it off to Samir Rajadnya. There are challenges. And before we go into that, just telling about challenges where we are. We have talked about how CXL can play a role in this memory subsystems. And we have a lot of work going on in CMS work streams in this space. However, if you look at it from some of the use cases that drive the higher bandwidth and the latency, there is a challenge just because CXL does rely on the PCI rates. And PCI today is a Gen 5 going to Gen 6 and then Gen 7, et cetera. But looking at where the many technologies are there, we do believe that actually we need to see how we can drive this aggressively for CXL to play a significant role. Just as an example, in another two years, we'll see that ethernet will be at 224 gigabits per second per lane. We will be at-- Gen 6 continues with the current rate. It's David Gen 6 that will be 64 gigabits per lane in 2026. Hopefully we'll start catching up because this is something that will be required to achieve that high bandwidth for scale-up networks as well as for any kind of a pool system that we talk about. And there are going to be other challenges that we start looking at as accelerators start communicating with each other. What kind of expectation is there from the symmetric coherence perspective? What kind of per-port lanes are required? And what kind of aggregate bandwidth we can achieve? Say PCI is limited to 16 lanes per port. That means you need something to bind it together to achieve the higher bandwidth. So as we look into the previous use cases, I think we need to look at how we provide those solutions for that high bandwidth, low latency. I think CXL already provides the right load store interface on the top.

And once we have that, I think we believe we'll have the case for composability. We'll have the high bandwidth network that allows us to basically separate out the memory from compute, compute against CPU and GPU. Both of them can connect to that memory, which is going to be pooled or shared. We can create that scale-up fabric, AI fabric, that we think we should address the challenges that we talked about. And Composable Memory System Group has a significant role to play here. We can talk about-- we have been working on the composability. We have been working on the different technical solutions required to get us there. And once we have those things coming from the use case perspective, vendor's perspective, we can drive them into the various standard bodies and create the blueprint for people to adapt. And this is where we really look to the CMS work to go and look to your participation. With this, let me hand it to Samir Rajadnya.

OK. Thanks, Manoj Wadekar. And good morning to everyone. So I'm here to talk about Microsoft use cases. And my talk is primarily around cloud use case. We also have software as a service, but I will touch that briefly at the end. So what is cloud? What is-- the basic question is, we have to give the same kind of performance as on-prem. If somebody has a server on-prem versus you buy a VM, you have to give same kind of a performance through cloud. So that is one thing. And the second thing is, when you look at cloud, what is really happening? This particular slide, what I'm trying to say here is, there are some problems and how CXL can help to solve these problems. So the diversity of VMs is increasing in the cloud environment right now. People are demanding various types of VMs. VM is nothing but a bin packing between the computer and the memory. And what happens is some customers get these VMs, but the underlying hardware can change. And if you see, right, what is one of the greatest benefits of CXL is, the computer roadmap is now detached from the memory roadmap. CXL gives us this opportunity where you can have a different memory technology, whether it is DDR4, DDR5, or any other custom memories that will come in the future, which is separated from the x86 or ARM roadmaps. So that gives us some good opportunities to solve some of the problems that we are seeing in the cloud. And the third thing is that people are renting these VMs, and there is also some VM-to-VM communication. And that kind of poses a challenge, but at the same time, CXL gives us this opportunity where we do not have to significantly change our hardware, but we can add memories and that can help solve these problems. So in a nutshell, what I would like to say is CXL is really a great thing, and it helps us to solve all these problems. But at the same time, right, the cloud, one of the main differences, there is a cloud and then there is a software as a service. Now if you look at cloud, these are the user applications that are landing on the cloud, and we do not know, right? It can vary one use case from another use case. And this is where we have to show this memory as kind of, you know, people are not going to change their applications based on the memory. So this is where we have to be as transparent as possible, make sure that the kind of performance that we are giving should not be affected by the hardware that we are introducing, specifically around CXL. Whereas if you look at software as a service, these are the software applications written inside Microsoft. We are total controller of software stack, and we can always deal with the two memory because we all know that CXL memory comes with some additional latency. People are talking about fabric attached memory that will add another level of latency. But for dealing with all of those, you need to have control over your software, and that is software as a service, and that will benefit from that kind of a two-level memory.

Now let me go to the next slide.

Okay. So I'm going to talk about -- Manoj Wadekar talked about memory expansion. This is really a great thing and pretty much needed for ecosystem. And memory expansion is something that is happening now. There are a lot of POCs. The controllers are there. And that is going to help this kind of a segment of the industry to fly. But what comes next after that is some of the things that I'm going to explain here, which is pooling and sharing, which will come eventually, right? So this is one specific thing that we studied internally. And what we found is there is a lot of standard memory behind servers. And when customers also rent these VMs from us, on many occasions, they do not touch that entire memory. So that gives us an opportunity. And as we all know, memory is almost approaching 50% of the server cost. So if you have such a big footprint, the cost of the memory, and if a lot of that memory is stranded, can we do something about it, right? This is where this idea came in. And the CXL gives us this opportunity where we can build a large memory pool and that is shared across servers. And now, dynamically, you can provide more memory based on the user application. So the graph over here actually talks about what kind of savings you can get. So if you look at that blue line, which where we are saying there is a 50% of the memory, which will be a CXL memory. And 50% is going to be local memory. So in that case, so if you go to a node size of, say, eight, you get something about between around 7.5% of the savings. And that's huge, right? Because the cloud has a scale and memory is coming to almost 50% of the server cost. So that is a significant saving. And if you are looking at different ratios, for example, 10% memories on the CXL savings will be a little less. But still, at the scale that we are looking at, it is still significant savings. So this is a paper that was published. And I'm just referring to that paper over here.

Look at it whenever you get a chance. So on memory pooling, there are two architectures possible. The picture on the left is a direct attach, a multiported CXL controller. And the second one on the right is the switch based. Both could fly. Each one of them have their advantages and drawbacks. So with the multiported CXL controller, the benefit is there is no actual switch in the path. So it gives you lower latency. But the challenges is around scale. Once you want to go to a bigger scale, you need to have some very good cabling solutions. These are the challenges.

On the switch side, it gives you extremely good flexibility. But you're adding another component in the middle. So that's a cost adder, latency adder. So that's the challenge. So I think people always-- everyone is excited about CXL. But I think what we really need to do is there are two things that are happening. You're introducing a new hardware. And at the same time, you are going to get some benefits, CapEx cost, TCO, and perform. And those are the disadvantages. So any time you're trying to figure out a solution, this balance has to be very important. Otherwise, what you will do is you may add more hardware. And the savings will not justify that. So that should not be overlooked.

So the main takeaways are-- I think what we are trying to say here is don't follow-- there is the triangle pyramid everybody draws. But what I'm trying to say here is it is entirely workload dependent. What may work for one use case may not work. So you have to really have to have your use case in mind. And system goals are very important. Have that view. And disaggregation benefits are highly dependent on workload. Basically I'm saying the same thing.

Now before I jump on to the next slide, I want to quickly touch on the other side of Microsoft, which is software as a service, where we have a lot of large memory databases. Everyone talks about AI, right? But let me just talk about large memory databases. I think this is where we control our software stack. And if you look at CXL 3.1, for example, which brings a lot of new benefits such as shared memory, I think there is some value there. We all need to look at that. And I believe that memory sharing will help us with our software as a service. But to get there, we have to have the basic building blocks to work. So memory expansion has to happen. Pooling is nothing but expansion of memory expansion. And then we need a very healthy ecosystem. And that is why I think the way we are going to expansion and pooling and sharing, these are very logical steps. So with that, I think Manoj Wadekar will come here and talk about benchmarks.

Since we are running a little behind, we are going to skip this. A lot of this is covered in the white papers that are just published today, it looks like. So I will hand it off to Vikrant now. Same call to action. Join the group, please, and contribute. So Vikrant. Thanks, Samir.
