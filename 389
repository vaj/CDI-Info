
Okay, good morning, everybody. Welcome to day two of the AI tracks at Storage Developers Conference. This is the first of our tracks—or first of our sessions in the track—for infrastructure for AI. We've been talking about applications for AI and data for AI. We're now switching over to infrastructure. In hindsight, I kind of wish we'd done this presentation yesterday, as I noticed that there's a lot of similar themes running through the talks. The talk today is about 'Storage for AI 101.' This is some food for thought to get you thinking about your infrastructure for AI as we go into some of the later talks today.

The presentation today is being brought to you by the SNIA Technical Council and the AI Task Force from the Technical Council. SNIA, of course, is your host for the event today, and the Technical Council does a lot of work putting together the agenda for this conference. So, we want to put together some material to give folks an introduction—a little bit of foundational material—around thinking about storage for AI. And I see that this is actually one revision old on the slide deck. One of the things we're exploring is how SNIA can serve you with vendor-neutral solutions to gaps around AI. We appreciate the folks that came to our BOF last night, and we've got a lot of good input about things that SNIA might be able to work on in the future.

We do plan to try and enhance this talk. We're considering creating a webinar and creating some additional information—making something that you can link people to—if the educational material we're bringing to you today is interesting. So, to make sure that you recognize us when we're out in the hall, we put our pictures up here. The status bar is showing up at the bottom there, covering up part of it. So, both Craig and I have our bios available at the SNIA Technical Council webpage. I'm Curtis Ballard with Hewlett Packard Enterprise, a distinguished technologist with HPE, and Craig Carlson from AMD will be taking the second half of the session today. So, if you've got suggestions for improvement to the content you see today following this presentation, please stop us in the hallway and help us out.

So, what is this presentation supposed to be? This is trying to be a high-level introduction to how AI uses storage. We're hoping to be foundational material for the other AI presentations. And, like I said, some of the presentations yesterday presented things very similar to some of the ideas you'll see here today. This is food for thinking about how to create your storage infrastructure for AI. In the time that we've got available today, this is not going to be able to be a deep-dive education on AI techniques or architecture. Some of that we saw yesterday with some of the storage for AI. We're not also doing education on storage architectures. A common theme of SDC is storage architecture and storage techniques—that’s not this presentation today—and we're not able to do a deep dive into variations in how storage uses AI. Like I said, we're trying to set some foundational spots for thinking here.

And one of the questions we've been asked is, 'What kind of storage are you talking about? Are you talking about direct-attached storage—disks inside of the server running your AI algorithms? Are you talking about external storage? If you're talking about external storage, is this block storage? Is this file storage? Is this object storage?' Well, what we're finding out is that the answer is some variation of, 'Yes, all of the above,' or the famous, 'It depends.' Most of the focus for storage for AI is around the two in the middle there: file storage and object storage. We had a session yesterday on using fast object storage with AI. Today, we've got a session about SSDs for AI, a session about HDDs for AI, and a session about object storage for AI. And exactly what is the right storage ends up being somewhat dependent upon exactly what it is you're trying to do. Your storage needs will end up needing to be tuned and will change for different AI workloads and different model sizes. I found it interesting as I was working on some of these slides. I went through them with a number of people, and almost everybody looked at them and said, 'You're missing this,' or, 'That’s not what my model is doing.' Because if you really want to do AI efficiently, you end up having to use some general concepts, use things you've learned, and optimize your solution for what you're doing.

Got some terminology here—I won’t try and go through all of these. This is helpful for people looking at these slides afterwards. I think many of these things you've already seen before. And so, we'll go on and look at some of the material.

So, this is one of the key things that those of us that have been in the storage world for a long time have had to figure out. We've done storage for a lot of years. Many of us in this room have been doing storage for 20-plus years, and there’s some common themes that we've run into. For different types of storage workloads, you get your speeds and your feeds, and then you go back and you calculate that this is the speeds and feeds that I need for my storage. And you build it, and you move on. That’s not the case with workloads for AI. If you're trying to do just one phase of AI, you might see some of this. Even in a single phase, the workloads tend to be much more variable than what we've seen in the past. A full AI pipeline is a multi-phase workload with very different characteristics through the pipeline. You're probably starting to hear the rise. Somebody commented to me just—I think yesterday or Monday—'Isn’t it interesting that people aren’t talking about storage systems so much anymore? Now people are talking about storage platforms. And my storage platform can do this well, and it can do that well.' And you're seeing that because, to do AI well, you have to be able to do multiple things well. And that’s either going to be a storage platform that’s able to do different types of things at different times, depending on what your needs are. In some cases, if you really want it optimized, you may have to be combining multiple different types of storage to get the best platform of what you're trying to do. And one thing to think about is your optimization goals may be different. We've always had this to a certain extent—that if you talk to people that are trying to build storage systems and figure out what they're really optimizing around, it’s usually something higher-level than the storage. But frequently, we've been able to just convert it to speeds and feeds. We always get stuck on speeds and feeds. Today, we're seeing that it’s speeds and feeds to accomplish—it’s a specific result to optimize for a specific thing. The big thing that we're seeing—and I think you probably saw some of it yesterday—is that we're trying to optimize for the GPU utilization instead of optimizing for transaction response time. Now, GPU utilization optimization may mean high-speed transaction response time, but not always. Some of the aspects of keeping your GPUs highly utilized have to do with more streaming workloads than some of the things we've been looking at before—for example, for high-performance loading of GPUs or checkpointing from GPUs. The second one that I'm hearing is people want to optimize for the efficiency of their data scientists. If you're doing training, tuning with data scientists that have to have deep understanding of your data and have significant expertise to tune your AI workloads, those are high-dollar resources that you want to optimize. They're very highly parallel operations in most cases—different types of parallelism through the different phases, but they're frequently highly parallel with multiple workers doing similar things simultaneously, overlapping. Sometimes, if you've got very sequential workloads but highly parallel with a lot of simultaneous users, things start to look random again. And so, you have to keep that in consideration. And then, like I said before, performance and the capacity varies widely for different AI tasks—or this probably should say for different phases of the AI process.

So, let’s go into our little storage phases of AI graphic here—one perspective. Contemplate it. Having AI generate this—but I thought it might be kind of a novel twist to have poor-quality, engineer-drawn graphics when we're kind of getting used to those cool AI graphics. So, when I first put this together a while back, I ran it by a few people. And they did not agree with the different phases I'd put out here. And I finally figured out that one of the differences in perspective is I was trying to think about the effects on your storage. This is the storage phases of AI. If you go out and do a web search on phases for AI, you will find hundreds of diagrams that look a lot like this. And there’s a lot of different subtle variations. But this tries to break things up into the different phases of an AI pipeline that have very distinctly different storage characteristics. I’ll go a little bit further as we go on. But I was interested to see—and kind of pleased to see—that some of the presentations yesterday, their phases of AI that affect storage ended up looking almost exactly like this. We've got some form of data ingest—you have to bring your data into the system. You've got a phase where you have to do some data cleaning. We'll look at these things a little bit more later on with some feature engineering: model training, model evaluation and tuning, and then inference. And the inference on this diagram is overly simplified. I tried to pick a middle level. I've seen a lot of presentations where inference, to me, looks like a magic 'then a miracle occurred' bubble—they've just got an inference bubble out here, and everything magically happens. Inference runs on a trained model, depends on a trained model—you have to have data going into your inference system. And you really want to be driving some business value out of that. And then, of course, you have to retain some records on what it was you did. It’s one of the things—a theme that I've been hearing through the conference this week—is that the data archive phase doesn’t show up in a lot of thinking about storage for AI. But it’s a growing need.

So, the model-building phases—you've probably seen this—seems to be where all the tension and the hype is: huge amounts of money, and interest, and excitement going into all of the phases of building the model. And, like I say there, it’s hard, and it’s fun. And there’s a huge amount of work there. It’s making extensive use of our data scientists. It’s taken up all the compute resources that everybody that’s building compute resources can ship. It’s certainly caused a huge draw in storage resources as well. And, of course, we all know about the demand for GPU resources. And what are they working on? They're generating a trained model. But for most of us, this isn’t generating business value unless your business is selling a foundational model that can be repurposed by somebody else—for example, a large language model. I was pleased to hear during one of the presentations yesterday, they were pointing out that same thing—that especially, like, the large language models, the cost of creating a new large language model is getting pretty insane. I don’t think they used the word 'insane,' but compared to how much money I make—I have?—yeah. So, in most of our worlds, this, on your accountant spreadsheet, shows up as a nasty word called 'overhead.' Lots of fun and interesting things here, but that’s where all the attention is today. We tried not to focus too much on that in this presentation.

But where your business value is generated—really, for many of us—is going to be out here in the inference phase. You want to be able to run your production data through a trained model running inference and generating business value. Ideally, you're going to have a trained model that you can use over and over and over and over again. Now, you're probably going to want to be continually refining that trained model. And I said this is a simplified diagram over here of the trained model. I liked one of the slides yesterday—I think it was in Prateek’s presentation from AMD. This inference phase, in some respects—lay it on its side, and it ends up looking like the workflow that we see for the model-building phases. For the inference phase, you've got your data coming in. And your data needs to come in—and your data needs clean. Your data needs prepared into AI food, and then it needs run through your inference engine, and you want to tune it. But to our customers, we hope that it can look like this: 'Then a miracle occurs.' Pour your data in here, and we give you your business value. This needs to make extensive use of production data—the real data that you're running your business on every day. Still takes compute resources, still takes GPU resources, but we want to generate business value. And it’s still hard to get right—still fun. So, there’s starting to be a real growth in focus on what you have to build for getting your inference right and generating your business value.

So, what are we trying to do here today? Trying to get people to think a little bit about, 'How does AI change my storage needs? How do I have to think about things differently?' And there’s a lot of different examples.

I pulled out one example here of a real-world case where somebody that I met—just kind of random—was really surprised at how trying to do AI completely changed a part of their storage needs. So, in this example, we were talking about data ingest. This was somebody that I met at a social function, totally unrelated to computers or engineering. And it happens surprisingly often—you're sharing, you know, 'What do you work on? What do I work on?' And this person was working on running a data center for a manufacturing company. And he was telling me, 'My latest problem is that we've started experimenting with AI, and it’s totally disrupting my infrastructure. And my big problem is data ingest,' which really piqued my interest because I had just got a report from one of our VPs from a visit with an analyst—believe it was Gartner at the time—that in his report, he had said that the analyst told him that everybody already has data ingest, so you don’t need to pay any attention to data ingest. Everybody already has archiving processes—we don’t need to pay any attention to that. The things that are really different are the generating models and tuning models, and the inference. And then I run into this person that’s telling me, 'I had a huge problem with ingest.' So, they had the system—but is it the right one? They were capturing their business data, but how did the AI affect what they were capturing, how they were storing it, and how they accessed and used that data?

So, for example, of this company—I haven’t seen him again, so I don’t have permission to use his company name here—but before using AI, they had a very simple model. They were generating huge amounts of production data. It flowed into a process they called their 'business logic,' and out of the business logic, they were generating—he called them KPIs—a few, you know, just some measures of how the business was running and some records. Those were fairly small—was a very small percentage. She said single-digit percentages of the data that was coming in were being pulled out and put into their records and database, and a huge percentage of their data was discarded. Highly sequential data coming in, determined by the rate that their manufacturing facilities were generating data, but then they just did random reads, a small amount of data, discarded the rest of the data, and created all kinds of space for more data.

Well, then they started experimenting with AI, and they came—they experimented just a little bit—and they were looking at AI analysis of the data that they had coming in. And so, now they find that the AI uncovered all kinds of value in data that they were throwing away. So, they had the production data coming in, same business logic, same pre-AI business data being saved out, but now they created a model which was AI-enhanced business logic. Creating new business opportunities was the thing that he called out that they really found valuable—that was in data they were throwing away. So, that AI-enhanced business logic was generating data insights for them. So, then immediately, they start talking about, 'We can’t be throwing this data away anymore.' So, now he’s trying to put together a data valuation AI to try and figure out, 'What data can I throw away? What data do I have to keep?' And his discarded data got really small. And he’s got a huge pool of saved data over here that has to be retained. And he’s still got the input data that’s mostly sequential, still random reads from a small percentage of that data. But the amount of the data that they were using ballooned dramatically. They were touching a huge percentage of the data where previously they were just cherry-picking a few pieces. And they have this huge amount of saved data that they need to keep for the future. And it totally blew up his data ingest because they were counting on throwing away the data and generating more capacity. So, this is in just one space, one customer. And there’s things like this happen throughout your IT infrastructure as you're trying to bring in AI. If you're starting from scratch, building everything new, that’s great. Most of the people I've talked to aren’t.

So, then, trying to take a look at the phases that we talked about earlier—what do the storage requirements look like for the storage for AI phases that we've just talked about?

And, like I said, this is going to have to be fairly general, of common themes. You don’t have to talk to too many different people that are implementing installations before you find that they're all a little bit different. But some general principles. The data-cleaning phase—I’ve broken it out as a phase of its own that’s got specific storage requirements. This is where you have your raw source data that has to be prepared for use in AI. Depending on what type of environment you’ve got, you’ve got logs, you’ve got pictures, you’ve got videos, you’ve got documents, et cetera. From the person I was talking to a while back—you know, their manufacturing facility—they had sensor data. They had videos and pictures that they were capturing of the manufacturing process. They had X-rays—all sorts of different types of data. And so, in the data-cleaning phase, you know, they had to clean out noise—things that are true, just extra noise—logging human-readable stuff that you don’t need anymore, deduplicate your data, normalize your data. One that’s starting to get hot that we forgot about for a long time—thinking about privacy and the ethical considerations of the data that you’re analyzing—doing things like anonymizing personally identifiable information, removing bias. Some of that stuff happens in the cleaning phase now. As far as it affects storage, this data is read from your ingest storage. And the best practices that are seeing emerge is that the data is read from the ingest storage, and then it usually gets stored into a higher-performance, different type of storage for the next phase. So, it gets written into the storage for the feature-engineering phase. People are frantically trying to automate with AI this phase. And I've shown over on the left general principles on types of capacity performance that you need for these types of systems. We're seeing that you have to have high capacity because you're capturing everything. The write performance—it’s usually highly sequential in terms of really high performance. Most environments really aren’t too bad. And then the read performance—similarly, you're reading small amounts of the data out—that tends to be highly random. And so, some general themes in the data-cleaning phase.

Feature-engineering phase—this is where you have your data scientists that are serving as translators. They're taking your raw data that came out of the clean—this is now cleaned raw data—and they're turning it into what I call 'food for AI.' As we all know, computers run on numbers. They can’t process anything but numbers. If it’s a document, you have to convert it to some sort of a numerical representation—usually some vectoring and putting in databases. So, you have to convert the data. The data scientists are frequently exploring the data and identifying patterns, outliers, relationships. When I've talked to data scientists that really know their stuff, that phase just amazes me. These people can look at data and figure out patterns, relationships that I had no clue were there. So, a good data scientist is really valuable there. Again, people are trying to automate a lot of this stuff with AI. You're splitting your data for training testing. You're doing feature extraction. You're doing data transformation—often highly parallel with a bunch of people working at the same time. In terms of requirements, your capacity’s down a little bit—you've cleaned up some of the data, thrown some away, your write performance tends to be a little bit higher—requirements about, we're calling it roughly mid-range write performance—read performance similarly, and the writes and reads tend to be very random for feature engineering.

Model training—there’s been a ton of coverage on this one. I don’t want to get stuck here, but we all know that GPUs are driving the cost of our systems these days, and maximizing that GPU utilization is optimizing your investment. And it’s optimizing the use of our resources too, in many respects. So, you want to design for balanced architecture—balance your storage performance with the GPU requirements. Early on, I was seeing a lot of recommendations for, 'How do I build my storage for training?' And recommendations were to figure out the absolute maximum possible performance that your GPU farm could hit and make sure your storage can do at least 30% more—or at least that much speed. It’s not always the most efficient use of your resources. So, you can’t see it on the screen here, unfortunately—it’s in the slides. There’s a really good presentation that was done at the CMSI Summit that I've got linked on the page there. John Cardante with Dell did that, and it covers how to design for balanced storage performance. Some of the things you want to think about—you know, you would think about your data sources. This tends to be where you figure out, 'Are you primarily file? Are you primarily object? Can you use either one? What’s driving whether you're file or object access?' A lot of cases, you end up having to do both file and object. If you have known workloads that you're going to be running, then you can really match your storage performance to the workload—and that’s something that I think gets missed frequently. A lot of the smaller environments that we're seeing—there is a specific application for what they're trying to do. They know what type of machine-learning algorithms they're trying to use, they know some characteristics. So, if you know that, you can look at GPU benchmarks that show peak performance for the various models. ML Commons is a good source for that. Figure out the size of your training examples, and you run some math, and you can figure out the required read bandwidth for your storage. Like I said, the presentation linked at the bottom has a lot more depth on doing that. If you're creating a general-purpose GPU pool, then you might need to support the GPU maximum read speed today. And, as we probably all know, for high-performance GPUs today, you can be looking at, like, a gigabyte per second per GPU. So, you can have some pretty extreme storage performance requirements. And again, back to the start—what type of storage is this? External, internal, object, file? Again, that ends up being influenced by the size of your datasets, the type of data you're working on, and how you can optimize the cost and the performance of your system.

And then, there’s been a rise in thinking about, 'How do you build storage for model training when things can go wrong?' Early, several years ago, the conventional wisdom that I saw was that your storage for model training has to be high-performance—'free storage, nobody cares about writes.' That’s not true anymore. The writes have become critically important—and particularly for checkpointing. As soon as the models got to where they were running for a long time, we saw in a presentation—I think it was one of the presentations later, I don’t remember if it was this morning or last night—there was a presentation that talked about the failure rate of GPUs in the Meta-Llama training, and it was pretty high. And training runs for weeks, and the cost of restarting is very high. So, you want to be able to do checkpointing. Checkpointing files are written sequentially—frequently multiple sequential writes in parallel. During that time, most of our technologies today for checkpointing—your training’s paused. Your GPUs are sitting there losing money—or not making any money, at least. For checkpoint restoration, that’s all reversed. You've got high sequential reads. In that case, it’s almost always multiple parallel streams loading a set of GPUs in parallel to get back running as fast as possible. And again, that presentation from John Cardante has some good information about how you can size your system to meet your goals. Capacity here tends to be kind of mid-range. Write and read performance are off the charts, and it tends to be highly sequential. And we've seen highly sequential for both object and file—different characteristics of a sequential object versus file, but it tends to be highly sequential. Evaluation and tuning—so, evaluation is measuring how well the results of the model match your expectation. Key one: accuracy—how often is it correct? We wouldn’t be here today if we couldn’t get AI to be correct at least part of the time. Precision and recall—it’s kind of roughly a measure of how often wrong versus wrong. And there’s a variety of measures such as your F1 score and your area under the curve—your receiver operating characteristics. In the presentation earlier this morning, we saw a discussion of area under the curve. And you have to do tuning based on results you're getting, which basically means adjusting your hyperparameters—other aspects to improve your evaluation scores. Output of this—you get a dataset containing the model parameters, which is generally, you know, an internal representation of the neural network. And that model parameter size tends to be constant based on the number of weights. So, in your training-tuning phase, again, we're seeing that you need kind of mid-range capacity—write and read performance as fast as you can get it. And this tends to be a little bit more of a sequential workload.

And then the inference phase—this is: run your production data through the finished model to generate business value. We all need to generate business value so that business value can contribute to paycheck value in the future. So, inference—I've been kind of surprised. I've had some questions: 'What’s inference?' It’s almost right there in the name, but I put it in here. You're inferring information from the data. There’s lots of different types. And this phase—maybe more than any of the others—really varies depending on what you're doing. Retrieval-augmented generation from large language models is a hot one today—that seems to be getting the most attention. There’s a lot of predictive analytics, computer vision, and anomaly detection. There’s high value and a lot of uses of AI for predictive analytics, computer vision, and anomaly detection—and we don’t talk about those much these days. And, like I said, the access pattern can vary. I put 'depending on type of inference'—it can really vary quite a bit. For example, RAG often produces a random workload that looks a lot like some of the workloads we've seen in the past for databases. Here, your capacity tends to be, you know, kind of calling it a 25%. Your write performance—similarly. Read performance has to be high for the inference and tends to be very random.

And then a phase that I think gets overlooked a lot—archive phase. Often overlooked because it’s not a core part of AI, and everybody thinks they understand their long-term data retention, but our needs are growing. There are some regulations that mandate storing information for AI applications. And the amount of mandated data storage is increasing as regulations grow. This is similar, but it’s not traditional archive. The big thing is that we're seeing that most people that are doing this want to bring the data back periodically and either use it for more training or get some more insights off of it. The performance needs vary, but really, you want just fast enough—it’s not really designed for performance. One thing I find interesting is the industry does not yet have accepted terminology around it. I tend to see 'archive' most of the time, but people have been doing archive for 50 years. Like I said earlier, this is similar, but it’s not quite archive. Kind of the term that I've been hearing a lot lately that I think seems to work the best is 'cold storage.' And this is something that we're seeing as a continually growing dataset—it’s just going to get bigger and bigger and bigger over time. So, you've got to have low-cost and low-carbon-footprint storage. Some of the other things that SNIA is working on—some of the other sessions this week—are looking at things like DNA and optical storage. We saw silica and ceramic storage yesterday. And maybe some of those technologies will help with this archiving problem. You'll see here in the capacity space—it’s just a completely different scale. So, I didn’t even try to put it on the same type of capacity chart. Instead of 100% capacity, this is like 600%, 700%. You end up just having huge amounts of capacity—almost all sequential. A little bit of random read, but it doesn’t turn out to be a huge amount.

So, that’s a quick overview of some of the phases and considerations on that. Then, we've got a little bit on tools and technologies—some of the considerations for building your next infrastructure. I’ll invite Craig Carlson up to go through that for you.

All right, thank you, Curtis. So, a little bit on tools and technology. I have probably 13 minutes to do 25 minutes of material, so I’ll go a little bit faster than I was planning on. So, calculating performance—we have all these great models, these systems, and, of course, you always want a benchmark. And one really good—if you don’t already know about this—one really good resource for that is ML Commons. If you go out to their website—just do a search, mlcommons.org—but you can do a web search. They have a lot of different categories of publicly available benchmarks. They have benchmarks for training, for inference—all the way from different types of devices: mobile, data center, edge. They also have some for storage, and they have some training algorithm benchmarks as well. So, you can see different models—how they might compare.

A little bit on accelerators. We all know that the GPU does a lot of heavy lifting—it needs to have some help every now and then. And I'm going to talk about three categories of accelerators: the SDXI, how that could fit in, computational storage, and GPUs.

So, SDXI is a—a data mover standard developed by—being developed by SNIA. And the nice thing about SDXI is it’s not a new concept. You know, we've had data movers for—for decades. You know, DMA engines have been around for ages. But the thing about DMA engines is that they typically are specific to the system they're running in. So, the way you program them, the way you set them up, how they operate, is typically different for each—each component. SDXI is an attempt to bring that to a standard software interface so that, you know, once you encounter one of these, you know how to run it. You know how to—you know how to set it up. You know what to expect. And it also then opens itself up to some expanded possibilities—such as adding functions, compute functions, to the SDXI. I have a couple examples here. If you went to the BOF last night, we came up with a whole bunch more. So, those guys have a lot of—a lot of work to do. Shyam is—he’s busy writing right now to kind of figure out how to do all that. But, you know, some of the—some of the basic ones might be encryption, decryption, compression, decompression. We had some for filtering—you know, other—all sorts of other—transcoding—all sorts of other ideas that came up last night. So, this is an area that I think is going to be very interesting and—and could be a lot of help in doing offloads for AI.

Computational storage is something that’s been developed by both SNIA and NVMe. And it’s an—an open platform for adding compute functions to storage devices. The idea is that you do your computation closer to the data. The less you—the less you have to move your data—and this came up in—in the AI BOF last night—about how expensive it can be just to move data in terms of—of—of resources in your network across the network. And so, the—the closer you can do your computation of the data, the more—the more you can get a lot of benefits from that. And—you know—it’s—it’s a lot of the similar functions that you might see for SDXI—such as encryption, decompression, compression, data filtering—which is something that I think that we had a talk on—on something like that yesterday. And then the—also preparation for training. There’s a lot of pre-work that is very mechanical that you could have a lot of these devices doing.

Of course, there’s GPUs. These are the big guys in the data center. And—you know—you—anybody who goes back to their—goes back to supercomputing—I came from the—from supercomputing—supercomputing days—it’s the same stuff. You know—it’s big vector processors. You have these processors that are doing massive parallel calculations. They're typically one simple calculation done hundreds of times at the same time in one clock step. And this is very amenable to—to AI calculations. Also—the—the—the memory in these—in these GPUs tends to be HBM—which is—you know—faster—of course, more expensive—but a couple times faster than you might see in DDR.

GPUs—now—any time you say this—especially in AI—when you say 'typically,' someone’s going to come and tell you you're wrong. But—very often—AIs are more effective for training, but they can be used for inference as well. You know—we don’t necessarily have the big GPUs in our phones or whatever they're doing. Some of the AI functions now—they're doing the inference based on other technologies—but you can use it for that. GPUs—like any—any technologies—do have a downside. They're harder to program—they're not a CPU. So—you have to have—look at different ways of programming them. They can use a lot of power—which—of course—raises your—your—your costs and your cooling requirements. They can be very expensive. If anybody’s priced some of these things these days—it’s—it’s astounding. You can buy yourself a house and a couple nice cars—and—and—of course—any time you have an external component—or a component that’s not the CPU—moving data in and out of it can be—can—can induce latency. And so—you may not always want to put all your data problems on the—on the GPU—depending on how big they are and what kind of—what kind of calculations they're doing—because it may not be worth it just the—the time needed to move it in and out.

So—the—the other component is network patterns and storage patterns. One adage that we have for compute—compute—compute systems that I always—that you—you—I always get reminded of—is, 'You're only as fast as your slowest part.' And unfortunately—a lot of times—the networking and storage components—just due—due to the nature—the physics—the latency—how they work—are the slowest components. And the—you know—storage devices are—are limited by the—the time to read and write from the media—or the—the flash—what have you and networks are limited by latency—the issue here is that they keep you from the goal of keeping the GPUs fed—because that’s your high—your high cost. That’s where you're making your money.

A little bit on some of the data patterns—you know—if you look at—especially the checkpoint time—if you look at—I think there was some—there was something I’d say—yesterday saying, 'If you include checkpointing, the GPUs are probably about 40% busy.' The rest of it’s probably waiting for checkpointing. And then—of course—you have to also ingest data for the next step—or epoch—that you're doing.

The other thing is that the—excuse me—the checkpointing creates a very bursty traffic pattern. And this has a lot of ramifications on your network design—especially when it comes to congestion and being able to handle bursts of data that—you know—you may not have a traditional network that you definitely have in these type of networks.

And so—which network am I talking about? If you're looking at a data center—and once again—when I say 'typically,' somebody will come up and say, 'Well, that’s not how we do it'—but this is a very typical AI network. You have on the—on the far side—on the close side over here—you have what you call your—what was typical of the front-end network—which would be your public LAN—your LAN—which—you know—you is very very standard networking technology you're probably you're probably going out to the cloud to store some of your data—or you might have some local object file storage on that side. Then, in the middle—in the gray box—you have your scale-up network—which would be a network which connects the accelerators—GPUs—and that’s typically a very, very high-performance—but it doesn’t have a lot of function. It’s really just meant to move the data between those. It’s not going to have—you know—a lot of different types of protocols that it’s trying to manage. It’s very dedicated—and probably over-provisioned—to moving that data. And then, on the scale-out network—the back end—is what you would have to connect the racks—or the pods—and that’s the scale-out network. And that’s the network that sees a lot of the burden from checkpointing and things like that. And for that network—you really want it to be very high-performance—very low-latency—very—um—air—air—um—have good error recovery and very—uh—um—have good congestion management. So that—so that when you have these bursts of traffic—you're not losing frames because the switches are getting congested.

So—for the scale-out—um—there—obviously—there are some—there are some—uh—proprietary solutions, but there’s a—a new solution that’s being developed—which is Ultra Ethernet. Um—and it’s an open project being developed under the—uh—Linux Foundation. And the—the goals are—uh—to—uh—scale to millions of—million nodes—um—having the most recent congestion management—uh—protocols—uh—low-latency—uh—protocols—uh—highly reliable with—um—built-in error recovery at multiple layers. And—of course—you also want to have security that’s built-in from the beginning and not an afterthought. Um—the Ultra Ethernet Consortium is a membership of—I think it’s probably about 90 members now—bringing the best techniques learned over many years and if you I think one of the first attempts to standardize this type of thing was the Data Center Bridging—which turned into a Converged Ethernet. That has some good positive spots. There’s been a lot of advancement since that time in how to do some of these things. And that’s the experience that’s going into this group. The specifications for Ultra Ethernet are expected to be completed by the end of the year—at which point they will be publicly available.

The other one—which is kind of the newer kid on the block—which is for the scale-up network—is Ultra-Accelerator Link. And Ultra-Accelerator Link is an accelerator—or a GPU-to-GPU connection—which focuses on connecting the DDR—HBM memories in the accelerators. It’s very simple—low-store operations—low-latency—high-bandwidth—for the hundreds of accelerators that you might have in your rack. The initial specification is looking at data rates of 200 gigabits per second. And the—and—once again—this is a group of companies that have come together to use their experience to build this very, very high-speed network. The—and the plan is that it’s going to complement the Ultra Ethernet. Now—this group is just getting started. You'll probably see some more public information available in the next couple months—including a website—which they're putting up right now. So, if you have any questions—you can ask me. I can’t talk about it a lot—but I can answer some questions.

So, if you look at—and Curtis mentioned this—the storing of all of this stuff—the three types of storage typically used are cloud and object—and that’s usually for your model data—and then block. And a lot of the things that I've seen—block is very commonly used—and this is a lot of times NVMe drives—for the checkpointing component. You know—model data doesn’t necessarily need to have that—that super low latency—but for checkpointing—the block storage is beneficial because you have very low latency—and that’s something that the UEC—Ultra Ethernet Group—is looking at right now.

Another component is the CXL—and how memory tiering could help a lot of these data centers. You have what you call the 'memory wall'—and being able to put enough memory in these systems—CXL has some promise to alleviate some of the issues—especially since you can—if you can compose that memory in memory pools—you can use it a lot more effectively instead of having it stuck in just one system.

Storage challenges—you know—performance, scalability, and reliability. Of course—you need to have things—you want to keep those GPUs fed—you want to be able to scale to some of these data center sites which are very, very large. Reliability—you know—you don’t want to lose any of your checkpoint data—because that’s very expensive. And one of the things that we're looking at is how SNIA can help with these.

One thing I wanted to mention is that SNIA does have a data pattern repository—and we're now looking to try to get some AI traces in this. It’s typically been not AI—but we do want to fill that gap. It’s a very useful tool—especially if anybody’s done any—if anybody’s done any simulations—you know it can be very hard to find sample data patterns. A lot of times, companies and data centers consider this to be proprietary information. What we're doing is collecting information that people can use publicly and openly.

So, that’s all I have for today. Any questions? I don’t think we have much time. Okay, thank you. Thank you.
