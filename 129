
Sounds good. Exciting to see a nice crowd here. I'm going to talk about what we think is needed to make the next wave of computing happen, some of the key trends that need to be supported to make that happen, and especially how optics is going to enable that.

So I think this chart is something that you've seen a lot. I'm not going to talk about how the parameters, the number of parameters in AI, ML, algorithms are growing. Some say 35x in two years. Some say 100x in three years, whatever it is. It's growing fast. But what does that mean, really? What it means is we really need to scale the amount of compute, the amount of memory that you closely knit together to solve these problems, build this infrastructure that can efficiently train and do inference, and so on and so forth. So if you look at some of the numbers, a single accelerator or a single GPU can probably handle something in the order of a billion, maybe 10 billion parameters. And some people have said, well, let's make it bigger, broader, maybe put five GPU die in one super chip or whatever you might call it. And still, you get to maybe a 2x, 3x kind of scale. But you really need trillions of parameters to handle trillions of parameters. Again, what does that mean? It means you need infrastructure that is, again, tightly knit, that has hundreds of terabytes of memory that is holding all these parameters that are flying around across lots of accelerator chips. The latest is hundreds that are knit together in the latest generations. In the future, we need thousands of those knit together. We need exaflops of compute. And this compute may be actually also distributed across different types of devices, GPUs, purpose-built accelerators, CPUs, and so on and so forth. So what does that demand? It demands really an efficient way to interconnect all these distributed elements that carry compute capability, that carry lots of fast DRAM or HBM memory. So how do we get there?

And if you look at, again, the latest and greatest architectures out there, you may recognize this picture from one of the leading vendors of AI infrastructure solutions. And on the right-hand side is the latest solution where a node or a chassis consists of eight GPUs which are interconnected with lots of bandwidth. The red oval shows the bisection bandwidth that exists between eight GPUs within the chassis. And then the blue oval shows the bisection bandwidth for node-to-node or chassis-to-chassis connectivity. And you'll notice that if you look at the numbers, the blue oval offers half the throughput, half the bandwidth that the red oval offers from a node-to-node connectivity perspective. Is that ideal? Is that by design? Perhaps not, although there are ways to reduce the amount of data flying between nodes and stuff like that. So what's the reason for not putting full bandwidth? It's things like cost and power and latency and efficiency and so on and so forth. So really, if you look at links like CXL and NVLink, high-speed service, whatever the higher-level protocol is, we really need a very efficient interconnect that sort of breaks that distance barrier as well.

So if you look towards what we need to do to build larger clusters, which involve thousands of XPUs, I call them, we really need topologies which allow very high radix and also high-throughput links. So each node or each element, maybe it's consists of an accelerator and a CPU, or maybe the accelerators are separated from the CPUs and interconnected also. But really, what we're talking about is terabytes per second coming out of these chips, multiple terabytes per second connectivity, and then the switches that sort of interconnect all these elements. We're looking at tens of terabytes per second, at least in the coming generations, as you aggregate perhaps eight or 16 of these elements together through some of these switches. So again, some of the topologies drawn here are not the only ones possible. There are other partitions, other ways to organize things.

And I'm not going to go into details of exactly the topology. And this picture actually adds one more element, which is sort of disaggregating the memory on the left-hand side of that picture there. You can have a pool or a shared memory that all the elements, the compute elements, are able to access and partition in various ways as workloads change. So this is, again, creating another frontier or another connectivity challenge that we need to solve. And again, the demands are pretty high. If you look at the amount of memory you can pack either inside the package or on a substrate or right next to the chip, it's going to be limited. Maybe you can push it, but there's some point at which you're going to start to say, I need to escape this. So that's where the problem lies, the biggest challenge lies in our perspective.

And really, the package-level integration is already baked in and people are already doing that. And if you look at some of the trends that are already being realized, the socket has become more capable. People are putting 12, 15 die onto a package, maybe six HBMs, four compute die, and so on and so forth. But then how do you expand beyond that? How do you interconnect those things? That's what drives this need for this new I/O for what I call scale out.

So one way to approach that is to look at chiplets, optical I/O chiplets. And if you look at this picture, what we are showing is in the middle there an SOC die surrounded by four optical I/O chiplets. And each chiplet offers a certain number of fiber ports, in this case, eight ports per chiplet. And the interface between the SOC die and the optical I/O chiplet could be something very efficient, something like UCIe, BoW, something like that, which is half a picojoule per bit, those kinds of numbers for that interface. And then you go directly optical out of the package. So that's what really enables the next wave of computing, in our view. And we also believe that the approach of having an external laser, an external light source, really solves a lot of the concerns with respect to the thermal environment in which the laser needs to live, and so on and so forth. Because when you are a chiplet, you're really inside the package next to a pretty hot SOC die. And hence, putting a laser inside the package is almost impossible. So we have a multi-wavelength light source. So we produce, let's say, in the ones that we are demonstrating today, eight wavelengths. We use eight wavelengths. And we put data, carry data, over eight wavelengths on each port. So you can think of it as eight times eight carriers per chiplet. So you have 64 carriers. And then the packaging side of things, you can choose your packaging technology. You can use any of the 2 and 1/2 D type of stuff, silicon interposer, EMIB, and so on and so forth. Or even an organic substrate-based packaging strategy. UCIe Standard, for example, works with a standard organic substrate package.

And this chart here on the left-hand side really talks about what is the value proposition of this approach. If you look at the electrical line that is sloping downwards as we go to the right, and the x-axis here is distance or reach. And on the top, I've labeled in-package, on-board, and off-board. So if you think about it, a lot of copper is used for in-package and on-board, especially at very high speeds. You talk about 100 gig per lane, or maybe going forward 200 gig per lane. You're not going to try and do cables going any kind of distance with 200 gig per lane. So electrical does have challenges to go beyond the board, especially at very high throughputs. And really, optical is how we get there. But if you draw a metric on the y-axis which talks about a composite of bandwidth density and energy efficiency, it really starts to emerge that electrical at very low distances, like in-package distances, is great. But once you get to even several centimeters, inches, and meters in particular, it breaks down. So what the chiplet solution offers is a way to bring essentially the metrics that the in-package electrical interfaces-- let's say USR, XSR, or even UCIe kind of metrics-- to the optics, which opens up the distance and reach.

So the goal is to see how we can make it possible to go beyond-- I'll go here-- not be constrained by the package or a substrate, but have a rack scale interconnect that can really start to look like a collection of elements that are almost on the same package or very close to that. The latencies, the BERs, the power efficiencies are such that, and the throughputs are such that, you're not thinking, oh, how far is this thing and how far is that thing, and how much latency am I incurring by going this distance, and so on and so forth. So can we have a new definition of a socket, or maybe a superchip, or whatever you want to call it, that spans a rack, perhaps, or multiple chassis with this kind of connectivity? So that's the idea. That's the goal.

And if you look at the technology, this multi-wavelength, micro-ring-based chiplet technology, it has a lot of room to scale. We're not even talking about having to do a feasibility study to see whether the next node is possible or not. We already demonstrated that we can scale the number of ports. You can easily go from eight ports on a chiplet to 16 ports on a chiplet. You can go from eight lambda per port to 16 lambda per port. We have demonstrations that are showing that. Per wavelength, we can also go up. Today, we are using 32 gig NRZ per wavelength. And if you actually multiply those numbers, eight ports times eight lambda and 32 gig per lambda, you end up with two terabits in each direction. And bidirectionally, it's four terabits per second. And we have a path towards doubling that several times over. And you can put multiple chiplets in a package, which is another degree of freedom you have, of course, with some limitations in terms of how much shoreline you have. So there's plenty of room to run with this approach.

So we think this is a great way to go. And by the way, this is not out in the future. We have demonstrations we are showing. We recently showed at Hot Chips. And we have a booth here. Please check this out. And one realization of an AI accelerator is an FPGA. And we have integrated our chiplets into an FPGA package. And we are showing connectivity between two FPGAs in a card form factor like this, which delivers multi-terabit connectivity between two accelerators, in this case, illustrating that with an FPGA card.

So with that, the call to action is to see how we can help develop this optical I/O ecosystem. And one of the things we really need is to see how we can standardize some of the form factors, the fiber management solutions, the connectors, and so on and so forth. And of course, some of the packaging infrastructure, testing infrastructure is happening. And we are driving some of that. So if you would like to talk some more, we have folks at the Intel booth with the FPGA card. Please check that out. You can see how that MCP looks like with two of our chiplets next to an FPGA. Our website there and my email address if you need to send any questions.

All right, thank you. Thank you, Elke. Ali, please go ahead.

Question I had was that you're running this at 32 gigabit per second. And I would imagine that at least you're going to consume about a picojoule per wavelength. Wouldn't it be more efficient to run this much faster? And if you run much faster, would you be able to do that with micro rings?

Yeah, so there are lots of trade-offs in terms of running faster. You may have to go to a higher modulation rate, I mean, complex modulations, which might also consume a little more power, maybe incur more latency. So depending on what you're optimizing for, you may choose different things. We have chosen 32 gig NRZ for now at least. And that allows us to operate without a FEC at all, and which is a great thing. We don't need a FEC, and we can achieve 10 to the minus 12, 10 to the minus 9 kind of BERs, which is what some of these applications need. When you have a memory disaggregation, compute to memory interconnect, you don't want to have a lot of latency. You don't want to have bit errors happening often, which need retries and things like that. OK, thank you.

Thank you. Just one question from me. We've seen Ayar Labs technology for a number of years, a lot of demos, a lot of incremental improvements. But the deployments haven't quite started yet. So what do you think— I mean, personally, what do you think the main challenge is for getting those products adopted massively?

Yeah, so I think the inflection point in terms of the need is really starting to ramp up with the generative AI and all that, pushing the number of parameters and all those kinds of things. And I think we are seeing a lot more traction with customers and thinking about deploying this technology. There are some challenges in terms of the ecosystem, like I said, which need to come together. And we are seeing that moving along as some of the larger players start to sort of push on that front.
