YouTube:https://www.youtube.com/watch?v=VvKEHq3xjUw
Text:
Thank you, CXL Forum, and thanks, MemVerge, for hosting this event. Glad to be here. Today's topic, we're going to talk about the CXL switch and how this CXL switch can enable better computing, and also that applies to AI machine learning. That's a lot of usage case today. My name is JP Jiang, and I'm come from a startup called Xconn Technology. So Xconn Technologies is an IC startup in the Silicon Valley. The company was started three years ago in 2020, so we're about more than three years and a couple of months old. So let me go to today's topic.

So first of all, we are a chip company. So we build the CXL switch. The switch we build, we already have the IC. So this is the IC was a sample back last summer. So this is a CXL 2.0 switch. And this is also a PCIe Gen5 switch as well because everybody knows the CXL is built on top of the PCIe technology leverage of the existing PCIe advanced feature and many years of IP development. So the switch we're having is a, have the largest lane count. So this switch has a total 256 lanes. And the total switching capacity is reaching to two terabytes per second. This switch is, at the beginning of the design, we kind of targeted the CXL as our main application and also the data center, all this AI computing as our main market for this switch IC. So we didn't put any thought into the other usage case for our switch. We're just a single mind to focus on this data center computing area. That's why we be able to optimize a lot of the design. So for example, the CXL, everybody today here is talking about the memory usage case, right? So at least we see the memory expansion, pooling, sharing. That's probably the first wave of the application for this CXL. But CXL continue to evolve with many advanced feature to try to fit into the modern computing. So later, of course, the CXL going to expand to many other usage cases. The first usage case we see, and we agree with many experts here, is through the memory pooling, through the CXL to do this memory application. And for the memory application, we also understand this latency requirement is very critical. Because all the customers we talk to, all the other hyperscalers, they are super, super concerned about the latency they incurred through the CXL. So that's where we very much optimize our design to achieve the lowest port-to-port latency, basically through our switch. And all these memory applications, our switch only add a very minimal latency into this memory path, memory process. Another thing is the data center, they also require very low power consumption. That's where we also optimize our architecture to achieve the lowest power possible. And through our switch, because it has the largest lane count, largest port count for the data center usage, we'd be able to reduce the PCB size, reduce the TCO overall for the end user as well. So today we have our silicon, not only the first silicon A0, we already have our A1 silicon in our lab. So the chip we already have worked today with the processors, Intel and AMD processors, which are the CXL 1.1 at this point, of course these are two companies. They didn't stop here, they continue to develop the next generation processor. And also we work with a number of CXL memory controller, memory device, make sure the interop issues are resolved. So we're moving along quite well with the processor, memory device, and our switch kind of forming an ecosystem, be able to provide a solution through the processor, through our switch, and be able to access the memory device. Then we're working with the upcoming CXL 2.0 processors to make sure we get to work with them next year when those processors are getting available into the market. And also people are mentioning, you know, a lot of computing devices, they are still, majority of them are PCIe based. So we do see the coexistence between PCIe and CXL going to be for a very long time. So our switch, actually the nice thing about it is it'll work with hybrid mode, basically mix the CXL and the PCIe. So for many years to come, our switch would be able to still bridge the two worlds until everybody embraces the CXL world, right? But I imagine that that takes some time to get in there. So as we are a chip company, we already have the samples, and those chip samples are available. And right now, the first generation switch chip is going to go to production first half next year. So that's kind of, in general, our plan. So this is the first time I'm here in Europe, so I want to introduce this switch chip to everybody to make you be aware, you know, we are, the CXL world is chugging along, and we are building our switch. And so this ecosystem almost formed with, you know, processor switch and the device and many other devices that come in very nicely.

So let me move on to some very simple usage cases to introduce this usage of our switch. So first of all, we talk about the memory expansion and some usage cases that Shunji also talked about the in-memory database, right? So in-memory database, we think, you know, the in-memory database, a lot of times they require a huge amount of memory to be attached. So some of them require, you know, 10, 20, or even 30 terabyte or even bigger kind of memory footprint. And in the past, it would be very hard to get into that kind of large size of memory size because for CPU socket, you only have the limited number of DIMM channels available. And expand beyond that is possible, but it can be very complicated and costly. And so this CXL switch, we can kind of expand that channel, right? You can imagine that how many switch port you have is how many DIMM channel you added to that. So our switch more looks like an amplifier, you know, through one of the channel, you amplified it to many multiples of channels. So our chip is going to provide a total of 32 ports. So if you all use up these 32 ports, you expand the 32 DIMM channels that immediately add a lot of memory. So very simple case, our switch would enable this memory expansion tremendously to a very large scale. So I won't explain further the expansion, but it's very simple to understand. So next, I want to talk about this memory pooling, which we see the most popular usage for this CXL memory application. So memory pooling not only work today. So on the left side of the picture, that's today's picture. All these hosts, they are supporting CXL 1.1. And the devices, the memory devices are single logical devices as of today. However, in the future or next year, we do see these hosts going to the 2.0 and the memory device going to support a multiple logical device, MLD. And our switch work in both cases. In the today case, our switch kind of virtualized all the memory pool that's behind it to form, to manage through a Fabric Manager to be able to manage these big memory pool formed by SLD devices. But in the future, our switch also complying to the CXL 2.0 spec and be able to work immediately with the MLD and the CXL 2.0 processors.

So also, some concepts borrowed from CXL 3.0. In the 3.0, we are enabling this switch cascading or inter-switch connectivity. And our switch today can also support this cascading. So in certain cases, our customers, they want to build a very large size memory pool. And just 32 ports, our single switch is not enough. So in this case, we can do the cascading of a number of switches to enable them to form a very big memory pool. And of course, in the next generation, 3.0, we're going to support a Fabric network, which enable an even larger size of memory pool. All right. So those are the applications.

Let me move on to the hardware we're having. We had-- so people can easily see the chip, the silicon, which is in the final package form. You can see on this picture, top is our silicon. And we also have this reference design hardware in a kind of chassis, in a rack, in a chassis form, and showed here. So in the middle, of course, is our switch, which has a small heat sink on top of it. And then we have many, many ports spanning out of this IC. So this is our reference design platform. We've been developing using this platform to do a lot of validation and also development.

So let me show you the memory pooling, since today's topic, we're centering around this memory application. So this is the picture or the architecture diagram that we're using in our own development. So again, in the center is our switch. And the top, the red colored, that's the CXL host, like those hosts offered by Intel and AMD. And for the memory pooling topology, because this memory pool is being shared by multiple hosts. So here we just use two hosts to represent this development platform. And in case that's our setup in our lab, of course, once you have two hosts working, you can further expand it to four or eight hosts. And on the downstream part, we have two memory expander device. Then we have a management host that implemented the Fabric Manager function that works with our switch. So this total diagram or architecture kind of captured our development platform we're using to prove the concept of this memory pooling, development of Fabric Manager, all these things to work together.

Let me show you another picture, which is the hardware setup we're using in our lab. So I put some of the reference design hardware boards here. So from the CXL host, we have some retimer board that expand or kind of connect to our switch. In the middle is our switch, the reference design platform we just showed. And we also have built some PCIe or CEM slot expander or the EDSSF expander board to enable the downstream port to be populated with many, many more CXL memory devices. And on the management host, we use a small board that's basically powered by ARM processor that we're doing all this management Fabric Manager software running on that board. So this kind of setup kind of allowed us to do all this memory pooling proof of concept. And we've been successfully bringing up this system in our lab and be able to do the memory pooling.

So next one, I actually use a demo video. So let me try to play the video. But I want to enter certain... Okay, so let me see if I can play it. So let me just talk about this demo kind of scenario. The real demo is in the MemVerge booth. So we have a setup there. If anybody is interested to see the real demo video, you can go to the MemVerge booth to see the video. And also I'm going to provide a link on the YouTube to allow you to see that on the YouTube as well. So this picture diagram shows there's two hosts, host A and host B. And these two hosts has allocated the, you know, HDM, which is going to use a CXL memory. These two segments of HDM through our switch actually map to a CXL device. So both these two hosts would be able to share this one single memory device. So we here, we just use one memory device to show, you know, through the switch, they can share this memory device. But of course, memory pooling, we are going to put multiple of CXL memory device into this pool. For this demo purpose, we just use one single device. And these two segments from host A and host B, they actually map to the different location in this CXL memory module. All right. Let me see. So this, all right, so this is the real setup in our lab. As you can see in the middle is our switchboard, which is have a picture in the previous slide. And on the left side is one of the Intel Sapphire Rapid server processor. And also right side is another one. And you can see the cable through the cable app connect to our switchboard. And on the switchboard, we have one single memory module that plug into the CEM slot. So this is the setup we use in our lab to show this demonstration. You go to the next. And we're running some test software. And using the, on the screen on the left side, the upper left side represent a host A. Sorry, the upper left side represent a host A. And the lower left side represent a host B. So both of these are two hosts that they can discover and locate the CXL memory module. Then we run some application that basically write some data into the CXL module and read from that memory location. So from the host one side, it write this text file into this CXL memory and then read it out and display it on the screen. And same thing on the host B or host two side. So this is a very simple kind of demonstration of memory pooling on the one device. And as Charles earlier mentioned, there could be memory sharing through the software, through the usage of, let's say, MemVerge software. That's possible. We're working with MemVerge on the next step to demo this memory sharing as well, as much as the higher performance of this CXL memory pooling and sharing to demonstrate the further usage case for this memory application. Because audio couldn't play out enough, I'm not going to play this video here, but you are welcome to stop by the MemVerge booth to see the real video.

Let me move on to the next slide. So this is what's happening today. Of course, everybody is looking at the future, right? In the future application of CXL 3.x, what kind of usage case is going to get enabled? So the diagram is simply from the CXL consortium. And the CXL consortium, they're talking about what's the usage case to support during the CXL 3.0 or 3.1 stage. So what you get from this picture is you see this switch get cascaded. And they even have this concept of a spine switch and also the leaf switch. It means all these bigger clusters of computing network is further connected through this switch to form a very big network, a fiber network, to do this computing. And also you see multiple of these devices like an accelerator, CPUs, memories, NIC card are all connected through the CXL switch. And we believe this is the future for computing, especially CXL provides cache coherency and a very high speed, a very efficient connectivity between all these heterogeneous computing units. So this is going to be our next generation product going to offer to support all these features as well. And we do see this has a lot of traction and a lot of support in terms of supporting the AI and machine learning that everybody is chasing these days. And we do see some new architecture or new emerging device that could form and to support this kind of application.

So let me go to a little bit of imagination of into the future, what we see going to be. So anybody familiar with this picture probably can recognize this is a little bit similar to what's right now, a lot of AI computing, the platform usage. There's a number of accelerators and the CPUs, memory device, storage device, NIC card, they connect it. However, it's a little bit different, right? So I put the CXL switch behind this accelerator and also between those heterogeneous computing devices. So what we understand or what we see is the CXL switch can provide a lot of capability to support this AI computing. So this just represents one single node of this computing. And on the top, we also put a link to the CXL spine switch. Basically through this kind of interconnect, you can connect a larger cluster of computing. So the switch will provide a memory expansion, pooling, and sharing for this functionality we already talked about. But also the switch behind these accelerators, they actually have large enough bandwidth to support what we call the all-to-all connection. So all-to-all connection kind of enables all these accelerators to fully exchange the data between themselves. And through multiple or through the CXL 3.x switch, the PCIe bandwidth is large enough to support all this. And you can have multiple switches to support this activity. And in the CXL switch to that connecting the heterogeneous computing units, we also see this switch can also support this all-reduce or all-gather kind of operation as well. Then people also talk about the CXL memory and they talk about the in-memory computing. I think this makes a lot of sense because all this all-reduce or all-gather computing, they're actually just doing all this in-memory compute. That would be a good fit. So our switch would be able to support all this. And the switch will also be able to support up to 4,996 nodes for the CXL 3.x devices. So that's kind of into the future. And this is the end of my presentation. I'm open to answer any questions if you have.
