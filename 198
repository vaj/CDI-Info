YouTube:https://www.youtube.com/watch?v=qJgWidihl-s
Text:
Fantastic, so yeah, so Anil, let me give a quick intro. I'm VP of business development for ZeroPoint.

And so the first question is, yeah, what is,  who is ZeroPoint? ZeroPoint is a IP licensing company based out of Sweden. And we do one thing and we do it really well,  which is memory compression across the hierarchy. So this is a portable solution,  it's complimentary solution to DRAM,  it's complimentary to CXL,  and it's complimentary to, in general,  what the ecosystem out here is aiming to do,  which is boost the capacity and performance  of AI systems and cloud computing in general. So our IP, and I'll show you some more details,  will boost the memory capacity by two to four X  without, of course, inserting additional memory banks. And then this technology is a thousand times faster  than software solutions, which are prevalent today. And as a result of this,  you can lower your total cost of ownership by 20 to 25%. And essentially, this technology is also used,  or can be used in consumer applications. And by the way, we have actually provided this  to smart device customers where the impact is different,  which is better battery life and user experience,  whereas we're talking about data centers out here,  so it's really the focus is on total cost  of ownership reduction.

So just a quick overview of the IP products  that are available out here from the Zero Point portfolio. So if you look at the memory hierarchy picture  that's drawn out here on the left,  so there are several IP products. We offer products for SRAM, on-chip cache compression,  and then a main memory expansion and compression. So those are the two products out here,  the SuperRAM and ZiptilionBW. But the one we'll focus on today  is this product called DenseMem,  which is geared towards CXL expansion.

So even before I jump into the solution,  let's first talk about what really is the challenge. So we've heard from now several presenters over this day  about the challenges facing the explosion in AI,  the cost of data, and we saw MemVerge  put up some of the parameters  for the effective cost of these systems. And we know memory is the largest spend in the data center. Now, out of that, and in addition to memory,  the hyperscalers today are spending  significant amount of resources  employing software-based compression. And both Meta and Google have published papers  detailing out all the gory details  of all the different compression, decompression algorithms  that run at scale in the data center. So that's one challenge. This can add up to a pretty penny  if you look at the infrastructure outlay at hyperscale. On the second side, recently, and when I say recent,  this is at the end of October,  the OCP consortium actually came out  with the hyperscale CXL tiered memory expander specification. And so this, we believe, is a critical thing  for the hyperscale adoption of CXL,  and that's why it's made its way into a spec.

So, of course, with challenge comes opportunity. And the opportunity out here is,  and as described in the hyperscale,  the OCP CXL tiered memory expander spec,  is the opportunity to add a compressed CXL memory tier  into our devices. So the opportunity really is to deploy a new tier. We already have two tiers with DRAM and then, of course, CXL. Now we can have an additional layer,  which is DRAM, ordinary CXL connected DRAM,  and then a compressed DRAM memory on the same CXL device. And if you go through the spec,  there's really three tenets  which are the focus of this spec. One is reduction in the TCO. The second is energy efficiency and sustainability. But then there's also a third angle,  which is to preserve software investments. So, of course, hyperscalers and enterprise customers  have invested significantly  deploying solutions, software solutions. So, of course, we don't want to have to throw away  all those solutions and start from scratch. So those are the basic tenets,  and there's more detail provided in the OCP spec. But the key takeaway is this spec  has the potential to remove the barriers  that all kinds of TCO barriers that maybe,  we've heard several people talk about in the past,  in driving this CXL adoption broadly  at hyperscale and enterprise customers.

So let's, doing a double click into the requirements,  if you read this specification  for the tiered memory expander specification. So, like I said, the tenets are pretty clear  and it needs to support legacy compression algorithm. So there's an explicit section on compression,  and that calls out the algorithms that need to be supported  and then the latency and bandwidth specification. So, again, we heard from Xconn  and we heard from several other presenters  talking about the heavy focus on latency. So it's no different for compression capabilities. If you're going to do a compressed CXL tier,  the latencies need to be extremely stringent  as specified by the hyperscalers,  and the bandwidth efficiency  needs to also be significantly high. Now, in the spec, calls out for LZ4 as a minimum,  but it leaves the door open for innovation algorithms. So if you look at some of the standard algorithms  that exist today, like LZ4 and Z-Standard,  so these are all geared towards the microsecond domain. And of course, as we all know, for CXL,  we need to be in the nanosecond domains. And if we can do even better,  if we can go down to single digit nanoseconds,  that's even better to help support and add new features,  add new innovation to drive adoption of CXL technology.

So the proposed solution, if you go through the spec again,  is if you think of a CXL type three device,  like a memory expander. So in addition to your normal uncompressed tier,  which is what you would have as on a standard CXL device,  the proposal is then to add a compressed tier. And the way to do this is to add a hardware accelerated  compression decompression capability  inside the memory controller itself. So if you look at, if you zoom into this CXL controller  device, what it would look like is essentially  a piece of IP block, which could compress and decompress  data as it's coming in at line speed,  and then plugging in nicely in between the CXL interface  and then on the other side to the,  before the data hits the DDR interface. So that's really the crux of the solution.

And so the question is, where does ZeroPoint  come into this picture? So the ZeroPoint solution, we actually have an IP block,  it's called DenseMem. This is OCP spec compliant,  it's hardware accelerated IP block,  which actually does three things. It does compression, decompression of course,  but then also manages compaction  and transparent memory management. So that makes it kind of smooth to integrate it  in the software stack and also to integrate it  into a diversity of memory controller, SOC devices. And while meeting the spec, ZeroPoint,  the DenseMem IP delivers two to four X compression,  which has been tested on a variety of data center workloads. And in addition to the minimum algorithm support,  which is called out in the spec, which is LZ4,  we also have a proprietary algorithm  which operates at cashline granularity. So that's one thing to take away from here is,  the algorithm is the compression, decompression algorithms  that exist today in the market or in open source. They all operate at a page block granularity,  which of course, at the speed of CXL,  when you're doing load stores,  and there were several discussions earlier  comparing load store access versus RDMA. So in this case, again, it's no different. If you want the highest performance, lowest latency,  you have to be able to operate at the cashline granularity. So you're not waiting for entire blocks to be pulled in  or to be delivered before you can do compression  or decompression, which is why a lot of these algorithms  are great for storage use cases,  but the latency is too high in the microsecond. We need to do this in nanoseconds. And that's what the ZeroPoint solution achieves.

There's a question for you. Is ZeroPoint involved in enabling the software ecosystem  to adopt CXL hardware-based compression? If so, please explain what is being done.

Absolutely, I think that's a great question. So one of the things we are working on is,  of course, making sure that the solution integrates nicely  into the existing Linux framework ecosystem,  the CXL drivers that are being developed. So yes, we do have a plan to develop a test driver,  if you will, which can act as a baseline  for other customers to adopt it. Or maybe in the future, we might even upstream this. But this is something to where we work with the end users,  because a lot of the hyperscalers may have custom solutions,  but different enterprise customers  have their own requirements. And actually, you'll see that's one of the call to actions  I have at the end is to call for collaboration  to accelerate the development of the software driver stacks  so that such capabilities and such innovation  can be supported for our end customers. So I'll come back to that.

But yeah, this is an OCP compliant IP block solution. And then I was talking about the total cost of ownership. So when you think about the dollars per gigabyte  and you look at the rack scale,  the TCO models that we can create,  and of course, certainly we saw that from a member  at some of the total solution. So this is where there's an opportunity  to reduce the total cost of energy pretty significantly,  of 20 to 25%. And this is what end customers are really looking for  at the end of the day,  to run these LLM models that my intelligence showed earlier,  which are fantastic. But then you have to be able to do it  within a compelling total cost of ownership. And of course, removing some of the excess cycles,  for example, CPU cycles,  that could be used somewhere else. For example, a hyperscaler could rent out those CPU cycles,  which are being used for compression, decompression today. And then monetize those kinds of get higher utility.

So we did have a demo actually at Supercompute,  and this was an FPGA based demo. I won't play it in the interest of time,  but there is a YouTube link. But let me put it out here. Yeah, so this is an FPGA based demo,  which actually shows the compression,  decompression capability working within  the framework of CXL, which was implemented in QEMU. So you can see the demo actually on the YouTube link,  which, and maybe I'll just stick to this. I won't run it live right now. But essentially what we were demonstrating was,  inline transparent compression and decompression. And this number out here,  this is showing the memory expansion factor. And we're running a variety of  different data center workloads. And then also splitting up and managing the memory  in two different tiers, uncompressed and a compressed tier. And then essentially also showing kind of a high hit ratio  on the DenseMem IP page cache. Once we've decompressed cache lines that can then be used,  and you don't need to necessarily go fetch them from DRAM. And of course the performance numbers here are low,  but this is an FPGA implementation with QEMU. But the same IP is targeted for production  and it will run at full CXL line speed. So, but at least this was the same RTL,  which was used to demonstrate it in operating in FPGA  in the CXL stack.

We have another question. Nilesh, where is the power savings coming from? Is it by offloading the compression from host CPU  to CXL device based compression?

Yes, that is one source of power savings,  but the other source of power savings is essentially  having this kind of compressed data essentially stored. So you can store more with less. That's the second angle. And then the third angle is when we already have  this decompressed data available in our cache,  that's providing additional benefits  by not having to go out to DRAM. So yeah, there's like the TCO model that we computed  kind of takes a holistic viewpoint  into how we can save power  and then also provide the performance  that's required from the expected.

Okay, let me move on. So, like I was saying, we have a characterized,  and by the way, in the slides, there's a link,  it's a YouTube video link,  which shows the demo actually running  and it shows the diagram of how we emulated the CXL  in QEMU with this implementation in FPGA. And so yeah, so across a variety of workloads,  what you're looking at out here, there are three colors. The first one is the, let's just start with the yellow bar. That's LZ4. So that's a four KB block. That's our implementation of LZ4. So that's our estimate where LZ4 will land  in terms of the compression ratio  for various data center benchmarks. The second one, if you look at the purple bar,  that's actually our cache line granularity algorithm. So that's the one that's operating at extremely low latencies  and like I said, we will have both these algorithms  as part of the IP block. So that can actually deliver  fairly competitive compression ratio. There is some trade off, as you can see  on some of the workloads  where the compression ratio is lower,  but it's doing this at extremely low latency,  which is critical for many applications. And this will be a kind of an option  that the customers will have to pick and choose  which algorithm to apply for different use cases.

So wrapping up then and with a summary  and a call to action. So the key takeaway is this DenseMem IP,  this is OCP compliant,  it's portable across the latest process nodes. It's also portable because it's an CHI interface,  so it should plug into most SoC plumbing. And then this will be production ready mid 24  and we've verified the performance  with several data center workloads. So really the call to action to our community out here  is one for controller manufacturers  is to collaboratively address the hyperscale OCP requirements  so we can accelerate the update  and deployment of CXL technology. And then the earlier question that was asked saying,  hey, how do we integrate,  how will all this work in the software stack? So of course, we have already been developing  the software solution and the driver integration,  et cetera, on the CXL side. But this is where we are also looking  for a community collaborators  and having deep insight into the end user workloads  and environments, for example,  MemVerge showed some of the end use applications.Yeah, so yeah, that's where we would like to collaborate  with this ecosystem to accelerate this uptake  of these innovations, which is what the end customers  are asking for it. So let's deliver these capabilities joint. So with that, I'll wrap up and hand it back to you, Frank.
