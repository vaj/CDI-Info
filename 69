
Greetings, everybody. Good morning. My name is Debendra Das Sharma. We'll talk about PCI Express 7.0, 128 gig.

So here's the agenda for today. We'll start with an introduction, talk about how PCI Express has evolved in a backwards compatible manner, then talk about both 6.0 and 7.0 in some amount of depth, then talk about PCI Express technology storage, form factors. We are working on optical-friendly PCI Express evolution going forward. So I'll give a little bit of an update on that, talk about compliance and conclusions and questions, of course.

So the table here shows the evolution of PCI Express more than two decades. You'll look into every generation we double the bandwidth. And you will notice that between two to three, it was not a doubling of the data rate. But with the encoding change, it's fundamentally a doubling of bandwidth per pin. And five to six was the other major transition. We went from NRZ signaling to PAM4 signaling. And we went from-- you'll notice that correspondingly, there is a change in the encoding from 128B, 130B to flip mode. We'll talk more about that. So effectively, of course, PCI Express moves with the industry demand. But on an average, doubling the data rate every three years in a fully backward compatible manner. It's the I/O interconnect across all segments of compute, whether it is PC, handheld, server, you name it, right? IoT, automotive, all of those things use embedded-- all of them use PCI Express. And the impressive thing is that it's one stack in the same silicon. But it works across all of these segments using different form factors. The other thing is that it's fully backward compatible. So what it means is that if you take a PCIe Gen 5 system today, let's say there's a x16 slot which is capable of running at 32 gig. And you have an old card which is a Gen 1 card, maybe a x1, runs at 2.5 gig. You put it in that slot, it will still work. And currently, from a 7.0 specification point of view, we are at ref 0.3. Targeted timeline for 7.0 is 2025. We give projections, but in general, been pretty good at meeting those. So in general, when I started PCI Express, right in the beginning, we used to think that three generations, one decade, it's success. And clearly, we are way past that, which is good. Because the industry doesn't have to go through a transition, right? These bus transitions are challenging. When things don't-- you're used to a particular style, and then suddenly your cards don't work, because you have to have a converter or something. So as long as we can evolve in a power-efficient, cost-effective manner, we will continue to evolve the technology.

So this shows the speeds and feeds, as I was saying earlier. So we support multiple widths. So in the columns, you've got the widths there, by one, by two, two lanes. x1 is single lane, x2 is two lanes, four lanes, eight lanes, 16 lanes. On the row, you've got the different data rate, and correspondingly, you are going to get the different bandwidth per direction.

So what's driving all of these bandwidths? Of course, it's the usage models, whether it is AI recently, automotive, cloud, enterprise, PCI, mobile, IoT, storage, all of these things. These are driving the demand for bandwidth. So on the device side, you have, for example, networking. That's moving from 800 to 1.6. And that requires you to keep moving the data rate, because it's a single connect that you need to sustain. Accelerators have an insatiable demand for bandwidth. So do FPGA, ASICs, all of that. And the other interesting thing that has happened is PCI Express, the name as it indicates, it's a peripheral component interconnect. And what I tell people is it's no longer just in the periphery, it is in the center. It's in the center of the action in a given system. Of course, the peripheral components do connect using PCI Express. But in addition to that, if you look into the usages, we use PCI Express 5 for CPU to CPU cache coherency mechanisms, proprietary ones. We use it for memory bandwidth. So that's where the alternate protocols come in, CXL. So with CXL, effectively, PCI has become a consumer of memory bandwidth to being a producer of memory bandwidth. And that's a very significant paradigm shift that's going on, because in order to get over the memory wall, we need a lot of memory bandwidth in the system. Now, if you're producing the bandwidth-- of course, you're consuming the bandwidth for good reason. All of these things are consuming memory bandwidth. Now it's producing memory bandwidth. So that means that the demand for the bandwidth will continue to grow at an even more rapid cadence. As the compute capability grows, of course, exponentially, which is happening, so does the I/O bandwidth. And especially if you're producing memory bandwidth, then that is going to become even more. And then platform is already at hundreds of lanes of PCI Express today, hundreds of lanes. Just PCIe, we have CPU sockets give out 128 lanes per socket. If you count all the cache-coherent links, it's many times more than that. So the question is, how does the evolution work for us? We are, as you are going to see, that we are still cost-constrained. It's not like four lanes of networking that I can say, OK, just give me the highest speed you can. I would, of course, love lower cost. But here, cost will continue to be the king, because it is across all the segments. Performance is the king. Power cannot make a trade-off there, especially with 100. So with a ubiquitous I/O, none of these will give up. And that's how we need to evolve the standard in a manner where it requires a lot of innovation. And it's possible for us to innovate. The good thing is that we've got volumes. So with volumes, you can bring the best innovation to be viable in an economically viable manner, so to speak.

So now let's look into 6.0 and 7.0.

And as I said, both are flip modes. So every few generations, we may change the encoding mechanism. We don't change because we like to, but because we are forced to in order to continue with this cadence of delivering the bandwidth. So what are the key metrics? It always helps to start with the key metrics. And this is something that we do before specification work starts. We give guidance to the workgroup, saying these are the criteria that must be met. Otherwise, people will innovate in different directions. So this gives you basically the guardrails. So of course, the data rate, we need to double the data rate. That goes without saying. Now, NRZ or PAM-4 at 64 gig, it's very well known that it's not going to be NRZ. So instead of debating PAM-8, PAM-16, just go with PAM-4, which is what is there, PAM-4 signaling. Latency, it's a critical criteria, especially if you are doing not just for PCI Express devices. These are load-store interconnects. So latency is, of course, important. Normally, if you are a PCI device, your access latencies are-- for memory access latencies, those are in the 200 to 300 nanoseconds kind of range. Much more stringent if you are accessing memory as a CXL device or using for cache coherency. There, every nanosecond counts. So again, because PCI Express is so centrally deployed, not just a lot of PCI Express devices which are latency critical, but we have got applications that are uber latency critical. So for that, what we said is less than 10 nanosecond adder. This was, again, going back in the time when we-- I think it was 2019, if I remember correctly, when we started working on Gen 6 for transmitter plus receiver. So one direction, no more than 10 nanoseconds. So single digit nanosecond impact, because we knew that PAM4 will mean forward error correction, which means higher latency, all of that stuff. And it's clear. We cannot afford 100 nanosecond of latency like the networking standards have. They're solving a different problem. There are 100 nanosecond doesn't matter. Here, 100 nanosecond means we might as well go home. There is no solution, another 100 nanosecond. So bandwidth inefficiency, again, less than 2% inefficiency. And you will see how we measure bandwidth inefficiency. This is the real data payload. Even header, we consider as overhead. Your FEC, your CRC, your encoding, all of those are overhead. So after taking real bandwidth efficiency into account, we need to be less than 2% than prior generation, because that's how we are going to deliver 2x the bandwidth. Reliability, again, because we've got hundreds of lanes, and this is a load store tightly coupled system, my reliability in this link, not counting software, just in this link, in hardware itself, needs to be very close to zero as measured in failure in time, which is how many failures you get in one billion hours. And just to give you an idea, silicon fit tends to be in the three-digit, four-digit number. So getting less than one makes sense, because you're not really counting. In other words, what we are saying is that if you look into any component, PCI Express's contribution towards making the reliability any worse should be in the noise, effectively. Channel reach, again, cost-effective platforms, and also a lot of platforms have keep-out zone, because people are going to put their memory DRAM next to the CPUs. So those DIMMs need to be nearby. So PCI Express slots are going to be a little farther off. So it's going to be the same channel reach as PCI 5.0. We just don't want people to go and redo their platforms. Low power, very critical, because if you've got hundreds of lanes and you're on a power-constrained world, which we all are, we need to make sure that we are doing no worse with respect to power management states than the prior generation. So similar entry/exit latency to the L1 state as before. But in addition to that, what we said starting with Gen 6 is-- I was talking earlier about the L1 I was talking earlier about the linear power consumption. We need to do something about the bandwidth consumption being proportionate to the power consumption, as much as possible. Clearly, these are higher speed links, so you won't be able to turn your circuits on and off on a dime. But at least we want to get some more operating data points. So we do really well in PCI Express with respect to the idle state. That's how we are in these handheld segments. Otherwise, your battery is going to be dead long back. PCs can deal with milliwatts, so these things need to deal with microwatts. So we do really well low power. We do really well in the highest power. So the question is, what about in between? So this is what we are talking about L0P. We added a new state. We'll talk about that. Plug and play, fully backward compatible. We talked about it. And then, of course, high volume manufacturing ready. These are going to scale to hundreds of thousands of systems every year, probably millions of systems. So hundreds of lanes in a platform, in each system. Cost effective, and we talked about scaling to hundreds of lanes in a platform. None of these are negotiable. You can't say, hey, I'll meet everything. But that reliability, that number needs to be 100 feet. Like, I don't have a solution. So none of these are really negotiable. So we need to have the right trade-offs to meet each and every one of these metrics.

So let's talk about PAM4 signaling, which is there for both 64 and also we are moving in that direction with 128 gig. Effectively, NRZ signaling means that you send zero or a one. And then there's a differential. So P goes one, N goes zero. So that's a one. Other way around, it becomes a zero. So that kind of stuff. And it's like a single eye, effectively, if you look into the picture on the bottom right. PAM4, what you do is that you say that I keep in a unit interval, which is within one clock period, if I want to loosely translate that as, OK, because the Nyquist is different. So don't be too particular. So unit interval is the right terminology. But in one clock, instead of sending a single bit, if I send two bits, that becomes PAM4 signaling. So effectively, you've got three eyes there, as you can see. So you've got 00 at the very bottom. 01 is the next level between those two eyes. 11, which is deliberate. It's not a typo. And then 10, because we are trying to make it to be gray coding. So that way, even if there is an error, you move over, affecting at most one bit. These are small tricks. That's well-known. So why PAM4? It helps with the channel loss. So now what happens is that I have the-- at 64 gig, I have the same Nyquist frequency as 32 gig. So it helps with my channel loss. Not completely, but it helps substantially. But then the challenge is that with the reduced voltage levels, with the eye height and the eye width that we have, we are more susceptible to errors. Because you now have three eyes instead of one eye. So you are more susceptible to errors. And that's why we did gray coding to minimize. And then we have techniques like pre-coding to minimize the number of errors in a burst. Burst error happens because typically people deploy, especially in these applications which are latency sensitive, they deploy things like DFE and CTLE circuits because those tend to give you lower latencies than digital signal processing, ADC kind of circuits. And the downside of that is that an error will become-- starts to get getting into a burst because you are using the prior bits to figure out what the current bit needs to be. So if the prior bit had an error, then it keeps on propagating. So you're going to get a burst that way.

So then that brings us to the question of, OK, what are the error characteristics and error assumptions? And by the way, some of these are new because we went with PAM-4 signaling. Our error threshold has just become a lot worse. So we said, OK, let's start with something called a first bit error rate. Not BER, but first bit error rate, which means that what's the probability that I'm going to start with a bit in error? And then after that, I have to deal with the burst. And I count that as one occurrence. Even though in that picture, you see in that wire, there are five bits in error, consecutive bits. Actually, those are UIs in error, OK? Five UIs in error. So spans over 10 bits. But even if I get that, I still count as one. So it's a little bit pessimistic from that point of view. But that's the reality, is that you're going to get one occurrence. It's going to keep on continuing. And then, of course, there is the lane-to-lane correlation, which means that there are some things that are some type of noise sources are common across all the lanes. Like you've got some voltage level fluctuation or something that's going to impact multiple lanes. So we need to be cognizant of these type of correlations. So some of the techniques we deployed in the spec is we constrain the DFA tap weights, things of that nature, a lot of circuit techniques that we put in the spec that is going to mitigate that. Not eliminate, but mitigate how long it is going to go. It still is pretty large. So then the BER is nothing but the actual bit error rate will be much higher than the FBER.

So then the question is, what approach do we take? So what you see here in the diagram is, again, I'm not worried about how many bits I'm correcting. I'm saying that I'm correcting a single incidence of this burst. So single FBER, correct. That's the one in the blue on the top. Two burst, correct. And this is on a 256 byte transfer. You can pick a 512 byte. You're going to get similar this thing. If you have three burst, as you would expect, of course, your probability of retry is supposed to go lower and lower as you're able to correct more. So that's obvious. Now, what we did was that we plotted the number, 10 to the power minus 4 for the FBER, minus 5, minus 6, minus 7. Now, an interesting thing that happens there is that what you see is if you move-- if you are at 10 to the power minus 4, single burst correction results in very unacceptably high retry, meaning even after correction, there is an error, which means that you will result in retry. So that link is practically going to be unusable. So you then have to go for the better error correcting mechanism. But then what we know is that error correction is an exponential problem, because any time you're correcting something, what's the possibility? How many permutations you are dealing with, right? So if I can correct one symbol, then the number of permutations is n choose 1. And then within that, you've got-- if your symbol is 2 to the power 8 bits times 2 to the power 8, that's the number of permutations. So it's n times 2 to the power 8, 60, whatever, 256n, n being the number of symbols, right? If it is 2, then it becomes n choose 2, and then 2 to the power 8 square. So then that becomes a square of that number, n square, right? 3 becomes a cube of that number. And not only is n getting cubed, but also that 2 to the power 8 is getting cubed, right? And that's standard permutation combination. And that means that your circuits are getting increasingly more complicated, which means that your latency is going to be really, really high. So long story short, we said we want to be on that blue curve, but we want to get to a point where it's going to be viable for us. And that point happens to be 10 to the power minus 6, because then my retry probability is going to be around 10 to the power minus 6, somewhere in there, OK? So in other words, in every million packets, if I have to retry one packet, that's fine. That's an acceptable loss in terms of performance. But if I keep my latency flat versus I have to add 100 nanoseconds, and as you will see, we try to keep even our replay at less than 100 nanoseconds, or close to 100 nanoseconds. So then it becomes a question that, hey, would you rather have one in a million failure where in that failure you are paying an extra 100 nanoseconds or so of latency penalty versus everybody pays 100 nanosecond penalty? It's a straightforward question, right? The key challenge is to make things work at 10 to the power minus 6, because what it means is that you are making a lot of demands on the channel. And that's a very reasonable trade-off that we went with. Because channels get better over time. Given the volume of PCI Express, people will always innovate on better materials, better packaging technology, better all of those connectors. Once I pick the wrong number with a 10 to the power minus 4, I am paying that latency penalty from now till eternity. So that's the rationale.


So once we have that, so then the question is, OK, I need to do some form of error correction. It's not just CRC and replay. I have to do error correction. So if you have to do error correction, what we know is that error correction works on a fixed number of bytes. I mean, you can make error correction work on variable bytes, but that becomes even more challenging. Then it becomes you have to go with some elaborate scheme, saying, hey, how big is this packet if it is a variable-sized packet? And PCI Express has had variable-sized packets, right? And we have done CRC over variable-sized packets. So this time we said, OK, it's time to make a clean break. We'll go with FLIT, which stands for Flow Control Unit. And it is a fixed number of bytes. So as you can see on the picture there, it's a by 8 example. You can do the by 16 in a similar way or a by 4 in a similar way. It will be less in terms of UI, wider or narrower, and it's going to go longer. It's 256 bytes. You will see that-- we'll come to the rest of the things later, but you will see that there are ECC bytes, ECC 0. There are two ECC-- three ECC 0 and three ECC 1, six bytes of ECC at the very bottom. So what we say is that FEC will be on that fixed number of bytes, 256 bytes. That's where the forward error correction will happen. Now the question is, once you do forward error correction in the FLIT, where do you do the detection? Well, I can do the detection still at a variable packet level, but if I'm correcting, doesn't it make sense for me to build my detection mechanism at the same place and make sure that my replay happens at the FLIT level, right? So that's the natural progression in which it went. So if you do correction at the-- it's better to do correction at the FLIT level at a fixed boundary, which means it's better to do detection at the fixed boundary, which means that retry happens at the FLIT boundary. And that's a departure from the prior generation. But again, this is limited in the physical layer. We know how to deal with those. And once you are doing this at 64 gig, then the question is, OK, what if you change the frequency dynamically to 32 gig at an NRZ or 8 gig or 2 and 1/2 gig, right? Because that can happen. Power demand or link became unreliable or whatever, right? Well, once you're in the FLIT mode, it's easy to always be in the FLIT mode, because then all that you are doing is you are just changing the frequency, as opposed to going through, figuring out, hey, did I flush everything? Did all the transactions go through? All of those things, right? And do a complicated handshake. Plus, you may not have that time to do all that complicated handshake. You might have to go back and undo things, which is hard to do in hardware. So it's much easier to say, once in FLIT mode, always in the FLIT mode. So we talked about the FLIT, 256 byte. So if you notice, there are numbers, 0, 1, 2, 3. It goes all the way to 235. So 236 bytes are for transactions. This is called transaction layer packet. And again, TLPs are variable size in PCI Express. That doesn't go away. That stays. So 236 byte is TLP. We do six bytes for data link packet. And the data link packet is basically things like credit, link level management stuff, like ACK, NACK, all of those things. And that means that no TLP or DLLP CRC, no sync header, none of that. And then you've got eight byte CRC. It's a fairly robust CRC. We went from four byte CRC per packet to eight byte CRC every 256 bytes. So what it means is eight bytes of CRC guarantees eight bytes of error. Any eight bytes in error after your ECC correction, it's guaranteed detect. That's the Reed-Solomon code. Beyond that, it's a probabilistic detect. But the probability of aliasing is 2 to the power minus 64, which is practically zero. And that's what you are going to see in the FIT number later on. Now, the other thing is that this is-- so we moved away from scheduling transactions, scheduling data link layer packets to giving people fixed slots. So this is like a train is going, and you know that train leaves every so often from the station. You don't have to rush. And you've got fixed bogies for different things. DLPs goes there. DLLP goes there. So I just removed all those arbitrations. I just removed all those sync header, framing token, all of those things. And that's what you are going to see is that it's going to contribute to a better link efficiency, even though it's upfront we are paying the cost here. And we also did-- the other thing it does is that it does reduce, actually, your queue sizes, because now you've got a guaranteed ACK NACK. You've got a guaranteed credit once you have guaranteed, as opposed to if I'm scheduling. And here's the thing. When I size my queues, I have to size assuming, OK, when is the other guy going to get a chance to send me some credits back? Or when is the other guy going to send me an ACK back for the things that I'm sent? Well, I need to be prepared for microseconds, because that's what the spec allows. And somebody might get greedy and say, hey, these are overhead. Let me try to reduce my overhead. So I make my queues a lot larger. But now with this fixed allocation budget, I don't have to worry. At the spec level, it's mandating that you give credit at this interval, which means that I can design to a much more aggressive round trip in nanoseconds, hundreds of nanoseconds kind of number, versus tens of microseconds kind of number. Flit latency, again, accumulation latency, this is at 64 gig, 2 nanoseconds for a x16. And you can do the math, 4 for a x8. If it is 128, 1 nanosecond for a x16. It's a straightforward number. And I think that's the thing. Other benefits that we got out of this Flit mode is we had an option, a chance to reorganize the bits within the TLP. So we have future-proofed it with new TLP arrangement and much more easier to parse, even at the transaction layer level. Results in significantly better efficiency. And then the other thing is that we have done is credits and all of that. We do this notion of pooling of credits, which should reduce the queue sizes. So a lot of very forward-looking things. In fact, if you look into a PCIe Gen 6 only design versus a PCIe Gen 5 design, if you just look at the storage requirement without the backward compatibility, the storage requirements in terms of the FIFOs and everything else on a Gen 6 is going to be lower than that of a Gen 5.

All right, so now how well do we do? And there are three things that I want to show. And here, I assume the retry time to be 200, even though I said that we are targeting 100. So just assume that people don't do it. In general, if you expect 100, I would design to 200. If you expect 200, I would design to 400, so that you're working reasonably well with people that have got variable latencies. Can't really mandate everybody goes to that yellow number. But so retry time 200 nanosecond, you will see that in the column. I have listed 10 to the FBER number, minus 4, minus 5 just for completeness sake. We are in the 10 to the power minus 6. That's a spec-mandated number. So with 10 to the power minus 6, the retry probability of a given flit is, like I said, about 1 in a million. So it's 5 times 10 to the power minus 6, which means that the retry probability, even assuming you didn't take advantage of the selective retry that we have in the spec, the retry probability over the retry time is 0.005. And that's a very reasonable bandwidth loss that we can encounter. So it's 0.05% of bandwidth loss because of retry, and that's fine. And then the fit, which is the failure in time, how many failures we get in terms of in a billion hours, that's somewhere around 4 times 10 to the power minus 10. So beyond 10 to the power minus 2 or minus 3, it's all in the noise at that point, because as I said, silicon fit tends to be in hundreds or thousands kind of numbers. So the contribution of the link is practically 0. Spec, if you do the spec things the way it has been constrained, then the burst length is going to be less than or equal to 16. You can, of course, it's a probabilistic number. Your burst can go higher than 16. But in that case, the probability of that is significantly lower than 10 to the power minus 6, which means that you are more likely to get another FBER. So that's the thing.

So how well does it do? So last slide was showing how well do we do from a retry probability point of view, from a fit point of view, all the reliability metrics, as well as those kind of stuff. Now the question is, how well do we do from a bandwidth efficiency, which is a performance point of view? So there are two aspects to performance. One is bandwidth efficiency. The other one is latency. So here we are going to look into the bandwidth efficiency. By the way, 128 gig is going to be a straight doubling from 6.0, because we are not planning to change anything. It's going to just soon completely into whatever we have in 6.0. Just the data rate goes up 2x. So the comparison table here, it's a bandwidth scaling versus the data payload size. And the data payload size, what we mean is how big of data are you requesting? So things that are popular are reads or writes. It's a load store interface. So it's going to be reads. It's going to be writes. Now you've got other things like messages, this and that. Those are not really performance critical. It's going to be reads, writes. So read and write can be-- the payload can be anywhere from-- we mentioned things in DWORD. A DWORD is four bytes. So it can be-- one DWORD is four bytes. So it can be anywhere from four bytes to 1,024 DWORDs, which is 4k bytes, which is about-- it's a page, right? Typical page is four kilobytes. Of course, you've got super pages these days. But traditionally, it's 4k. And the way we do the scaling is we say, hey-- so recall that what I said, right? The header, I count as overhead. We count as overhead. Anything CRC, this, all of those are overhead, OK? It's the real data that gets transmitted. So you will see that there are three things there-- 100% read, 100% write, 50/50 read/write. So for example, when you do a read, you send the read request. That's considered overhead, because data didn't really come back, right? It's only in the completion when you've got the data. That data is the non-overhead portion, right? That's the good stuff that you're sending. The rest of it is all overhead, right? Necessary, but overhead. So basically, small packet all the way to the large packet, right? So what you will notice is that normally, I said that 1.98 is what I would be happy with. What's happening there? We are getting more than 2. How is that possible, right? I just doubled my data rate, and how come I'm getting a bandwidth of more than double the bandwidth, right? And it has to do with the inefficiencies that were associated with the prior encoding, especially for small packet size. Effectively, think about it, right? If you are transferring only 4 bytes of data, well, you've got 4 bytes of CRC. Doesn't matter how big the data was. You had 12 bytes of completion header. You had another 1D word, I think. That's 4 bytes. 4 bytes of framing, right? So that framing 4 bytes, the CRC 4 bytes, right? All of those are now amortized in a 256 byte quantity. You're not sending those so naturally for those kind of payloads. Your efficiency just went through the roof. As the payload size increases, you will see that those efficiency gains start decreasing. And then once you're in a very large payload, your efficiency is less than 2, right? And again, remember I said that we don't want to be less than 1.98, because then we are not really doubling the thing. And you will see that 1.98 is roughly till about 512 bytes or somewhere 256 bytes. Beyond that, it goes to-- it's not quite 1.98. You're not quite 1.85, but you are somewhere in there, right? So the question is, OK, what's happening here? And that becomes inevitable, because it's like one of those things that you try to get to 1.98. In a lot of cases, you have beaten it by a significant amount. In some cases, you are not quite at 1.98. You might be somewhere around whatever 1 point-- I don't know exactly what that number is, but somewhere between-- definitely above 1.8 and looks closer to 2. But definitely it's not 1.98 in the very extreme points. So is that a problem? And the answer is no, not really. Why? Because practical systems today, the max payload size is at 256 bytes, now going to 512 bytes. So which means that-- and by the way, client systems are at 128 bytes max payload size, which means that they are at 32, that number, and 64. And within that and beyond that, we are definitely doing really, really well. And it's one of those-- and if you happen to be at that high number, it's fine. It's not that big of a loss of bandwidth efficiency, the 1 point from a 9.8 point of view. In most cases, we are doing better than 2, which is the good news. What it means is that small packet payload becomes very efficient with the flit mode.

What happens to the latency? So again, the latency is, as I said, we want to be less than 10 nanoseconds. So you have two bookends here, one x1 and x16. And then you've got different data payload size. And what the latency is, we take into account all the accumulation, latency, CRC, everything. So smaller payload, that's why you will see that there is a larger number, because even though it's 4 bytes, we have to wait till the 256 byte boundary. And I have assumed that always my data payload is in the very first one. So it's a very pessimistic way of looking at it. By the way, we expect-- so these are the numbers, just from an accumulation, latency, and all of that point of view. So you'll see that there is one case where you are above 10. Otherwise, it's less. And then you will see that there are negative numbers. What does negative number mean? What it means is that I'm doing better than I was doing in the prior generation. And why is that? Because if I'm transferring data at a faster rate, the data arrives faster. So that's basically what you see is being reflected there. That's not the whole picture. We expect to see, realistically, a lot of latency savings, because the barrel shifter kind of logic that we had with 128, 130, those are gone. Framing logic is gone. So with those, we expect to see a good 5 to 10 nanoseconds of latency savings on top of this. So latency, we are expecting to really come out ahead.

So in general, we went through all of these. But in general, we meet or exceed the expectations that have been set up for us. Now, let me switch gear and talk about unordered I/O.

So far, PCI Express is and will continue to be an ordered protocol. Anytime you are doing load store, it's ordered by its nature. So it's producer-consumer ordering model. But the way we enforce producer-consumer is through a tree topology. And every entity along the way in hardware and forward is that ordering. And that's how the whole producer-consumer thing works. Now, delivering a single stream performance has become challenging, especially once you are doubling your data rate. It's not only the challenges at the five-layer level. Not only is the challenge at all the materials and all those will continue. At the end of the day, it's the hardware. All the materials and all those will continue. I mean, that's not going away. But there is only so much that your internal-- like if you have a NOC kind of design, whatever is your internal interconnect in the chip, every part, it can only deal with so much of bandwidth. Because you're not going to really increase. Just because your PCI is going up 128, your internal frequency is not going to go from 2 gig to 4 gig to-- or you're not going to increase the data path with tremendously. So what we have done is we have come up with this notion of saying, OK, we'll maintain the producer-consumer ordering model. But we are going to move to something called an unordered I/O. In other words, the enforcement of the ordering moves to the source. The source knows what it is doing. When I'm generating transactions, I know which rights are there and which flag needs to sense those rights effectively. So I just make sure that those rights have reached the destination. And then I can send my flag. So that way, it's still producer-consumer. I just don't need everybody in the path to enforce that ordering. So that's unordered I/O. But again, we are backward compatible. So the existing model will be there. And the way we do that is that we have eight virtual channels. So we can always designate some non-virtual channel 0 to be this unordered one. And that gives the path to effectively deliver that higher performance without any of the chips' internal processing limits. Plus, it also opens the path for us to be in a non-tree topology. Because again, the reason we were in a tree topology is because of everybody along the path enforcing ordering. Now, if the enforcing moves to the source point, then transactions can go in any order anywhere and using any path. And I'm just going to make sure that the ordering enforcement happens at the source.

This has got some more details. I think I talked about it. But effectively, as you can see, it's truly unordered. This fabric completely goes unordered.

I think these are on the storage side. So I think the key here is, of course, it's well-known that storage has moved to PCI Express. And why not? Because we're delivering a huge amount of bandwidth. There is a huge number of lanes in any given platform. We talked about that. And very low latency. So what's there not to really like about it?

And then this is just showing some of the same thing in terms of looking at where PCIe is versus others.

And same picture here. You can look at them.

So from a storage point of view, RAS is very important. Of course, all of our-- as we said, we have a very high level of RAS. All of our transactions are protected at the hardware level. We talked about the failure in time, which comes from the CRC capabilities we have. We have things like error injection mechanism. We have very elaborate error logging, error reporting, error containment, all of those things. Advanced error reporting. And then, of course, we have the hot plug support within PCI Express. So everything that-- and not only that, I mean, we always are looking for if there is a need. We are very quick in terms of bringing in an ECN and incorporating it for the future ones.

And this is an example of DPC, EDPC that we did for the storage segment, especially as we moved from this HBA-style storage to a native PCI Express-style storage. What happens when one of those things goes away? You don't want to bring down the entire hierarchy. So that's what the downstream port containment mechanism that we introduced. I believe it has been more than a decade maybe now. But we did introduce that after some time during 3.0.

The other thing is, of course, I/O virtualization. And this is undergoing more innovations recently. So you have this notion of virtual functions, physical functions that you can assign directly to VMs. And with IOV, we are trying to even take it one step further. We have support for things like address translation services. You can store the IOTLB entries in the local devices and give translated address requests. So all of those things we support. And virtualization is first and foremost, right, in terms of we started with I/O virtualization, I believe, back in 2006, 2007 kind of time frame. So fairly well deployed. And as I said, we are looking into some enhancements going forward also.

Security. I say that the job of security is never done. But we have done-- we work very closely with DMTF and a lot of link encryption mechanism, end-to-end link encryption. All of those things are comprehended in the PCIe specifications. So IDE, all of those things are there.

And this is just a pictorial model. You can look at it, which parts are from DMTF. That's in the green, and which parts are from PCI Express in the blue over there.

We support a wide range of form factors, and again, which is expected because we go into a lot of places. So not everybody's form factor here is very different than a form factor here in the server. So these are the list of them. So BGA, M.2, U.2. Probably U.2 is less popular now. CEM continues to be pretty popular. SD Express, and then a bunch of E1 and E3, EDSFF form factors, which run on PCI Express as the base interconnect.

We're also working on cable topology, both internal cable, like for example, in the usage model here.

External cable also, like disaggregating of resources with CXL.
Basically, we want to extend from board level to the rack level and potentially beyond.

And in that comes the question of optical. So PCI Express, by the way, it's an electrical specification. It will continue to be electrical, at least for 7.0, potentially beyond that. But can we evolve in an optical-friendly manner? And we just recently kicked off an optical work group. And the intent here is that we want to make minimum changes on the root port and switch side, because we want to leverage the existing ecosystem and all that good stuff. Everybody needs a EIC/PIC kind of thing, so there will be some requirements that we are going to put on the EIC side of things. Electrical or electronic IC. And then we also recognize that there are a multitude of optical technologies that are in the market. Let the best technology win or best technologies win, whichever way it is. We are going to evolve PCI Express in an optical technology neutral manner. We're going to take the sideband information and embed that within the PCI Express link as main band, so that you don't have to dedicate a separate optical fiber to send sideband information. Those will be embedded, we are thinking, in the skip order set itself. There is enough redundancy there, and those get sent periodically. And that's basically the way in which we expect that this optical evolution will happen. And this is, again, something that we are investing to make sure that we are going to enable optical usages. As I said, our aspirations are from periphery. We went to the center of the platform, to now we want to go to the rack outside of the platform. As long as we do a good job at it, of course.

And these are showing some examples. You can do that optical through a riser card. You can package them on the same package and call it co-packaged optics. Or you can just put your photonic one outside. So different possibilities exist.

We have a well-defined compliance mechanism in PCI Express. We've been running the compliance program for decades now. That basically ensures our interoperability story. So everything gets tested from the physical layer level, link layer level, and transaction layer level. We do a pretty thorough job of doing the testing and the compliance, this thing.

So in summary, we've got this single standard that covers the entire spectrum. We are not stationary. Neither is the world stationary. We are always evolving. We are always innovating. 950 plus member companies. I'm waiting for that day will be 1,000. It will be a big celebration, I hope. 1,000 member companies. But predominant I/O, which means that there is a lot together. We have all been innovating on this. We have all been working on this, too. And we love the technology. So we want to make sure that this continues. And we will continue to innovate. It has been a great journey.

So with that, any quick questions? Yes?

Since you're staying with PAM-4, the frequency goes up to 2, right? So do you reduce the cable length that you get back?

So the question was, since we stay with PAM-4 from 64 gig to 128 gig, the Nyquist frequency has gone up 2x, right? So 64 Nyquist for 16 gig, which means 128 gigs Nyquist is going to be 32 gig. It's going to be a challenging thing. Yes, cables will help us. But we also want to still do it on the motherboard. And the way we get around this is, as always, everybody's budget starts shrinking. So what it means is that your packages need to do better. And this is like a generational story that we have had, right? Your materials need to get better. And again, because of the volumes, things that were considered to be exotic, very high-cost material, because of the volume, they became more and more mainstream. And they will continue to be more and more mainstream. And that's where the economies of scale help us. But yes, a lot of innovations on the connector side and all of those things. So there is a lot of work to be done. But we are confident we will make it happen like we have made it happen for the last two decades. It's never a straightforward journey. It always looks like something is looking really-- this time, maybe we are not going to get lucky. But always we come out ahead. So thank you. All right, so thank you all very much. And I'll be outside if you want to ask me more questions.
