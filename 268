
Good morning and good afternoon. Thank you for attending the CXL Link Level Integrity and Data  Encryption webinar. Today's webinar will be led by Raghu Makaram, Principal  Engineer at Intel Corporation, and David Harriman,  Senior Principal Engineer at Intel Corporation. Now I will hand it off to Raghu to begin the webinar.
 
Thank you. Good morning or afternoon. So CXL is a Coherent Low Latency High Bandwidth  Interconnect. This is used to connect CPUs, TPUs,  high-throw ranges of memory. In these usage models, high-value data  moves across the link. So it's essential to secure this data as it moves across the link. CXL Integrity and Data Encryption, the CXL IDE,  provides this protection.

So today we'll explore CXL IDE in detail. We'll start with a brief description  of what the threat model is that we are looking at  and how IDE addresses these threats. We'll follow this with how to know who is on the other side,  the device at the station, and key exchange  so that the keys to the main network IDE are established. And then we'll dig deeper into how CXL IDE is implemented,  both at the CXL.io protocol and for the CXL.cachemem Protocols. And we'll conclude by looking at the other handling  implications. Let's dig right in.

OK, from a threat model perspective,  the data we want to protect is the protocol transactions  that are going to cross the link. So if you look at the diagram on the right,  it shows an example system with a host that's  connected to a CXL endpoint. There's a switch in the path, and it  could be multiple endpoints connected below the switch. The primary asset that you want to protect  is the data for both the protocol transactions  and the data in header that goes across the link. And of course, any typographic keys  that are involved in setting up and using the IDE  would also require protection. In terms of adversaries, we focus  on the physical adversaries. These are skilled or simple hardware adversaries  who can snoop the data going on the link  or modify it to inject, collect data, and replay it  at a later point in time. There's also other things, like swapping out a device  or removing a device. While the threat model, the IDE is  designed based on the physical adversaries  as a benchmark for designing the IDE,  IDE itself is a foundational building block  for other types of threats. This could be trusted software or network adversaries. In these cases, IDE becomes a valuable building block,  a tool that can be used to secure systems. For example, if you're trying to build confidential computing  systems. Now, a TCB or Trusted Computing Base  sort of defines the set of things  that you must trust in order for the IDE to work properly. Clearly, the implementation of IDE in the root code  and at the endpoints would be within the TCB. Now, the agents that configure these endpoints of IDE  would also be part of the trust boundary  because they are the ones that are configured in this case. Now, these switches bring up an interesting case. As shown in the diagram on the right,  you see that the CXL.cachemem for IDE  sort of terminates at the switch. More than a new session is set up between the switch  and the endpoint. We will talk more about what this means  and how it's suggested to use later. But from a CXL.cachemem for IDE perspective,  the switches would be TCB. When physical adversaries are in scope,  developer services is something that could easily be done. Just unplug the device and you have a developer service. So defense against such developer service attacks  is not in the scope of IDE.

Let's look a little bit about what the protocol stack is. It's used to kind of know who is on the other side,  attest to their goodness, and perform key exchange. The device-- all of these messages  for enumerating the device or attesting to the device  flow over CXL.io protocol. And the CXL PCIe IDE ECN, that's used on the CXL.io side,  and CXL-IDE established by the ECN  describes sort of the mechanism that are used. These sort of build on the DMTF SPDM, or Security Protocol  and Data Model, specification. The SPDM defines this method for exchanging messages securely  over the SPDM section. PCIe established a mechanism called secure object exchange,  which really is a mailbox registers on the endpoint. And these can be used in a secure way  to send secure SPDM messages. And the CXL IDE, both on the .io side and .cachemem side,  would leverage these DOE mailbox. There are other mechanisms, like an alternate path  is available to SPDM, to use MCTP, binding,  or use SMbus. These alternate mechanisms are available for--  these are defined by DMTF for specific usage. So they just can leverage these alternate mechanisms as well.

The device attestation, like I said before,  start by building a top of PCIe, and SPDM specifications. And this starts with what I call a hello message,  really getting to know who's on the other side,  sort of understanding their capabilities,  negotiating algorithms. That would be later used to do other things. So at this point, you sort of know  what the other person is capable of. And then you kind of get who's on the other side  by a certificate call, which returns a certificate, which  has really, in the devices of the endpoints,  unique identity. This is sort of who the responder claims to be. Like, this is my certificate. This is who I am. To confirm the identity of the responder,  like really, are you surely who you are,  there's a challenge response protocol  that's built where the requester sends a challenge,  and the responder finds the challenge  to prove that they are indeed the owner of the private key  that's associated with the certificate. And then there's a challenge response. Get on both sides now. Who's on the other side? And now, if there's additional information to be had,  things that could be more dynamic,  things like configuration information or firmware  revisions that are used, those could  be obtained by using the measurements, the check  measurements protocol.

So with that basis of knowing who's on the other side,  the IDE establishment, key establishment  builds on top of that. The first step in that is to, in key exchange,  to set up a secure session, the secure SPDM session  between the request and the responder. Once the establishment is set up,  the secure session is set up, it can then  be used to transport additional information  between the two sides. And this could just be things that just need authentication,  but things that need both encryption and authentication. For the IDE key establishment, of course,  the IDE key stems are secret. So these would be encrypted and integrity protected  and sent across the secure session. The secure session itself has a notion of heartbeat  to keep it alive. And then periodically, the secure session can be updated. Of course, the secure session, once it's kept alive,  could also be used to update the IDE keys,  or refresh the IDE keys periodically.

Let's now look at--  talk about how keys are established,  but what are these keys used for? What are the properties of IDEs? CXL.io IDE follows the PCIe IDE. CXL.cachemem IDE is defined in CXL spec. There's a lot of commonalities between these two. Both of them use AES-GCM 256-bit keys for data encryption  and integrity. So this is how the protocol data is protected. And there's a-- part of the AES-GCM  is this integrity value that gets transmitted. And this is a 96-bit MAC that's transferred or transmitted  for integrity protection. The-- both-- and these are the keys, right? The AES-GCM, these 256 keys are the ones  that were exchanged previously. We talked about the key exchange protocol. Now, both CXL.io and CXL.cachemem really  protect the protocol sets, right? And anything that is more control messages or control  information, things like DLLP, sequence numbers,  or link CRC in the case of CXL.io,  or control flits, or link CRC in the case of CXL.cachemem  protocol are not protected by IDE. These are flow independent of the IDE. So these are some common things between the two. There's another commonality, right? Both CXL.io IDE and CXL.cachemem IDE  have the ability to provide additional robustness. They can have a crypto engine that  will fight things like hardware, software, and crypto engines  are protected by an encrypted link to CRC. We'll talk more about what this means  and how this is done later in the presentation. Now, let's talk a little bit about differences between the  two. CXL.io IDE has these two variants,  link IDE and a selected streams IDE. Link IDE protects all the protocol traffic  that goes across the link. Whereas a selected streams IDE is more selective. It can protect a subset of the data  or a subset of the protocol traffic  that's going on the link. And it could just be an assignable device interface  like a pass or maybe it's another device function that's  targeting a given endpoint. And only that component is protected by IDE. The selected streams IDE allows an ability for the switches  now to be configured in what's called flow-through mode. In this case, the switches don't need to implement the full IDE  protection. Of course, they need to do certain things  like making sure all the ordering rules are maintained  within their stream, but they don't  need to implement the cryptography in mobile IDE. And this sort of selected stream could then  be between the report and the endpoint,  kind of passing through the switch. So now, you look at CXL.cachemem IDE. CXL.cachemem has only link IDE as well  as notion of selected streams. And in the case of CacheMem IDE, the unit of protection  is a protocol flit. So the entire flit is protected by IDE. There's a little bit of flit header  that's only integrity protected but not encrypted. And the rest of the flit header and all of the data  in the all data flits is encrypted and integrity  protected. So in case of CXL.cachemem IDE, only the switch  must support link IDE because that's the only thing  to support in CXL.cachemem. And this is a very deliberate choice. CXL flits, you think about, pack a lot of information  because opcodes and data and these could be--  multiple of these are packed in one flit,  and each one could be targeting a different endpoint. And from a switch that implements CXL.cachemem protocol,  it really needs to look deep into the flit,  unpack the flit, unpack and understand  the opcodes and data involved, and then repack it  before solving that. So that's-- an IDE flit can just be transparently switched. So the CXL.cachemem IDE, if there's a switch in the path,  has to implement only tagging. Now let's go further and understand more details  about and explore CXL.cachemem IDE.

CXL.cachemem IDE requires a couple of minor protocol enhancements  to enable the user ID. There are two parts. One is a new header slot. This is a slot to carry the MAC information. And header slots normally carry any six bits of information. So this H6 header slot, all of the information in the slot  is MACs. There's 96 bits of MAC that's transported. The IDE also introduces a new control flit. And this IDE control flit has three sub-messages. And let's walk through this. The IDE TMAC-- truncated MAC--  is a way to terminate a MAC uppercursor. So what is MAC uppercursor? What does it mean to truncate it? And we will talk about those in the upcoming slides. IDE start is a message that's used to trigger the use of keys. So the idea we talked about earlier,  how the key exchange protocol and the two sides  agree on what the keys do. Once this configuration is set up to keys on both sides,  it can trigger the transmitter to say,  OK, it's time to start using the new keys. And somehow, the receiver needs to know  when the switch is happening. So the transmitter sends the IDE start message. And after this message has been sent,  all subsequent protocol flits will  be protected with the new key on the transmit side. And the receiver, when the receiver receives the IDE start  message, it knows that, OK, anything  that's going to come after this is going  to be protected with the new key. This allows the receiver to know precisely and keep  in sync with the transmitter. IDE idle is a message that's sent really like a no-off. And it's needed in some cases really  to ensure the sync that started up with the IDE start  remains and the Rx and Tx remain in sync in various cases. I'll give an example of how IDE idle gets  used when we talk about the truncated MAC later.

Let's look a little bit into the cryptography that's involved. So the underlying crypto is AES-GCM. And when you look at AES-GCM, in this definition,  there's two components or elements of this. There's the cryptographic keys that  are used to detect the messages. And there's an initialization vector. Now, the keys, if you look at the diagram on the right,  there's a key that's used by the transmitter,  let's say the Tx upstream, are these green keys. The transmitter will encrypt the data and integrity  protected and sends it across the link. The receiver must now have the same key  to be able to decrypt the integrity of the transmission. So Tx upstream and the Rx downstream,  right in this direction, both of them  should have the same green keys. Now, in the other direction, from Tx downstream  to the receiver on the other side,  there'll be other keys, very different set of keys,  these yellow keys. And Tx downstream and the Rx upstream, in that direction,  they join together on a path as a key. And these keys, the keys are established  using the SPDM session we talked about earlier. And the key refresh protocol using the ID start  allows the keys to be refreshed without a loss of data. So you'll be using the old key and then  you send the ID start message. At that point, both the receiver and transmitter  can switch to be using the new key. So that's one aspect of the-- one element of the AES-GCM. The second element of the AES-GCM  is the initialization vector. You have a trusting construction. This initialization vector is 96 bits wide. And as shown in the diagram here,  there is a specification-defined format  for the initialization vector. And the lower 64 bits of this IV is the monotonic counter. And the idea is that the transmitter, right,  so the initialization vector needs  to get updated every time the MAC of the integrity  value is sent across. So this monotonic counter gets updated by the transmitter  every time it transmits a MAC. And the receiver, when it receives the MAC,  would update the monotonic counter. So this allows the receiver and the transmitter  to stay in sync. The initial value of this initialization vector IV  is set up during the key exchange protocol. So you start up at the same value. And then every time things get transmitted,  the counter gets updated. So the two sides stay in sync. This means this initialization vector  doesn't need to be transmitted across the link. Just by the ordering of the protocol  splits across the link, you guarantee that the two  sides can stay in sync. And that saves some bandwidth, because you're not sending  additional 96 bits over the link.

OK, let's look a little bit more into how the underlying  cryptography operates here and how  the components of the thread get mapped into the GCM elements. So in case of CXL.cachemem, the integrity value of the MAC  is accumulated across multiple threads. And the set of flits over which the MAC is accumulated  is called a MAC Epoch. And this MAC Epoch can define a number of flits. This is the aggregation flit count,  which defines how many flits are in a MAC Epoch. There are two modes of operation. And these modes determine what the aggregation flit count  would be. So in the case of--  the two modes of operation is the containment mode  and the skid mode. In the case of containment mode, the idea  is that the receiver must receive all the flits,  must receive the MAC, check the MAC before any of the data  in the--  or the opcodes in the flit are released for further processing. And what that means is now the receiver  is going to buffer these, waiting for the MAC  to be refined. So in the case of containment mode,  the aggregation flit count is set to 5. It sort of gives us a good balance  between how much bandwidth overhead is needed  for transmitting the MAC and what the latency and overhead  and buffering requirements are on the receiver side  that's going to wait for the MAC to show up. And since the flits are never released till the MAC verifies,  this mode kind of gives you the best security. The second mode, called skid mode,  here the aggregation flit count is much larger. It's 128 flits, which greatly reduces the bandwidth overhead  because the MAC is only sent now every 128 flits as opposed  to every 5 flits. In skid mode, the receiver is allowed--  they don't need to buffer up everything. The receiver is allowed to release this data  for further processing without waiting for the MAC. This means that the skid mode has much lower latency. But since this data is released and consumed  by the MAC check, there's this risk  where there's an interest, there's  those inflicted on modification that's later detected by MAC  check. It may be too late, and the data is already being consumed. So this requires a careful assessment. And the solution is tackleable, jumpstandard tradeoffs,  and the value, the lower latency and lower bandwidth  overhead to bring. The diagram in the bottom here shows  how the elements of flit get mapped into the components  that NIST defines for the AES-GCM. So there's two parts of the input to the AES-GCM. That is the additional authentication data. This is the information that needs integrity protection  but is not encrypted. And then plain text, which is both encrypted and integrity  protected. So look at this example. See the very first flit on the left is a header flit. So the flit header with the T2 base  is mapped on to the AD, which is only integrity protected. And the rest of the flit content is mapped on to plain text  because it needs both encryption and integrity protection. Flit 3, you can see, is exactly the same. It has a flit header, and it has flit contents. Flit 4 is more interesting. It's an all data flit. So the entire flit, there's no header. So all the 64 bytes of the flit is  going to be encrypted and integrity protected. So it gets mapped out of the plain text portion of AES-GCM. Now, flit 2 is interesting. It has a header, and this header carries--  the header slot carries a MAC. This MAC is really for the previous MAC event. It's really for the flits that are not shown here  right there on the left of the diagram. And logically, this MAC belongs to those flits,  but it's transmitted later. And since this is part of the previous effect,  it is neither encrypted nor integrity  protected in this effect. The MAC for the flits in this effect, the ones that  are shown in this screen, would actually  come later to the right at some point. This diagram also shows how the AES robustness feature works,  and I'll talk about that more later.

When you look at sending back-to-back traffic,  this is the case where there's continuous stream of flits,  and it's advantageous to use the MAC slot to send the MAC. Post-requirement, of course, is you only have all the flits  in the MAC at birth, and then the MAC for the sub-effect  will follow after that. And the order in which the MACs are received  should be sort of the same as the order in which the epochs  are sent. So you send the MAC for the epoch 1 first,  and then you send the MAC for the epoch 2. Generally, the requirement is, as soon  as you send the flits for the MAC at birth, the MAC for that  epoch should follow immediately. But it may take a little time for the transmitter  to compute the MAC. In this example, the flits in MAC at birth zip 1  are encrypted, they're processed,  and then it takes one flit time for the MAC to be ready. But you don't want to introduce any bubbles in the transmit  pipe, so the flit for MAC epoch 2, flit 0 of MAC epoch 2,  is probably sent on a link while the MAC for the previous epoch  is getting ready. And then the MAC is ready in parallel with flit 0,  and then it gets transmitted as part of the header in flit 1. So this is the example that's shown in the figure A. Now, if you look at the case shown in figure B,  what it's showing is, OK, the MAC got ready exactly the same  as before. So flit 0 of epoch 2 was transmitted in parallel  with the MAC, was getting ready, and it was ready at that point. But in flit 0 of MAC epoch 2, there  was a multi-data header. So when there's an empty edge there,  it takes-- the subsequent flit would all become all data  flits. So it could be up to four all data flits, which  means this MAC, even though it's ready,  the yellow MAC come out to auto-link  till flit 5 of the epoch 2. And at that point, it gets sent as part of the header slot. So this means that the MAC could get delayed beyond one flit. So in order to minimize how long the receiver has to wait,  the spec defines a limit. So the latest a transmitter could send a MAC  from the network would be the sixth flit after the epoch X.

Let's talk about TMAC flit that I had alluded to earlier. This comes up in cases where the traffic is not continuous. For example, let's say the link is going idle,  and it's a containment case. So one flit was sent from a link,  and that link is going to be quite--  the receiver has not made--  the receiver has received this one flit,  but they don't have the MAC. So they don't want to send it. The receiver has received this one flit,  but they don't have the MAC. So they don't want to wait. The receiver could wait indefinitely. So the spec defines a mechanism called  a way to terminate the MAC appropriately and say, OK,  I sent you one flit, but I'm not going  to wait for five flits or 128 flits. I'm going to send a MAC right now. And this will allow the transmission of the MAC  where even though fewer than aggregation flits come,  but the flits have been sent. And this is done by sending what's  called the IDE control flit, the TMAC flit. Once the transmitter has decided it's  going to terminate the MAC appropriately,  it has to first send the TMAC flit. And before it can send any new protocol flits that  would really block the next MAC, because it  can't get into a lead between the one flits and the MAC  flit. In a typical implementation, when  you're doing crypto at this high speed,  it's typically pipelined. But you're trying to do a full line rate,  let's say of 64 gigabytes per second with a by 16 bit. So in these cases, the crypto engines are typically pipelined. And it's possible that the SDCM to precompute the crypto  output that would minimize the encryption, decryption  frequency. But when a MAC is terminated early,  the receiver will need some time to clear out  any precomputed tasks so it can get ready for the flits  that go off to the next MAC. So this amount how long it takes with a feeble,  let's say, configurable, you know,  discoverable parameter from the receiver. And it gets configured into the transmitter as truncation delay. And the transmitter really must wait for this amount of flits. And this wait is generally filled with an IDE idle flit  so that the receiver can get ready to receive protocol  flits to the next MAC epoch.

Let's look at how encrypted plaintext CRC that I alluded  to earlier gets used. So this is a mechanism to provide  robustness of crypto engines, right,  to improve protection against hard or soft  and other intermultiple crypto engines. This is integrated into the MAC check mechanism. And this is a mechanism that's used  by the MAC checker to check for any potential  and if you look at the diagram on the left,  it's how the transmitter would,  sort of what the transmitter would do. So it has the plaintext content,  generates a CRC on it, this gets the CRC. And the CRC gets appended with the plaintext. So I have the plaintext and it's corresponding CRC,  the whole thing gets encrypted. And the ciphertext along with the encrypted plaintext CRC  is used to compute the MAC. So all of this is now ready to go out. In the case of CXL.io,  you would actually send the ciphertext  and send the encrypted plaintext CRC. So that is CRCs. In the case of CXL.cachemem IDE,  there are really no bits to send  the encrypted plaintext CRC. So from a protocol perspective,  these bits come out to send. And it also turns out that if you want to send  the encrypted plaintext CRC,  you actually save on the MAC. Because you're not sending it to  any additional bits of the link. Now this means because the encrypted plaintext CRC  is not sent across the link,  the receiver now needs to do some additional work. The receiver receives the ciphertext,  it has to decrypt, generate the plaintext,  compute the CRC on it, and then re-encrypt that CRC  so it gets fed into the MAC calculation  and gets verified. It turns out in AES-GCM,  it's really an XOR at the AES-GCM control output. That's how encrypt decrypt works in AES-GCM. So this means the receiver side  just sort of comes down to doing two additional XORs  in the path to MAC computation. So it's not really onerous,  why it's needing to transmit these bits of the link.

Okay, let's move forward,  talk a little bit about how the link is handled. So the baseline set of, you know, link handling. The link CRC protects against transient  that has happened on the link,  you know, things that could be due to electrical  or other reasons. And there's a retry mechanism built on top of this CRC  that would retry the flex case  if the CRC was detected. IDE has no impact to either the link CRC  because the link CRC is not protected by IDE in any way  and not on the link retry mechanism. So from an IDE perspective,  these things happen independently as they do,  you know, with or without IDE. And IDE only sees flits that have been,  that have passed those CRC checks. So that are not only good, that came on the link. IDE itself of course, introduces its own set of error. Right, so anytime you have a, say an integrity error,  because it's due to a thread that flipped a bit  from the link, there will be an error  that's discovered by the receiver. You know, as we talked about earlier,  IDE accumulates the back across multiple flits. So when the error is discovered,  it's not clear,  which cannot be domestically,  no way it would happen. You know, which flit was there,  whether it was data or header. So all of the accumulated flits would be dropped  and an error would be locked. So once this error is locked,  all subsequent IDEs would be trapped  and also be trapped till the link is released. In case of, you know, device that has a lot of state,  there's a couple of ways to do it. You may have some cryptographic state,  that's pre-computed,  or there may be plain text data sitting around. And there are a couple of strategies  where this could either just be cleared,  or these could be, you know, access control  so that they don't leak out. And from a link reset perspective,  let's say the link is reset  either because of an integrity error of the IDE  or in case just the link gets reset,  all of the secrets in the device  and all of the IDE keys get reset. And then there's a need to kind of go back,  start over again and do the full device authentication,  key exchange to reestablish the IDE connection.

So today we reviewed some of the capabilities that CXL,  I know what CXL offers in terms of security,  talked about how device attestation works  and how you know who's on the other side  and the mechanisms that are needed to do the key exchange. This is built on, this uses the CXL.io interface  that is sort of built on top of the SPDM  and DOE mail analysis. We talked about how CXL.io protocol,  the IDE for it follows the PCIe ECN  and then looked at some of the properties of CXL.io IDE. We dug into the details of CXL.cachemem IDE,  but really the detail definition  that's in the 2.0 specification. We talked about the two modes of MAC transmission,  the containment mode and skip mode  and how the IDE is implemented in a efficient manner  to get the link identity and confidentiality  that's low bandwidth and has low latency impact. And we also talked about the use of a different  plain text CRC for providing to do internal business. So thank you all for listening and O to you.

 Thank you, Raghu. We will now begin the Q&A portion of the webinar. So please share your questions in the question box. We can start with the first question. In the figure on slide 11,  if a key switch exchange process happened  between the MAC epoch one and the MAC epoch two,  when exactly will the IV counter be configured  at each TX RX port? Is the RX side still using the old IV value  to decrypt the first flip of the MAC epoch two?

So if you look back at this definition of AES-GCM,  all of the flits in the MAC epoch  really have to be processed together. And that would mean there's one key, one IV  for all the flits in the MAC epoch. And I would say that the key switch really has to happen  at the boundaries of MAC epoch. So all the yellow flits would use one key and one IV  and the green flits would use a different key  and different IV. Realize that the MAC for the yellow flits,  even though it's coming later,  would have been processed with the IV and the key  from the previous epoch. So one kind of way to address all these  would actually be to do a truncated MAC  and end the MAC up accordingly  because you need a new key.

Okay, is there an impact on latency  of CXL.mem read and write commands?

It's the, you know, the latency in fact  would depend on the mode of IV that's used. We talked about those sort of two modes of aggregation. There was a containment mode and skip mode. When skip mode is used, you know, there's no latency impact  because as soon as the flip is received  it's decompensated and sent forward  so that the latency will be minimal. In containment mode, of course,  the flits are held back till the MAC check completes. There would be an additional latency  because the flits have to arrive,  the MAC has to arrive and all this. This has to be checked out. Good question. Let's see, the next one. Could you please?

You know, the keys on the fly question  actually goes along with the epic question. I think maybe speak to that one.

 Sure, I'm trying to find it.

At the bottom. So when keys are changed on the fly  then what will be MAC calculated on?
 
Yeah, like I said, the MAC is always  based on the keys and IV that was used  to protect the flits in the record. So if you look at how, you know,  AES-GCM, you know, defines the app name  and you can't take all the flit content  that gets back into AAD or into plain text. And this is processed with a key and an IV  to generate the MAC. So this whole unit goes together as one. The key switch happens, new flits  will get encrypted with the new keys. There'll be integrity protected with the new keys. How about that? Good question.

 Let's see, are there requirements to distinguish  ID related link resets from non-ID  or implementation specific?

In general, I think you could say that  any reset, when there's ID enabled and getting used,  when the linkage reset, any of the secrets  that were there from the previous, you know,  recent epoch, so to speak, has to be protected. Like, you know, when you come back up  to reset, it's not guaranteed you're talking to the secret. So the ID keys will get cleared  and any of the pretexts of crypto state  will also have to get cleared.
 
Does the traffic require to be back-to-back? If it's sequential traffic, how is ID expected to behave?

So it doesn't need to be back-to-back. There can be, you know, bursts of traffic. So you could send, you know,  we did this example that I have on the screen. It sent flit one, and then maybe some amount of time  before flit two arrives or flit three. The flits are clearly ordered at the link level. So they get in that order and the receiver sees  in the same order the transfer is sent. There may be practical reasons, like, you know,  if the link is getting idle for a very long time,  then maybe you need to terminate the MAC appropriately  and send a TMAC so that, you know,  in containment mode you don't have very large latencies.

Raghu, do you want me to take this one about-  - I was just hoping you would, please. Why is selective stream not supported in CXL.cache?
 
Yeah, so, you know, the key observation here  is that in PCI Express, the TLP,  the transaction layer packet,  which is the routable unit is the basis for IDE. And so, you know, setting aggregation aside for the moment,  the connection end to end between two ports  implementing selective IDE is,  I'm sort of making air quotes,  it's simple because you take the entire unit  that comes into the switch,  determine which port it goes out and pass it out. Now, this figure here actually illustrates nicely  how CXL, of course, is a very different structure  where in general, you're going to be applying the MAC  across a relatively large number of flits  that are potentially all routing to different places. And this essentially precludes  having a selective IDE mechanism applied to this. And, you know, if you turn that around and say,  what if you applied the MAC to each individual flit? Well, it would be a very significant impact  on the usable bandwidth, right? So it's not a desirable place to land. So essentially, this is something that falls out  of the natural difference between a flit based protocol  like CXL.cachemem and a larger, you know,  packetized protocol like PCI Express,  which of course then applies to CXL.io as well.

Okay, well, you know, David,  do you want to talk also about the EDPC link reset? What does it mean? Does it mean we need to restore CXL.cachemem IDE  after an EDPC link reset on CXL.io?

I think that the, yeah, sorry,  actually I'm trying to find the question. You know, in general, the IDE connections on CXL.io  and CXL.cachemem are managed independently of each other. And the reset cases of which there are many,  I think require some thoughtful treatment  in a platform and application specific way. But as a general statement,  they're not really tied to one another per se. Now, obviously there are some things  about the configuration of the connection  that are established over CXL.io  that if they are changed probably in turn  impact the CXL.cachemem connection. So I think that,  well, that's probably all that I can say  without a whiteboard. Raghu, I mean.

No, I was just gonna say that, you know,  in many cases when you start doing down-port containment,  it would also mean that the link is going down  or the device is getting unplugged somehow. It would mean that the entire device is going away. It's not the ideal link of the CacheMem. Like, I mean, if it was in a system, like you said,  if it was carefully thought out  and only one side is going down,  it may be possible to keep the other one alive. Like, but not only to treat CXL.io IDE  and CXL.cachemem IDE as sort of two parallel  independent entities.

Yeah, I think that's the overarching concept  that really these things are independent  at the level of the protocol,  which doesn't mean they're independent  at the level of some higher application concept,  but those are implementations or scenario specific,  if you will. And there's an overarching philosophy here with IDE,  which is IDE is just protecting the connection, right? It's necessary to have appropriate security policies  at both ends of this connection. And so again, as a very general statement, right? It's simply a requirement of use  that if the security properties of, say,  the .io connection change in a certain way  and that impacts the security posture  on the CacheMem connection,  then the device or the host needs to ensure  that that's enforced,  but that's sort of outside the scope of IDE per se.

Right, so I'm gonna take the monotonic counter question  just so that it's clear. We'll start with the final slide. So,  so the monotonic counters use this part  of the IV construction. So when you look at a AES-GCM,  the IV is updated every time a MAC is transmitted. So, and this means, right,  when the transmitter, you know,  encrypts and computes a MAC,  it's using the keys and a particular value of the IV. And then the receiver has the same key,  it also needs to align to the same value of IV. Otherwise, the encryption will come on incorrect  and the integrity verification will fail. So the use of the monotonic counters,  you know, let's say both sides start at one,  the monkey. At key establishment time,  both of them have it as one. Every time the transmitter sends,  it's five flits, it sends a MAC. The receiver sees the five flits, it sees the MAC. The next MAC at birth,  the transmitter is updated the monotonic counter to two. The receiver, having seen the flits of the first at birth,  can now update itself to two. So that way, the two sides will always stay in sync. But that's sort of the general use of the IV  and the advantage of the counter  is that it's fairly straightforward  to keep the two sides in sync  without having to send it across the link.

Is it possible for a device using CXL.mem or CXL.cache  to optionally participate with the security protocol?

Because CXL.cache and CXL.mem are both currency protocols  and they're coupled by the cachemem IDE. So a given slate, if you think about it,  would have some CACHE transactions  or some MEM transactions  assuming the device is in both of those,  like it's a type two device, for example. So in that case, no,  it's not possible to separate those out independently. But if the device was just purely a type three device,  it could then say,  I only need to implement .MEM protocol  and that would get protected by IDE.

 I think maybe there's also a good segue  to the is IDE optional question.
 
Yeah, I was gonna go to that. Go ahead, Dave.

Yeah, well, I was gonna say,  just to expand on what Raghu just said. So CXL.io has its IDE capability. CXL.cachemem has its IDE capability. They're both effectively and independently. Anyway, they're both optional. But obviously if an application requires security  of the kind that IDE provides,  then that application can't meaningfully work  without IDE being supported on both sides and turned on. And again, this gets back to the idea  that both the device and the host  are required to enforce a security policy  that's appropriate for whatever they're doing,  where that's somewhat application specific. But so for example,  a device that requires IDE for secure operation  would be obligated to determine that IDE  is not only supported,  but is in fact enabled and operating  before it enters into any sort of secure operational mode. And of course, if IDE were to go down,  then it would disable that secure operational mode. And this is essentially  an upper level protocol requirement.

Okay, I'm gonna take this question on slide three. Okay, let me go back to slide three. Will a switch have to interleave GCM streams  with different keys from it to each point? Are the program ones in static?
 
It would depend on, what's the protocol name?CXL.io IDE has the ability to have multiple streams. So, and you have stream IDE and CXL.io. So, data belonging to different streams  or TLPs belonging to different streams  are protected by different keys. And the switch, it would naturally flow through the switch  in an interleaved fashion. But the switch would not really be doing  anything special there. But when we are doing central cache mem,  it's a link level protocol. So, that is a link IDE going from the root port  to the switch, and then from the switch to the endpoint. And those would be, you know,  there's one set of keys between the root port and the switch,  there's a different set of keys  between the switch and the endpoint. And all the flits, all the cache mem flits  would all be protected by the corresponding IDE.

Let's see, is there a debug mode for CXL and PCIe,  LAI to handle IDE? Dave, would you like to take a shot at that?

Sure. So, this is, so first of all,  I assume we're talking about LAI,  like as in with a logic analyzer  or exerciser type test equipment. And this is something that is essentially outside the scope  of the PCIe and CXL IDE definitions themselves. But we are actually working on a proposal for, you know,  essentially a uniform way of handling this,  where of course, the thing that's difficult here  is that test equipment would typically trigger  on information that in this case  is now actually going to be encrypted. And so even just to display, of course,  what's going on on the link,  it's necessary to be able to decrypt packets,  which means being able to agree on the key,  either because you have perhaps a set of agreed upon keys  that's arranged in advance that are specifically for debug,  or because you have a mechanism  to essentially leak the key to the test equipment. Now that can be done offline. There's a more difficult problem,  which is again, if you wanna trigger on information  that's encrypted, then of course,  that processing has to be done real time  or close to real time. But that is something that is being worked on.

 Let's see, I'm gonna take the first one here. In Skip mode, it seems that protocol flits  are released for further processing  without waiting for the MAC. While this might be good for latency and bandwidth,  how does it work if the MAC is found to be incorrect? Does this not increase security and risk  due to a lag in the MAC check?
 
So I agree with the observation completely, right? Yes, there is an additional security risk,  and that needs to be carefully weighed  against the benefits. And in fact, the threat model,  that's under consideration. So they didn't want to see specific types of threats  that they particularly say they're concerned about,  and what are the benefits of the additional latency  and bandwidth that will be obtained. So yes, it does require careful analysis and thought. Any other thoughts, David?

No, I think that covers it.

Okay. Let's see. Can you specify what is expected from IDE firmware  implemented outside of CXL IP? That is, what is negotiated on the mailbox of CXL.io? Where is the definition of the key exchange  and agreeing on starting counters?

So there is an ECN for the CXL.cachemem protocol,  which discusses the key establishment protocol. For the CXL.io, the use of DOE mailbox  and how the keys are negotiated are covered  in the PCIe IDE itself. And so yeah, we walked through the device attestation  and key exchange flow,  and I would expect some kind of an IDE firmware  or a device security manager to be involved  in that negotiation and agreeing of the keys  before they get programmed into the crypto engines  themselves and apply to CXL IP.
 
Yeah, and it's probably worth mentioning as well  that PCIe IDE established a key management protocol,  which is kind of the starting point. Then in CXL, I believe the capabilities of that  are somewhat expanded. And then the idea is that we would continue to evolve  both the PCIe one and the CXL one  in this kind of a step back and forth fashion  so that they're essentially at the same level of capability. But I do anticipate that both of them  are gonna grow some additional capabilities  beyond what was defined  in the initial key management protocol.

I think this one, could you please refer to MLD CXL.mem devices serving multiple servers? I think the question is,  how would IDE work with a MLD device? It's a phenomenal idea, right? I mean, the way I view an MLD device  is that there are two holes,  but multiple holes connected to it. And if IDE is required,  there will be an IDE connection  from each holes to this device. So that would be one mechanism  where the device is directly connected  to multiple holes in each one. You know, you can have multiple links,  one going to each server,  and you have an independent IDE with each one. Clearly the device, of course,  would have to further understand  some of the implications at the usage model level  and the overall system level  of what it means to have all these,  you know, different servers sharing the same link  and the same device. And, you know, what if some of them are enabling IDE  and others not, sort of the underlying implications  of how do you guarantee the security properties  of the usage model request. So that would be some, you know,  considerable thought that has to go into building  such an MLD device. A second option for an MLD device  would be that there would be a switch in the middle  that the switch would actually be the one  that's multi-headed talking to different servers. In this case, of course, again,  there may only be one IDE link,  but it may be servicing information from,  you know, sort of the switch is sending the data  from multiple hosts to one endpoint. And again, in this case,  the device has to carefully weigh  and understand the implications at the system level  of how things are getting shared.

Hey, being mindful of the time here,  we will wrap up today's webinar. So thank you, Raghu and David,  for sharing your expertise. We couldn't address all the questions we received today,  but we will be addressing them in a future blog. So please follow the CXL Consortium on Twitter  and LinkedIn for updates. The presentation recording will be available  to watch on demand on the CXL Consortium's YouTube channel,  and the slides will be available  on the CXL Consortium website. If you would like to learn more about CXL,  please join the CXL Consortium  and download the evaluation copy  of the CXL 2.0 specification  and engage with us on Twitter, LinkedIn, and YouTube. Once again, we would like to thank you  for attending CXL Consortium CXL Link Level Integrity  and Data Encryption webinar. Thank you.
