
All right. So today we're going to talk about unleashing the full potential of CXL and focusing primarily on type 3 memory buffers and such, but it applies to the overall CXL ecosystem. Get started.

For agenda today, we have is a quick review of our CXL journey. It's happened so far. And then how do we deploy at scale? We want to take this to the next level and take it to deployment in the data centers and racks and blades and such. Exciting developments and then impacts to workload, then the CXL ecosystem itself.

All right. So 2019, we get this first spec. We are now at the cusp of release of CXL 3.1. 2020, we were all looking at use cases. You could do memory sharing, pooling, expansion, memory tiering, all kinds of stuff. 2021, we had design concepts. And the slide is actually from OCP 21, where we are looking at a similar design to what Kiran just presented earlier. 2022 is the sampling year. You know, exciting. Most CXL memory buffer vendors are sampling both devices as well as cards and E3 modules. And we are in 2023 now. What are we doing today? We are validating across different CPU, GPU hosts, across different memories. And we are trying to put this into real existing architectures. So where do we want to get to in 2024? 2024, we want this to go into racks. And what do we need to get there, right?

Here are some building blocks. So the first thing we want to do is interoperability, right? So we have compliance at the electrical and the protocol level. But then to take it to the next level, we want to be able to mix and match any of these rows and still be able to interrupt without any issues. To add to that, we want to be able to plug in with switches. We want to be able to plug in with reach expanders and still be able to have robust interoperability.

Form factors. We just had a presentation from Kiran and Anant that talked a lot about form factors. What I have here is what exists today, right? We have the DIMM-based modules, which is one kind. And it gives some flexibility in terms of the DIMM supply chain. You can use the previous generation DDR4 memories. It enables reuse and supports higher density. And then you have all of the E3.S modules. And while they are slightly lower in capacity, what they do is they leverage the existing infrastructure from the back planes, NVMe back planes.

What we need next is RAS. And this one is really important when it goes into racks and blades and server systems that go on forever and ever. One of our customers came up to us and said, hey, if my memory fails in the field, I do not want to go walk there and unplug it and service it as much as possible. And that's what RAS gives you. So what do you want to do? You want to have your hardware have all of the hooks built in to do good error detection and correction. You want to be able to support fault detention and isolation. This is what I'm talking about. You should be able to offline your memory and still keep operating as much as possible till a point that you do need to service the actual hardware. Error handling and reporting-- you want to log everything. You want to log your reports with errors. And you want to do this in such a way that it has minimum performance impact. And then customizable management framework-- you want to reuse your software development kit, integrate into existing infrastructure, which is either your BMC or host management infrastructure. At the end of the day, what we want to get to is 99.999% uptime and improve the TCO.

Taking RAS to the next level, if we have in-band tools, we have this SDK integration into the BMC, and we do continuous monitoring, what we can actually get to is predictability. If we monitor over enough amount of time and all of the temperature and the voltage sensors and such, the idea here is to be able to predict failures before they happen. So while we have all of these building blocks together, what's exciting right now is deployment.

So if you look at our current form factors-- and we have a CEM add-in Card form factor-- you can actually use it as is in existing SAP HANA hardware, where you can double the memory capacity and add memory bandwidth 1 and 1/2 times per socket with the CXL. And what does this give you? It lowers your TCO. You're now talking about adding memory to just two sockets instead of the four with a DRAM-only case.

The next thing that we have is hardware interleaving. This one is also really interesting in the sense that you can take eight native channels of DDR memory. You add in two of our add-in cards, the CXL A1000 cards, and a total of 12 memory channels appears as one big NUMA node that has 50% more bandwidth, 50% more capacity, with 25% less latency. And then you take these configs and you use real-world applications.

And the impact to real-world applications is there. We are seeing up to 50% improvement for memory-intensive applications. And they go across various fields, starting from AI, computational fluid dynamics, data warehousing, ocean and weather modeling, and many, many more.

So what do we want to do? We are at the cusp of volume deployment. We have done all of this work so far. And what we really want to do is get to the next level.

So call to action. We're basically all in this together. We have seen that the value of CXL is evident. So then the focus is now on at-scale deployment. One of our presentations earlier talked about architecting hardware designs that support all of the CXL protocols and do it in such a way that it's easy to adapt in existing data center environments. We want to move the ecosystem. The ecosystem readiness is one thing that we really need to push for right now, where all of the required CXL drivers are incorporated into our OS hooks and such so that the solutions are plug-and-play. And then we want to do the POCs. And this is some of the work that CMS is really aiding with. We want to be able to do POCs where we support memory sharing and pooling. And we do interrupt with switches, as I was just talking about. Existing system designs, DCMHS and MSIF, that we just spoke about. I think some of the PMM modules that Anant spoke about, I think that makes a lot of sense in terms of what we want to explore next. And then we also want to be able to use existing infrastructure. We want to use OpenBMC. We want to use Cloud Firmware Development and then Fabric Management.

With that, I guess, come to our booth if you have time. There are a lot of my colleagues who are giving additional presentations and talks later today and tomorrow. And if you have any questions.

We have time for a couple of questions.

Hi. In some of the pictures I saw, the expansion inside a chassis. But what about rack level with switching and all that? How is that coming?

Yeah, right. So the device itself and the cards, they are designed in such a way that they can support any of them. I think memory behind-- CXL memory behind switching is one of the POCs that I'm talking about. And there are switch vendors like XConn and others that we are trying to work with to get that on flight. And so if you actually come here next year, I'll probably be talking about that.

