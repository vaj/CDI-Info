
I thank Adam for a wonderful setup  for what I'm gonna be talking about at this point,  which is optimizing storage and memory hierarchies,  largely focusing on CXL. My name is Andy Banta. My official title is the Storage Janitor,  and I work at Magnition right now, among other places.

So one of the problems that we run into now  with modern storage and memory systems  are that we've built a huge hierarchy  that has lots of different steps,  lots of different tiers in it,  anywhere from register-based memory  all the way out to optical-based drives,  that type of thing,  and figuring out how to configure this appropriately,  figuring out data placement,  figuring out how to size the various different tiers,  figuring out what interconnects to use  can be a larger task than an engineer  or a team of engineers can figure out. SNIA has been driving an awful lot of these changes  and adding an awful lot of the different pieces  that end up in here, making this problem worse.

Most people don't really know what Magnition is,  so let me just give you a quick overview. Magnition has built a framework  for simulating various different tiers of memory  and has come up with algorithms  for figuring out data placement  and sizing and interconnects. We can do this in the simulations. We can also do this real-time. For those who aren't familiar,  Magnition has a handful of patents  and some award-winning papers out  on the subject of doing data placement and sizing.

So right now, the engineering world  has a bunch of different disciplines in it,  like we have electrical engineering, chemical engineering,  mechanical engineering, civil engineering, bioengineering. All these different things use simulations  to come up with answers to their solutions.

Why can't software engineering and system engineering,  system design engineering, use the same approach? You do this because it's cheaper,  it's far less expensive  than trying to figure it out from scratch. You can come up with a lot more flexible possibilities  than you can building out hardware. As Adam was talking about,  there aren't CXL devices out there  that you can necessarily test on right now,  but if you simulate them, you can do them. An aspect that, one step beyond what Adam was talking about,  was that you can't necessarily test at scale,  even if you can get a few of the devices,  you can't necessarily build out at-scale systems  that you wanna try. So it makes sense that we should start using  simulations in software design and in system design,  rather than just in physical design practices.

I mean, the values are huge. It's faster to prototype, it's cheaper,  it's a lot easier,  you can come up with a lot different configurations,  and it's the same practices that should apply  to any other of the engineering disciplines  that are out there. This is just showing a stepwise approach  to solving a problem and coming up with a better answer  through running simulations,  rather than trying to build it out on your own.

One of the cool things about simulations  is it tells you things  that aren't going to be intuitively obvious otherwise. So this is a genuine graph that we got  from one of our simulations,  where we're comparing latency and cache size  in a particular configuration. And you can see that there's this huge knee  in the center of it,  where even if you expand your cache size out eight times,  you haven't really improved your performance at all. This curve is invisible until you actually go through  and do the simulations to find it. You can't just intuitively say,  this curve is going to exist. That is why the simulations come in handy  for this type of thing.

So, let's talk a little bit about how we go about this. Magnetion has put together a framework  that will build out deterministic behavioral simulations  by using different pieces,  potentially like the pieces that Adam was talking about  in the previous talk,  piece them together as you want. Magnetion has a library of components that are already. We can also draw on public source or open source systems  that are out there. And typically we will work with companies  who were interested in designing  their own proprietary pieces to fit into these simulations. When you build out these simulations,  you can go in and say,  here are the things that I want to vary  to try out the various different simulations. You can also plug and replace components  to try out different things. And while you can run these against benchmarks  or synthetic data,  we also have the ability to play back actual traces. So you can try these against genuine applications,  genuine workloads that you would be interested in running.

So the whole idea here is that we build a framework. We're working with a little company in Berkeley  named Lingua Franca to do this framework. As I mentioned, we have a handful of components  that are out there already. We have like an SSD simulator and a DRAM simulator,  an HTTP simulator, various different things. One of the easiest examples to show  how we would put these simulations together  is using something like a cache simulation. So we kind of look at a cache as a decision point  on whether there's something that's stored in the cache  or whether it's not stored in the cache. So we have an input and in this diagram,  we're just gonna say that a hit goes down  and a miss goes off to the side. With the cache, you have various different media types. So typically this would be a fast media type  and this would be a somewhat slower media type. And what we do is that we simulate  those various different types of media. So it could be an SSD or DRAM or whatever. We also have this idea that there's interconnects  from the input to the cache and from the cache  to each one of the media. So we just call these wire components. And we can build this out to be a multi-layer cache  if we want. We can build, this is a fixed diagram. Jay asked this question when I first presented this. It's like, it was, this all you do? And it's like, no, these can multiplex,  these can fan out, these can be,  the components can fit into any type of framework  that you want. This is just an example to explain how this works.

So when we get into the media component,  see, I fixed this in my slides, but it's still here. So the media's components can be memory, disk,  cloud storage, network storage, whatever you want. We have a handful of ones that are out there. MQSim is a SSD simulator that's out there  that we've adapted to work with our simulation. There's also various different DRAM simulators  where we can simulate contention and whatnot. So the idea that there's going to be parallel access  or contention or queuing on any one of these devices  is simulated as well. And one of the nice things about the simulation  is these media devices don't actually need  to store anything. All we need to do is say whether the data  that we're interested in is there or not,  and all we need to do is simulate the delay  in getting the response back,  because we don't really care what the response is. We just want to know the delay in getting the response back. So in this case, we could have something like DRAM here  and an SSD here and a network storage box  or cloud storage in this example. Similarly, the wire components would be memory buses,  ethernet, disk adapters, whatever would be something  that would interconnect these various different devices. They can even be something that would transform a request. Like in this case, we could have something  that would be coming in as an HTTP request  that would get translated into an ethernet request. They work for a variety of different memory buses. We can calculate hops and topologies  if there's various different lengths of things here. And again, all we have to do is introduce the delay  associated with it. We don't need to actually do any of the real work  that would be happening here. So the simulations can work far faster  than they would in real time. So in this case, we could have something like a URL here  and we could have RAM bus or DDR to connect our DRAM  or potentially even CXL, something like a PCI bus  between the two different caches and NVMe here  and something like iSCSI ZRS3  to go out to our final storage.

In addition to this kind of layout,  a cache itself has a handful of different components in it. So we can go in and we can change the algorithms  that we play around with in a cache. So we can change the allocation, the lookup  and the eviction algorithms in a cache, for example. And we've actually found one of the examples  I'll talk about in a few minutes. We've actually found that just changing the algorithms  in the cache has far more effect than changing the size  or the type of media that the cache talks to.

What I mentioned earlier,  while you can run synthetic workloads,  we are much more interested in doing playbacks  of actual workload traces. So you're getting information on the workload  that you're interested in,  not just saying what a benchmark would say. So some of the traces that we've looked at so far  would be content delivery networks. If you're trying to put together an AI system,  you could do like a learning or inference workload. If we were trying to put together a storage array,  you could do something,  use something like a storage trace as your workload. And for HPC systems,  you could use something like a electronic design automation  or gene splicing or whatever to be your workload as well.

And we have a variety of different ways currently  that we can read in playbacks. We can use the vSCSI trace from VMware. So this would just be all the virtual SCSI commands  that come into a virtual disk on a SCSI device. We can use like an ethernet trace  that you would get from Wireshark. We can use an HTTP trace  that would be potentially a dump from an HTTP server. We can use something like memtrace  that would actually be tracing all of the reads and writes  into a stack of memory.
 
So let's talk about some of the different ways  that we would potentially be using these simulations.

So these are just general purpose cases. The idea that you have a fixed cost build material. So you're trying to build out a system  and you know how much it's going to cost. You can run a simulation against this and say,  okay, here's how we optimize this cost  into a list of building materials  for the application that I'm trying to build for.

Another example is you already have some infrastructure  or you already have a data center  and you're trying to repurpose it. What's the best way to configure that data center  for the workloads that you're trying to do?

There's also the idea  that you're trying to get rid of wasted resources. You've built out a system  and or you're trying to build out a system. If you go out and start trying to constrain the resources  to see which ones really don't make any difference  on your outcome,  it's a way of discovering wasted resources.

You can try to come up with the best performance  for a given workload.

Most commonly, people will try to come up with  what is the best performance  for a variety of different workloads. I have all these different things  that need to run together. What is the optimum setup to do this?

While these graphs that I've shown you  have tens of points in them,  what we actually do with our simulators  are run millions of experiments  and end up with graphs  that give you millions of data points. You can try to start finding trends in that. It's not just a small sampling. We run typically millions of samples  against our simulations  to come up with the best answers. Because we're running this many simulations,  it means that you can vary  quite a few of the very different,  you can vary quite a few of the  decision points or components  or other pieces that you can vary in your system.

Some specific work,  specific use cases  would be something like a CXL system.

I did a talk at SDC in December  that spent, the entire talk was spent  talking about CXL use cases. Just to bring this out in particular,  following on with what Adam was saying,  once again, thank you for that great lead up. This configuration, if we were talking about type 3 CXL,  would have potentially seven different NUMA nodes  with anywhere from tens of nanoseconds of delay  to hundreds of milliseconds of delay. The only way that you can actually figure out  the best way to use that  is to run some simulations on the applications  you wanna use on it  and figure out how to spread them  across your various different NUMA nodes.

Another example that we've run into  is the idea that somebody is trying to build out  a hyperscaler or a large-scale HPC system  to do simulations. They're gonna have a variety of GPUs and CPUs  and Ethernet interconnects and HBM interconnects  and potentially CXL memory hung off of your CPUs. What is the appropriate way to build this type of system out  to be able to make the best use of it  for the workload that you're trying to do? This might not be the same  if you were trying to do design automation  versus whether you're trying to do AI learning. You would wanna run the specific workload  that you're trying to learn  to figure out what the specific example is.
 
Again, I know this is way too small to read. In my slides, I had nice big highlight arrows  pointing out what the various different pieces are. But this is a content delivery network example. Here would be the web traffic that's coming in. This would be one of many regional points of presence. And this would be somewhere where the source material is. So the point of presence is this big box here. And the point of presence has a handful of servers in it. And each server has DRAM in green and SSD in blue  in case you can't read the text that's there. As I said, I had nice big red arrows on it  to point out what they were previously. And I don't have those anymore. So I'll just have to do this and this and this and this  to point it out to you. This is the most recent experiment that we worked on with. It was done as a research experiment  in conjunction with a handful of content delivery providers  that are out there.

And we tried doing various different things  like varying the number of servers  or getting rid of the number of servers  that had DRAM as the cache  and only had some that had SSD in them  or varying the size of the SSD and DRAM. One of the things that I really didn't show in this example  is one of the most important changes we made  was to vary the cache algorithms. And varying the cache algorithms  had more effect than doing any of this. And just so you know, one of the key things  that you wanna do in a content delivery system  is be able to get a cache hit within these pops  without having to go to the origin. Anybody who's tried to look up a three or four year old  SDC video from YouTube, it takes numerous seconds,  tens of seconds for it to actually get to you,  which is why that you would want something like that cached.

The final thing that we really wanna focus on  is the ability to get understandable results. And typically these simulations dump stuff  into a huge database and you need somebody  to figure out how to pick through that database  and figure out what it means.

What we've done is we've assembled a notebook  of graphics libraries where you can say,  for this experiment, I wanna look at it in this way. And this is some of the actual results  from our content delivery network. And you can see that there's about a factor  of three difference between when we run a fixed topology  and when we run a variable topology. This is just one of the easy examples  of finding out what we wanna find out,  showing it graphically. We have a huge variety of different graphs  that we can print out or display. We have heat maps. I didn't wanna get into a whole lot  of examples of what we have.
 
What I did wanna point out is this simulation  has proven results. We worked with a major storage company. And before they started working with us,  they were running a couple experiments a day. After they started working with us,  we were able to simulate about 50,000 experiments a day. And over the several month engagement we had with them,  they ran more than a million examples. And anybody who's ever worked with storage technology  knows that getting double digit performance increases  in your cache is huge. And in some cases, we were getting almost a 50% improvement. So this is a real world result with Magnition  working with a storage partner  who won't allow us to use them as a reference. One of the two major storage companies in the world,  if that narrows it down for you.

So again, this is just a quick overview  of the various different pieces that Magnition has. And on my slides, I actually added  my contact information here. But you can reach me at andy.banta@magnition.io. I am on Twitter, if anybody still uses that, @andybanta. I am on LinkedIn. You know, feel free to contact me anytime. And I appreciate your time. Questions?

Very nice, thanks. We actually funded some work at Georgia Tech. It's called Spatter. So what we do is we take pin tools  and get memory address offsets and links  that we read or wrote across thousands of machines  doing these complex simulations. So we simulate too. For the reason that we can't test what we do. And you can just think about what that means. But we also are generating a ton of memory traces  and making them available because memory traces  aren't classified, but applications are. So we are hoping that there'll be lots of sites  that generate these traces. And then Spatter will replay them. So if you ever wanna replay them against a simulated system,  there's a replayer out there that will replay it. So we play in this game a lot. HPC plays in this game a lot, as you said. And there's a source of lots of data that are memory traces.

Okay, excellent. You're familiar with the I/O Traces  Technical Working Group in SNIA?

But I don't think that does memory, does it? It just does I/O.

I am not sure. I know that they are very interested in I/O traces  and they keep asking Magnician for us  to share some of the I/O traces  that proprietary companies have shared with us  with the caveat that we use them only to use  in our own simulation. And it's like, no, we can't give those to you  based on the agreement we've made.

Right. If they want memory traces, I can generate those.

Well, absolutely. I mean, memory traces are awesome examples of things to do. And I think especially apply to,  even though this is the Storage Developer Conference,  it seems to be,  the idea seems to be that memory is becoming the new storage.

Thanks for the talk. And the thing I would be most interested in,  from your analysis, you're mentioning CXL  from tens of nanoseconds to milliseconds. If you could publish classes of applications  and roughly how much latency they can tolerate,  that would be pretty big, I think, for adoption, right? And give people kind of targets to look at  and real expectations of what could happen  in a faster manner, right? So I'd be very interested in kind of that output.

And I agree entirely. I think this ties in very nicely to an awful lot of the work  that the SDXI group here is doing as well,  because I think one of the things  that's going to fall out of doing that type of analysis is,  okay, we know where we need to move the memory,  how do we move it? And I think that what they were working on  is a big part of that.

So we have instrumentation that will tag  the pieces of the memory trace  so that we know what parts of the codes  are generating those things. That might work really well  for what you're talking about, Adam,  where applications are big and complex. And so knowing what part of the memory  you could put into CXL versus what parts you can't  and what that implies to your application  seems to be super important. So there's kind of a path to get there. Wonder if we could promote it somehow. Anyway.

 Absolutely, thank you for the information.
 
Yeah. Yeah, I think that there are two papers,  one from Microsoft and one from Google  that kind of goes into that on classes of applications  that are latency sensitive or not. One of those papers from Microsoft,  they assume a static memory,  so there is no promotion,  the motion that just ping certain parts of the code  on fast memory or far memory. So that really helps. And back to how,  do you have a simulation for that? Like if I can get memory addresses  and parts of the code,  can I plug in and understand  what parts of my code could be hosted  on far memory or not?

The answer right now is no. We have the framework  and we certainly see the application  of how it would work with CXL. We have not pursued anything other than just  making sure the framework understands  that CXL could be one of the workloads  and how to piece it in there. As I said, the most recent work that we've been doing  has been on content delivery networks,  but the simulation framework is certainly capable  of handling anything that can be broken down  into components.

Yeah, in this case, I also work on,  instead of, not instead,  but with latency comes the bandwidth on the storage  where there's a point where the cache  becomes a bottleneck for bandwidth.

Absolutely, yes.

So you could just be better off  just not having cache in the multiple devices.

And that's entirely a possible situation  and that would be one of the things  that we could definitely simulate.

Yeah, when you mentioned the MQSim,  is that a proprietary software?

No, it's a university put together open source project  that we've just adapted to work with our setup. It's nice because it's one of the cool ones  that has done a really good job  of figuring out contention  and it actually works like you would think an SSD would  and it also takes into account  that once you've entirely written the SSD,  the characteristics change  as you start doing garbage collection on that.

Sure, yeah. All right, thank you.

All right, thank you, Andy. Let's thank Andy. 
