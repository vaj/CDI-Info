
Hi, my name's -- is this working? My name's Jeff Hilland. I'm only Jeffrey when I'm in trouble, so maybe I'm in trouble. I don't know. Let's see. There we go. So what I'm up here to talk about today is the challenges of managing the CXL ecosystem. There's been a lot of focus on the control plane, or on the data plane rather. You know, speeds and feeds do matter. Getting the data in and out in a secure manner certainly is important in the industry. But somehow you've got to manage all of this, get the control plane isolated potentially. And so that takes a lot of work from a lot of different -- not just companies, but a lot of different organizations to make it happen. So really it's -- you know, there's about six or seven organizations. There's a couple more I'm not going to talk about today that are also working on it, because some of that work's a little nascent. But so really what are our, you know, CXL, PCI-SIG, DMTF, OFA, OCP, and SNIA. If that's not enough acronyms for you, hopefully you'll have had your fill by the end.

Again, I'm a distinguished technologist at HPE.
I'm also president of DMTF, and I'm the co-chair of the SPDM working group and one of the people that put Redfish together and took it to the industry.

So what I want to do today is provide a roadmap of all the open communities out there and let you know what's going on, because there's a lot of folks doing a lot of effort to try and get everything working together in order to manage CXL fabric. And to try and do that in an interoperable manner in a multi-vendor community is certainly a challenge. And of course, any work you can do to help carry this along and improve the state of the art, we're certainly willing to take any volunteer efforts. It is a volunteer community. You know, we all have day jobs as well, but certainly some of our day job is to try and make some of this stuff work.

Font problems. So what's the infrastructure management dilemma? Really, there's a couple of problems that we're facing right now. How do we take all of these industry standard, off-the-shelf components coming from a bunch of different vendors, put them in a box and make it all work without writing a million lines of code, trying to leverage as much stuff as we've done already in order to solve the management problem, and still allow for differentiation from everybody in order to -- because we're all in it -- to try and make a little money at this as well. And yet, you know, you look at all these interfaces in between the box. Some of these we've been working on for a while, like, you know, in between a BMC and a host, or in between a BMC and a bunch of devices. But we're also breaking apart this sheet metal as part of this, and that's something we haven't typically done before. Once you start plugging things into a fabric, it's been very easy to control manageability information inside the box. You've got I2C lanes and PCI lanes and things like that, and you've got a BMC to manage the power and thermals and those kind of things. But once you put a switch in there in a separate enclosure, you've taken your fault domain as well as your control plane and split it between two different boxes. And so, you know, to try and put that together and -- so now what's the BMC's job? Is it supposed to be managing those things on the other end of the wire? It can certainly see them. Is that a good thing? Is that a bad thing? Those are the kind of problems we're facing. And then, you know, what do those interfaces look like? How do I actually get to those things? And then where exactly is my, you know, control head nowadays, right? If it used to be the BMC was a thing it could do, or BIOS, or even the workload running on the processor. But now if I've split my control plane between a bunch of boxes, I need some kind of external entity out there in order to manage the infrastructure as well. So how do you put all that together?

So going through the different organizations real quick, obviously CXL has some bits in there that -- in order for you to manipulate and control and manage CXL devices. And the two main ones are the CCI and the FMAPI. And the CCI is used for managing CXL memory, and that's, you know, a good interface for getting the power and thermals and health data and media and poison and security and QOS and all of that information, you know. So it's good to see that as a standard. And then the FMAPI commands are used to manage the switches. There's also a bunch of events and logs and things like that. But certainly we have all the information at our disposal to build something. But the way you get to these things are through either the mailboxes or MCTP. So there's two mailboxes, a primary one and a secondary one. One of the problems is not every expander out there is implementing the secondary one. And that's a challenge if you're a systems integrator trying to do a firmware first strategy. So, you know, these registers aren't -- these mailboxes aren't reentrant, so you can't have two people beating on them on the same time. So if system firmware wants to beat on one and the OS wants to beat on one, well, you can't have two things talking to the same device at the same time, right? So that's a challenge. And then you've got the MCTP access to these methods, right? That's sideband access. That's how the BMC will get to it. And that works, certainly works. But it also doesn't match the ecosystem that we've been investing in the BMCs for the past couple of decades. So it's a very bitwise interface, and what we've been investing in the BMCs is a self-describing, self-contained interface. One of the things that customers tend to do is they'll mix and match anything they possibly can together. And so once you start getting very, you know, bitwise interfaces, we have the same problem with NVMeMI. Every time I upgrade my NVMeMI firmware, I've got to update my BMC firmware. And that puts things together in a very lockstep method. And you end up having to upgrade everything at the same time and make sure all the firmware and firmware interfaces match. And that can be a challenge, particularly in a fabric, because now I've got a switch in the middle, and again, multiple switches in a 3.x time frame. So I could actually have firmware mismatches and things like that. So we need to manage all of that as well. And so, you know, it would have been good to have a self-contained, self-describing interface, but we've got what we've got. And, you know, it also didn't help that this is all new code for every BMC out there. Now, that's not a bad thing, right, because, you know, pushing the state of the industry is something we do all the time, and adding new code is fine. But every time there's a new CXL thing out there, you're going to have to update the BMC firmware. And we spent the last couple of decades to try and get away from that. So it's not an insolvable problem. It's certainly something that we can do, but that's one leg on this elephant and one set of challenges.

The PCI SIG has--PCI cards typically have both PCI--have PCI normal commands and then PCI VDM commands for managing traffic. And so VDM is actually a kind of a nice thing, and it turns out you can encapsulate VDM in CXL-IO. So we expect a lot of the devices out there to be able to handle a lot of the PCI VDM commands that we've been building for the last 20 years. And I'll get to what those are largely in the next slide. So, you know, again, PCI devices also have either USB or I2C lines going to them for manageability so the BMC can get to them. And then the BMC also has some PCI lines that will tunnel through the south bridge for PCI VDM commands. So you can take both small and large management messages through the entire ecosystem, using all the ecosystem that PCI has been working on for its lifetime. Yeah, the sideband signaling I already talked about, and then the PCI devices already support most of the MCTP commands. So most of that's on the next slide. And then CXL-IO, you can actually tunnel PCI VDM. So I covered all of that.

This is an eye chart, and I apologize for the diagram on there. There's a standards effort called PMCI and DMTF. And what these people have been doing for the past 20 years is putting together that control plane ecosystem that's inside your box of sheet metal. And it's a suite of standards. There's two main standards, MCTP and PLDM. And MCTP is a low-level, small message transport that things like SPDM go over, but also PLDM goes over. And the reason this one's important is it's basically the communication mechanism that we use for doing all of the sensor data, firmware update, security for SPDM. It's got all of the device management, inventory, control, those kind of things. And there's a couple of methods in there to do that. And all of that work takes place in the DMTF's PMCI working group. FRU is going through a modernization there as well. So if you look at FRU data, it's very tight and packed. We take a text file, we pack it down into a binary file, and then for somebody to digest that information, they have to expand that binary file back into a text file. And that's kind of crazy given the storage sizes of today. So we're looking at putting just straight text down in there, or JSON or something raw that devices can just eat. And we've done the same thing with all of this stuff going on in PLDM. There's a couple of data models there. There's an MCTP type 2 data model, which is really good for sensor data if I'm trying to do a fan control or power or temperature. It's a very simple data model for extracting that information. When you get to complex systems that really need to be object-oriented, that have relationships between the objects like you see in a CXL fabric, that's where you want to use something called RDE, which is Redfish Device Enablement. Really all RDE is is Redfish packed in a binary form. We compacted it down and made it tight for low firmware footprint environments to be able to handle. One of the beauties of it is it's completely self-contained and self-describing. You can take a RDE device, PCI device, plug it into an RDE ecosystem, and with zero changes on the BMC, it just works. All the information flows into the BMC. A request comes in from a client. It'll take that request, encapsulate it in a binary format, send it off to the device. The device basically does a Redfish command, you know, show me your temperature, show me your, you know, how big is your DIMM, what's your relationship, how many DIMMs are in this machine, all of that inventory information. And then flows back to the BMC. The BMC turns it back into JSON from binary and spits it back out. There's no BMC changes required for that infrastructure. And if you've got any OEM extensions, they're completely pass-through. So there's no time to market difference either. And so there's a real advantage in using RDE as opposed to a PLDM type 2 model or a very, you know, bespoke binary model. And then there's firmware update. If you're a systems integrator, there's a couple of ways to do firmware update. CXL invented their own for that. It's missing some of the things that have been in -- that are in the PLDM firmware update process. You know, we've spent a long time getting PLDM firmware update right, and it's already working in a multivendor community for PCI devices and all the other devices out there. So that's preferred for the BMCs. And all that code is going into OpenBMC anyway, so it really doesn't benefit us a lot to have yet another firmware update mechanism. And then every modern BMC out there supports most of these protocols. In fact, I don't think there's a BMC shipping that doesn't support everything out there except maybe RDE. And there's kind of a split in the community between those that really like RDE and those that are very comfortable hand-crafting every data model out there to take all those PLDM Type 2 records and create a Redfish object out of it.

The other thing that's going on in the DMTF, of course, is Redfish. You've probably heard a lot about it for the last few years. It's a RESTful protocol. We started out where everything was just JSON, and we've got like four or five different schema languages now. One of the reasons for its success is we separated the protocol from the data model. So we can really go over any protocol. That's how we were able to do the RDE is it's just a different transport. We just do a binary data model. What you invest in is the data. You don't invest in the schema language or the transport. If you can describe that payload in any schema language, why not go ahead and do it? So there's a bunch of mappings for that payload. And then there's a suite of self-validation tools out there as well and documentation generators. So if you can create a Redfish profile, you can generate documentation automatically out of it, and that same profile can also drive your conformance test. So I'm speaking Redfish, and it could be either a device or a fabric model. If I can describe that in a Redfish profile, I can do all my validation automatically as far as conformance to the Redfish profile. What that helps with is interoperability. So I can guarantee my Redfish implementation for any individual switch is the same by running the same test suite against it and using the same profile. We started supporting CXL and Redfish in 2020.3, and basically that's the year 2020, and it's our third release, and we do about three or four a year. So you can kind of point to the timescale for any of those Redfish releases. There's white papers out there and presentations. We just released a YouTube last Friday on CXL fabric management. There's three more fabric videos coming here over the next month. Hopefully we'll get them out in the next couple of weeks. So if you don't understand Redfish and how it works with CXL, go ahead and take a look at the white papers and the videos. There's updates that are coming. We just added the latest support in 2023.2, and then, you know, one of the other beauties of Redfish is it's got all the other stuff, right? So, you know, if I'm putting a PCI device out there at the end of my CXL fabric, that should all just work. We've already got the data models in there for that as well. And then we just released, it's out as a work in progress, what we've been calling the Rosetta Stone, and that's a CXL object. Remember that CCI and FMAPI. How does that map to every individual Redfish property in the Redfish data model? So, and the way we do Redfish is all on GitHub. So as we add things to Redfish going forward for CXL 3.1, we can add that mapping to the Rosetta Stone at the same time in the same pull request and then just release it all at the same time. There's a lot of work that still needs to go on in the Rosetta Stone. It's a really good document if you know CXL and you need to put things into Redfish, but it doesn't have where all the rest of the Redfish properties will come from. So like product and things that you may be getting from Fru or maybe getting from somewhere else, it's not going to tell you how to get those. It's really just a CCI and FMAPI mapping document.

There's another group out there called the OFA, Open Fabric Alliance. If you're familiar with RDMA or InfiniBand, this is the group that does libfabric. And a lot of these, when I say they, it's them is us. It's a lot of the same people circling in the same orgs. So the guys, some of the folks working in the Redfish Fabrics Task Force are over in OFA actually working on Sunfish. And what Sunfish is, is an open fabric management framework. Remember that box that was outside all of the box. Something has to glue all of these pieces together. If I'm getting some of the information from a BMC and I'm getting some of the information from a fabric manager outside the box, how do I recognize those two devices are actually the same device and represent them as a single object to the rest of the management ecosystem? And that's what Sunfish-- it used to be OFMF and they renamed it Sunfish. And it's kind of a trend, if you're a Redfish-y thing, somebody calls you a fish. So they went with Sunfish. I like Starfish better, but Sunfish was where they went. So anyway, this is a code to manage multiple fabrics. And it isn't just limited to CXL. Originally, they were doing a GenZ one. They were doing an InfiniBand one, a Zephyr one, Omnipath, and-- gosh, another one-- Slingshot. So there's a need, especially in the HPC community, to not invest in custom code for every fabric out there. So you'd like the fabrics to be as interchangeable as anything else. So by having a largely single-level data model that's useful out there for creating memory pools and memory regions and things like that together, and it's mostly the same as I go from fabric to fabric, it's only the very nitty-gritty kind of code that's down there and the details of an object that are very fabric-specific that my high-level code won't have to deal with. But when I've got a problem, I really want to know that information. And that's what the Open Fabrics Alliance is working on.

Lastly, OCP-- well, not lastly, but certainly OCP. There's a couple of efforts going on here as well, particularly some submissions that have come in right around the Global Summit here. There's work going on in multiple areas, largely in the hardware management stream. So if you haven't paid attention to the hardware management effort going on, the project there, there's a couple of work streams to pay attention to. The biggest one for me is the profiles group, because that's where we really expect-- remember I said we need all these profiles to go into the interop validators, right? So I want profiles that describe a CXL fabric, profiles that describe a CXL memory component. All of those individual component profiles that we can then validate against the Redfish interface programmatically and make sure everybody's interfaces are exposing them the same way. You can then take those profiles and wrapper another profile around it and say, "Okay, this is how I want my FAM drawer to look. This is how I want a CXL host to look." And so there's work going on to create all those profiles as well. They've already started a number of them. They've got some that describe like a pizza box server and things like that. And so those are all helpful and well and good. But when it comes to CXL, we really need a more component-oriented approach that you can then gather together and validate different boxes that way. There's another spec that went in as well called the satellite management controller spec. This isn't for managing satellites. It was probably a poor choice of names. It's really a subsystem management controller. So remember how I said every PCI device out there supports things like PLDM and MCTP and RDE? The problem is there's no one spec out there that says, "You know, all that optional stuff in the spec, what's the mandatory stuff I need to do to build a product? Man, I don't have time to read all this spec." Well, that's what the SMC spec is. It's really Google and Meta and Samsung and HPE got together and put all our requirements for, "Yes, here's the MCTP individual commands you must support and how you have to do your EID and all the other individual little details of building a product." That's all in that SMC spec. We left the SPDM requirements out because we didn't have enough time to put in talking with the security project. But I know they have some sets of requirements as well. And so the next version of that spec, we want to make sure that it includes those as well. And then OCP CMS, I'm pretty sure everybody in this room has been paying attention to that. If you didn't attend the track yesterday, go through and watch those. Some really interesting information going on there, particularly later in the day. The last presentation I thought was pretty eye-opening as far as folks actually implementing the CXL Redfish protocols.

Last one is SNIA. And SNIA is the Storage Networking Industry Association. They basically did an extension of Redfish called Swordfish. It's like, 'Well, why does that matter for memory?' When we went through and we did the NVMe mapping and some other stuff, basically you can do most of that. In fact, you can do everything you need to for an NVMe device with Redfish. But if you start doing advanced things, you're going to need Swordfish. And really, it's just a few objects. But what they do really well is pooling. So I can create a memory pool in Redfish, but it's really just a bunch of devices. It's not classes of service. They don't have bronze, silver, gold, or anything like that. So what the Swordfish spec has is a bunch of NVMe block file and object support. And to them, it doesn't matter if that storage thingy is backed by spinning media or a bunch of chips. To them, it's a pool is a pool is a pool, and it's just backed by something else. So if you want more advanced concepts, go to SNIA. And so getting to where the answer is, this is kind of what it takes to build that system. 

And if you look at each one of those wires, whether it's I2C, I got some stuff from Open Compute, some stuff from PCI SIG, some specs from DMTF, some from CXL, depending on the wire I'm going over. And so this is how the industry is really working together to put everything together and make it work. There's a little bit of other stuff going on in the industry. We certainly expect JEDEC to come into this ecosystem at some point. They were too busy for the first round, but given that they're fixing to release some specs, we're hoping to get some of their time as well. And of course, work going on in UEFI as well to make some of this work.

So call to action, get involved. If you have any questions, reach out to any of us that are involved in any of this. Get involved in the open communities. And Sunfish could use a lot of work. So if you have any time and a willingness to code, OFA is completely open. You can just join it, just kind of like OCP, show up to a meeting and help contribute. Any questions? We have about one minute. Is that a lot to swallow? All right, thank you.
