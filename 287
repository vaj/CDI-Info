YouTube:https://www.youtube.com/watch?v=LK8ITlL3Veg
Text:
Hi, my name is Torry Steed. I'm a Senior Product Marketing Manager from Smart Modular Technologies, and today I'm going to be presenting some information about CXL memory expansion modules and the advantages they provide. So let's go ahead and jump right in.

So I think everybody's pretty familiar with CXL at this point. It's been a very popular technology for quite a while, but I do want to kind of level set with one quick introduction.So CXL, or Compute Express Link, is an industry standard open protocol, and it's intended to connect CPUs to either accelerators or memory expansion devices. And it's actually three new protocols, CXL.io, CXL.cache, and CXL.mem, and those are the three new protocols, and they all operate on the same physical interface as the PCIe bus does today. So same voltages, same pinout, everything's the same.But it's a new protocol that operates on top of it.

And depending on how you combine those three different protocols, you can actually create three different types of devices. They're not very inventively called Type 1, Type 2, and Type 3. And just for the purposes of this discussion, the one that we're going to be focusing on and the one that the industry is largely focusing on today is the Type 3 device that uses CXL.io and CXL.mem, and it's used to create memory expansion devices.So it's a way basically to put DRAM devices and connect them across the PCIe bus to a CPU, right?So that's what we're going to be focusing on. 

So what do those devices actually look like, right?So there's a couple main different form factors that are being introduced in the marketplace today. The one on the left is EDSFF, or Enterprise and Data Center Standard Form Factor, and those are E3, which is shown in the device, an E3 short or an E3 long, that's the dot S or dot L, or E1 devices, which are kind of like NVMe devices. And the E3s are kind of like the new version of a U.2 device, right? So that's a new industry standard. CXL is taking advantage of those form factors, mostly in the E3 form factor today, because it's best suited to add memory because of the power envelope and because of the space that you have in those devices.The other form factor that's emerging currently is the add-in card form factor, sometimes referred to as CEM, or C-E-M, Card Electromechanical. And those, there's a number of different ones that are becoming available. Smart has introduced some as well. But those are the standard kind of add-in card that you would see for like a graphics card or a NIC, or those types of devices that have been in use for a long time. And the nice thing about those is they're readily available in systems today. So if you can get a motherboard, off-the-shelf system motherboard that supports CXL, you can actually plug in a CXL add-in card today and start playing with it, experimenting with it right now, because these devices are available. So that's kind of what they're going to look like in terms of memory expansion. Obviously, there's more nuances than that. 

But where do they get installed? So the E3 and E1 devices typically get installed in the front of the systems.They're removable and ejectable. What we're seeing so far is because the infrastructure for CXL switches isn't fully developed yet, there are at least one on the market and more coming. And those will be coming in terms of the mid-plane, so that you'll be able to attach lots of CXL devices to the front, which we know that's coming. Today, what we're seeing is a small number of the front bay devices are being enabled for CXL. So one, two, three, four bays in the front of the system will be enabled for CXL, and that will support like that, those E3 devices today. Then what we're also seeing on the right-hand side, you'll see this is kind of where the add-in cards go. And the nice thing about that is we've got a lot of servers today that are configured for accelerators because of the AI growth. And one nice thing you can do is you can actually remove one of the AI accelerators and drop in one of those CXL memory expansion add-in cards.And so you can kind of trade off, do I need accelerators in this application? Or maybe it's a database application. Maybe I don't need graphics cards accelerators. Maybe I just want to maximize my memory capacity. And you can use CXL add-in cards in a system like this to do that. So it gives a lot of nice flexibility. 

So what applications are driving CXL? Obviously, the big one, the elephant in the room, of course, is artificial intelligence. Now, a lot of the memory, the heavy memory use for AI is driven by those accelerators. So you actually need memory attached to those accelerators. And CXL is not really a big player at this point in that space. That's still kind of the proprietary interfaces. However, a system that's either training or inferencing for AI does also need a significant amount of main memory. And that is a place where CXL can help.So we'll get into the details of that.But really what we're looking at is what applications need large amounts of memory and what applications need to eke out every little bit of performance from the main memory to gain advantages. And that does include AI. Memory, in-memory databases is another huge one.We also see some interest from video processing guys. Another one that I like is the biotech, the biotech industry. So there's a lot of data that's being generated in biotech, from things like DNA sequencing. And that data doesn't, they don't really want to store it necessarily for a long time. They want to gather it and process it quickly. And one way to do that is to store it all in memory, run whatever algorithms that you're looking to run, searches and things like that. And then it's much faster, obviously, if that can all be done in main memory. And some interest in the fintech as well. So all these kinds of applications will benefit from CXL.

So what are the main benefits? So I'm going to focus on three. I'm going to focus on three of the primary benefits that we get from CXL. The first is increased memory capacity.Next is reduced memory cost and then increased performance. So let's start with increased memory capacity. This is kind of the simplest one to explain, but let's look at an example.

So this is just an example of an AMD Genoa motherboard from Supermicro, right? So first, don't assume that any of the examples I'm showing support CXL today. You actually need to do a little bit of research. This is just for illustrative purposes. But this is an example of an AMD Genoa system with a single socket, and it supports a total of 12 RDIMMs today. And that's the way we've been adding memory to systems for decades now, right? And up until CXL arrived, that was the only way to add system memory. 

But now, once you've maxed out those 12 sockets, you can actually go forward and add CXL devices. So in this example, we could conceivably, in this motherboard, add two, let's say, of those add-in cards consuming 32 lanes of CXL. And that would give us an additional 16 DIMM slots, right?Because each of these add-in cards I'm showing can support eight more DIMMs. 

So that actually increases our overall system memory from 12 DIMMs to 28 DIMMs, which is an overall increase of 133%.

Another quick example. So this is a dual socket CPU. This is a Sapphire Rapid. So here, each of the CPU sockets can support up to 16 DIMMs, which is quite a lot. So you can get a total of 32 DIMMs in this system, 

but you can still benefit from CXL if you have a very memory-intensive application.In this case, let's say you have these bays in the front that are designed to support add-in cards of various types via a riser card, and those add-in cards are installed horizontally, right, with a riser card.And in this case, we could add up to four of these four DIMM add-in cards, because each one is a single width. So you could add an additional 16 DIMMs to this system, which would increase your overall capacity from 32 DIMMs to 48, which is still a 50% increase in the amount of total system memory. And that kind of is independent of what density of DIMMs you're using, right? So if you're using CXL, you're going to have to add an additional 16 DIMMs to this system, right? So if you're using 64 gig DIMMs, you still get, it's about the additional DIMM sockets that you're able to add via CXL. Now, you can do a similar thing with the E3 form factor devices. Like I mentioned before, each one of those can go in the front bay. Those are, because their capacity right now is about 128 to maybe 256 gigabytes, it's equivalent to each of those is like adding another DIMM. So if you're able to add four more, it's like adding four more DIMMs. Now, once CXL switches to the front bay and switches come out and you can fill the whole front of the system with CXL devices, you'll be able to add a lot more, but those are kind of still in development. So I'm really focusing on what you can get right now, what's available. So that covers kind of memory capacity, right? How we can, it's pretty straightforward. You know, you just, you add CXL and it gives you additional capacity in the system, which can give you all kinds of benefits. 

The next one that I'll cover is the reduced memory costs.So for this one, at first glance, it's like, well, wait, how does adding CXL reduce my memory costs? When I add a CXL module, I have to add a CXL controller, which is an additional cost to the memory. And that's absolutely right. If I'm just talking about adding 128 gigabyte module, let's say, if I add an RDIMM versus adding 128 gigabyte CXL device, the CXL device is more expensive. No question. So how in the heck do you get a reduced overall memory cost? 

Well, you have to look at it from a system perspective.When you do that, let's take an example. Again, it's the easiest way to illustrate. Let's say I have a single socket CPU that this one has eight DIMM sockets. And let's say I want to build a one terabyte system for that CPU. In this case, I would have to use 128 gigabyte DIMMs in order to get up to a terabyte, right? Because I've got eight sockets, I need 128. So that gets me to my one terabyte. The trick is, if you look on the left, I'm kind of showing a chart and this changes all the time write the exact ratio but the point is, as you go up in memory density on a single DIMM, the price per gigabyte goes up. That becomes especially apparent if we have to use TSV memory or through silicon via memory, 3DS memory, right? So in this case, I'm showing all the way up to 128 gigabyte DIMMs that are SDP, single die package parts. Even then, there is a price premium for going up to the higher density modules. And that will always be the case, right? If you're pushing the envelope getting the most dense memory it's going to have a price premium. So we can take advantage of that, right? So if we're knowledgeable about CXL and we know how to take advantage of that, we can instead design our system where those same eight DIMM sockets instead use 64 gigabyte DIMMs. So we're paying half the price per gigabit for those DIMMs. And then we can add CXL. We can add a single eight DIMM CXL memory expansion module, which gives us another eight DIMMs. We put 64 gigabyte DIMMs there as well. Now we're getting 512 gigabytes from the main memory, 512 gigabytes from the CXL, and we have a total of a one terabyte system again. But the catch is because our memory is so much more cost effective, we can actually save 40% on the overall system cost, right? For memory, the memory cost.That's pretty huge. And the nice thing is you can actually kind of turn the dial a little bit and you can even increase the system memory to a little bit higher density. Let's say you want to go to 1.5 terabytes, you can do that. Or you can go to less memory and save even more. So it gives you another dial when you're designing your system to trade off cost and memory. If cost is no object, great. But for most of us, cost is a significant factor when we're designing these systems, right? So that's how you can actually save money on a CXL.When you're building a system and it allows you to reduce the overall cost.

So the last one, kind of the biggest topic is the increased performance. So how can CXL help you improve real world system performance? 

Let's first talk about the elephant in the room and one way that it doesn't help, and that is latency. So when we, you know, if you've been paying attention to CXL before, I'm sure you've seen a memory pyramid like this, you know, main memory, so memory attached in an RDIMM directly to the CPU is around 100 nanoseconds of latency.CXL attached memory, because it has that controller, because it has to serialize and then deserialize the data, it is about double. It's about 200 nanoseconds. So you kind of scratch your head and go, okay, that's a big deal. I'm adding latency for my CXL memory tier, which is true. But I have a couple of points about that. The first is because we're adding, additional memory capacity. One of the things that we're doing is we're most likely in our applications, reducing the amount of times that the system has to swap with disk, has to go out and access or store data on the SSD. So we should really compare CXL latency to DRAM, but we should also compare it to NAND, because if we're avoiding those SSD accesses, then it becomes critical that we are able to reduce, you know, latency compared to NAND. So it's significantly less than, it's significantly less than NAND latency.

The next thing I'll talk about for, with regards to latency is something called the memory queuing delay. So in modern CPUs, we have many, many cores that are all trying to access memory through a relatively small number of memory controllers. And we can't really put more memory controllers on the CPUs because we're pin limited.At this point, each memory controller takes a significant number of pins. So it makes it very difficult to add even more. And as those cores get more and more heavily loaded, they start trying to access all, access that memory controller at the same time. And when that happens, we see a bottleneck, right? And so that bottleneck is kind of like cars when the number of lanes is squeezing down and you're, and you're limiting your traffic. 

So how does CXL help with that? Well, you can actually add, add memory to the PCIe bus using CXL, right? That's what we talked about before. So by doing that, you're essentially giving the cores and the CPU alternative paths to access memory. So it's almost like the, the side, side highway right next to the freeway that lets you access memory as well, right? So, so you, so you can have multiple, you're adding more paths. Overall, you're adding more bandwidth to access memory. So that can alleviate, that can alleviate the latency hit as well, because when the cores get heavily loaded, the most dominant factor in accessing memory or when in the memory latency becomes that queuing delay. So adding CXL can help alleviate that. 

One last thing on, on latency. So right now we're looking at CXL memory devices attached directly to the CPU, right? And so that adds latency because of the CXL controller and because of the serialization and the protocol that has to happen.When we get to the next phase in CXL 3.0, or with those CXL switches that we've been talking about, you actually are going to have even more latency, right? Because the switch is going to add latency.So I just, that's something to keep in mind. I think there's ultimately going to be maybe two tiers of memory. There's going to be direct attached memory that goes directly to the CPU for via CXL. And there's also going to be switched as well, which is going to be larger for sure. Larger amount of memory, but more latency.So we're getting kind of these more and more granular tiers of memory as we add these different mechanisms for using CXL. 

Okay. Let's talk about a real world example for how CXL, how all these benefits can actually improve the performance. So this is an example taken from a really good white paper that was done by Micron and AMD. And I'm going to kind of paraphrase it, but I really encourage you, there's a link at the bottom. You can Google the CXL memory expansion, closer look on actual platform by Micron and AMD. It's a great white paper. But what they did is they took an AMD processor with 128 cores and it has 12 DIMM sockets available. So they fully populated it with 1296 gigabyte modules, giving them a total of about 1100 gigabytes, right? So a little over a terabyte of system memory. And then they added four Micron CXL E3.S modules.Each one was 256 gigabytes. So that's, four additional modules gave them another terabyte of memory attached to the CXL. Then there's eight storage devices, right? So that's kind of the test setup. 

And then they ran some applications.And so looking at the one on the left, the first application they ran was an in-memory database, an SQL application. And in this application, it was only consuming, even without the CXL, it was only consuming about 50% of the bandwidth. So it's not, it's not a memory bandwidth limited application. It's capacity limited, right? You want to put as much of that database into memory as you can and minimize your IO accesses. So by adding the CXL devices, they actually reduced the amount of SSD paging IOs that they had to do by, you can see here between 44 and 88%, which improved the overall performance of the database by 23%, which is pretty significant, right? So that one is a performance benefit purely by increasing the, the memory capacity on the system using CXL. The next application, the machine learning application was an Apache Spark application. And this one was also sensitive to memory capacity, but it's also sensitive to latency, right? So it's got a lot of rapid, rapid accesses.And so you would think that maybe CXL wouldn't give you as much of a benefit, because we talked about that, that larger latency previously. But it turns out the, the capacity was the dominant factor. So having that additional memory capacity in this application still generated more than double the performance compared to DRAM only, which is massive. And then the last application is a high-performance computing application on running Cloverleaf. This one was memory bandwidth intensive. So this application did not exceed the amount of memory that was in the system, even without CXL attached. It was within that memory envelope. But what we see here is that freeway analogy, right? So you see they were able to increase the memory bandwidth by adding CXL by about 33%, you know, that additional highway, and that netted an overall 17% performance improvement purely because of the increased memory bandwidth, which I find really interesting. So again, great white paper, go check it out.There's more and more information coming out all the time with these types of applications, showing the real world benefits that you can get right now with CXL supported systems and those CXL memory expansion devices. 

So the last thing I want to talk about is kind of future looking a little bit, right? So what we've been talking about so far today is a situation where we're just adding memory directly inside the server, right? CXL memory inside the server. You get all kinds of benefits from doing that. And I think that's going to continue.Going forward. But obviously, in the future, we are going to be disaggregating memory. And so I think those two are going to play very nicely together. I think we're going to have CXL in the server to really maximize both the capacity and the bandwidth that you can get from memory. But then I also see the use case looking forward to the CXL 3.0 timeframe and when those switches become more readily available, where we're going to disaggregate memory as well. And that is going to provide memory pooling.Memory sharing, where you can avoid stranding memory, which I think is an issue, but maybe not the critical issue. But those two, the point here is that those two are going to coincide. You're going to have CXL memory in the server as well as CXL memory that's disaggregated in a dedicated appliance too. And it remains to be seen what kind of benefits that we'll get from that combination of those additional tiers. So looking forward to seeing that.

Okay, so that's pretty much it. For additional information, you can contact us on our website there. And again, my name is Torry, and I'm happy to answer any questions you might have.
