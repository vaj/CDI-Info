
Well, thank you guys for coming after lunch. I know it is actually a hard one to make after lunch sometimes, so I get that. My name is Andrew Maier. I'm with Eideticom, and today we're going to be talking about P2PDMA, which is a peer-to-peer framework that's open-sourced and upstream in the Linux kernel. I'll get into why we're talking about it today, but basically some real-world examples is kind of the key point to this, but there are some significant upgrades as well.

So, a little bit of an outline of the talk. I'll give a little bit of a background of what P2PDMA is, for people that aren't aware of it, talk about the framework updates that I hinted at very subtly there. There are some significant milestones that have happened. Go over some of the user space interface examples, talk about some testing, and then some potential use cases, you know, SSD to SSD, NIC to SSD. Where can we use this in the future?

So why P2PDMA? That's kind of the first question. This came out back in 2018, and quite honestly, it hasn't seen a lot of traction until now, but I think it makes a lot of sense. PCI devices are just getting faster. We've got PCIe Gen5. PCIe is no longer the bottleneck of transfers now. Really, there's a lot of applications that are kind of looking at the fact that hey, DMA access can be inefficient. You might have a bottleneck on your DMA. You might have a thousand things running on your server that's using up all of your bandwidth of your DRAM. So how do we offload that to a different device? How do we move that more efficiently? And how do we do it in an upstream way? So you can kind of think of this as an equivalent that's the upstream equivalent to NVIDIA's CPU direct. It's just a more open source upstream general version of something like that. And of course, this is now supported on most modern CPUs and modern motherboards. So we've kind of seen a shift in the industry over the last eight years where it wasn't the most efficient path. There was a lot of caching involved on it. Now the vendors have started to kind of clean up that path. So everyone seems to be aligning and getting more on board there.

So what is P2PDMA?  Okay. Well, quite frankly, it's in the name. It handles peer-to-peer transfers to PCIe, between PCIe devices via DMAs. And it's an open source framework that lets you do that. So the traditional path... Oh, see, I should have brought an actual laser pointer. I'm going to try it with the trackpad. We'll see how it goes. The traditional path to have a DMA go between two PCIe devices is actually to go via the host DRAM. So it's going to do a bounce buffer on that RAM. You're going to have a transfer that's a write or a read from the SSD going into the RAM, and then a second one going into another device. That's fine. It's normal. It works. It's secure. But the more efficient way might be to just take that RAM out of there altogether. Why can't we just talk directly to devices? So that's what P2PDMA does. It allows you to get a buffer and a handle into different devices out there that will support it, like SSDs or accelerators or NICs, and transfer that data directly peer-to-peer. So it eliminates all of your RAM usage. And of course, the main part of this is we can't do any of this if it's not performant. So it won't make any sense if we can't get our full PCIe bandwidth or if we're not getting some kind of savings out of it. Obviously, eliminating host RAM can be efficient and is a savings, but you also want to gain benefits in other ways, like latency as well. So how do we accomplish this?

Well, since P2PDMA, it's essentially just an API. It's a very simple API for registering and managing resources. So with P2PDMA, any PCIe BAR can be mapped, but partial or in full, to be a peer-to-peer-capable memory. So using that kernel space API, you can do simple things like add the resource, check the resource. You can register your memory for other applications to use. The key part here is that it can now be used in both KernelSpace and UserSpace applications. So once a device provides and publishes that P2PDMA space, some other kernel, some other user space driver or whatever it is can access that. And then another point here is P2PDMA is actually already supported in upstream NVMe. This has been around for quite a while; it was already in the NVMe spec. I believe it was 1.2. I actually, you know what, I have a slide on that too.

So, spec 1.2 allowed it through NVMe controller memory buffers, and we heard a little bit about this with Donpaul and AirMetal just before the break here. But NVMe controller memory buffer is, simply put, it's just a BAR of PCIe memory exposed by the NVMe device. It was introduced in 1.2, so it's been around for 10 years now. But it's kind of obviously had some upgrades since then. So, the initial case, it was just simply a BAR. Now, there's a bunch of registers to handle stuff like virtual machines and IOMMU access and stuff. So, basically what happens is because it's supported for P2PDMA, all NVMe devices that have CMBs, their CMBs automatically get registered as P2PDMA devices, assuming that P2PDMA is enabled in your kernel. That is going to be a tongue twister; I'm going to struggle with P2PDMA saying it a million times today. I'm gonna try, but anyway, so yeah, the most important one that I want to point out here from some of the upgrades is in NVMe 2.0, there was a thing called CMBMCS introduced, and that is basically going to allow, along with a bunch of other registers, that's going to allow virtual memory access. So, you can have a virtual machine that will just pass in your PCIe device, and you'll be able to know your addresses internally for that, so your P2PDMA structure will still work.

So I'm gonna take a step back for those of you that aren't aware of what Eideticom does, just to introduce our product from that side, and kind of how it fits into P2PDMA, and then I'll tie it into, wow, there's timing on this slide, that's annoying, sorry. And put that into how it fits into the P2PDMA framework. So as Eideticom, we produce what's called a computational storage processor. So again, Donpaul kind of introduced the idea of it, and there were some talks this morning, but there's an idea of a computational storage device and a computational storage processor, the two separate things. The devices typically tie themselves to flash, they tie themselves to some kind of storage, versus processors just have the processing power in them. So Eideticom, we focus on the processing side, so we don't actually tie to storage, but we provide some kind of acceleration offload. But the key point to that acceleration offload is actually that we hide behind the processor. We hide behind a NVMe front end. So we also provide an NVMe CMB. So you can kind of see where I'm going with this, but natively we get P2PDMA support because of that CMB. So that's already enabled by default for every build that we do. And that means that essentially access to your accelerators on the data path can just be easily integrated using P2PDMA integration tools.

Okay, so recent milestones. So this is kind of the timeline of what's been happening in the P2PDMA world. Like I mentioned, 2018 with Linux 4.20 was kind of the first time that they got it introduced. And more recently, as in 2023 and 2024, we had, yeah, 6.2, we had the user space interface finally upstreamed. And I'll get into what that actually means for that, but it means that user space applications can now make use of full P2PDMA regions very simply. And the most recent, and I would argue, kind of the most significant and important is actually Ubuntu 24 has it enabled by default now. So if you go up and spin a Ubuntu 24 server, you will have P2PDMA enabled by default in that kernel. That means that if you plug in a no-load, if you plug in a KIOXIA CMB, something that has a CMB with it, you will get a P2PDMA space that you can use.

So, let's look at what the integration looks like. From the application side. On the kernel side, you have full fine-grain access to this. I'm not a kernel developer. I don't think I ever could be. I've tried many times, but not successfully. So yeah, so there's a lot of fine-grain control access to this that you may or may not make use of. But drivers can, it's actually quite simple—where drivers can simply register a P2PDMA resource by using that add_resource function. Then they can publish whatever memory they want to publish. And this is an important note: You do not have to publish your entirety of your buffer. If you only publish part of it, that other part that isn't published gets reserved to your kernel driver. So you can still use it as P2PDMA space, but it's reserved to you. What can happen then is a separate application, the kernel space application may say, "I wanna use that P2PDMA space. I wanna find some memory." So it can go ahead and probe the system and say, "Okay, tell me how many devices are here that had P2PDMA. How far away are they from me on the PCIe tree? What's the most efficient one to use?" Or you can just say, "Hey, I have a device. I know that I wanna talk to the no-load computational storage processor. Does it have P2PDMA?" And then, more importantly, you can then take it one step further and say, "Okay, allocate some memory for me." So then you have access to allocate and free and use that memory however you wanna use it.

The user space side, so that the kernel space, the user space side has basically been static since 2018. It hasn't really changed too much. There's been some additions here and there, but the most recent one is actually the user space interface. So this was the one that was upstreamed in Linux 6.2. And what this does is it lets any user space application make use of it. So yeah, after we went through many failed iterations of trying to upstream code, which is essentially what happens when you do it in Linux, we came up upon the sysfs approach. So you now get a sysfs entry, like a directory that has a couple different files here for you to be able to access anytime that you have a P2PDMA device registered. So in this example here with this screenshot, I've got a no-load card, which is actually NVMe 10. And this is just Ubuntu 24 running. If you look at Sys class NVMe, NVMe 10 and P2PMEM, you get access to an allocate, an available, a published and a size. So you can see how much P2P Memory it's advertising, how much is published, how much is currently available, which is important if there's more than one application using it. And then finally, I think the most important one is this allocate file. So this is not an informational print. That is actually the file handle that you get access to, to use that P2PDMA device. And all you have to do is simply mmap that file and you will get a pointer to that P2PDMA device. And I'll go into a little bit more of an example here.

So let's consider a pretty basic setup here, where we just have two PCIe devices. One of them is an SSD in this case, and one of them is a no-load, but it could be anything with an enabled P2P device.

So let's say we want to transfer data from that SSD to the no-load to do some processing on it, whether it's, I don't know, compression or analytics, or one of the other various products we offer.

But let's say we want to do that data transfer; we'll first have a transfer that goes from the SSD to the no-load, and then we'll have to do some kind of transfer that tells the no-load, 'Hey, your data's in your CMB, it's local to you; I want you to do this process on it.' So what does that actually look like from the code perspective? I think that's the interesting part; I want to dive into here.

So again, we have that sysfs interface, right? We have that access, we can read it, we can say this is what our size is, this is what our published amount is, this is what our available is, we can check all of the metrics on it. And like I mentioned, for the allocation part, all we have to do to allocate that memory is we open a file handle to that allocate location, and we do an mmap on it. So this data pointer right here, that houses a pointer to that P2PDMA space. So you can now go and use this in any place, any place at all that you have a buffer that is accessing a DMA. The caveat to this, which I did not mention on the slide, is that you want to be careful with how you access this data. So accessing it via DMA from another SSD, perfect, great, fine. If you want to do load stores from the CPU, now you're not so efficient. You have to remember that this location of this buffer, even though it looks like a normal pointer to main memory, is not in main memory. This pointer is in your P2P device. So you can absolutely do a memset or memcpy too, but just note that it will be slow. That path is not close to the cache, right? That's going over the PCIe bus. So again, think efficiency would be another PCIe device accessing that, not necessarily the CPU. But yeah, so this data variable now contains a pointer to that buffer. So you can use that anywhere else that you would use a buffer in your code. So think read. You can do a read with an SSD with a file descriptor, that's on that SSD, and you can just point in that data buffer as the buffer, and it will use that buffer instead. You can do an NVMe ioctl, a direct ioctl call, whatever it is that you want to run, that will run just on that buffer device now.

So, what does this actually look like then, if we want to do this transfer? Well, the SSD sending to the no-load CMB is actually going to be a read from the SSD. It's kind of backwards in the logic of it, because you think the CMB is actually ingesting data, but the SSD is actually pushing data via its DMA to the CMB of the no-load. And you're going to do that by just issuing a read to the SSD with the pointer of the P2PDMA.

The second command is a little bit more interesting. So this is the ioctl command. It could be done via just an NVMe write command, but this is just how we at Eideticom do no-load access, is through the ioctls. But we're going to trigger a NVMe write command, which is now, again, backwards logic. We're going to say, hey, no-load, you have memory that we want you to take and do something with. So the input to that data is now going to be the pointer of that data pointer, which is the same one that we passed in the previous command. So we have, again, that whole process of it from the whole application, and we have that data pointer, which is pointing to that region of memory, just like any other data pointer. And then the key point to this is the no-load is actually aware of its own addresses. So this would be the same as if you had an NVMe device. This isn't specific to no-load. This is any device that supports the P2PDMA framework. You have to be aware of your addresses. So if an address comes in and says, hey, I want you to do a DMA or I want you to do a PRP list poll from here, you have to look at that address. And if you know that it's your P2PDMA space, you're actually going to do an internal transfer. So you know that it's local to you, you can handle it slightly differently. But the key to all of this is that you're not actually breaking any security from layering issues from the Linux kernel. So everything is actually being controlled by the CPU. So all of these commands, everything from the SSD read to the no-load write to whatever it is, everything is being controlled and issued by the CPU. It's just that the data flow is now either internal or over the PCIe bus. That's all.

So let's look at a test that we did with this. And I know I'm kind of jumping a little bit back and forth with some of this stuff. I did actually do quite a bit of work this summer on just testing the P2PDMA framework. And I had gambled on STC and I was working on stuff in May, hoping that I'd be able to publish some numbers, but I can't yet. We're still working with some partners on some things, but the ones that I can publish, I'd like to... That's what I'm going to get into here. But essentially, I wanted to test the framework and make sure that there weren't any PCIe bottlenecks to see what the latency would kind of look like and to see if it would just work in the first place. So let's consider a test case where in this specific example, we have four SSDs all sitting on the PCIe bus beside a no-load card, because that's exactly what we did in the lab.

We want to try to generate and test the P2PDMA framework against... well, basically pressure test it.

So I'm going to have all of these SSDs writing all of their data to the no-load CMB at once. Now, the way that I did that is I don't want to reinvent the wheel here. So FIO is a great tool for doing any kind of input/output access to drives and block devices. The nice thing about P2PDMA and that user space framework is that there really isn't any code that you have to write to do that. So I can just use FIO. FIO with the `iomem` variable, which was actually built into mmap from any file descriptor you want in FIO. And that will just simply work with the new user space integration with P2PDMA because that's how it works. It takes in a file descriptor and you mmap from it. So all we had to do was generate an FIO config here that does read commands to each of the SSDs and it points the `iomem` variable right at that allocate file descriptor. If you remember the one from the previous slide, that's it.

So running that, we actually got some interesting results. And these are the ones that I can share. We saturated the PCIe bus actually on the no-load side. So I had a gen four by eight set up that was running at the CMB, I believe it was just on a block RAM device for that one. So we actually saturated the PCIe bus of the gen four by eight, and not consequently. It was actually the saturation of the SSDs as well. So each one of those was doing four gigabytes per second. You obviously have to take into account that, you know, if you want to scale this up a little bit further, you're going to have to start looking into things like HBM or higher PCIe, like PCIe gen five by eight, gen five by four, whatever you need to run. But that is now our limit, not the SSDs pushing over PCIe infrastructure, which is quite interesting. So I know these, these words are kind of backwards. The whole write versus read in nomenclature is a little bit weird, but the right side was actually the SSDs writing to the CMB and the read side was pulling the data off. So doing NVMe writes, which is the opposite, but yeah, it's, it's reads from the CMB perspective. And of course, we verified all this by internal no-load counters. So we looked at our PCIe traffic and then saw all this, but the more important part, and the really interesting piece, was for this specific test, because we maxed out those SSDs, the performance was actually identical when we used P2PDMA or the host DRAM, it was, it was completely identical. The latency actually, I can't give you the latency numbers yet, but they were, they were also within noise range. So they were, they were quite interesting.

So let's look at a more realistic use case of this because nobody's going to want to run FIOs from their SSDs to CMBs and just leave it there. So, what, what can you actually use and do with this?  Well, let's consider the example where again, it's the no load in the path, but you have a host application that is trying to compress data or trying to do some analytics on the data. So you're going to send that data down to the hardware as an offload anyways. In the traditional path, you would send it to that hardware accelerator. The accelerator would then pass it back to the RAM and then you have that kind of double bounce buffer thing going on. And that's fine, but it's not the most efficient. But in the P2PDMA path, that second transfer out of the accelerator is actually just internal and it just goes right to its own CMB. So you get compression data being written right to the no load CMB and then you, the CPU can then say, "Hey, SSDs, I want you to go pick up your data. It's not in host RAM. It's actually in this CMB that we're, we're going to tell you about." But again, the main key part is the host. The host is orchestrating all of this. So it is still the one saying, "Okay, I'm going to transfer data to the no load. I want you to put it into the CMB and then I want the SSDs to pull it." So it's, it's really easy to coordinate on, on the CPU side. It's just, it knows the order of all the actions. So one of the main benefits here that I haven't really touched upon is we have gotten rid of this secondary, this bounce buffer, right? So we used to write that memory right to RAM, right to host RAM. And then read it kind of right back into the SSDs. So now we've gotten rid of that. So we are now lower latency. So we essentially got rid of one transfer completely. That that transfer to the SSDs is obviously still happening. But the one that's internal is, is way faster than going to host memory. And again, I can't get into the specifics just yet of how much faster, but I'll, I'll hint at those in the future anyways.

And you can think of it to the exact same. So let's, let's look at the exact opposite approach. Now you want to do decompression or something else. You can now just reverse your path. You can see, you can tell the SSDs, "I want you to write your data into the no-load CMB." And then you can tell the no-load, "Hey, I've got a bunch of data. I want you to process, throw it into the accelerator and then bring it back to the host." Exact same thing, same idea, and exact same savings on the latency side. So we've gotten rid of that bounce buffer on there.

Okay. And this is where the bouncing around is kind of going to happen. But I want to look at a—a couple of different kinds of use cases because obviously, being a computational storage processor company, we are keen on looking at computational storage applications. However, that's not the only limiting point for P2PDMA. It can kind of be used anywhere really. So if you look at a data capture appliance, this is something that we actually do with a lot of FinTech companies now where they'll have a ton of data coming in on a network card that they want to store to SSDs. But typically, they don't just want to store the data. They want to do something to it. So they want to do some kind of filtering, some kind of pre-metadata analysis, or actually—in all of the cases we've seen compression, which is where we've obviously seen our initial foot in the door.

So, if you look at the traditional sense, what's happening is: data comes in on the NIC, goes to the host memory, where it's released to the application.

The application then does something to it. Like I said, compression; generally, that's what they do. They do some kind of LZ4 or the like, or analytics.

And then it would then write the data to the SSDs. That's the classic case.

But if we add the hardware offload into the case with peer-to-peer DMA, this is where it gets really interesting again because now we consider the fact that we can actually just hop from the NICs to the NLU to the SSDs without using the host memory at all. And of course, in the end, there's some kind of latency benefit from it. Now I know this is kind of strange because we're adding in a second device or a third device in here to those ones that already has two. But I promise in the end it will make more sense. So let's consider the setup where we have an NLU, SSD, and network cards.

We can then DMA. So, in the capture case, we're going to DMA the data directly from the NIC to the no loadCMB, so that exact same DMA is still happening. The CPU is still saying it's just that data pointer thing that I talked about.

So the data lives in the CMB. The host now tells us, “Okay, do your offload to it.” So do whatever they want to do to it, which is again, first time was compression; generally, it can be anything.

And then that data gets stored to the SSD. The main benefit here is actually on the compression side. They were taking a lot longer doing the compression, using a lot more resources on the CPU.

So we have given them a latency benefit from the overall transaction side of, of capturing that data. Of course, it also reduced the CPU load from, from doing that calculation. But the main part with the P2PDMA side is that we got rid of the DRAM usage. So it's, it's able to just go directly from there.

So where do we go from here? Those are again, my biased points of what P2PDMA could be used for. But really, if you take any two PCIe devices, that live on a bus that you have to transfer between these, could be used, so that the applications are kind of endless. And I know there's a, there's a huge focus in our industry on cough, AI cough, if anyone's noticed. But if you think of something like AI where it has an inference or a training engine, basically, you're transferring a lot of data from a NIC to a GPU or from a storage to a GPU or from a GPU outwards to it, to the NIC, all of that traffic can be done with peer-to-peer DMA, and all to enable it, it's, it's remarkably quite simple. You just have to be able to have devices that support it. So if you have an NVMe CMBs or if you have a GPU with a large BAR that you can use as a sync, so there's quite a few applications on the AI side, on the training side, as well as, as well as the inference part. But there's obviously anywhere you have any kind of hardware offload of any PCIe device, this just makes sense. So you put in some kind of data analytics, some kind of analysis, some metadata gathering, capture, whatever it is, query analytics that you're doing. You can have those transfers happen in peer-to-peer any kind of data capture obviously that I showed before the case that I was mentioning about AI where it goes from NIC to GPU kind of sounds a lot like, or Jeep NIC to storage, I guess, kind of sounds a lot like GPU direct to storage kind of is, it's just the upstream. Open source version of it. So anytime that you have a transfer that's going from a, from a PCIe device to another PCIe device, it just kind of makes sense.

And that's kind of mostly all I got for you guys today. But the, the key point that I really want you to take away from this is those milestones that I kind of talked about. So I mean, P2PDMA has been around, like I said, for eight years or whatever it is now. But we're really just seeing a lot of traction lately with that user space interface. It's being upstreamed in, in 6.2. And again, if you, if you use Ubuntu 24, you you're using P2PDMA, it's, it's in there. So you know, plug in, plug in a device and it'll just work. That's that's kind of the key points. We as a company that's related to computational storage have been seeing a lot of interest lately of people wanting peer to peer DMA. My job now is to figure out why, but that's kind of the interesting point, right? There's no, there's no limit to this. You can kind of think of anywhere where you want heavy DRAM usage, you need to offload this somehow. So yeah, again, we, we looked at some testing, we looked at the full bandwidth. The other point I was going to make, so integrating into FIO was, was trivial, just using that iomem variable in the user space interface, which you could do in Ubuntu 24. The other tests that I talked about with the NIC did involve some changes that were currently upstreaming now. So they're not available. They're actually in the RDMA driver itself, but that'll support things like perftests. So perftests already allows you to do mmapping of P2P that's been upstream for quite a while, but the, the lower level drivers of the RDMA and the NICs have to have to support it. And then, yeah, we looked at a couple of use cases. I mean, these are, again, these are just our use cases for computational storage, but I'd invite you to get creative and see, you know, where it would make sense to necessarily offload from a, from your perspective.

So yeah. So again, complete that upstream and then do more of this and push and ask more companies to get more involved. That's kind of the key here is, you know, this infrastructure is now upstreamed and supported and ready. We just need more companies to sign on and release drivers and stuff that have access to this. And of course, we've been working with a couple of companies that way as well. And then of course, just continuing to test. So anytime we have modern new CPUs and motherboards, typically the vendors are getting better at this. They're testing it on their own. But we want to make sure that everything supports it going forward.

And I kind of hinted at it. So I did do a little bit of work this summer on it. I did release a blog post for that shows pretty much the results that I talked about today. But I would keep your eyes peeled in the future. I'm hoping when this partnership that we're dealing with now ends, I'm hoping we can publish a little bit more. So that would be stuff related to NIC transfers. So either NIC directly to storage, NIC through hardware acceleration. Those are kind of the numbers we're looking at now. And I'm hoping to compare latency and bandwidth numbers on the CMB that way. So yeah, keep your eyes peeled. That's it. So does anybody have any questions? Yeah.

So slide 16, when you do the I call, why you minus the number of blocks by one.

Yeah. So the question was in slide 16 here, why do we subtract one off the number of blocks here? That's actually just how NVMe works. So NVMe is a zero-based protocol. So when you say transfer zero blocks, it's actually one. That's yeah, that's just how the driver works. They do that with, I think, starting LBA as well. It's all zero-based. Any other questions? Yeah.

Another question.

Yeah.

So, when you transfer data from NIC to storage using this, does the NIC have to be a SmartNIC? Because I often need to read the package for compression, decompression, encryption, that stuff.

Yeah. So the question is, sorry, I'm repeating for the camera and everything too. The question is: when you go from a NIC to storage, from a network device to storage, do you have to use a smart NIC or something like that? Yeah. Do you have to use a smart NIC or something on that approach to do the compression? The answer is no, but it's kind of up to the vendor. So if, you know, Mellanox or NVIDIA decided that they just wanted to add in driver support for P2PDMA, they could absolutely just start writing their network interface code straight to the SSDs. If they want to do some kind of compression or otherwise, then yes, you would need a smart NIC or a hardware offload in that case. Right.

So the question is, who would basically control the network stack? Is that correct? Yeah. That's a struggle with it. So, if you are just purely doing that, you would probably need to transfer some of that data to the host. So, it would make sense to me. Or use a smart NIC in that case, or have some kind of subprocessing on a, on an offload module, or or whatever you're doing.

Do you see a need for that, uh?

Yeah. In that case, I mean, so that the question back and forth was, was, is there a need for, for something like that? Um, I think that, uh, adding a smart NIC into that equation kind of already makes sense, right? Because you're, you're doing your software stack already. You could just be doing that stack on the NIC itself or, or a DPU for that matter, or, or whatever processing unit you have. Yeah.

Oh sure. So the question is how do you know when the DMA is completed if you have to do an fsync? Is that what you said? I'm saying, um, yeah, so you don't have to, uh, typically when you do an ioctl call, they're blocking. Um, so again, the, since the CPU is controlling everything, you know, at the point that when the ioctls returned, the data has actually been transferred. Uh, I mean that's up to, you know, the firmware of whoever's like inventing the no load or whatever, or device they can't return to say, "Hey, I've transferred the data" when they haven't actually done it yet. So yeah, but the short answer is since the CPU is the one controlling everything, it's, it's actually quite simple. They don't have to sync. Yep.

The one thing I don't agree with you, is that PCI Express can ever be fast enough for some people.

Sure. Yeah.

We're working on gen five. And we use 32 lanes because some chips just give us all the 32 lanes.

Yep.

Um, and when you go back to the slide where you talk about four SSDs, that's actually an interesting use case because one of the things we start seeing is the SSDs seem to be peaked at four lanes, right? One, two, three, four, 16 lanes. That's your card right next to it. We're using similar cards.

Yep.

So do you see any oddities when you work with, oh, you see the app? Oh, okay. So you used AMD for that.

Yep.

Because we gave up on using Xeon for those kinds of setups.

Right.

There were some oddities we didn't understand. And I just wondered whether you, you, you were able to identify the oddities.

Absolutely.

It worked like a charm with this AMD app.

It did. Yeah.

We had glitches, performance glitches, with Xeon.

Yep. So, the question is, did we see any oddities or related, basically?

Bifurcation. Sorry.

Bifurcation. Yeah. Yeah. Yeah. Did we see any oddities? We had some oddities with, with Intel cores, and, and why did we choose to use the CPU that we did? Absolutely. We did. We ran into so many roadblocks and so many oddities. I would say it kind of started out; since 2018, we've been working on things like this. So Intel CPUs in particular back then were really bad at doing peer-to-peer traffic. That was just a known fact. I think AMD was ahead of the curve there. In fact, AMD was at Intel. I think they had a very small cache between their two PCIe devices. So if you tried to transfer data, it would just essentially crawl to a halt or just not work. And then you'd get random errors back on your PCIe device. So we have seen some, I would say we've seen some improvement lately where they're not using switches anymore to do that. The previous answer to that was yes, we would use a PCIe switch that would alleviate all the issues. But AMD kind of has been the best approach for this so far. I think the new, I don't remember the name of the new Intel's now, it's, which lake are we on now? I can't remember. It's yeah, some lake. The latest versions, I think it was from Ice Lake onwards. We saw quite a bit of improvement. I can't talk too much more about the scaling of it. We haven't actually done a test on Intel's that was more than I think five or six devices writing at once, but the hope is that they're, you know, they seem to be innovative. I think that's a good thing. Yeah. I think they're getting on it, but yes, we went through some, yeah.

Two things. One, if you care about bifurcation, look at PCIe NI, which is in development in PCIe today. That's in the protocol; so if you're a member, go play. And also, in PCIe now, you have variable maximum packet lengths if you aren't limited to, you know, the number of packets.

The question is, can we take advantage of variable TLP lengths, essentially? And I actually wasn't familiar with...

It's actually not TLP.

It's not TLP.

It's actually packet size. MTS is. Things get limited down to 256 bytes.

Yeah.

Or you go to 4K if you don't have to talk to the root anymore.

Yeah, I see what you're saying. Yeah, if you can take advantage of dynamic, dynamic packet sizes by not talking to the root, which is the smallest version of it. That was a big pain point for us. I'll be quite honest with you. Seeing a 256 or, you know, God help you, a 128 show up as your MPS size definitely limited our performance. I didn't know there was actually dynamic approaches, but we have seen some significant improvement lately in root ports actually supporting higher numbers.

That's why we want PCIe 6.

That's why we want PCIe 6. Perfect. I like the plug; it's perfect. Yeah, to be honest, I think we would just be able to take advantage of that because it's just over the PCIe protocol. So, yep.

Another comment goes with applications. There's a huge thing going on, which some people start calling confidential computing and confidential AI. Obviously, any data, even if it's encrypted, with today's standards, ending up in a CPU in main memory, is much easier to look at and, let's call it, hack as a figure of speed than if you transport it between two SSDs.

Absolutely.

Different physical access. You need to watch and listen to the PCI Express class, and I would imagine that this is one of your interesting applications.

That's a very good point. I didn't actually consider that one. So, the point made was that there's a security impact of being able to avoid the CPU. And that's a new path altogether. You can't necessarily hack it or listen on it as easily. You have to listen on the PCIe bus. Wholeheartedly agree. There's a whole point of confidential compute and ephemeral keys being more of a thing now, too. So, that is absolutely a really good use case for it. Yeah. Any other comments? Yeah.

Yeah, so this seemed really easy to do between devices. So, is there a question of trust, and who's allowed to DMA between devices?

Yeah. So the question is, is there a question of trust or who's allowed to, what's the permissions look like from DMAing between devices? It's kind of at the kernel level, is the answer. So you as the driver, as the provider of the P2P DMA space, again, you can reserve it for yourself. You can't go in and say that no one else, only application X is allowed to use this at the moment. So if you publish memory, anything can really access that memory. Once, of course, you have that memory allocated to you, nothing else is going to get access to it, just like in main memory. But if you have two competing applications, for instance, that are trying to allocate the entire amount, one of them is going to get starved. So yeah, the short answer is you can't really prevent it, but you can reserve it to your own applications if you need to. Yeah.

Oh, for the PCIe SIG.

Oh, okay, interesting. So, there'd be something on the PCIe bus that'd be sniffing the traffic, saying, "You can't do this packet." Okay.

It's using DMTF SPDM to do that work.

Yeah.

You typically do need some kind of bus management controller to deal with it.

Right, yeah. I'm looking at it, but my answer is purely from the Linux point of view, right? So, there's nothing in the Linux standard that would necessarily prohibit you from doing that. But there's absolutely stuff on the hardware side that you could do. That would make sense. Yeah, any other questions? Oh, yeah.

Did you have to turn off the access control settings on PCIe?

Yeah, so the question is, do you have to turn off the access control, or the IOMMU for that matter? I can answer both. The answer is no, not anymore. It used to be. We used to have to turn off the access control registers and security for it. Now it's just kind of more supported on native architectures.  For the IOMMU answer to that, that was the point where I made where it was NVMe 2.0 where they introduced those registers. The, if you're familiar with the way address translation works, essentially you get an address translation table when you create a virtual machine. Your addresses will change. So that NVMe register was invented to circumvent that. So when you start a new machine, it'll say, "Hey, this is what your address is." So the device knows whether or not, you know, the transfer is gonna be internal. So ACS and IOMMU can, well, ACS, they can both stay on. Yeah.

I'm sorry, related to what you just said, about the virtual access, CMV, ESC. So, does that mean you need ATS on your SSDs or your...

Sorry, ATS or ACS?

ATS, address translation.

Oh, address translation. Sorry, the question is, do you need ATS on your NVMe devices? Yes, so your device needs to be aware of what its address is. That's kind of the, it's not really a translation table, I would say. So in the Nolo case, the way I would describe it is we expose a BAR, BAR two or BAR four that has a specific address assigned to it. When we get assigned that address, we also get that NVMe, what's the name of the register? The MSC, MBS command. When that comes to us, we as the firmware take that, and we say, okay, we know that anything related to this address is internal to us. So that's our address translation table, is that one lookup. So yes, every command that we do when we send out data or pull in data, we have to look at that to compare it. So yeah, your devices do need to support that. Yeah.

When devices are across a CPU complex, yes, if you have to go through the room complex, that's... not ideal, so is there a certain layout you recommend?

Yeah, so the question is, are there certain layouts you'd recommend, and would you want to, in general, you want to avoid crossing over a CPU socket or going node to node, for instance? Absolutely, NUMA node awareness is still kind of a thing. That is why in the user space side, we don't have this on the user space side, so the kernel space side, when you do a find, you can actually look at the distance. So that'll give you a parameter, I believe, just in it, to your parameter to say how many hops away you are. So ideally, you would be under the same root complex or PCIe switch or under the same architecture from at least one jump. Further than that, you're just gonna get increased latency and you're up to whatever your architecture can do. We would work with someone like AMD for that to figure out what those numbers would be, so.Any other questions? This is good, I think this is the most number of questions I've had in a presentation, so. I appreciate it, you guys are great. And yeah, if you guys want to chat about it, feel free to look us up or talk to me this week, and we'll go from there. All right, thanks.
