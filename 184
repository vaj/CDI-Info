YouTube:https://www.youtube.com/watch?v=_adbBMEhdNQ
Text:
All right. So thanks, Frank. Thanks for the intro. And glad to be here. And I'm thankful to the MemVerge team  and for organizing the CXL forum, as always. So the latest update from our side,  before I dive into CXL-specific items,  is that we are now part of Broadcom officially. Broadcom closed the acquisition of VMware,  I think, end of November. And since then, we have been a wholly owned subsidiary  of Broadcom. But our charter's the same, which  is we continue serving our enterprise customers  with virtualization solutions. And we continue to innovate on newer hardware and newer  operating system features. And we continue to enhance our platforms. And definitely CXL is one of them. And we have been hard at work with CXL as well. And this presentation is all about some of the things  that we are doing with CXL as well. And just a quick disclaimer, some of the things  are confidential. It's not public yet. So I please request you to refrain  from sharing it in the public. But this sort of provides you with a roadmap view  of what we are doing. So again, I represent product management at VMware. And I work on the platform side.

So let's quickly talk about the problem statement. So essentially, what we are seeing across our enterprise  customer base is a few issues or pain points. So customers report that they are spending too much  on their DRAM memory. They are seeing sort of this imbalance between the CPU  and memory usage. So most of the customers today, for example,  report that they have spare cores of CPU  or they have CPU that remains underutilized. And they would like to see their CPU better utilized. Plus the fact is that a lot of our customers  actually deploy memory with certain forms of insurance,  which means that when they purchase  or when they actually configure their system,  they actually keep some memory aside and over-provision,  which means that for any peak demands or for cases  where the system actually runs heavy loads,  they need this extra capacity so that they  don't run into heavy performance problems for their customers. So what customers tend to do is they  tend to purchase this extra capacity of DRAM. Now, the issue is that for the most cases,  this DRAM is well-utilized. But they do face a problem of, for example, 90% of the times  this DRAM is sitting idle or not really used. So VMware has been working on software memory tiering. And now we are actually also looking  into some hardware-based solutions.

And our solution to the problem is  create a layer in our virtualization stack. So basically, we'll implement some memory management schemes  that will do active hot and cold page replacements. And that will ensure that the most active memory always  sits on DRAM. And the reason for doing this proactively  is to make sure that most of the workloads  continue to run without any drastic performance  degradation. So what customers get as benefit is  they're able to now utilize their more expensive DRAM  component more efficiently and deploy their lower performance  or non-active pages or non-active memory  onto the second tier. And again, this is entirely handled by the VMware stack  within ESX. And customers do not have to change their VM workloads  or container workloads to be able to make use of this. The other key point which I highlight here  is that what we have seen across our field  is in terms of active memory usage,  it's typically around 50% for our customers, which  means that whenever customers run their workloads,  their locality of reference or the most active pages  typically sit within the 50% range of the overall memory. So they might actually allocate close to 90% or 100%  of their overall capacity. But in terms of at any point, how much of memory  is actively being used for their workloads  or what the CPU is accessing is going to be in the 50% range. So it's another thing that that memory might be scattered  all over this range. And VMware will ensure that it brings the active memory back  into the DRAM so that DRAM can continue processing. So again, the key point is that customers  get a very good TCO reduction with such an approach. Plus, they don't suffer from any performance penalties.

So going forward with my TCO story,  here is a sample of what we see in terms of potential TCO  reduction for our customers. So if you look at the left-hand side,  this is the typical 2-terabyte DRAM DDR socket attached DDR  and the corresponding cost. And this cost has been derived from a popular OEM. And it comes to this total cost. Now, we do have a form of software tiering  that we are utilizing with NVMe. Although this is not CXL, but we think that the same things  are being leveraged or being planned  to be leveraged for CXL-based technologies as well. So we are basically taking steps here  in terms of first supporting a known NVMe protocol  and then going to the CXL. So with NVMe, what we see is if we deployed  sort of a 3-to-1 ratio, three parts of DRAM  and one part of the lower cost NVMe,  and taking dollar per gigabyte sort of cost for NVMe,  we get some cost reductions. But even further, when we get into the CXL world  and start utilizing an accelerator,  we actually start seeing much better performance, much better  TCO reduction. So we are working with some vendors right now. Of course, VMware division doesn't produce hardware,  so we are working with some vendors. And essentially, the idea is we will use them,  we'll actually work with them, partner with them,  and have sort of a hardware-software co-design  methodology where we can get further improvements  in performance and even further improvements  in the total cost. So the key takeaway is that as we  evolve from going from a pure software-based approach  to a hardware-software co-design,  we'll start providing customers more benefits  in terms of cost reductions.

So this is basically what we see in terms of evolution  towards the future. So we see a lot of activity around AI/ML kind of workloads  and need for bigger memory and how a larger capacity  of faster memory could actually help  some of the recent changes in AI/ML and analytics  type of workloads. So today, VMware is on a journey to enable CXL in the platform. And we will be supporting CXL 2.0. And starting from Granite Rapids,  VMware will officially launch our CXL 2.0-based support. So what that means is that vendors could actually  bring their devices and start enabling them on ESX. But there is only specific configurations  that VMware will support because we believe  that we'll be able to provide--  again, this is based on the return or the benefit,  the value for the customer. The best value is to support a specific configuration, which  is simple to deploy, plus it will give the minimal amount  of changes, both in the software and hardware ecosystem,  to be able to provide CXL. So we'll support CXL memory expander vendors and CXL memory  expander devices. And customers will be able to see larger capacities. And going forward, soon in the next few releases,  we'll also be starting to support the type 2  accelerator. This is the one where I mentioned about the hardware  software co-design, where we work with third-party hardware  vendors and start supporting a custom accelerator, where  we have VMware's IP actually running within the card  and basically doing an integrated memory  tiering between the host kernel and the device. So some of the benefits of doing this  is that we already demonstrated that we could get some VM motion  performance improvements. For a lot of our customers, VMotion  is an important application. So we'll be able to see some VMotion performance  applications improvements. We'll also be able to move to doing some initial pooling  and disaggregation. We'll also be able to do some edge type of deployments  with the accelerator and support larger AI model and AI ML  kind of workloads. And also start looking into some offloads,  where we could potentially offload more than just memory. But primarily, this device will be  used as a memory accelerator device, where  it will be able to work in coordination  with the host and the ESX kernel to provide faster, better  memory usage. And finally, we have this concept of a memory pool,  where we will use traditional ethernet fabric  and we'll be able to use these type of accelerators  in this fashion and create a logical pool. The idea is that this will involve lower lift and shift  for customers. It will even provide better security and a simpler  blast radius in the sense that we don't--  I mean, we don't have to worry about customers  losing an entire memory appliance, et cetera. So we will be providing support for such configurations  in the next three to five years. And basic idea is that now we can have configurations  where hosts can access multiple such devices  or multiple hosts can access a single device  across the network. And everything would be transparent to the host  and the VMs running on top of the host. Basically, the host will handle the memory management part  and VMs and containers, the applications on them. All workloads will continue running unmodified. So we already have a system DRS, which is our distributed  resource manager. And this will manage the memory across these multiple hosts.

So at a high level, how does our type 2 accelerator look? So we'll provide a single memory address space  to our VMs and containers to our customers. And they will be able to consume memory  as before with no changes. So they will just have to use a hardware PCI form factor  device. And it will attach and talk CXL protocol with the host. And again, this is something we are  planning for the Granite Rapids and onwards time frame  with the BirchStream platform. And here are some of the advantages that I list. Basically, use of an accelerator and as you probably  have heard from some of the earlier speakers,  there is definitely a lot of advantages  that the CXL protocol itself brings. Not just from a CXL.mem side of things,  but further with a lot of the other nice features  that CXL brings, such as being a memory controlling function  and being able to track memory on the host, et cetera. So those things are all brought together with the accelerator. Again, as far as the customers are concerned,  this will be a black box. But VMware will have its IP and it will integrate it  with the ESX kernel. So in terms of the benefits, again, the idea  is to provide lower TCO for our customers  and better CPU core utilization. So that with the larger memory, cores are better utilized  and customers don't have the imbalance that I mentioned. We'll also be able to reduce some of the OpEx costs. So host evacuation and memory management  across hosts and pooling and failure probabilities. Those things would be handled by some of the resource management  components within ESX. And they will lead to an improved OpEx  as well for customers. And again, we'll continue to evolve this model  and support more future use cases with pooling  and sharing, even supporting some form of standard memory  usage on other hosts. I mentioned AI/ML, for example, and analytics. And we could do some really good host application  or workload-based analytics and support for those workloads. And we will also be able to support,  as we go, CXL becoming a medium to support hardware  virtualization. So think that not only memory, but we also  start supporting some of the I/O devices,  such as networking and storage I/O devices.
 
OK, so let me try to answer this live. What is the latency that Ethernet Fabric introduces  on the CXL memory?
 
So it can range, Luis, anywhere from 10 microseconds  to 80 to 100 microseconds. But we believe that if we perform the tiering really  well in terms of moving hot and cold pages  and tracking it across hosts, and in concert  with the DRS algorithm, ensure that we manage the resources  well and the workloads are moved to the right host that  can support the latencies, we will  be able to support that kind of latency with the Ethernet. And we are continuing to do more experiments. We'll be running more workloads. And we'll be determining what type of workloads  actually suit this model. But so far, our experiments have yielded pretty good results. And even our most recent benchmarks  are showing some really good promising results. And the accelerator is itself being built,  keeping in mind the need for doing networking across hosts.
 
So David, are type 1 accelerators  going to be supported on current generation CPUs  or only with Granite Rapids?
 
David, type 1 accelerators will be supported only going forward  with Granite Rapids, simply because we  wanted to make use of some of the enhanced CXL capabilities  with CXL 2.0. CXL 2.0 provides much better RAS capabilities. And we believe we are also working in conjunction  with some of the vendors and OEM partners  who are actually aligned with us on supporting this with CXL 2.0  and with some of their platforms on Granite Rapids.
 
OK, so let me see. So in terms of the differences between NVMe tiering, which  is sort of-- you can consider both of these solutions  sort of orthogonal in the sense that it  will mean different, slightly different things to customers. So NVMe is just about plugging in an NVMe drive  into an existing host and enabling  software-based tiering. So it's a very simple approach. Customers can just take their existing platform. They can actually plug in an NVMe drive,  and they will start seeing memory tiering being used. We will also be providing some statistics, some information  on how the memory tiering is looking for the customers  so that if they start seeing performance issues,  they will be able to adjust their tier sizes  or be able to trigger moving their workloads to other hosts  using vMotion, et cetera. In terms of the TCO, it will provide lower TCO benefits  just because we will not be tiering a whole lot  into the NVMe. And we do recommend lower tier sizes on the secondary tier. With the accelerator, we are hoping  that customers are able to tier up to 100% of their DRAM  capacity. So it will basically be a 1 is to 1 tier with the accelerator. And in case of workloads, we believe  NVMe-based tiering is sufficient for VDI and VM consolidation,  general workloads that are more IT-like workloads. Whereas with the accelerators, because this is CXL-based  and uses some of the good properties of CXL protocols,  we'll be able to support a much better variety of workloads. So NVMe has higher latencies. With CXL protocol and memory caching,  we'll be able to provide better support and a superior  solution. So there is some CPU cost. The host CPU will perform tiering,  whereas with the accelerator, we could  move some of the processing to the accelerator device itself  so that we put less of a burden on the host.
 
Now let me see if there are any more questions. OK. So the question is, why do you want  to connect through Ethernet Fabric when CXL supports  switch and GFAM, which is better in terms  of latency of CXL memory?

Yeah. So Luis, the way we are looking at this  is to see what our customers can do as an evolution. So sure, CXL Fabric will evolve in the next few years. And we want to watch carefully how it evolves. And we don't want to force customers  to move into a complete CXL Fabric-based approach. We want to start them with a CXL-based approach. Start them with an Ethernet-based approach  and see how it goes. And maybe in the future, we will support CXL-based Fabric  for our customers. But at this point, we don't really  have that as something that we are seeing immediately. But maybe our views will change in the next two to three years,  where we start customers actually adopting  some of the CXL Fabric and doing some actual work  with implementing CXL in their environment. Of course, having said that, there is actually  a division within Broadcom that is--  I think, I believe it's our PLX division that's actually  building a CXL switch. And we would want to see how we could collaborate. And maybe there is an opportunity there  for the VMware division to work with the Broadcom division  that's working on the CXL switch.
 
Does the client need to be running  VMware to get memory tiering?

OK, this is a great question, actually. So initially, we are building these with VMware ESX in mind. But I didn't mention that eventually, we  want to actually move or evolve the accelerator  into a device that could become a general purpose device,  and which means that it could actually  have work with a host that actually might be running  a Linux or a Windows, even. So yes, actually, this is in the plan. And we do want to make this device--  you can think about it as a hardware-based hypervisor  or a virtualization device.

 So let's see. OK, I hope I answered all the questions.
 
So this is my last slide. So essentially, the summary is VMware  will support CXL expanders in specific configurations. And again, this is CXL 2.0. We'll be starting to support this from the Granite Rapids  and onwards. We are actually working with OEMs and some of our CXL  memory partners very closely on this. Memory tiering is definitely something  VMware has invested a lot on over the last few years. And we want to continue doing that and bring out  some new solutions that utilize memory tiering. And then finally, memory accelerators. So this will be our complete evolution  to CXL-based usage and CXL support,  where we'll have customers actually  getting a lot more benefits with CXL-based accelerators. So if you have any questions and any interest,  I see that there might be some customers in this forum. Please don't hesitate to contact me. I think we are certainly interested in partnerships. POCs or demos can also be arranged. And we are evaluating solutions right now  with different technologies. So I would definitely be interested in having  conversations with customers on seeing how  we can benefit our customers.

 Arvind, there's one more question.
 
 Yeah.
 
So Luis is asking, is the ecosystem  ready to push part of the processing in the type  2 accelerator and partition between host and accelerator? So the question is, if the CXL ecosystem is ready.

Yeah. So Luis, my experience has been that the ecosystem--  CXL is pretty open. It's an open standard. And customers or vendors and designers  will start coming up with creative ways  to use CXL accelerators and CXL-based devices. And it's really more about how you actually  can make sure that this is all seamless to customers. So as long as this is seamless to customers,  we believe that customers will use it. And then our OEMs and other partners  will fall in line to be able to use such solutions. Because in the end, anything that benefits the customers  is what will bring the pull towards getting  these solutions into the market and get some adoption. So with that, I think I've taken my time. Thanks a lot. And I definitely enjoyed this question-answer session. And looking forward to further sessions with MemVerge  and the CXL forum. Thanks a lot.
