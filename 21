
Good morning and good afternoon.Thank you for attending the CXL Consortium CXL 1.1 versus CXL 2.0 What's the Difference webinar.Today's webinar is presented by Danny and Elad from Unifabrics.And so I will hand it off to Danny to begin the webinar.

Well, hi everybody.My name is Danny Volkind.I'm the CTO and co-founder of Unifabrics.Together with me is Elad Schlieselberg.He's a system architecture also in our team in Unifabrics.

So what are we going to discuss today?First, we're going to do a brief introduction to CXL, hopefully a brief introduction.And so we just get familiar with the terms and definitions that are going to be used throughout this presentation.Then we're going to talk in general on the main enhancements added in CXL 2.0 and then go through a deeper dive into some of these.And as we are limited in time, we will also mention the major topics not covered in this session and then do a quick wrap up.

So since we are limited in time, here are the, and this presentation is an overview and not a comprehensive one.This slide includes the main sources of data related to the materials we will cover.And I encourage you to get deeper understanding of the mechanisms and features that are discussed in the sessions using these references.

So let's start.So CXL, a revolution in computer architecture, and that's really a big title, right?I mean, well, it is, right?Because if you think about it, basic server architecture didn't really change a lot in the last decades, right?I mean, if we take a 20 year old server, it pretty much looks the same as the one you can purchase these days.There are of course more cores and more DDR channels and faster IO, but roughly the same architecture.And so what's makes CXL special is that CXL is a unified bus, enabling a standard way of connecting to the IO caching and memory subsystems.And its biggest strong point is the wide industry support and adoption.That's the really big issue here.It leverages PCIe and it actually runs on top of a PCIe PHY, that's Gen 5 down to Gen 3 in the integrated mode.It's actually in most platforms, it will actually use the same PCIe slot that you're all familiar with and the same slot can be used either for PCIe or CXL.And we'll talk a bit about how it is possible in the next slides.It is optimized for high rates and low latency, and it's actually comprised of three dynamically multiplex protocols, that's CXL.io, CXL.mem or .MEMORY and CXL.cache.So let's take a quick look on what each of them do.So CXL.io is PCIe based, it's mainly used for initialization and DMA transfers, memory map type, all the goodness that we're kind of learn to love and appreciate in CXL, or hey, if you're in that side, and it is mandatory in CXL, so you have to implement this one.There's CXL.mem and this actually allows you to expose a memory, allows the device to expose a memory to the host and enable the host to do memory accesses to and from the device.And we'll show an example of how exactly it looks like in one of the next slides.There's also CXL.cache, which allows a device to make coherent accesses to the host's memory or cache.And notice that the coherency in both in CXL.mem and CXL.cache coherency is managed by the host.It means that all the complexity and perhaps the specialty in managing the coherency for the specific architecture is managed by the host itself.And so three protocols, all multiplexed on the same bus, and this is CXL.

So what happens if you don't need all these features?And this is where CXL allows you what we call a mix and match approach.And for that reason, CXL actually enabled, defined three types of devices.There's CXL type one device, which implements CXL.io, which is mandatory, as I mentioned, and also a CXL.cache interface.So this kind of device will typically be a caching or an accelerator device, which means like it's an accelerator that requires coherent access to the processor's memory, but doesn't necessarily require host access to its own memory.So typically that could be a smart NIC device.A CXL type two device implements all three protocols, CXL.io, CXL.mem, and CXL.cache.And that would typically be an accelerator device with memory.So an accelerator that requires access to the processor's memory, it can actually cache some of it, and then it exposes memory to the host itself and allows the host to perform memory accesses to the device itself.And that could typically be a GPU and an FPGA or a computation accelerator.And then there's a CXL type three device, which actually implements CXL.io and CXL.mem, no CXL.cache on this one.And that would be like a memory buffer or expander because it exposes memory resources to the host and allows the host to do memory accesses to the device.And that could be a device that is used for memory bandwidth or capacity expansion.

Let's move on.So let's talk about CXL evolution.So CXL 1.0 was introduced in March 2019.And September 2019, the same year, CXL 1.1 was published.And roughly a year later, CXL 2.0 was published in November 2020, and CXL 3.0 is currently in the making.So there is wide support, as I mentioned, and adoption in the industry.There are over 170 industry leading companies that joined the CXL consortium.And I think that's maybe what sets it apart from previous standards that tried to do the same.And the more important and exciting thing is that server CPU products that are supporting CXL 1.1 are already publicly announced and are due or expected later this year.And you can refer to the CPU vendors product maps to see when CXL 2.0 devices are expected.So I said I'm going to show an example and talk about how it actually works.And this is a good example.

So how will it look like?So a CXL device first and foremost starts its operation like a normal PCIe device.It will leverage the alternative protocol negotiation mechanism that is defined in the PCIe specification.And during the training sequence exchange, it will be detected as a CXL device.And then the platform firmware will produce, in this case, that's a .mem device, and the firmware will produce a system resource affinity table and heterogeneous memory attribute table, both are defined in the UEFI spec.These are ACPI tables.And these provide the OS information about the memory initiators and targets and the system and proximity domain.It also includes performance information, by the way, such as latency and bandwidth information.So this would assume a typical two-socket system.And in the upper image, we can see how the HMAT table is being parsed by the OS.And it is important to note that it is a plain vanilla kernel.It's a recent one, but it didn't require any patches or recompiling.And what we can see here that there are a total of two processor domains, processor domain 0 and processor domain 1.And there are actually four connected memory domains.And these are counted or indexed by 0 to 3.And now note that processor 0 and 1 has their own locally attached memory connected as memory domain 0 and 1, respectively.And also note that processor 0 has two additional memory domains.That's 2 and 3, indexes 2 and 3, which are two CXL devices exposing memory to the platform.Now in the bottom image, you can see that the platform actually exposes four NUMA nodes, two traditional NUMA nodes, like a compute plus memory node.These are domains or nodes 0 and 1, and two memory-only nodes.These are nodes 2 and 3.Now we can see that in this case, CPU 0 has 64 gigabytes of attached memory.CPU 1 has 80 gigabytes of attached memory.And there are two additional CXL nodes, one with 512 gigabytes, like half a terabyte of memory, and one with 8 terabytes of exposed memory.So we can see that the total memory of the system is 8.5 terabytes and some change.The bottom line is that the memory nodes look and are managed exactly the same as normal memory.And this is a huge advantage in terms of transparency and user experience, but it also introduces some challenges.

So, until now, we went over some of the basic concepts of CXL and got a taste of its goodness.Now let's briefly review the enhancements introduced with CXL 2.0, which will later be covered in the next slides.So first and foremost, it is important to highlight that CXL 2.0 is backward compatible with CXL 1.1.This means that a CXL 2.0 system enabled platform will support a CXL 1.1 device and that a CXL 1.1 platform, supporting platform, will work with a CXL 2.0 device to the extent possible in CXL 1.1.Sometimes it doesn't make sense.The main enhancement in CXL 2.0 is by all means is fan out.While CXL 1.1 always refer to a device interacting with a host, CXL 2.0 introduces switching and pooling.Another important enhancement is a fabric manager, and essentially it is a standard defined API to control and monitor the system.As it expanded the fan out, CXL 2.0 also adds mechanisms to do fine-grained resource allocation to hosts.And CXL 2.0 also adds important telemetry and load management features.Hardware support is expanded and adapted.And of course, security.Security is a huge concern.It's always raised by customers.And CXL 2.0 adds new security features, and there are much more.And we'll try to cover these in the next topics.

From here, I'm going to hand over the presentation to Elad, who will walk over each of these features.Elad, to you.Hi, guys, and thank you, Danny, for that introduction to CXL 1.1.As Danny had so accurately highlighted, CXL 1.1 gives us a lot of amazing features, but they really are oriented towards an architecture that is mainly single host in either single device or single host in multi-device.CXL 1.1 gives us the ability to connect as many devices as we have basically PCI Express ports, except now we'll be calling them, I guess, CXL 1.1 ports, or as CXL calls them, Flexbus ports.CXL 2.0 is about taking all these amazing features and scaling up.You want to scale beyond just the platform.And how much we scale really is based on how much we can fit and what we want to do.Now as we scale, we're going to want all these features to work just as well and efficiently as they did in CXL 1.1.And in order to do that, we're going to need features to manage that.We're going to need management features to manage our resources, to manage our hosts.We're going to need hierarchies so that every host at the end of the day sees a simplified hierarchy that it controls.We're going to want to be able to pool our resources as we scale up.There's going to be a lot more resources.It might not make sense anymore for a host to know specifically which memory it wants.And suddenly we're going to want pools of resources, whether it is memory pooling, whether it is device pooling.We're going to want pools of certain resources and we're going to want to be able to allocate them as need be.Now the bigger we scale, suddenly we're going to see a lot of issues and we're going to see a lot of features that CXL 2.0 takes into account to mitigate these issues.So we're going to talk about security and how to mitigate a lot of the problems that we're going to start seeing when we scale up.We're going to talk about telemetry because as we get much bigger, we're going to need sort of a way to know what's going on, who's doing what and what's doing what.And we're going to talk about hot plug because as we get much bigger, it becomes very important to be able to move resources around dynamically.

So the first topic we're going to talk about is switches.CXL 2.0 brings forward the CXL single level switch.Now what is a single level switch?CXL switch is an entity exposing multiple identifiable resources to multiple hosts.And this sort of brings about the fan out that really becomes a linchpin of what CXL 2.0 is for.Now like I said, 1.1 used a host to device architecture.It gave us a ton of amazing features.But now in 2.0, we can take all these features and connect them to different hosts.Obviously we don't have increased bandwidth.This comes at the same connection that we had earlier, but it gives us a lot more possibilities as to what we connect.Suddenly we can allocate resources statically in the sense that we can configure the switch beforehand to give us resources that we want, or we can do so dynamically.And how exactly we do that, we will see soon.

So the CXL switch gives us the ability to allocate resources as we see fit.And the first problem that comes with that is how do we allocate between multiple hosts?We don't necessarily want the resources in one host's quote unquote world to be in another host.We don't want that access to necessarily exist.And in order to mitigate that, a CXL switch sort of implements a virtual hierarchy architecture.Every virtual hierarchy includes its set of hosts and devices, which isn't necessarily apt on which physical connection you use.So a host is connected to a physical bridge, obviously an endpoint, and the host itself acts as a root port.But this endpoint can be in any virtual hierarchy.And the CXL switch creates these virtual hierarchies so that these hosts are isolated from each other.And the host decides which CXL device will be in which virtual hierarchy.Again, how that is decided, we will talk about soon.Now as I said, a host is attached to a physical bridge.This bridge internally is mapped using a PCI to PCI bridge to a virtual bridge.And this virtual bridge is then attached to something called a virtual CXL switch, which we will refer to as a VCS.This virtual CXL switch is what defines our virtual hierarchy.This VCS is then attached to a virtual bridge as well, or multiple virtual bridges, which are mapped again using PCI to PCI bridges to root ports.The CXL itself is root ports to multiple devices, and each one of these physical bridges are then connected to CXL devices.Now it is important to mention that the devices attached to the CXL switch can be any device just like they were before.It can be a CXL 1.1 device, it can be a CXL 2.0 device, and it can be a PCIe device.At the end of the day, the switch maps these out to the host such that the actual device doesn't need to know that the switch is there.And in that way, we can still use CXL 1.1 devices and PCI Express devices.Now as you can see, there is a little star over the 2.0.CXL 2.0 defines a single level switch and not a multi-level switch.And so the idea is that you map a device and not another switch.Now each device has a 16-bit field.And then it doesn't necessarily matter how many hosts and how many devices are actually mapped out.And so you can actually add another switch, but it will only look as though a flattened switch has been attached.And this isn't necessarily defined in the specification, but can be proprietarily made.

Now as I said earlier, these devices can either be statically mapped out or dynamically.And in order to do that, we will need a central managing feature.And that's where the Fabric Manager comes in.The Fabric Manager is sort of a set of APIs that can enumerate a system and decide how to sort of build these little worlds that we called our virtual hierarchies.And the important thing to note here is that the Fabric Manager is a logical element.And why is that important?It's only really defined in the specification as a set of APIs and interfaces.The Fabric Manager defines the devices, it enumerates them, it defines the virtual hierarchies and the virtual switches.And it can do all that that you can map out statically.But with the Fabric Manager, you can do so in a much more dynamic manner.But it's not necessarily a physical entity.It can be a host, it can be a BMC, which is on the board or on the switch.It can be a lot of things.A lot of you are probably thinking already of a sort of centralized managing system.You can think of a data center with a centralized controller mapping out multiple distributed Fabric Managers for all the switches in the system.You can think of sort of a cloud system where Fabric Managers hold data as to which customers paid for which resources, and then allows those resources to be dynamically mapped to set hosts.And these Fabric Managers are event-based.As devices are connected, the Fabric Managers know or sort of get an event and then react accordingly.They map these out, they enumerate the system, and they do a lot more things that we will talk about in the remainder of this webinar.

So this brings us to our next topic of logical devices.Now CXL 1.1, as I said, is sort of meant to be host and multiple devices.Now that's not entirely true.CXL 1.1 defines multi-headed devices, and it maps out how you can create a device that can sort of take care of multiple hosts and map out a set of resources for those hosts.But the thing is that at the end of the day, you are mapping a single physical device to multiple hosts.Now that we've moved to a sort of switch architecture, we can map these devices out as multiple devices.We can connect as many devices as we see fit and as many hosts as the CXL switch supports.And these devices, we now look at them as sort of virtual elements.They are connected to the switch, and each device is just sort of the way we would interface with said device.So a logical device is just sort of a set of logical interfaces that we can communicate with.And once we define logical devices as just a set of interfaces, we are no longer bound with the single device, single logical device.So we have multiple types of devices.We have single logical devices, or single logic devices, which are here SLD in yellow in the middle, which is basically a single logical device, which is what we had until now.

And we now have another type of device called a multi-logic device.A multi-logic device allows us to split our resources in the physical device, still be connected to the CXL switch using one physical connection, but suddenly be split up between multiple virtual hierarchies.And the way this would work is that the multi-logic device is sort of managed by the fabric manager, and the fabric manager can then sort of talk to the multi-logic device and map out its logical devices in different virtual hierarchies.Now every logical device belongs to one virtual hierarchy, as I mentioned earlier.But you can also separate the multi-logic device into multiple logical devices, into different logical devices in the same hierarchy.And the reason this is useful is because suddenly it might be more efficient to map devices based on the types of resources they have.Think of a memory expander that has both PMEM and DRAMs.And suddenly you would like to have those mapped out separately.You don't want this big bubble of memory where you don't know the kind of latency you can expect.And this is very convenient because then you can map them out into different types of resources, even though they are still using the same physical element and physical communication backbone.Now every MLD, or multi-logic device, can have up to 16 type 3 logical devices connected to it.These logical devices have logical device IDs, or LDIDs.And as I said, they can be up to 16.The LDID is 8 bits wide, and the bottom 4 bits define which logical device we're speaking to, whereas the top 4 bits are usually 0.The MLD head, on the other hand, has its own address, and that speaks to the fabric manager.The fabric manager sort of enumerates the MLD head as a separate logical device, and finds the nested logical devices underneath it, and separates them out between the devices.In the example we have here on the right, we have 3 hosts connected to a CXL switch.Each host sort of has its own virtual hierarchy.The purple host has 2 logical devices in its virtual hierarchy, one under each MLD head.The green host has only one, under the right MLD head.And the yellow host has 2 devices connected to it, 2 logical devices, one single logic device in the middle of the CXL switch, and one logical device under the MLD head on the left.I did put the MLD heads in red because they sort of exist in the world of the fabric manager.The fabric manager speaks to those separately.

Now that we get a much larger fanout, we also get a world that is much more complicated.We get a world where we have lots of devices and lots of hosts, and it's sort of hard to see what's going on.So CXL 2.0 brings forward the point of QoS telemetry.Now QoS telemetry isn't a new feature.It's not something, it's not a new idea.It's been out there, it's something that's been around for a while.It's been a problem since you've had multiple processes running on the same computer.The second you had multiple threads, it's become very important to know which thread is accessing which memory, and who's hogging up all the bandwidth, and who's hogging up all the resources.This type of issue has existed before.But at the end of the day, you can put something at the end of your computer and figure out how much the computer is using, and you can figure out how much of your memory you are using and what kind of bandwidth you can expect.And all these mitigation features can sort of be inside your host to give a sort of fair reach to all these threads.Now the same is true in the, I guess, old CXL 1.1, which is still pretty new, as you have a device that is connected to a single host.And because this device is connected to a single host, QoS telemetry wasn't as important.But in the example we have here on the right, you have an MLD with a single physical connection to a CXL switch, have multiple virtual connections to multiple virtual CXL switches.And you don't necessarily know what the other host is doing to the MLD.And suddenly, the bandwidth that you could be seeing from that MLD, which you don't necessarily know as an MLD, but the bandwidth you could be seeing isn't just based on what you're doing.And for that reason, CXL 2.0 brings a telemetry feature, and it embeds it pretty deep into the communication of memory transactions.So every memory transaction from any device in general comes with an added two bits, which tell you how much load is on that device.And that load is based on both internal load, think of an accelerator telling you how much its workload is actually being utilized right now, and bandwidth, how much it can actually send bandwidth in total and how much of the bandwidth is being utilized, as well as other features that can be included into it, like how much power and how much all these things are doing.So it's only two bits, but you're getting it in a very high rate because every memory transaction gives you sort of a four-bit range of how loaded the device is.The host can continuously calibrate how it uses that device so as to sort of avoid auto lapse.Now, it's not a mandatory feature.Obviously, since we will be connecting CXL 1.1 devices or PCI Express devices, these won't support the QoS telemetry features, but all the CXL 2.0 features or devices should support it or would like to support it.

Now the next feature we're going to talk about is hot-plugability.If you can think far back, when you wanted to connect the GPU to a computer, you'd have to shut down the computer, connect the GPU, turn the computer back on.It wasn't that bad.There are many reasons why it's probably a good idea to turn your computer off before connecting something.It's not outward-facing.It's not like a USB cable where you can just plug it in.Nobody really expects you to accidentally plug something in when the computer is turned on when you're dealing with PCI Express devices.The same is true for CXL 1.1 devices.Now PCI Express does sort of define an ECN for hot-plug.It's an optional feature.I'm going to be honest.I've seen it just about nowhere.But when you're talking about devices suddenly that are connected to switches, the problem becomes much more complex.In the example we have here on the right, we would like to connect an MLD memory expansion to the switch and replace an MLD head with memory expansion similar that is connected to two separate virtual hierarchies.And suddenly we would like to add a device.In order to do that, if we didn't have a hot-plug feature, we would have to turn off the CXL switch and in turn turn off all the devices as well as all the hosts.And we can easily see where this grows if each one of these hosts has another CXL switch connected to it and then we'd have to turn that off.And then we'd also have to turn off any devices attached to that.And suddenly this becomes a domino effect where we would have to turn off or power down massive systems.And this is not something that we would like to do in large quantities.So CXL 2.0 brings forward the hot-plug feature in two separate sort of methods.First of all, there's the hot-add feature.The hot-add feature gives us the ability to hot-plug in sort of any device, any supporting device that we would like.And it's important to mention that every CXL 2.0 device has to support hot-plug in downstream ports such that the host has to support the ability for hot-plug and the CXL switch then has to support the ability for hot-plug as well.The devices themselves do not have to support it, which gives us the ability again for backwards compatibility.So the hot-add feature gives us the ability to add the device sort of and then tell the Fabric Manager using the event of hot-add that a device has been added.The Fabric Manager then enumerates the device and maps it out into relevant hosts.So in this case, if we were to add a device and want to map it as the device we were replacing, we could just map the top logical device to the host on the right and the bottom logical device to the host on the left.And in that way, we get our hot-add feature.Now removal is a lot more complex.When we remove a device, until now, think of a USB device, not much has happened.But what if that device has memory?Nobody out there is trying to create a hot removal device, a hot removal mechanism for RAM inside a computer because doing so would be a nightmare.All the pages that you have mapped to it, you have no idea what's going to happen.And that's just for memory.Imagine if you have a cache-coherent system and suddenly you want to remove cache.Cache that could be sitting on the device that is mapped to memory on the device that the host could be using.And suddenly that becomes very complicated.So a hot removal isn't implemented in the same fashion.You can't just surprise remove a device like you can the addition of a device.And instead, this is done in two steps.And what we're going to find out is a managed hot removal.The device or the host will request a removal of the device.This can also be done directly through the fabric manager.And then the fabric manager will let everybody in the same virtual hierarchies of those devices know that a removal is about to occur.This is the first step in a two-step process of removing the device.And then every device then starts the second step of sort of taking care of all these things that needs to be taken care of.In the case of memory, this gives every device the ability to move the memory outwards.One of the mechanisms defined to do that is the global persistent flush feature.We haven't really spoken about it so far, but CXL 2.0 lets us, it adds support for persistent memory devices.So it can move all these memory that's about, all this memory that's about to be disconnected to persistent measures.Or it can move it to drives externally or somewhere else.It can move it to different memory, anywhere that's not on the device that's about to be removed.And it can flush any cache and write back any cache that's there.And again, this has to be supported by all downstream ports, again, for the same reason.

So the next feature we're going to talk about is security.And I think I said this earlier, the more we scale, the more types of threat models that begin to pop up become more and more important.We just talked about the hot plug feature, which means that anybody can just come and add their device to a CXL switch with many ports.And these ports can be outward facing using fast serial connectors, which somebody could just add a cable to.Flexbuses don't necessarily come in the form factor of the classical PCI Express device.Now we have multiple form factors, and somebody can come with just any device and connect it.And more than that, that device will have the ability to map to memory of an actual host.This device can then have access to the host and say it's a cache device, and also have access to the host's cache.The amount of problems this could create is staggering.And security is a very big issue, and CXL 2.0 didn't let this go without addressing it properly.And I think PCI Express 4 or 5 defines an IDE.There's a very good webinar on it, specifically, and I'll just touch on a few of the major points here.CXL leverages that IDE for PCI Express and uses it to create its own secure model as well.It uses the DMTF security protocol and data model, the SPDM PCIe IDE ECN.This is based on certificates.They can be verified using trusted sources.There is an asymmetric handshake for passing keys.This is used using ECDSA. ECDSA uses the NISTP-256 curve.It also, for the communication itself, it uses up down to link level encryption, and it encrypts the communication using AES-GCM, which comes handy with its own little map, integrated map for data verification and making sure that everything worked out properly.Because this is done, because this map is done for large amounts of, like, bulk packets, this can lower latency.And for that, there's also a skid mode, which is the ability to use this data before it is properly verified.This is a mode that you can opt to use.You don't have to, because this means that affected data or dangerous data can enter your system before it is properly verified.But if there is a problem of verification, this creates an event, which then triggers a sort of a domino effect to either get this data out or create an error.Now the actual security model comes in, is a point-to-point security model.The host, towards any device directly, can create a sort of secure link, and it can also create a selective stream, so that the host can create an encrypted stream of data towards a device without the switch actually being privy to what is going on.This is good in case somebody were to connect another device or another thing somewhere and not want them to know what is going on.

The last topic we're going to talk about is memory interleaving.And before we do that, what is memory interleaving?Memory interleaving is the ability to take the data that we are writing to our memory and split it up into pieces.And we do that usually in order to increase our bandwidth and lower our latency towards memory devices.A lot of memory devices have a certain downtime that they need to adhere to, so if we are writing large amounts of memory, we would like to write our memory in a way so that we're not using just one DIMM, we will split up our data in a way that we can write to one and then write to the next and then write back to the first one and then the second one.And we can interleave our data in this way as many ways as we would like.And CXL 2.0 gives us the ability to interleave our data as well.Now a lot of computers nowadays sort of interleave automatically.If you connect two DIMMs in two separate sockets to a classical user PC, most of the time the operating system will just opt immediately to interleave the data, because why not?In CXL 2.0, suddenly we are connecting massive amounts of memory.And we are connecting massive amounts of memory which isn't necessarily uniform.CXL 2.0 gives us all these pools of memory which can include different types of tiers of memory, each one with vastly different latency.If earlier we had maybe the ability in a PC to connect two or four or eight, in servers we could connect maybe 16 or 32 DIMMs, usually these DIMMs would be pretty uniform.Once we have memory tiering and external memory, there's no reason to have these memory uniform anymore.You can easily add 6% of PMEM, 12% of NVMe, 28% of fast memory, and whatever's left of even faster memory, and suddenly interleaving between memories that have different amounts of latency isn't necessarily a good idea.Imagine you were to interleave your data on DIMMs and at the same time on PMEM, and then suddenly one word will come quickly and the next word will have a large latency.And for that reason, CXL 2.0 maps out the ability to create latency groupings and the ability to sort of interleave our data in a smarter way.Now the interleaving itself can be done on bits 8 through 14 of the address bits of the memory requests, which allows us to sort of pick the granularity that we are interleaving.This gives us a granularity between 256 bytes and 16 kilobytes, so a very large range.And it lets us interleave the original specification of 2.0, it defines 8, but there's an ECN which allows us all the way up to 16, which is now part of the specification, as well as 3-way, 6-way, and 12-way, I think even 9-way, I hope I'm not saying that wrong, using multiple hosts to interleave as well.And that's a pretty nice feature.

So we've covered a lot of the major topics of CXL 2.0, but the specification is really big.There's a lot of new things.To put that into perspective, the CXL 1.1 specification I think is just under 200 pages, CXL 2.0 is I think 750.It's a much bigger, much broader specification, and the scale-up has brought with it a lot of features, and we didn't manage to go through all of them.Just a few of the major ones that we haven't gone through, speculative reads for lowering latency for eventual reads in the future, the way memory is mapped, or functionalities are mapped, which in CXL 1.1 is as a root point integrated endpoint, and in CXL 2.0 it's vendor-specific, much more dynamically like it was in PCI Express.We also didn't really talk about persistent memory support.I mentioned a GPF sort of offhandedly, but these are also big features in CXL 2.0, as well as power budgeting for on-board persistent memories.There's also enhanced error handling and isolation, such as security violations and surprise link down that we didn't talk about.I sort of mentioned skid mode very quickly.That is another thing we didn't talk about, as well as a lot of quality of service features that we didn't talk about.So there's a lot of things that CXL 2.0 gives us that we didn't manage to look to talk about.It is a very broad thing.I highly recommend you look into it further.Also the things we did talk about, there are webinars out there for almost all of them now and whatever there are and there will be.I recommend you watch those.They really are great.

Okay.So thank you a lot for the thorough explanations and the topics you covered.And as Elad mentioned, there are many other topics that were not covered in this presentation, and we do encourage you to refer to the specifications and other webinars.And there are really great materials online that were published by the consortium and others.And they are a great source of information and knowledge.So let's do a quick wrap up and talk about what we have seen in the session today.So CXL represents a major change in server architecture as it allows a standard, widely adopted way of connecting to the memory and caching subsystems.And this capability actually enables a huge amount of applications and use cases that were not possible in standard systems, at least before it was introduced.Now CXL 1.1 enables device level memory expansion and coherent acceleration modes and CXL 2.0 augments CXL 1.1 with enhanced fan out support and a variety of additional features and some of which we covered in this webinar.And CXL supporting platform are due later this year.This is really exciting.So it's actually happening.And you can expect CXL to be including your next generation of servers probably.And CXL greatly increases the amount of resources that can be used by server platforms.We showed several examples.And with applications and solutions that leverage CXL starting to pop up, there will be a need for mechanisms that can assist with managing and efficiently utilizing these new scale of resources.So thank you so far.And let's go to the Q&A session.

Thank you, Danny and Elad.
So we will now begin the Q&A portion of the webinar.
So please share your questions in the question box.
We did receive one question.
CXL 2.0 memory device connected to a CXL 1.1 host will appear as root complex integrated device or as a PCIe endpoint device?
So that's a good question.
It will appear as an RCiEP, as a root complex integrated endpoint.
Every CXL 2.0 device has to support a CXL 1.1 sort of way of communication.
And in that sense, it will appear that way.
Thank you.
Do we have any other questions?
Okay.
Well, thank you, Danny and Elad, for sharing your expertise.
The presentation recording will be available on the CXL Consortium's YouTube channel, and the slides will be available on the CXL Consortium's website.
We would like to encourage our viewers who are interested to learn more about CXL to join the CXL Consortium.
Download the evaluation copy of the CXL 2.0 specification and engage with us on Twitter, LinkedIn, and YouTube.
Once again, thank you for attending CXL Consortium's CXL 1.1 versus CXL 2.0 What's the Difference webinar.
Thank you.
