YouTube:https://www.youtube.com/watch?v=OErMP2GfvmU
Text:
All right, so my topic is going to cover the CXL switch. So thanks Frank for this opportunity to talk about Xconn technology and the product we are  working on.

So first of all, I want to kind of introduce the concept of CXL switch. Probably everybody knows about it, but I want to take this chance to introduce why switch  is important for this CXL ecosystem. So basically we believe for the CXL to be something called scalable. Scalable means you can be small, you can be big, you can meet your needs. Some kind of, let's say the AI computing system, their size is just small. They don't need so many GPUs, so many computing resources that can be small. Or memory footprint in that case, right? But then there's also the very large model that going beyond the terabyte to tens or  twenties or even more kind of for those kind of scaled off system. Then that pushes the limit to a much larger scale system. So the good thing about CXL is it will cover both small application as well as very large  application. And this scalability is very important. And the CXL switch provides this scalability for the system. So think about every CPU or device, they can only have a limited number of the CXL port. But it'll be very hard for them to directly connect to each other, right? But CXL switch provide a more like a hub that enable everybody to connect to it and then  direct the connectivity between all these devices. So I would say, you know, the switch would fang out to a large memory expansion, right? And a pooling. So that's where our switch, the Xconn switch has 256 lanes and total 32 ports. So that fun out of 32 ports can enable a system to be very scalable to a larger computing  node. So that's where the scalability means. So also the system architecture, if we, you know, without a switch, you would have to  go each node to node, kind of end to end connection. But once you have a switch, every device in this heterogeneous computing system go to  the switch and that makes the system architecture very simple, right? And very simple means you can be able to scale that system. Also the computing get into more like a fabric network. So we have a see a number of example where, you know, a single node resource is just not  enough to do all this computing. You want to, you know, up to hundreds and thousands of computing device on network together  to compose of a fabric network. And the switch is a vehicle to getting there. So our switch also support this multi-level kind of switching capability and the cascading  of the switch to get to a computing node up to thousands of computing devices. So here in the bottom, I just put some pictures from the CXL consortium that kind of give  you an example. You know, at today we are CXL 2.0 switch, but we're getting to the CXL 3.0 or 3.1 switches  in the next year or so. You can see the scalability and the fabric network kind of being, you know, conformed,  being enabled by the CXL switch.

All right. So let me move on. So let me just briefly talk about Xconn's CXL switch. Xconn has the world's first CXL 2.0 switch. So that product number is called XC50256. And it is also essentially also a PCIe Gen5 switch as well, because everybody knows the  CXL is a beauty on top of the PCIe technology. For this switch, we have 256 lanes as you read it easily from the power number. And total capacity becomes 2 terabytes per second. That's kind of combining all the lanes, all the ports, upstream and downstream all together. And this chip was tipped out two years ago. And we have the ASIC chip in our lab for a year and a half. We did a lot of verification. And so far, we have a lot of test data shows this ASIC has the lowest port-to-port latency,  which is required for the CXL 2.0 memory pooling and sharing kind of application. And also the lowest power consumption per port. Because we kind of, our chip design was just focused on this AI computing CXL switching  market. So we were ignoring some other applications. That's why we'd be able to optimize our design to reach a very low power consumption. And because it has a large radix, one of the switch can already handle a lot of workload  or design. So that kind of gives an advantage to the system designer to use one single switch and  do a very large computing node, computing design or architecture. The chip will be able to connect to, you know, there are going to be a total of 32 ports. Each port can be x8, but it can also be a x16 port. And we support a CXL fabric manager, which is very important for the CXL 2.0 memory pooling  and sharing. And also more important later for the CXL 3.0, 3.1 fabric manager. The chip we're designing right now can support cascading mode. Although you know, cascading is being defined in the next generation. We already see a lot of demand for these kind of cascading feature because our customer,  they don't want to just have one single switch connect to a limited number of memory devices. They want to have a, they want to build a very large memory pool system to enable their  application. And that can be enabled by the switch cascade. So this ASIC chip has been customer samples from last year and is still ongoing. And we are getting the production chip ready to go to production the second half of this  year.

So let me briefly talk about these CXL 2.0 applications. The one simple application is a memory expansion, right? So here is some very simple picture of how this memory expansion can be enabled. Our switch has a total of 32 ports. You know, if we use some port for the host connection and the rest of the port, all go  to the CXL devices. For example, you know, if we use a smart modular CXL module memory devices, their capacity  is going to one terabyte per device. So imagine in that case, you have a, you enable, you kind of attach 30 CXL devices, that memory  expansion go immediately to the 30 terabyte. So that's kind of tremendously scalability to go to a very large memory footprint. If you attach more switches, that's going to be multiplied by how many switches you  attach to it. So that's a very simple usage case. We can see, you know, today's computing system can very easily take advantage of a switch  and also the CXL memory devices.

But I feel like, yeah, everybody's high interest. They don't want to just enable a single node memory expansion. They might want to work to do a memory pooling and even later to go to the memory sharing. And for this picture, which is again borrowed from the CXL consortium, you see these multiple  hosts, H1, H2, they all connect to the switch through an upstream port. And on the downstream port, there's a number of CXL memory devices, which will form a bigger  memory pool. So here I just put a one switch here, but to build an even bigger memory pool, you can  use cascading a number of switches, which enable more ports and more attachment. And then another important element is the management or fabric manager platform. So this kind of system today, our Xconn as a company, we have been able to put together  a solution that works with the shipping processor, which are CXL 1.1, and also the upcoming server  processor, which are CXL 2.0 in this year.

So I talked about the CXL memory sharing, and memory sharing actually is a very important  application, a very critical application for the future computing. So the idea of memory sharing is, if you look at the picture on the left, the host 0 and  the host 1, through the memory pooling, they could allocate their own memory on the CXL  devices through our switch. The same color kind of memory block means that goes to the same color host. However, in this case, the memory sharing, you could allocate a block of memory space  that both host 0 and host 1 are going to access. That's what we call the memory sharing. So at this stage, at CXL 2.0 stage, this kind of memory sharing can only be supported by  the software, where the CXL 3.0 will really enable the hardware level kind of memory sharing. But even today, with this kind of software-controlled memory sharing, there are already a lot of  applications that can take advantage of that memory sharing. So MemVerge is our partner company, so they are specialized in the memory application. And they are already working on their software to enable this memory sharing capability. So the right side picture, I just borrowed some system architecture from MemVerge, which  they are working on this project called Gizmo to enable the memory sharing. So again, at this stage, the CXL memory sharing is possible through the software.

What we see the memory sharing that is important for the AI computing system is because the  memory sharing has a lot of advantages over the RDMA. For example, RDMA is one level much slower, much higher latency than the CXL memory. CXL memory latency is around 100 nanoseconds, but RDMA is going to be the microsecond, tenth  of a microsecond. And also the memory sharing, that operation model is more like a load and a store. So you don't have to transfer this data across the node to go through a network. That saves a lot of energy, but also simplifies your programming. Plus, CXL has defined already the memory coherency. That's going to give a lot of advantages for this memory sharing. With our switch, we'll be able to do this memory sharing to a very large scale. So in the previous picture, I showed 30 terabytes is very easy to get. And even more, you just add more switches. So some of this larger computing, AI computing, they could take advantage by utilizing multi-terabyte  of CXL memory, or even tens or twenties, thirties of terabyte of CXL memory to do the computing. And because everybody in this ecosystem bringing CXL together, in the future, the total cost  going to get lower and lower. So this kind of memory sharing through the CXL switch and the new software that enable  memory sharing will really boost the AI computing system tremendously.

So this one, I just want to tease a little bit on the future technology for the CXL switch  and what that can enable. So in the CXL 3.1, that standard, that spec was published last year. With the 3.1, we are really seeing all these heterogeneous AI computing systems coming  together to be united by this CXL 3.1 standard. So in this picture, you can see all the CPUs, all the GPU accelerators, memory, and the  DPU network card, and some of the new called in-memory or near-memory computing devices,  and even the storage devices, they can all get connected together efficiently by a CXL  3.1 compatible ecosystem with a switch connecting every component of this computing ecosystem. There is a lot going on with AI computing, and for this hardware and all these specs,  it's going to be available in the next year or two, three years. There's going to be more software work on this to enable this AI computing, for example,  some of the AI computing models to get all this data taking advantage of the CXL network,  CXL fabric. That's going to probably get developed in the next two, three years. But we do see CXL 3.1 has a lot of potential to really boost the AI computing system, to  really get the AI computing system architecture to the next level, to be lower cost, to be  higher performance, higher efficiency. So there's a lot of potential, a lot of work ahead of us. And Xconn being the pioneer in this CXL switch, we do have a roadmap going to have a CXL 3.1  switch, and also the PCIe Gen 6 switch in the next year. And we're currently working really hard to get the silicon taped out, to get our product  sample to the early next year, and wish to work with everybody on this exciting new computing  architecture and the system.

So I'm also going to use this chance to talk about our current product, which we mentioned  in the product page, the XC50256, which is the CXL 2.0 switch, and also the XC51256,  which is the PCIe 5.0 switch only. So we basically see right now it's a mixed kind of stage where you have CXL devices,  you also have PCIe devices. And the whole industry is kind of transitioning, more like the data center, AI computing industry,  they are transitioning from the PCIe interface to the CXL interface. But you still take time to get that transition done. So at this stage, we see our product, these three usage case can be enabled by the current  switch. So you could do a PCIe only, with some of the GPUs and the network card to use the switch  to connect. And also you could do a CXL only. But more likely, there's interesting implementation where you have both PCIe and CXL. You can also do this kind of design because our switch supports a mode called hybrid mode. The hybrid mode can operate half as PCIe or half as CXL. Or you can program it to be one third to be PCIe, two thirds to be CXL. That's totally software programmable. And we do see this kind of flexibility will enable more design to get the hardware done. And then later, if there's any changes, you don't need to change the hardware design. You just need to change the software configuration to be able to kind of migrate from the PCIe  to the CXL or from the hybrid mode to the CXL, that kind of transition.

All right, so this is my last call for action. I encourage everybody to join the CXL consortium to collaborate on the next AI and computing  system architecture with CXL technology. And also if you are interested in the Xconn technology, our switch, please contact me,  contact my team for a demo of CXL memory pooling and sharing. So here's my contact information.
