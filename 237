
Hi, my name is Paul McLeod. I'm a Product Director here at Supermicro. Today, we're going to talk about storage architecture  optimized for AI workloads. There's a lot to cover in this quick presentation,  and so we'll jump right into it.

Today, what we'll do is we'll cover  a brief backgrounder for Supermicro. We'll talk about those challenges,  and then the new thinking we have to have in order to do AI storage,  in order to really meet the goals of  these new large language models and get  the performance that people are looking for and  expect in their GPU infrastructure. Then we'll talk about some of the hardware innovation. Because Supermicro, everything has to do with hardware,  and we've always been driving hardware and hardware performance.

Supermicro as a company has been around for 30 years,  and continuously growing during that whole period. Our core competency, of course,  is server hardware. It's pretty much what we build. We've been building servers from the start,  starting with the motherboards, and then just  building all the infrastructure around that. Which brings us to today,  where we have these GPU systems that we've been working on  for more than a decade that have become the hot item. We've been building all the infrastructure to make those things so that  you can have highly efficient power and cooling. We have our own liquid cooling infrastructure and rack integration. Soup to nuts, you can get a solution from Supermicro for AI workloads. That includes storage infrastructure.

 To implement AI, there's a lot of challenge here,  as I said at the beginning.

Some of those things that we run into and that  everyone has to consider when they're starting one of these projects,  it is going to be large-scale. Yeah, you might start fairly small,  but for the most part, what we encounter is large-scale and  rapid growth as being one of the key elements. The data types, and this is the key challenge for storage,  the data types are mixed. It can be very large files and it can be a lot of  small files with high concurrency. The I/O pipelines, the workloads that everyone is  pushing at these GPUs can be very diverse. When you have something like that,  you're looking for centralized management. That's usually the reach goal. We have this little slide up here,  which was a poll from our partner Weka that pretty much  when admins were polled about what are  the challenges they run into when trying to deploy storage for AI,  data management really was the number one thing. You can't just start with what you've been doing and  keep going down the road and expect to be successful. You have to rethink that. The other factor with AI and with  these infrastructure that's high performance  is all this emerging technology. This could be NVMes and memory technologies,  and it can also be things like offload engines,  so the DPUs, IPUs. And everything else that goes into that infrastructure to take  the extra work that's moving data off  those expensive GPUs and CPUs that you'll be using to do the main job. All that stuff is something you have to consider  when you're building AI infrastructure.

You've probably all seen the workflow  that we're all trying to get familiar with in terms of this hot topic. This is from a SNIA source in terms of how the data gets processed. This is very basic. This is five steps. I've seen where it's five or more. Really depending on who you are,  what kind of data you're processing,  this typically is a DevOps operation where you develop  your model and then you feed that back into the system, so it's a loop. Each of these areas,  each of these categories from data collection all the way to  inference have different profiles in terms of a storage deployment.

The typical thing that people would do when they're thinking about,  "Oh, this data collection,  that's going to be large files and I'm going to be putting this into cheap and deep storage."  I'll pick this hardware to address that. For pre-processing, "Oh,  this is going to be something very similar to what we've done in Spark and Hadoop,  so I'll be using something that's a high compute to  storage ratio and probably still using low-cost storage."  Then training and validation and inferencing using, "Oh,  I have to use Flash to get that," and then archive, of course. But this mix usually will come with management interfaces  and basically a burden that has to do with different silos of data. When we think about this process,  we want to try to streamline it because there's going to be all kinds of challenges in  that GPU infrastructure that you'll want to be able to meet with a unified solution.

Because not only is this pipeline complex,  but it's essentially multiple pipelines. We think of those stages,  those are all high concurrency. I have multiple pipelines doing similar things and then some might be  more intensive than others on different segments of the workload. This really creates a challenge for both administration of  storage and for what you're going to select as a hardware platform. This slide comes from Weka,  which is really where we've found to be one of our best partners to solve this problem.

One of the things we like about Weka from the start is GPU direct certified. If you're building on a NVIDIA architecture,  you have access to the memory of those GPUs directly from the infrastructure. The other key factor from my mind is it's a tiered architecture that  allows that transparent connection between  slow and cheap storage and fast caching of storage. It's not really a cache per se because it is more of a tier. I have flash storage that is essentially talking to the GPUs. It's giving that super low latency performance with RDMA support to the GPUs. But I have in that same file tree,  because this is all under a single namespace and it's both  accessible from file protocols as well as object protocols. I'm able to have data that's back in  the object tier and pull it into that hot tier without any migration process. I can browse that data as if it were file. That's extremely important from a data management perspective. It's one of the key things that why we selected Weka as one of our close partners. The other thing that it has that's extremely important is metadata performance. When we're building something like this,  the metadata becomes the bottleneck for most of the applications. Anyone who's done HPC projects knows that now that we're in this AI flash era,  the media actually can move very quickly. But the data in terms of that's the metadata,  it can be difficult to manage. New techniques have been coming up. And  Weka is one of the first ones to come out with that. The other factor is this software-defined storage architecture  really fits well across our whole storage portfolio from  multi-node systems with a high number of NVMe drives all the way to  our top loading 3.5-inch storage you have a fit with the Supermicro portfolio.
 
We dig down a little deeper into the architecture. We start with the GPU layer where essentially all the work is being done.

We add that flash tier. This flash tier is the primary place where the GPUs are accessing the data. They're not trying to reach out to that slow tier. They are getting all their data and keeping  the caches full on those GPUs with this flash tier.

But then we have this object storage tier. The other nice thing that we get from Weka with the object storage is compatibility. Since it's just needing an S3 compliant storage stack to plug into it,  there are multiple partners that can provide that S3 interface. We usually defer to customers in terms of what are they using,  what are they happy with, we can work with you. It immediately becomes something very flexible for  the customer to deploy a Weka solution with  perhaps existing S3 storage that they're happy with.

If we look at this deployment from more of the physical topology perspective,  we can see that we have a very fast networking that connects to that flash storage. In this instance, we have 400 gig networks that are going to  all flash NVMe servers. That's that first tier of storage,  very fast and very low latency. Again, this is hundreds of gigabytes of throughput and millions of IOPS. That's something that will scale as you grow your system. Then connected to that is that slower object storage tier where most of our customers,  especially when they're talking about scale,  will deploy something like our top loading 60 or 90 bay storage systems. Which give you great throughput performance. I have a very large object store. It can actually move large datasets quite well. But you wouldn't want to have a low latency,  small IOPS type environment running on that. Then of course, all the networking that we do,  and we deliver these systems as full turnkey rack systems,  the rack plug and play program that we have is key to making these projects successful. We build up the infrastructure for our customers in  our rack production right here in the US and then ship them and they're all ready to go.

When we look at that workload,  one of the things that WEKA has come back with a number of  customer environments is contrary to most people's ideas of when we're  talking about large datasets and unstructured data,  everyone thinks of this being very large file. But majority, when we're talking about large language models especially,  is it's smaller file. It's not 4K IOPS,  but it's in that 100K to 400K range,  which is still pretty small and you would still want something that has  extremely good metadata performance to deliver those IOPS. Because as I said earlier,  the actual request for data and then the acknowledgement of data,  all those things are what infrastructure in AI gets bogged down with. It's not the actual data movement. It's not that we don't have very fast media devices,  it is that we have a permission structure or  essentially the handle to grab that data becomes the blocking thing in the workflow. We've been shipping PCIe Gen 4 for the last year and a half,  two years, and now we're starting to ship PCIe Gen 5 infrastructure. You can really see this progress and how the technology and the hardware technology  has advanced to a point where it's beyond the capabilities of the software. So we really want to have something that will address that capability.

We don't want to be the bottleneck or building a hardware architecture. We want to find partners to deliver the solution and the performance they need.

So essentially, Supermicro has been doing this since the start with compute platforms. Then in the last decade,  I've focused in on the storage side with NVMe. So really NVMe was the inflection point for Supermicro to say,  "Hey, this is new technology. This is where we can have an advantage."  When others were doing systems with two NVMe device,  we had launched servers that had 48 NVMe device. Again, this was because we knew at some point in time,  there was engineering resources there that had to,  we knew this was coming, right? You're going to need lots of storage and it's going to be NVMe. So why not get ahead of the game and start working on those projects? So with that,  we embraced EDSFF,  EDSFF, the standard that came out of SNIA for flash media. So this in our 11th generation of our servers,  we had E1S, that's the short version of the 1U media,  and E1L, essentially what was available on the market in the EDSFF generation to start. Now that we're in our 13th generation,  we have E3 systems. So E3 devices in that same format,  which essentially gives us, again,  performance because it's PCI Gen 5,  as well as a capacity bump in infrastructure. What we inherited there was CXL. So CXL is emerging technology that we really have also taken and brought close to our heart.

So why would I select EDSFF? At this point, most people haven't even heard of EDSFF. I'm always surprised because you see,  we've been talking about it for quite a while,  but I still go to trade shows and I'm blown away that the technology hasn't been fully embraced. So one of the reasons you would want to do this is because of signal integrity. We're at PCI Gen 5. These devices will have the best signal integrity. So again, that equates to the best performance. They also, by design,  have where we have a right angle connector where airflow can come through. So we're not really getting in our own way by putting a backplane. But there is essentially the difference between the EDSFF and the traditional U.2,  is the use of a backplane that sort of blocks the air,  giving us more air to the system so that we can then use higher TDPs on those systems and drive performance. Because we're going to want to put a DPU in there. We're going to want to put a processor that can drive flash media to its limits. And we're going to need air to move through that at this point in time.

The other thing that we did with our pedestal architecture was design something that was very balanced in terms of the media and then the networking. Because we know what people want is to have access to those SSDs for their AI workloads. And they will want that to go through a balance of bandwidth from the media to that network. Essential for an AI workload or anything that's going to need high bandwidth and low latency performance.

So our architecture from the start had that. And then we inherit because this new emerging technology of CXL is,  you know, the memory vendors have decided that EDSFF is the way to do this. And so this is the E3-2T device. So it's similar to the flash device, but it's a little bit thicker. So it's an E3S, a short device that's thicker. And then it's a memory device. So this isn't technically storage. It's memory, right? So it's not a persistent memory device. It's a volatile memory device that we can expand. So certain applications like in-memory database, et cetera, are advantaged by this technology. And future looking composable infrastructure is really the end goal for CXL and the CXL projects as they are. So that gives us a platform that both delivers memory performance and NVMe persistent storage performance.

We take a look, a little closer look at these systems. This is an example of our AMD line single processor, and we can fit up to 32 of these E3 devices in this system. So that's basically getting to a petabyte of storage in that 2U space.

And then in Intel, we have a dual processor, again, full memory support, so 32 DIMMs. So excellent overall server platform that you would expect from something like a flagship compute platform. This is our storage line, and this is the E1.

And then finally, the E3, where you can see we've implemented the larger 2T base. So this is essentially the first to market product that will support these CXL devices. And we're really excited about the projects that are coming for CXL in the next year. Really some great innovation happening there on the memory front. And interesting data points in terms of throughput and memory performance and all these things that we haven't been able to have on a single processor or a dual processor.  You know, the memory size available in one of these systems.

 So we have a white paper on our website that talks specifically about implementation of the Weka solution and our hardware.

Also, other resources in terms of the Cibermicro storage portfolio, and again, contact information, et cetera, if you have questions about it. So thank you for allowing me to present today, and I look forward to future SNIA events.
