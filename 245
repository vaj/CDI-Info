
Memories are driving big architectural changes  in computing. I'm Jim Handy, and I'm going to be talking to you  about those changes. Tom Coughlin couldn't make it because  of a scheduling conflict, so I'll be covering this by myself.

We're going to be covering an awful lot of information  very quickly, so hold onto your hats. First, I'll be talking about emerging memories, and then  memories disaggregation, and how CXL helps with that,  and then processing in memory, new kinds of memory chips. And I'll talk about chiplets and UCIe,  and then finally about AI and how it's going everywhere. So first, let's talk about emerging memories.

This is a chart that was lent to us by Intermolecular. It shows the similarities between various emerging  memory technologies. It's broken into a left-hand and right-hand section  called FEOL for front end of line,  and BEOL, back end of line. Those are semiconductor terms, and it  means either you're building something on the silicon,  or you're building it up in the metallization layers. And you can see that most of these emerging memory  technologies are being built in the metallization layers. The only one that's not is the ferroelectric FET. But the others, ferroelectric RAM,  ferroelectric tunnel junction, magnetic RAM,  resistive RAM, and phase change memory  are all kind of like new materials  in between what would be plates of a capacitor. It's either a ferroelectric material,  it's a magnetic material, it's a resistive material,  or it's a phase change material.

Now, the question is, what makes these emerging memories,  and what makes established memories established? And the first thing is that established memories  have a large market. It's usually well over a billion dollars. In the case of DRAM and NAND flash,  it's close to $100 billion for each of them. And the emerging memories typically  are very small markets, under a billion. The next one is that established memory technologies,  there are worries that there's a scaling limit,  that Moore's law is going to stop for these. Whereas for the emerging memories,  then the scaling limit is years out. It's very many process generations out. The thing that makes the established memories thrive  is that they're on a well-established process. Everybody knows how to make them well,  and how to make them cost effectively. Whereas for the emerging memories,  because they involve new materials,  then the process is not well understood. Now, you know the names of all of these established memory  technologies. The only two that are persistent are NAND flash and NOR flash,  and they both have incredibly slow write cycles. But in emerging memory technologies,  everything is persistent or non-fallible. And so-- and they're also-- they have a very, very fast write  speed, very close to what DRAM has. And so because of that, it could change computing architecture,  and that's what Intel's Optane tried to do.

And we believe in a report that we brought out,  I'll tell you about in a second, that there's  going to be a very large market for these emerging memory  technologies. It's going to take a while to get there. And so we're expecting to see it hit the billion dollar mark  somewhere around 2033 for discrete products. But wafers that use memory technologies,  use NOR flash today, and then SRAM later on.  Those actually are going to convert over  to emerging memory technologies. And so we're going to see an awful lot of growth. And so instead of DRAM's 4.5% average annual revenue growth  rate, and NAND flash's 6%, we're going  to see a 31% growth rate for standalone,  and we're going to see a much larger growth rate for wafer  based embedded memories.

The impact on computing for emerging memories  is going to be profound. And Optane laid an awful lot of ground  for that by bringing persistence closer to the processor  and setting up with SNIA and with other organizations  ways to use that. The persistence will move within the processor  as processors start using cache memories that  are built out of emerging memory technologies,  and important speed and cost implications. There will be important speed and cost implications to those. I will be talking to you in more detail  later on about how emerging memory technologies are going  to replace SRAM as caches. And with it, they'll bring compelling power savings  because there is no DRAM refresh, which  is a major portion of the power consumption in a server. They're persistent, and so they can be powered down  periodically, which means that they can bring even more power  savings. And there's this byproduct heat that  needs to be taken care of, and so that's going to be reduced. And so you'll end up with lower OpEx. And in some cases, particularly in hyperscale data centers,  there is a reason to expect that maybe higher CapEx will  be spent in order to get the operation costs down so  that you have a lower total cost of ownership.

We talk about all of these new memory technologies  in a report that Tom and I wrote called Emerging Memories  Branch Out, and it is now available. There are a couple of links that you can see in the PDF  that you can download of this presentation  later on if you'd like to get very knowledgeable about these.

So shifting gears, let's talk about memory disaggregation. Why is that a problem? Why do we need memory disaggregation? Well, it all comes down to stranded memory. Stranded memory is a problem because it's wasted resources,  and I'll show you why.

In a typical system, you'll have a bunch of servers  tied onto a communication path, and each one has a memory. Now, the memory size for the server  ends up having to be as big as the biggest application that  is going to run on that server. Even if the server spends  most of its time running an application that  needs less memory.

So you end up with servers that have that much memory used  when the large application is not running on them.

But typically, the large application  will bounce around to different servers  with virtualization like this, and you just  have to have that much memory in each one. And so that leaves you all of this memory that's unused,  which is called stranded memory.

Now, the way to get rid of this stranded memory  is you have a system where everybody has a smaller memory,  and you have a shared memory pool outside of that. And then as the program that needs the larger--  the application that needs the larger memory  moves from server to server, you simply  reassign that memory from one server to the other  according to whichever server will need it  at the particular time. So CXL actually sets up this communication  between the memory and the servers, and by doing that,  it eliminates stranded memory. That's why there's so much interest, especially  in hyperscale data centers, for CXL today.

CXL, this diagram is kind of unwieldy. It's a picture of a CXL system for CXL3,  and it shows switches talking to switches talking  to memory from the hosts. And we put the picture of the Starship Enterprise  up here because memory is disaggregation's final frontier. Servers got disaggregated by virtualization,  and then storage. And now memory is the last part of all of this. And CXL 2.0 and 3.0 both support switches. CXL 2.0 supports a single switch. 3.0 supports a configuration like this. They support memory pools like the ones  that I showed you in the prior slides,  and so there's no more stranded memory,  and that can give you savings in the data center,  both in the energy used by the extra DRAM,  but also in the capital costs of the DRAMs.

And so after disaggregation happens. We see that there is the possibility  that DIMMs will become obsolete because people will not  need to upgrade the memories in their systems. They'll just upgrade the size of the pool  by adding more CXL modules. And so you might see systems that just have HBM and CXL,  or soldered down DRAM like is used in portable computing  today. That could very well become the way that servers are made. It does support very uneven virtual machine sizes  that you can have small memory VMs,  and you can have other VMs that have  colossally large memory sizes. And so that all opens up new horizons for computing.

So let's talk about processing in memory. This is an interesting new field,  but actually it's kind of been around for a long time. But different people have different ideas  about what processing in memory is about. Some people say that it's DIMMs with a DRAM and a processor  chip instead of just DRAM on the DIMM. Some say that you have to put the processor  inside the DRAM chip in order to make it processing in memory. And then I've met other absolute purists  who say that you can't just have a processor alongside a DRAM  inside the chip. That the processor actually has to be a part of each memory bit. Finally, you've got the extreme end  of things, which are analog neural network chips. These are chips that try to mimic  the function of the brain by using memories as linear parts. But I'm getting ahead of myself. Let's first talk about the DRAMs.

But the goal of all of these is to reduce the data movement  within the system. If the data doesn't move, it can be processed faster. It can use wider buses. And it doesn't consume anywhere near as much power.

So I'm going to be talking about each of those in order. The first is the DIMMs with an internal processor. And there are two examples that I'm going to show you here. One is from a company called UPmem. And the other is Samsung. Samsung has their AXDIMM, which is an oversized DIMM that  has a processor built onto it. And an UPmem has a PIM with a data processing unit in it. So these both are things that are  intended to accelerate AI applications. And they should help out in the future.

Then you move the processing inside the DRAM chip. And Samsung has the same architecture. But they actually put it into a custom chip  inside of a high-bandwidth memory and HBM module. The kinds of DRAM stacks that surround the processor in a GPU. And so you can see that these even and odd bank arrays  are actually DRAM arrays. And they've taken out two DRAM arrays inside the DRAM chip  and stuck a processor in there. There's another company, Natural Intelligence,  who spun out of Micron technology. And it was about 11 years ago that Micron  introduced this chip. It was a processor in a DRAM chip  that they called the Automaton. And it is processing in memory for networking problems.

The networking problems it solves  are just really challenging. This is a diagram that Micron used  to promote this part back when it was early on. It also accelerates those NP hard problems. And it's really-- the processor can do an awful lot. But it requires an awful lot of support  in the way of rewriting software to make it work for it  and that kind of stuff. And so it's struggling a little bit right now.

The next is putting the processing  within the memory bit cell. This is the-- like I say, this is  a very careful look at how things are. And you've got a couple of memories  who are-- or a couple of companies that  are doing that with memory. This is just a diagram of a regular memory cell. And that cell would be where you'd  put one bit's worth of computing. You can't really do very much with one bit. But matches, you can do really well and very quickly.

And so we've got a couple of companies. We've got GSI, used to stand for Gigascale Integration. And they've got what they call their Gemini APU,  or associative processor unit. And then you've got Macronix, who have a NAND flash chip  they call the FortiX. This is a 3D NAND. And so you're looking at a side view of the NAND here. Both of these do matches.

GSI has run some simulations of a server based on a Xeon Gold,  I think it was, Intel processor. And they showed that it could operate--  the Gemini could operate on certain problems  10 times faster. And it would cut the system power down  to half of what it is. What I think is interesting about that system power thing  is that the system power of the APU, which  is what their Gemini is, an associative processing unit,  that power is roughly the same as the idle power  of the server. So for very little power, you're getting an awful lot of ability  to recognize matches.

Finally, we have analog neural networks. And these are a very odd field, because what  you do with the memory is you use linear voltages going in  instead of digital voltages, and then do matrix algebra on them  in the memory cells. So it's an old idea. It's been around for a long time. As a matter of fact, Intel introduced a product  with this in 1989. It's seeing renewed interest with emerging memory  technologies. It does matrix algebra instantly. Every single bit is doing a mathematic operation  at the same time. And so you get a single cycle math  for the entire matrix multiply. It's a slow cycle. It's a linear chip. But if you're doing millions or tens of millions of processing  steps all at the same time, it ends up  being much faster than doing processing at gigahertz speeds. It's got very simple operation, but it's a little bit difficult  to set up. It's hard to program these chips the way  that they are currently designed. But it could be a good accelerator  to work with a standard CPU. And you might even be able to use a very unpowerful CPU  along with a number of these to do very powerful AI tasks. It fits emerging memory technologies  well because each one of these little resistor-looking things  that are inside the diagram at the bottom right,  those all need to be variable resistors. And emerging memory technologies do that very, very nicely. There aren't very many products in this area,  though, and there's been a ton of research. And we'll have to see if it ever moves past that phase. The diagram at the upper side I just  thought was kind of pretty. It shows in varying colors the weights  that are stored inside each of these resistors. And so you'll see that the color indicates  what the weight is like. That as it moves from red, which is a lower weight,  to green, which is a higher weight through the yellow  color, that that ends up just kind of giving  a graphic illustration of how it is.

So we think one or more of these technologies might succeed. It's a perspective for offloading tasks  from the processor. And it could even be the main processor in smaller edge  applications with just some low-end CPU or microcontroller  to perform the housekeeping. It could make the need for CPU performance increases  take a slower path. And it allows the more complicated tasks  to be offloaded to the PIM chip. And then the processor really doesn't have an awful lot  to do after that. So you should have an overall cost savings,  because these processors in memory  are intended to be very inexpensive solutions that  have a somewhat dedicated way of performing. And then the performance scales with the addition  of PIM chips. I think this is a really important point.  That if you want to have 10 times the performance,  you just add 10 times the PIM chips. And you don't need to worry about the interplay  between one and the other. So it really should be a boon to people who want to scale up.

Switching gears, I'm going to move to chiplets. And chiplets actually also play in  with some of these technologies. The reason why chiplets are suddenly becoming popular  is because NOR flash has stopped scaling. It no longer shrinks with process shrinks  after 28 nanometers. And SRAM scaling is slowing down,  which I'll show you graphically in a little bit. So the embedded PROM function, which NOR flash  has been serving so well for such a long time,  is moving to an emerging memory. And embedded SRAM, including cache memories,  is likely to become persistent.

So what becomes persistent in all of this? Well, we're familiar. This is the memory storage hierarchy. I think you've all seen this when I presented it before. But it shows a price per gigabyte on the bottom axis. It shows the bandwidth on the vertical axis. And then it just shows different things-- tape, hard drive,  SSD, each one gaining in speed. And to fit in the memory storage hierarchy,  things have to follow this line. They have to be cheaper than the next faster technology  and faster than the next cheaper technology. So the things that were used to being persistent  are in this bottom circle. And that's tape, hard drive, SSD, and 3D cross point,  while it lasts is in that area.

Now we've got things on the processor chip  itself, which are poised to become persistent. So the L1, L2, L3 caches. And thankfully, the SNIA emerging memory  or non-volatile memory programming model,  that can handle persistence that close to the processor. What's interesting is that DRAM is probably  going to be the last holdout for a long time.  Because it's always going to be really cheap. And so we're going to see systems  where DRAM is the only non-persistent layer  in the system.

So UCIe, the UCIe Express Consortium,  has been formed to develop a standardized  interface between chiplets. This is Intel's Ponte Vecchio chip. I could have shown another one. This one actually uses HBM DRAM stacks around the chips. Those ones in the upper right and the lower left corners,  I believe, are I/O chips. But the other smaller ones are HBM stacks. And then the two very large rectangles  are the two processors. And right now, the interface between the I/O chips  and the major processor chips, they're proprietary to Intel. And NVIDIA uses proprietary interfaces. AMD uses proprietary interfaces. That means all of them have to make their own chips  to do these functions. And as chiplets become more widespread,  they'd probably rather have things  that are more multiply sourced. And so that's why chiplets are going to come in there.

UCIe is like CXL for chiplets. Now, UCIe will probably also be used for memories  as cache memories start using emerging memory technologies. And also before that, because we're  going to see an era where the logic process goes much farther  than the SRAM process. And so you'll see mixed processes  used in SRAM process that will also  be a more efficient process for SRAM than logic processes. And so you'll see logic and CMOS dividing up  into different things. And then you'll start seeing the memory chiplets being  built in a memory process. And you'll get significant diarrhea and cost reductions,  not only because of using these different processes,  but also because of competitive sourcing in the marketplace. So it will cause chiplets to become commoditized. One memory chiplet can be used by multiple logic companies. And all vendors' parts should be equivalent to each other  so that they should be almost interchangeable.

And so that finally brings us down to our last bullet point,  and that's AI everywhere. So where does AI fit in tomorrow's world? These are different parts of the computing and communication  channel that we're used to. You've got the data center on one end. You've got the end point, which is very often  an individual with a cell phone on the other end. And between all of these, you've got communications links. I'm not saying it's going to be a satellite every single time. It's going to be a wireless network or Wi-Fi or something  like that. But graphically, it was easier to put this together. And each one of these communications channels  chews up a bunch of bandwidth. So how do you get that bandwidth down?

You do it by adding AI to every single one of these points  in the processing chain. Somebody told me a quote once that I've never  been able to look up and find out where it came from. Which is that the bandwidth required in a channel  is inversely proportional to the amount of intelligence  at either end. I learned that, I think, in the '70s. And it's very true that if you put AI at the end point  and it processes pictures and says, this is Jim's face,  this is Tom's face, this is whosoever face,  then all you have to do is send the name of the person  back to the edge server. And if the edge server is collecting information,  it might be able to say, oh, these  are all people from this and that neighborhood,  and then just send 12 people from that neighborhood,  13 people from that neighborhood back to the communications  channel. And that kind of thing can distill down the data  to the point where almost no bandwidth is required  because so much processing is going on at the end. We're expecting to see that happen with AI. And with the sudden--  it seems like everybody's pushing on the gas pedal  for AI now. That should be happening in the very near future. We're going to see all of these technologies  that I just discussed being used in this. You're going to see CXL used everywhere up to the edge  server. You're going to see processing and memory also  used in those applications. You'll see at the end point, you'll  see the processing and memory chips  used some or also neural networks  for recognizing things. And you'll see emerging memories peppered all over this. And so it's going to be a lot of change  that AI is going to enable these other technologies  that I spoke about. And it's also going to be something  to help us use that bandwidth more intelligently. And so AI will ease the bandwidth requirements  that we need. And that's really something that's  a very important point about what it brings to the party.

So it brings faster response times  because things will be computed closer  to where the data is generated. It will reduce bandwidth requirements, which I've  harped on enough, I think. But it also gives you the opportunity  for higher data integrity because if the endpoint  receives something it wasn't expecting,  it can challenge that. It will give improved security because you've  got the opportunity to build in more smarts and things. And so overall, it will be a better user experience.

So you have a new bag of tricks to use  for computing architecture. Emerging memories for persistence and power savings. CXL will bring you pooling, persistence,  and more as it fights disaggregated memory--  or as it enables disaggregated memory, sorry. Compute in memory will help with scale and performance. And then chiplets will give you amazing new processors  because it can use these chiplets to build out  processors that couldn't be built using standard single chip  architectures. And then AI will accelerate response  while reducing bandwidth. And it reduces the amount of centralized data processing  that needs to go on. So hold on to your hats. A lot of change is coming very soon. Thank you very much.
