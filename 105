
Okay. Again, thank you very much. I am Kevin Yasumura, and I lead MEMS and optical switching at Google. I will be presenting on our optical circuit switches. Thank you for having me, and I would obviously like to recognize and thank all of the collaborators, some of which are listed below.

So first, what is a light wave fabric? So this provides our direct optical connection between the networking endpoints. It is comprised of primarily the optical circuit switch. I'll share a little bit more detail here. But we also have to work with the WDM transceivers that have been co-designed to work with the OCS as well as the circulators, and you'll see why that's very important shortly, and obviously the hardware and software control plane. What we get from this is a highly reconfigurable, extensible fabric for both our primary data center network and for machine learning systems. This gives us a large number of benefits and performance improvements, obviously for cost and energy efficiency. And because of this conference's focus, I will be highlighting here for machine learning, it gives us the ability to regularly and repeatably run very large node jobs on our TPU clusters, and for particular systems and particular algorithms, get very large speed increases up to a factor of 3.3, as you'll see. So I'll be going through very briefly some of our experience developing light wave fabrics at scale, and then I'll talk a little bit more about machine learning applications.

So last year at SIGCOMM, we disclosed to the world that we had upgraded Jupiter. We had gone from purely electrical packet switching, and instead we had replaced our top layer network with optical switches. This runs in Google data centers every single day worldwide. And those optical switches now connect our aggregation blocks as well as our fabric border routers, which then goes to either other campuses or the rest of the world. And by doing this, we have achieved very significant reductions in both CAPEX and power consumption. And again, I want to highlight that this is only for the networking layer. Obviously, compute takes more power. And this gives us a large number of benefits in terms of expansion as we grow the data centers, as we replace things, topology engineering for traffic patterns. Obviously, the DC traffic patterns are different than ML traffic patterns. I'll talk about that briefly. And it also allows us to support a heterogeneous mix of optical transceivers as different aggregation blocks are installed or removed from the data center.

What we've begun disclosing this year is that in addition to our DCN layer, we are also using optical switches in our machine learning clusters. So let's first take a look at this 4 by 4 by 4 cube of TPU chips. And let's consider that to be a fundamental building block now of this TPU cluster. If you look at this block here, there are six faces. And we connect those six faces through optical switches. And they wrap around to either themselves or then connect to other blocks in order to form different types of jobs that our customers require. And in an extreme case here for the TPU v4, in which there are thousands of TPU chips, you can arrange it, say, in this 16 by 16 by 16 cube pattern. Or in another extreme case, you could say, well, I'm going to take all those and just put them in one big line. And so now you can treat the TPU resources as a resource that's available depending on customer use and also if you have different sections that are down within the data center. And so that gives us a lot of benefits, especially in terms of utilization. We don't have stranded capacity, modularity, ease of deployment. We can bring these TPU clusters up without having to populate all the TPU chips in it from day one. We can bring it up gradually. And because we don't need to know physically where they are, we just handle it logically. We can still build customer jobs.

So let's now get into some of the hardware that we've used to build our lightweight fabrics. So first, I'm going to focus on the optical circuit switch, which is near and dear to me, of course. This is a very simplified schematic of the optical circuit switch. And here I'm only showing the optical core, the heart of the OCS. And you have a number of input fibers, and you have a number of output fibers. In this simplified diagram, I'm showing two MEMS mirror arrays with only three MEMS mirrors. Three is not very much. But if you were to rotate the MEMS mirrors, you can then change where your traffic patterns go. And so for our MEMS, that was developed in-house as well as the optical circuit switch itself. They offer tens of milliseconds scale switching time. That's a mechanical time constant. Nothing you can do about that. It's also non-blocking, which is very important if you're going to run ML jobs for different customers in the same cluster. It gives us near perfect isolation of the connections, which gives us good partitioning, which means that you could have a portion of the compute resources, those cubes, remember, that are running jobs, and maybe the other ones you need to reconnect or create new jobs or new topologies. And we can do this all dynamically in real time while we have a certain portion of it fixed and static. And overall, a MEMS-based solution balances all the requirements that you would expect, which are obviously cost, scale, performance, and for us, given our scale, manufacturability. It's not enough to produce 1, 10, even hundreds, even thousands are nothing to us anymore.

So this is a diagram showing now the end-to-end connectivity for a machine learning application now. So we have a TPU on the left and the right, and we have the optical transceivers. As I mentioned, this has to be all done and it has to work together because now we have the OCS in the middle that then creates the different topologies. I'm only showing one connection here, obviously. And what we've also done is we've integrated the circulator within the transceivers themselves. So that's another level of integration that we've done specifically for our use case. And one of the other benefits of optical switches is that because we're never actually looking at the photons, they just pass through, they bounce off the MEMS mirrors. It is data rate and encoding agnostic. So it future-proofs us and the data center for a wide number of things, and we can use the same OCS for multiple generations of applications. But it does give you other challenges, in particular, higher loss because now you have the loss of the switch and you also have the loss of the circulators. And because we also run the photons bidirectionally, so that cuts in half the number of optical switch ports that you need overall, you also have to deal with multi-path interference effects. And I'll talk about that briefly in a second here.

So now let's look at a little bit more of the innards, the heart of the Palomar optical circuit switch. So this was developed in-house. I led the team for that. Let's take a look here at the left. We have a fiber collimator array. It's just a bundle of single-mode optical fibers. And we have 136 fibers on the left, 136 fibers on the right. We have a lens array, which then focuses each of those fiber beams. And let's just follow this green data path. Just use that as an example of one of the many, many pads. There are obviously lots more of these such beams that are crisscrossing each other in an actual OCS. But it first bounces off a side optic here. It then bounces off of a mirror in the first MEMS mirror array, bounces off of a middle optic, bounces off of a second mirror in the second MEMS mirror array, another optic, and then to the target output fiber. And then again, don't forget that light is actually going both directions here. In order to monitor the positions of those mirrors and keep the insertion loss good and obviously the system healthy, we have an out-of-band, what we call an injector module, which shines light on each of the MEMS mirrors. And then we use cameras in order to monitor and watch where each of those mirrors are. And that's a very important feature and decision that we made because it is obviously very manufacturable. We don't have to add fiber taps. That adds cost and complexity to this type of a product. And on the right, you can see an actual picture of the core. And obviously, I've taken the top off here for this picture. Same components here, injectors, collimators. The MEMS here are a little hard to see, but you can see the adapter boards. And then what I did do here, the blue cables are obviously the high-voltage cables that bring the voltages and control signals to each of those MEMS mirrors.

So as was mentioned previously, let's talk about two of the very, very important performance parameters for an optical circuit switch. First is insertion loss. Second is return loss. So here I have a plot, an example of one of the many Palomar OCSs that we've manufactured. Our lifetime requirement is 3 dB. That includes aging, shock, thermal, et cetera. But as manufactured, the mean here is less than a dB. And this particular unit had a tail that maybe went about 1.5 dB. And this does include the front panel loss as well. Because we are running Bidi optics, return loss is very, very important to us. Our spec is 38 dB. And this particular unit had about 46 dB as well. So everything works.

In addition to the OCS, next is the transceivers. So this shows five generations of transceivers that Google has developed. I won't get into all the details here, except obviously speed increases as does the encoding scheme changes, as does the data rate. And I want to highlight that all of these transceivers are custom to Google and our application and our link budget requirements.

And that is because, again, we have the optical circuit switch and the circulators that we need to deal with. So we deal with this by basically running things at higher speed. We have better extinction ratio components. And we also do higher levels of integration. And here in this particular example, we're highlighting that we've actually put the circulator within the transceiver itself. We also have spent a great deal of effort on custom DSP algorithms and error correction that is on board within the transceiver itself in order to make the overall network work.

So let's now step back and talk about machine learning again as a specific example. This is just a 4 by 4 grid. Let's just assume, again, that we only have 16 of those cubes. There are obviously a lot more of them, but we'll just pretend we have 16. And let's, again, envision that one of these, shown in red here, either that cube is non-functional or maybe it's just not available. Maybe a customer is running a job on that cube, and they just want that cube for a very, very long time. In this scenario, if you had a fixed network topology, you are now limited what you can do with the available resource, especially since you need to do a wraparound topology for a machine learning job. So how we deal with that is.

because we have an OCS, we can basically pretend it doesn't exist. 

And we can reconfigure them in any way that we need to for any job that's available, provided you have enough resource cubes available for your customers. And this is shown, again, also in this plot. There's a lot on here. But the dotted curves are the effective throughput through a TPU cluster if you do not have an OCS. And it shows that when you have failed chips and other issues, it is very, very hard to make large jobs. Statistically, it becomes very difficult. But the solid line shows the impact now of having an optical switch. And now you can, at a much higher frequency, create jobs that you want, because you can basically reroute around any issues that you have within the data center.

So let's now talk a little bit about our cost and performance metrics. So obviously, if you don't do anything, you just have a direct connect technology. That's just one. That's the baseline. So by having a lightweight fabric, we've added 6% to our cost. So it's very, very minimal. And only about 1% to the power. It doesn't add very much at all. And that increase in cost and/or power is obviously nothing compared to the benefit that we gain in terms of availability, customer satisfaction, and also we don't have resource stranding within our data centers. If we were to build this with a traditional EPS fabric, it would increase the cost by 24% and increase the power by 10%. So obviously, the lightweight fabric offers significant advantages both in terms of overall cost, power, and then also availability in our systems.

So let's now go into a little bit more into the details of different LLMs. If you had a very, very large LLM, we call it LLM 2 here, and it required all of the resources here, the best way to handle that is just build that big cube that I showed on one of the earlier slides, the 16 by 16 by 16. There are other LLMs, however, where the optimal configuration is not that perfect cube. And in this instance, LLM 1 in this long stick-like configuration actually gets a factor of 3.3 speed up in that particular use case. And there's another one, and there are many more-- I'm only listing three here-- that actually gets a 1.5 factor speed up relative to a base cube configuration in this maybe like decocards-like configuration here. And so the point here is that not only are we seeing significant speed up because we have a lightweight fabric and the optical circuit switch, but it also future-proofs us for future developments in machine learning and algorithms, because we can now create the optimal configuration for whatever job comes forward as the industry moves forward as well.

So call to action, if you'd like to learn more about TPUs. And also, we have announced recently TPU v5. I've provided some links here. And if you have any specific questions, you can email, sorry, at the authors at that email address. I'll hold it up for a sec here. I'm not going to read through all of this. I just want to say again that it's been a fantastic opportunity to build optical circuit switching at scale at Google. We believe it is the industry-first large-scale deployment of optical switches. And as a MEMS person, it makes me very happy that I have millions of MEMS mirrors running every single day worldwide right now. OK? Thank you.

Thank you, Kevin. And I see a question from the audience. Go ahead.

So you said you have these running real-world traffic all over the globe today. Have you taken these statistics on how often the switches are being reconfigured, how often the MEMS are moving? How much is it actually being used other than initial configuration and what could be done? How much?

So we have a number of papers on this. So for the large-scale, the DCN-based application, it's more on the timescale of hours. So that's not something where you're switching all the time as you're doing backups and other types of things. So it's a very slow type of switching.

So it would be safe to characterize that as hourly reconfigurations?

Something like that. Yeah. Now, the machine learning, however, is very different. That's a very rapid-based configuration. And again, it depends on the jobs. I don't have all the statistics, but I will say that it's now gone from hours to minutes timescale now.

OK. All right. Thank you.

I do have to worry about the MEMS mirrors and how often they move. So yeah.

So you said the optimal configuration depends on the particular model. Is that purely dependent on the model size, or are there other details of the model that contribute to the-- 

There are more details about the model that determines the optimal size. And I'll be honest, there are a lot of smart people that focus on that. So I'm the MEMS and the optical switch guy. Yeah.

Thank you.

So from the point of view of reconfiguration time, do you see that as an issue for either ML workloads or other workloads in Google? And I'm trying to understand, where does this go since this is mechanical? Are you looking at solid state going forward? And if there's specific technologies that you'd want to talk about?

Right. So right now, no. We are not limited at all by a millisecond timescale. There are other things in the data center, checks, and other job queuing functions that take much longer than that time constant. So we're not concerned about it. And right now, we don't see an immediate need for shorter than millisecond timescale switching.

All right. One more question. OK.

Yeah. Thanks for the talk. Very nice. Just a question about scalability. What is the port count you desire to scale, number of ports, and also going speed up? Obviously, system budget shrinks when your bar rate goes up. So can you talk a little bit about scale out?

I heard your first question. I didn't quite catch your second one. But let me answer the first one first. So we actually picked the size for historical reasons. There's nothing magical about that particular size. It could have been 12 more or 10 more, whatever. So that was, again, remember, we had Jupiter. And we had a lot of data centers already pre-built. And that size worked for that data center. So that's how that came up. And then that OCS was then also used for a lot of these machine learning applications. But as things go, this is old work. There are different things going on right now. And can you repeat your second question? I didn't quite hear it.

Your second question is, if bar rate goes up, typically, optical link budget starts to shrink. So that will be a potential problem.

Yeah, it is. It will be a problem. And we have to watch that very carefully. Thankfully, both the optical switching function and the transceivers were all in one group. And we all sit next to each other. And we talk. And we work it out. Yeah, but it is a valid point.

All right. Thank you, Kevin. Let us thank the speaker one more time.
