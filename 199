YouTube:https://www.youtube.com/watch?v=WKURedYwswQ
Text:
So I think most of you are probably familiar with Synopsys. We are the leading IP provider for interface IP. And so any of you folks that are building SoCs probably have interacted with someone at Synopsys. So I'm the product manager for the PCI Express and CXL controller IP. And I'm going to talk about our solutions specifically for CXL.

So first, I just have two quick slides with some interesting information about CXL use case adoption. Some of the adoption information is not publicly available. It's information we collected at Synopsys. And then I'll go into the Synopsys CXL IP solutions, some interop and proof points, and then a quick summary.

So one, you may have seen something like this out there, but there's a lot of interesting use cases,  some of which have been discussed for CXL. This is just one that always kind of interested me because it's taking a really pervasive technology of DDR  and saying, OK, DDR is great. And as we need more and more memory to do acceleration or do different things,  do I want to keep adding more and more? As you're adding more and more DDR channels, it gets really complicated. They need to be the same. They take up a lot of space. So one of the ideas has been discussed in some activities taking place in making CXL based memory modules. And so you can imagine a chip on the left here. And if I wanted to add a whole bunch more memory,  we can move over to an architecture more like on the right where we replace some of we keep the DDR,  but we replace additional ones with something that's CXL based. By doing this, we put a bunch of CXL interfaces on the chip. And now we go off chip and serial link through what would be a PCI Express 5 or PCI Express 6 physical layer  and connect to a CXL memory or storage device. And another advantage of this, in addition to the expandability of it, is that this SoC,  potentially, if you have multiple use cases for your SoC, this could do more than just connect the memory. Right. This could be anything that has a CXL interface on the other side. And there's a lot of other interesting use cases. There's a whole bunch of discussion of fabrics and stuff that I'm not going to go into,  but there are things that we support here with our IP and Synopsis.

I wanted to share this curve because I think it's really illustrative of what's happening in the CXL space. So CXL gets a lot of buzz. There's interest, as shown by the MemVerge event here and some of the stuff that's happened at the flash memory summit, etc. But what we're looking at here, this is purely Synopsis data. This is the adoption, the number of licenses per year. I've taken off the numbers so that we're not giving away anything that's proprietary, but just to give you a relative sense. And what we're seeing here is not only is CXL use growing, but it's accelerating. You can see this is a nonlinear curve here. So where we are today at Synopsis is we have over 130 CXL controllers and PHYs. And what I mean by CXL PHYs, I think people on the call know that CXL uses the PCI Express physical layer. So it's using a 32 gig PHY or 64 gig PHY. But when these are coupled in a solution license, we tend to refer to these as CXL PHYs, even though they're really the same as PCI Express PHYs. So we're at like over 130 licenses now. And this includes, surprisingly, and it's even surprising to me as my product is we've got over 20 licenses for CXL 3 already. CXL 3 seems like a really new spec. If you look at it, it's been out for a while. CXL 3.1 has come out, but there's a lot of flux, right? There's already errata being discussed to CXL 3.1. So it's been a difficult spec to nail down, and I think a lot of you might be familiar with the pain that we've all gone through with that. But we've already got to over 20 licenses for CXL 3. And another interesting thing that we support for CXL is IDE. So you may be familiar with the integrity and data encryption ECN that came out for PCI Express, and it's also there for CXL. And we have customers using IDE for PCI Express 5, for PCI Express 6, for CXL 2, and for CXL 3. We already have multiple licensees for CXL 3 with IDE. And in our IDE solution, I have another slide on that, but I'll just touch on it here. We have the ability to support encryption and decryption on any of the three protocols. So you can have it on the .io and the .cache .mem or just on the .io if you prefer, depending on what your chip architecture requires.

So Synopsys got into the CXL IP space very early. We were the first to announce a solution. We have a longstanding and deep relationship with Intel. And as a result, when it was clear that they were going to take an internal Intel spec, it was called Intel Accelerator Link,  and turn that into an external public consortium-based protocol spec called CXL,  they brought us in and said, "Hey, would you like to support this as an IP provider? Because we really want to drive this into the SoC community. We want to promote the standard and it would be great if there was IP available for this."  So we jumped at that opportunity, said, "Yes, we would. We'd like to do that," and immediately started working on that. And that's given us an opportunity to kind of get ahead of the other vendors out there with CXL solutions. We started earlier. So as we've deployed CXL, sure, a lot of folks have it now, but ours is like a second, third generation,  because we started back in 2019. Originally, it was a 512-bit CXL controller. We have 32 gig files. Now we have the 64 gig files for CXL3. Another important part of the Synopsys story is the verification IP, too. Since we have a complete verification IP, we do provide some verification IP licenses,  two of them to be exact, when you take a CXL controller license. So this helps you validate your CXL configuration that you create using our configuration tool called Core Consultant. Also helps you connect your controller to your PHY and just validate your whole little subsystem. It's not enough. It's not sufficient. It's not intended to be your SoC verification tool, but it is something that comes with the IP. And today, we support CXL2, CXL3, and also CXL3.1.

So this one gives you a little bit more of a view of what we consider when we talk about the CXL2 solution. You'll hear Synopsys talk a lot about complete solutions. And for us, that means even more than just a PHY and a controller. It means PHY, which is shown here, controller, which is shown here, the IDE module, which is over here. That's an optional piece that you can use to do the encryption/decryption, as I mentioned. But it also means a couple other things, the verification IP I already mentioned. But then we have this other thing that we call IP subsystems. This is an optional service that Synopsys provides for our customers that want to do any number of things in varying complexity. So they might want to be building, let's say, a highly bifurcated controller or a series of controllers that bifurcate. Some of them are dedicated CXL, some of them are PCI Express. They have complex clocking and reset logic, or maybe they even want to integrate all that or some subset of that with PHYs. And we can do all of that as an optional service so that what you receive as an end customer for your SoC is a completely ready-to-go,  drop-in piece of IP at the subsystem level that's already verified. Then the last one on here, something we call IP prototyping kits. We've been doing this for a while. We've done it for PCI Express. We now do it for CXL as well. So if you've been to some of the shows, we've shown our CXL IP prototype at the SC23. We've shown the PCI Express one at DesignCon, at DevCon going back a couple years, at PCI SIG DevCon this year we're showing it. So we have hardware based on what we call HAPS. Today it's HAPS 100. It's our current version. It has the Xilinx UltraScale FPGAs in these large boxes. I'll show a picture of it later. And we can put our controller IP into those FPGA systems. And then we come out and connect through a pipe interface, it's a modified pipe, get a little bit wider, to our daughter cards that have our  Synopsys PHYs on them. And so we get a complete PHY+ controller hardware solution. And that's what we do for demos. We take half of that, root port on one side or an endpoint on the other, to the compliance workshops to get on the compliance list. We take that for our interoperabilities that we do. We interoperate with like Teledyne LeCroy. Gordon was on here. We've done a lot of work with them. We've done with other vendors. I don't want to short anyone, but pretty much all the equipment vendors and instrument vendors we've done interoperability with. We've done interop now with Intel. We've done it with other CPU vendors as well. So those same hardware systems that I mentioned are not only used for demos at marketing shows, they're used for the interop, they're used for the  compliance. And then we productize them as something we call an IP prototyping kit. So if you want to do your own IP prototyping, you can take these IP prototyping kits. The basic IP prototyping kit in quotes is a soft deliverable, so we deliver that to you. That's so that if you have HAPS prototyping systems already, you can use those and you don't have to buy another one. Because sometimes we have customers that have large numbers of these and they prototype a whole set of IP that's going into their SFC. So they don't need to take hardware each time they take the IP prototyping kit. But it's nice because it's the same thing we use and we make that available as part of a solution as well. So we support all the versions of the standard here. The IDE support is there. We support PCI 5, PCI Express 6.1 and 6.2 is just around the corner. And one of the other things we do at Synopsys and we've always done is we support host mode and we support that in our first iterations of every IP. And that's really important for us because we're very early out the gate with our IP to support the early adopters. And so there's nobody usually for us to interoperate with. We've had Gen 6 for a while now. There's been nobody to talk to for a while. Now there are starting to be companies we can interoperate with. But when we first come out, we have to be the device and the end host. And that's why we typically support host and device immediately or as a dual mode so it can be programmed. And then we also support Switchport. One of the customers that already talked today here, Xconn, talked about their fantastic CXL2 switch, the first CXL2 switch in the industry. And I think as you know, that was based on the Synopsys IP. So we have a great collaboration with them to get this kind of technology out into real world applications very quickly. As we've expanded to CXL3, we've extended the data path of our controllers all the way up to 1024. We can support by 2, by 4, by 8, by 16 links natively. We support the Type 1, Type 2, and Type 3 devices. And we have some options here too. You can configure this if you want. You can create your controller as a Type 2 and then you can at boot time say, "I'm going to advertise I'm a Type 1 only," or, "I'm going to advertise I'm a Type 2," or even say, "I'm going to advertise a Type 3 only."  So there's just some flexibility if you want to have different SKU numbers for the same part. Or you can save a very small amount of gates and just optimize this at compile time and just say, "I'm building a Type 3 device. I know I'm a memory application. I'm never going to use a .cache," and you can go that route. The other thing we do is we support a couple different interface options. If you look at the controller, I have a slide coming on it, but you've got the CXL side of the stack, which is the .cache and the .mem, and then you've got the CXL.io or the former PCI Express side of the stack. On the PCI Express side, we've always had a Synopsys native interface. I won't go into the details of how that works, but that's been an option for the lowest latency for a lot of our customers. And then customers that are connecting into an AMBA fabric for their chip, they often take the AMBA bridge on that, so they take the AMBA version of that IP. So we still support that for CXL. And then on the CXL side, the .cache .mem side, we also have a native Synopsys interface, which just looks like the channel interface descriptions. If you go into the spec and look at those with the host of device channels and the subordinate to manager channels, et cetera, for .mem. But now we have another one that we call the CXS interface. This is an ARM specification. It had CXS.A originally for CCIX, then they had CXS.B for CXL, and they've got CXS continuing to propagate now supporting CXL3. So I'll show a picture of what this looks like, but this is another option on the CXL side of things to interface. It works really well if you're plugging into the ARM ecosystem. If you're plugging into like a CMN700 Campos or something like that, then this is just a direct plug and play from Synopsys into that. It also allows you to support some interesting things. We do have a few customers doing this that are in silicon that have taken the design, and they're supporting what we refer to as CCIX over CXL. Some people refer to it as CCIX 2.0. More generally, CCIX 2.0 really means CCIX over 68 bite flits or CCIX over flits. And for CCIX over CXL, what this means is that once you use this kind of CXS interface, it means you're just taking a simple credited streaming interface,  and we don't really care what's in those flits. You've packed the flits yourselves. You send them across the wire. So one of the options is you can pack a CCIX protocol into those flits. And so you can effectively create a CCIX system that includes the symmetric coherency that CCIX was famous for with the low latency that you get with CXL. Kind of the best of both worlds without having to delve into the initial complication of the back-end validate channels and stuff like that. We build our solutions based on the silicon proven PHYs, and these are all proven now, multiple foundries, PCI 5 at 32 gig, PCI 6 at 64 gig. And then we have multi-protocol versions of these, too. So we have like 112 gig PHYs that you can run at 64 gig somewhere in your chip if you want to use it as PCI 6. And then we have the 64 gig kind of dedicated ones as well. We've done interoperability with the major host providers, including Intel Sapphire Rapids. And one thing that I haven't highlighted a lot in this presentation, but I do want to point it out, is we have a whole bunch of features in our IP that are intended to target these advanced ARM-based servers. These include some of the interfaces like the local translation interface, the MSI GIC, where that's the MSI interrupts mapped to the generic interface controller from ARM. And then more recently, something they call the ARM confidential compute architecture. So if you're following what's happening in the encryption/decryption and IDE world, this is something kind of on the host side. It's a trusted execution environment that together with T-disp on the device side creates kind of a complete secure link all the way from like a hypervisor down to a virtual machine. So they seem to be the first ones that have really defined this in a publicly available spec. They call it ARM confidential compute architecture or CCA. And we support that in our controllers as well. And when I say we support that in our controllers, these things are really living kind of on the CXL.io side of things. So they support it in the PCI 6 controller IP and they're supported in the CXL controller IP. We did the industry's first two-party public interrupt demo and compliance demo with CXL with the Teledyne LeCroy exerciser analyzer. And you saw that earlier in one of the slides. Great piece of hardware. And we were able to show compliance. We were able to look at the protocol and do all kinds of cool things with that.

I'm not going to go into a lot of detail on the security modules. We have some other experts in our company that would be happy to talk to you about that. But I just wanted to show this one slide because what we have is they're co-designed. So we have these IDE modules. It's a different team that designs that with the security expertise and encryption/decryption expertise. And then we have our controller teams. But we've agreed on the interface. We've designed it together. We interoperate them. We basically give them to you as two different IPs that then plug together through an interface that we've defined. And you can see they have a connection here up at the transaction layer. And then they have another connection down here. So there's a whole bunch of complexity here that I can't go into because I'm not the crypto guy. But we have these solutions available. And we have, as I mentioned earlier, many, many customers for CXL2, for CXL3, and for PCI5 and PCI6 as well.

Here I'm showing the two primary different interface options that are available. And there's some pretty interesting stuff here. If you're not that familiar with how CXL works, you've probably been through some of the spec presentations and stuff. But essentially on the right-hand side of both of these diagrams, I'm showing what is effectively the PCI Express 5 or PCI Express 6 controller,  which is referred to as the CXL.io path. So this has really got a few enhancements in it. It has to operate slightly differently. But first, all intents and purposes, this is a PCI5 or PCI6 controller. It's the whole thing. And then over on the left-hand side is a new protocol stack for CXL. It's a lean protocol stack. It's a very low-latency protocol stack. It's what enables CXL to give you this really low-latency performance. And this includes kind of a link layer and transaction layer. And in our native interface version, it comes out as CXL.cache and CXL.mem. It's not this simple. It's really multiple interfaces. As I mentioned, there's host to device, device to host, and there's subordinate to manager, et cetera. So there's a whole bunch of interfaces that come out that make up these two simple arrows I'm showing here. And then on the other side, we come through the ARB MUX, and we can select from this stack or this stack, put these together,  and then we send them out through the logical file layer through a standard pipe interface,  one of the beauties of the PCI Expressl spec is it uses the standard PCI Express files. On the right-hand side, it's the same thing, but now we're talking about one that's focusing on this CXS interface. So if you look closely at the difference, primarily what we're doing is we're taking out most of the transaction layer logic,  or what ARM likes to refer to as the upper link layer logic. That gets handled, including flip-packing and unpacking, up here in the coherent mesh network from ARM. Or it could be your coherent mesh network or your NOC or your application, whatever it may be doing. But you're taking this simple credited CXS interface here, and you're coming through the stack in the same way,  but you don't have the transaction layer, instead you have the CXS interface layer. So it ends up being about the same in terms of complexity, gate count, and separator,  but you get a little bit lower latency when you take this version. But the trade-off is you're pushing some of the complexity up here. So in terms of system latency, maybe you don't get any lower, but you get a lower controller latency. And again, on the right-hand side, this version allows you to do CCIX over CXL. It's a direct plug-and-play for the advanced CMNs, the 700 and what comes beyond that. And it allows you to mix and match, too. So you don't have to use just the options I'm showing here. You can actually take the native CXL.io and the native, which is what we're showing on the left. You can take the native CXL.io, which is the PCI Express, together with the CXS,  which is not an option I'm showing here. Or you can do kind of the opposite and take the AXI with the native, or you can take what I'm showing on the right-hand. So there's four different possibilities, and they're all configurable by you when you're building the IP.

So one of the things we do at Synopsys, we pay a lot of attention to this, is interoperability, testing, and validation. Because we're usually very early, we have to have this ready very early. So as I mentioned, we've been testing our first demos of PCI Express 6. We're back in 2021 and 2022. We had it all over. 2023, we showed it. So we do this early and often. When we first do it, it looks kind of like this. Here, we're showing a HAPS 100 system as an endpoint, another one as a root complex. And we're coming out of these cables that are pipe-connected to a daughter card with a Synopsys PHY. We're going across a small backplane and connecting here. So this is an end, what we call an end-to-end demo, where we can send flip traffic across. We can analyze the throughput, inject errors, do things like that. And below, we're showing the same thing that we did for a PCI 5. Different form factor. It's a HAPS 80, not quite as slick-looking as the HAPS 100, but effectively the same thing. But down here, we're showing a use case we do all the time, too,  which is got the Teledyne LeCroy interposer in here. So this is the interposer with the exerciser analyzer so that we can look at the packets,  we can inject errors, we can do all those fun things that Gordon was talking about earlier. And we've done those. We do interop at virtually every workshop. We've done lots of them so that we've got lots of endpoints and RCs qualified at 4.0. As most of you, I think, may know, as of today, there's only a 5.0 compliance workshop. The 6.0 compliance workshops aren't happening yet. There's events that are going to be pre-FYI testing, then there'll be FYI testing,  and then eventually there'll be 6.0 real testing for a 6.0 integrated list. As of today, 5.0 is all there is. And so what we've done, and this was partially on the request of our customers,  they're like, "Hey, I'm building PCI 6, but there's not a lot of PCI 6 stuff that I can talk to. So I need to make sure that I can plug this into somebody's PCI 5,  as an example, Sapphire Rapids or something, and make sure it's going to work."  So we went ahead and took our 6.0 PHY controller, both RC and EP configurations,  and got those on the integrators list. So you can go to PCI 6 5.0 integrators list to see our stuff there. And we do have the most entries of any IP vendor on the PCI 5 because we do this so much. This includes the 6.0, it includes CXL2, and we were the first IP vendor to have the 5.0 host  on the integrators list as well.

This is showing the demonstration and kind of what it looks like here. Our CXL, normally our hardware prototyping kits and our hardware validation compliance are based  on a by one, our controllers are not FPGA oriented, they're ASIC oriented for maximum performance. And so we end up having to slow them down to run them in FPGA, and we can't run the full width. So normally we're running a by one with a Synopsis PHY card, like I told you. What we did for CXL instead is so that we could have a x8 that would plug  into a x8 Sapphire Rapids host, we slowed it down to Gen 3, we ran x8,  and instead of using RFI, which we'd already proven for 32 gig, we used an inbuilt Xilinx PHY,  which is the GTH inside here. So this was a Gen 3 speed, but running CXL protocol x8,  and this is what we validated a whole bunch of protocol stuff with. It's what we interoperated with Sapphire Rapids as well.

This is just revisiting the announcement from Xconn, the Synopsis and Xconn collaboration,  where they developed this 256 lane switch using IP. This is a great achievement, first pass success, and a really complex design if you think about it. And also we're moving forward together for CXL3 as well. So we're looking forward to having...hopefully we'll have one  of the first CXL3 switch chips out there too. And those are key things. When you look at all the diagrams, I'm sure you've seen tons of them showing CXL  and how it's taking over the world, it's enabling all these fabrics and all this other stuff. One of the things you see in virtually every diagram are switches, right? So we need switches and they have to be out there before all this great stuff can happen.

Some other proof points, we did the industry's first 64 gig link up. This was not a full interop like I was talking about before where we're passing flits. So before, this was the first partner that was available with something that could talk to us  for PCI Express 6.0 once upon a time. And in this case, it allowed link up. So we could link up to L0, we could do speed changes, but we couldn't pass flits. So we did this back in January. Now since then, we've moved on, we've done the whole flit transfer.

Another thing we were really excited about is at the Intel Innovation Forum,  we together with Intel, we took their 6.0 test chip, which they weren't sharing widely,  and we interoperated that with the Synopsys PCI Express 6.0 solution. So we took our fine controller, connected it to their test chip. Their test chip is just kind of like the PHY plus the PCS. So it enabled us to do link up to L0 and then do speed changes. And we were just running these speed changes over and over and over, thousands and thousands  and thousands of times without ever falling out of L0. So really great robust demo that we did at the Intel Innovation Forum.

I mentioned the PCI 6.0 solutions that are on the integrators list. This is important, I think. Again, this is the only proof point there is for PCI 6.0 today. And these are the four that we have on there. We've got the PHY and the controller for RC and EP. And you can go to the SIG integrators list and look at that at any time.

Okay, so I wanted to kind of end just looking at what we're doing for CXL3 and looking at 3.1.
 
We have a lot of features that we support. So I have a table that's way longer than this talking about the PCI 6 features that we support. In fact, it's like five or six pages of this kind of stuff. There's just so many features that are supported for PCI 6. And the reason I'm mentioning that is that those features as PCI 6 are available as part  of the CXL.io within a CXL3 controller. So when you license a CXL3 controller from Synopsys, of course it supports PCI 6. We support PCI 6.1. 6.2 is coming up. And we have all these features in here, these different data paths for the different widths,  the native CXL interface, the AXI that I mentioned, 256-byte standard FLIT support. You may have heard of the 256-byte latency optimized FLIT support. We support that as well. Device host and dual-mode support and switch port support. That's another feature and it's got two flavors, HBR and PBR. PBR is port-based routing. Multiple logical devices are supported. The two key ECNs are back and validate cache scaling, which help us get a degree of symmetry  there. L0P, which was supported in PCI 6. It's something in PCI 6, but now in the CXL3 controllers it can be supported in CXL3 mode. And then we have PBR support for endpoint. This is specifically targeting GFAM applications. And then we have the IDE support in both of our interface types. So for the CXL3 native and for the CXS interfaces, we support IDE. And then this last one, this is one that was surprising. It showed up in the CXL3.0 specification. They call it direct peer-to-peer to host device memory. It required UIO to work. And UIO was an ECN to the PCI Express specification. So we support this and it requires UIO to operate. So it requires multiple virtual channels and unordered I/O support, the PCI Express 6 or  the CXL.io. And we support that.

 As we move forward to 3.1, there are some new features in there and there's a lot of  errata. So we have this support for the applicable 3.1 errata because a lot of the errata really  affect the upper layers beyond the application layer, like beyond the controller up in your  application or even in firmware. But we support direct peer-to-peer for CXL.mem. So that's a little bit different than the one I was talking about before. TSP, this is, if you want to make a comparison, this is like the CXL equivalent of Tdisk for  IDEs, kind of enhancement IDEs for virtual machines. Extended metadata trailers and header logging as well.

So in summary, Synopsys is offering a really comprehensive CXL portfolio. We've got huge amount of proof points, customer data, customers in silicon. We've got some that are sampling or in production. We've got the broadest portfolio. So we've got the CXL controllers and PHYs for 2.0, 3.0, 3.1. Advanced coverage of the PCI 6 features and CNs, PCI 6.1. And then the PHYs are in a tremendous number of foundries. I didn't include that information, but I think there's like 16 or 18 foundries in the 32  gig variety. And for the 64 gig, we've got four or so, or maybe more. That number will keep rising. We believe we have the lowest risk solution because we did this so early and we've been  through so many designs with customers already. We've already got over 120 licenses. We've got over 350 PCI Express 5 licenses, and we now have over 50 PCI 6 licenses. So this is really a lot this early in these spec evolutions. And the reason this is important is just because it means that by the time you utilize this  from Synopsys, it's really robust and it's really low risk. Then we have the complete solution, including the IDE, the verification IP, the PHYs, the  controllers, switch port, dual mode, extremely low latency. We're very competitive. We're below, well below the specification of 20 nanoseconds round trip time. Customer proven timing closure in lots of different processes at gigahertz so that we  can run one gigahertz for 32 gig or one gigahertz from the 64 gig at 64 bit pipe. Utilize data path architectures for all these different configurations. We have something I didn't talk about at all, but it's a huge advantage to our IP. It's a very extensive set of RAS DES or debug error injection and statistic features that  are really important for these early spec rollouts where you might want to debug your  silicon. You might have to debug what's going on with your partner. You can inject errors at all different layers of the stack and you can record events and  then analyze what's happening. And then of course we have the AXI bridge interface and the advanced ARM support.

So I'll just wrap up with this final one to find out more about Synopsys CXL IP solutions  if you're interested. If you already know that you have a Synopsys sales rep or field application engineer that  you've been working with, please contact them. They'll be able to connect you with anything you need. And if I need to get involved in a conversation, they will bring me in. And if you're not sure about your sales rep or FAE, feel free to contact me directly. I've just included my email address in there for that purpose. And I think that's it. So if there's questions, I'd be happy to entertain them or we can wrap up.
