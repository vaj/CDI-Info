
Hello, I'm Jonmichael Hands, and I'm  the SNIA SSD Special Interest Group Co-Chair. I'm going to give you guys the EDSFF update for the SNIA  Compute Memory and Storage Summit.

To recap, EDSFF has now changed its name  from Enterprise and Data Center SSD form factor  to Enterprise and Data Center Standard form factor. The big change is now EDSFF is going  to allow other device types. And the first new device type that we've seen is CXL,  but there'll be more device types allowed. Like, as you may know, something like OCP NIC  shares the same EDSFF connector. So there's a whole family of form factors  that can be in these standardized form factors. But we're going to talk about SSDs mostly today  because I'm an SSD guy. E1.S is really designed for compute SSD. What does compute mean? Compute really means server. It means that these are going into mainly hyperscale servers,  because the hyperscalers designed  E1.S to fit vertically in a 1U server. They've designed them to be modular. So there's different options for E1.S  if you want to have it more M.2-like without a heatsink,  or if you want to have it in a standard backplane that fits  vertically in a 1U server. Obviously, it supports hot plug in different thermal  environments. But it was designed to be very modular and scalable,  like M.2 that was used in hyperscaler, where they just  wanted to scale the number of drives when they wanted  to scale the performance. And quality service and isolation,  but have the same performance as U.2. You can have E1.S that go all the way up to 25 watts  in a much smaller form factor. The E1.L and E3.L are really these high-capacity optimized  form factors that have just a lot of flash on them  and are designed for things like-- maybe  think about high-capacity QLC or high-capacity TLC that's  designed for storage workloads, not compute workloads. So actually storing user data where  the data has to have data protection and replication  and all that good stuff. So E3.S is really your mainstream form factor  for enterprise servers. Again, servers have to support 1U and 2U variants. And they needed a form factor that can fit in a 1U  horizontally or a 2U vertically. And also, when they were designing it,  wanted to make sure--  when Dell and HP and I worked with the folks that  wrote the original spec, the idea  was that you want to be able to have this standard version,  the E3.S, which is 7.5 millimeters,  be able to be smaller than U.2 so you can scale the number  of I/Os to get performance. So a lot of the modern CPUs have 128 PCIe lanes. Or if you have a two-socket server,  you have a lot of PCIe lanes. So the idea was we wanted maybe some back planes  that have options to have more I/O in the front. And so one of the main benefits of E3.S  was its performance scaling of just the number  of drives per server. Obviously, E1.S is much faster than M.2.  Because of the much higher power envelope. But the big benefit of the scaling and EDSFF  comes from the rack unit per server, per number of drives  to be able to scale the I/O. EDSFF was designed for PCIe Gen  5, for Gen 6. It also supports CXL2 at 2.0 and 3.0. We'll talk about today with CXL memory modules. The first we see is E3.S 2Ts. And EDSFF is designed to be more thermally efficient,  so easy cooling, different thermal options  for different server environments, and flexibility  to reduce TCO. And I have some really cool use cases and examples  I'm going to give to show that.

As far as the specs, EDSFF comes from the SNIA SFF group. And the SNIA SFF specs are the ones  that define the actual EDSFF form factors. There's three form factors for the actual physical dimensions. So there's E1.S, E1.L, and E3. Those specs have not been changed much recently. E1 specs haven't been touched since 2021. E3.S specifications, E3 spec 1008,  was updated in 2.1 to add support of NIC sidebands,  2x1C. So if you wanted to have a standard backplane  with the by 4 connections, but have  a device that has multiple. And then clarifications around some of the stuff  that they would need for maybe some CXL stuff. 1002 is the connector. So this is the actual high speed connector  that defines what's put on the backplane,  what's put on motherboards, what's put on the drives. And it's interfaced agnostic, meaning  that a bunch of other use cases beyond just EDSFF  could use this. But you'll see it's mostly used in EDSFF. The additions to the 1.5 spec were really  to add Gen 6 support. There's some errata, obviously, being  able to have this connector be able to be used  in high speed cabling systems. So again, there's been updates there. The pin and signal spec has been updated to 4.0  to officially support PCIe Gen 6 and CXL LEDs. This is the pin list and placement for PCIe and CXL  that basically just says where they all go. As for when you're building one of these EDSFF SSDs,  what does the pin out look like as far as PCIe signaling  on the connector? The next is this SNIA SFF TA1034, which is the pluggable  multipurpose module. Now, this is a general purpose form factor. The first that we're going to see  is potentially the ability to take something  like an OCP NIC, which already leverages the SFF TA1002  connector. It uses the X16 variant of that for most of the OCP NICs. But being able to have it in a pluggable form factor,  that's how pluggable like an E3. The last is the SFF TA1023. And this is the thermal specification. So as you can see, in a lot of the EDSFF specifications,  power has now been moved to an informative specification.  So meaning that it's no longer thou  shalt be under this power in the EDSFF. It's really about the different thermal and airflow  requirements to cool a specific power on a device type.

Thank you, Supermicro, for letting me kind of steal  the slide from them. But it really shows off the benefit  of superior signal integrity. So when you can reduce the number of connections.  And back in the day when we looked at PCIe topologies,  when you had PCIe connections that could come from the PCH,  that can come from the CPU, that can come from retimers,  from switches with three drivers behind them,  and there's all these different types of PCIe connector  topologies, in PCI Express, you have a loss budget  for the entire channel. And the more connections you have,  the more loss you introduce. So one of the things that they utilized  in one of their server designs was  having the mainboard connection for the SFF TA1001C connectors  basically directly to the cable. And they're saying reduction in 40% of the signal loss  across that topology. So this is a way to significantly improve  signal integrity without a tremendous increase in costs. I know I've talked to customers, and one  of the big hurdles for adoption for that Gen 6  and other higher speed protocols are going to be  are basically cost. So you're going to have to be able to design  these new interfaces, new speeds,  at the same or lower cost as generation before. And EDSFF allows you more tools in the toolkit  to be able to do that. Compared to, say, for instance, U.2 backplane,  where you're having an extra hop, an extra connector  on the backplane, the U.2 drive itself  has a higher insertion loss than an EDSFF drive. So overall, the signal integrity of an EDSFF design  is going to be much better. And overall, again, at the system level, what that should  do is reduce system costs. And airflow is much improved because you  don't have this big backplane that's blocking all the air  with the cutouts. You can see the connectors are going right to the drive  backplane, and you have free airflow  that goes between the drive.

So we'll talk a little bit about E1.S trends. Thank you, Storage Review, for letting me pillage  some of your pictures. As we've shown in some of the previous CMSI EDSFF webcasts  last time, we went around the OCP Global Summit  and took videos of all these servers. Obviously, Meta and Microsoft were big proponents  of the E1.S and have designed E1.S across their entire fleet  of OCP servers. And so you can just go to the Open Compute Contributions  website to see all the different server designs,  such as like Grand Canyon. Even a hard drive machine has E1.S for the caching drives. So E1.S is very pervasive now in the hyperscale. And you can see now other platforms in the enterprise  that are bringing maybe not OCP type designs,  but bringing these standard E1.S to 1U servers. And I think, again, the E1 is designed for 1U servers. It really shows the storage density in these servers  cannot be matched by U.2. You just have-- it is obviously just the optimal form  factor for a 1U server. You can see here in the middle, one of the new trends  is that now it's not just Intel and AMD. You have NVIDIA as a major platform vendor,  as AI is now the heart of the data center. And they have adopted E1.S as the form factor  for these high speed training servers. And you can see in this design, you have for E1.S here. And again, a lot of people are--  a lot of these AI companies are using EDSFF in training. They're using them in inferencing  to have stuff local to the GPUs. They're using them in that training phase  for checkpointing. So you need a very high performance E1.S,  but they don't want to take up a lot of space. And it's really quite literally the perfect form factor  for them. So I'm very excited to see, again,  not only EDSFF in traditional CPU platforms,  but also now moving into the GPU and AI platforms.

E3.S, again, thank you Storage Review  for letting me steal some pictures. This is just a cool picture of one of the Dell servers. And basically, you can see here just in a standard server  now, they can fit eight drives in a 1U server,  but it's not taking up the full backplane,  so there's better airflow on the other side. So there's just a bunch of different options for E3  that make it really suitable for-- you can see here,  I'm showing the 1U and the 2U variant for a standard 2U  server, which has some drives in the front of one  of these storage bays. But it just shows the flexibility  that this E3 was really designed for these mainstream enterprise  servers. Where you have a bunch of--  again, a standard server might have--  in the past, might have just had CPUs and some DRAM  and a few SSDs. Now there's GPUs, there's more NICs, there's more accelerators,  there's DPUs. There's just more stuff in the server. So these 2U servers, saving that space  and saving that real estate with E3 is going to be huge. And then also, obviously, you have the ability  to create these storage servers with insanely high density.

I'm going to give a really cool example of how  you can use some of the flexibility in E1.S  for different thermal environments. So the E1.S spec for 15 millimeter and 25 millimeter  go all the way up to 25 watts. Now you may want to use 25 watts if you're  looking for the highest performance,  and you have a few drives per server. Or if you're looking at PCIe Gen 6,  you may certainly want to use up to 25 watts. But if you're looking at a storage-based server, where  you're looking at TCO, well, a storage--  this server I'm going to give an example of  is a super micro server that supports 24 E1.S 15 millimeters. I'm going to show an example of running them at 25 watts  versus running them at 16 watts. So if you think about what hyperscalers have figured out,  they have figured out that they want  to run the drives at least a higher temperature  than standard, basically at the upper limit of what  the operating temp range is. They can run the fan speeds at a lower speed. Now if you look at a--  the fans are going to be limited by whatever the highest power  components in the system are. So if the drives are running hot,  then the fans will ramp up to 100%. This is this scenario on the left here,  running the drives at 25 watts. Now if you reduce the drives to 16 watts,  you obviously reduce the power from 25 to 16 per drive times  24. That's this reduction in this light purple. But the thing that's counterintuitive  is if you can reduce the fan speed from 100% down to 30%,  then there's actually a very large reduction in fan power  as well. So in this scenario, we're able to actually save  26% of the power on this single storage server  just by reducing the SSD power, and nothing, no change. Now 1U servers are obviously notorious  for these very, very loud, high-performance 40-millimeter  fans. They can consume up to 30 watts. In this specific server, they have eight of those fans. You can see here, fan 1, dot, dot, dot, fan 8, and 24 SSDs. Now again, these-- so not just from a thermal perspective,  but from an audible and noise perspective,  it's a significant reduction.

Now if you take that and you pop it  into our very friendly SNIA  TCO model,  I have limited, basically, drawn up an example rack  with two PDUs. These are 15-amp PDUs running 208 volts. And a standard PD limit for this kind of rack  would be 15 kilowatts. And you could say one server to 16 servers,  however many servers are in that rack. Now if you're limited in that rack by power,  you can actually fit 45% more servers per rack  if you're estimating-- if you cap the drives at a 16 watts  versus capping them at 25. And so that's a pretty significant reduction in TCO. If you look at--  you pop these into the SNIA  TCO model,  we're able to see a 29% reduction in TCO. Now obviously, if you're running a storage server,  one of the most important things you can do  is run higher-capacity drives. So if we look at this on the right,  we move to 15.36-terabyte drives,  and that even further reduces the TCO. But you still get a very good reduction on the 15.36-terabyte  drives when you're running 24 of those  by running them at 16 watts versus 25. So this is just a nice example of how  something as simple as a little bit of SSD power  can significantly impact rack-level TCO.

I was very excited this year. I went to the CXL Developers Conference,  and I went to MemCon. And it was super cool to see all these new CXL devices. So Samsung showed off. These are a couple of pictures  I took on my phone. They showed off a CXL hybrid memory module  that's supposed to have some SLC flash plus some DRAM  and these super caps in here and maybe a different kind of CXL  use case with some NAND. They showed off the CXL memory module with DRAM. And SK Hynix and Micron showed off the same at their booths  at the CXL Developers Conference,  these CXL memory modules. This is just really cool. And what they talked about at the CXL Developers Conference  quite a bit was these use cases for CXL improving memory  bandwidth and memory capacity. So again, there's going to be other use cases like memory  pooling and other expansion and all this stuff. But for the CXL memory modules, for the CXL 2.0 stuff  that we have today, it's really about increasing  the amount of memory per system and looking  at workloads. That need more memory and not just memory  capacity but also memory bandwidth  because CXL can expand it. Now, what better form factor to do it in than the E3.S2T?

So these CXL memory modules, there's  now a JEDEC standard called JESD317A is the latest version. And that's available for download on the JEDEC site. But it defines the EDSFF form factors  for use in CXL memory modules. So you have an E1.S with a x8 connector. So this is a little bit less-- it's not as typical as a E1.S  with all of the SSD when you have a x4. They have an E3.S2T with a x8 connection. That's, again, the 2C connector. And you want x8 because you want more PCIe bandwidth. Again, CXL uses the PCI Express electricals for the bandwidth. So the same eight lanes will give you  more bandwidth at PCIe Gen 5. And then they have defined the E3.S with a x4 connection. So exactly the same as an SSD. And so you could make a CXL memory module that's  backplane compatible with SSDs.

And again, thank you, Supermicro,  for letting me steal this picture from them. But this is just a really nice example  of how you can mix and match CXL and SSDs with EDSFF form  factor. You can see here in the server, they have these slots that. Again, I think one of the things that people may have overlooked  when the EDSFF spec was invented  was that you can have a slot that is compatible. Where  you could have two slots for E3.S,  or you could have one slot for E3.S2T. And they can be backplane compatible. That was impossible. U.2 15 millimeter and 7 millimeter  weren't compatible with each other,  meaning you can't jam a U.2 15 millimeter  into a U.2 7 millimeter slot and just take up the other slot. But EDSFF was designed with this backwards compatibility  in mind. You can see here in the front of the server,  they have this one with maybe four CXL memory modules,  and then eight SSDs and a 1U. And so this is just a very cool config  to allow you to mix and match these devices.

Well, it's my presentation. I get to rant a little bit. One of the things that just drives me absolutely insane  is the lack of workstation and high-end desktop adoption  of EDSFF so far. We need to fix this. Oh my god. Nobody wants to buy a $15,000 workstation with M.2  or hard drives. Please, please, please. M.2 is great for boot, but not suitable for high-end  workstation workloads. Now, EDSFF, the drives themselves,  like the hyperscalers, have gotten low capacity,  one and two terabyte drives that are extremely suitable  for boot. They're extremely suitable for any kind of workstation  workload. Quite frankly, power is not an issue. The difference between M.2 at 8 watts versus, I mentioned,  E1.S with 16 watts, when you're talking about these systems  with dual 4090s at 450 watts each or 350 watt TDP CPU,  the couple of watts difference between for EDSFF  is definitely not going to be an issue in these workstations. So again, this is not to hate on Dell. I love Dell. I actually have the earlier version of this,  the Threadripper. I have the 7865. And again, my only major gripe with this workstation--  I really like the workstation-- is that it's very hard  to put NVMe drives in there. You can't put enterprise NVMe drives in there. Now, they could certainly delete these hard drive  bays or these flex bays with the M.2 things. They have this little M.2 carrier thing. And just put EDSFF in there. That would be a perfect use case for E1.S for 6  across the front of a high-end workstation that already  has a lot of PCIe lanes. So please, if you make workstations, adopt EDSFF. It is really the perfect form factor for workstations.

Now, Forward Insights has let me--  agreed to let me present this. So thank you, Greg, at Forward Insights for always  the really informative market updates. And so you can see here, this is percent units on the left  here and percent petabytes on the right here. It's really useful to look at both,  because obviously, sometimes the petabytes,  something like an E1.L where the average capacity  drive is much larger, this will show  the adoption much better. So you can see here, as we get into--  we're approaching 2024, 2025 here. Now, by units, we're already about 15% going into next year. Now, by form factor in petabytes,  we add a huge chunk of E1.S already in 2024. So you can see the purple and this pinkish are E1.S. And 2024 is estimated to be almost 20%  of the petabyte volume. And you can see all the way up to 2028,  when we actually eclipsed 50% of the total volume. Now, this is-- I want to say, to me, this is actually  really disappointing. We started. I was at design.  I was on the team that did the ruler. And we were designing the first EDS form factors, E1.L,  in 2017-- at the end of 2017. Having E1.S is in production all the way from 2018,  and now we're in 2024. Now, in the hyperscale, you can see it happened very fast. This shift from M.2 to E1.S is obviously  happening much faster in the hyperscale. Because they've just designed E1.S and all the platforms. U.2-- originally, we were thinking maybe U.2 wouldn't go  to Gen 5, or the insertion loss or electrical costs  would be prohibitive to system cost of U.2 at Gen 5. And certainly, I don't really think U.2 at Gen 5  makes very much sense when you have E3. It's just a better form factor. Unfortunately, there's all these slots out there  that supported U.2, and so all the SSD vendors  had to make their new drives also compatible with U.2. And you're not going to make a Gen 5 drive--  and not going to hobble it down to Gen 4  if the spec will allow you to go to Gen 5. So this, from a platform side, has been kind of frustrating. But from the system side, now you've  seen that we have systems available from all  the enterprise OEMs, from hyperscale systems that  support E1.S. I don't think we'll  see an issue going forward. In Gen 6, it'll be a much harder transition,  because U.2 is just not coming along for the ride. And so the transition to Gen 6 platforms and CXL 3.0  platforms will really drive the further adoption of EDSFF. Again, just griping a little bit,  I would have loved to see faster adoption. I would have liked to not see Gen 5 U.2 deployed  at any enterprise OEMs or any servers. I think that just slows everything down. I realize that a bunch of SSD vendors  already have U.2 drives. And so if you're a platform vendor,  you're not going to make a system that just doesn't  support all the drives that are already in production. So these form factor transitions are always  just very, very tricky. But it is finally happening. We are seeing the volume pick up on EDSFF. And there will be a future where that's  the only major form factor. So I'm actually even more bullish than this outlook  here that Forward Insight has provided.

So with that, in the backup, I've  included some slides that I'm not qualified to talk about. But this is from the SNIA  SFF committee  directly on some of the updates of what's happening. Just for the folks that aren't following.  I did mention the pluggable multipurpose module that's  really designed to be utilizing EDSFF in this new form  factor for all general purpose devices, the front or the rear. There's a new internal high-speed cable  and modular connector system, updates to the SFP+ and QSFP  connectors, mini link 4 by 8 shielded connector.

These are the new specs. And then the revised specifications,  I did talk a little bit about some of the EDSFF ones before. But this is the full list of things  that the SNIA  SFF committee has been up to. And they have been very busy. So with that, yes, thank you again  to the SNIA  SFF committee and all the hard work for the EDSFF  specs for drive vendors like myself who just utilize them. And it's very, very important that we have these. And there's an industry standard for SSD  and a standard form factor. And with that, thank you, guys. Please rate the session. And let me know what else you'd like  to see in the EDSFF updates. Thank you.
