
Good afternoon. This is Brian from S3 Platform. We just built 256-LANE CXL Switch, so it's my pleasure to present you this new solution.

So today I'll talk about system architectures and hardware design, software features, RESTful API and also the GUI. And of course, we will show you some of the very first test results in the latency and the bandwidth.

On the right side you see the physicals in the rack. So there is a compute node and there's memories put in a separate box. In that separate box they put six memory modules in and all the chassis are interconnected by CDFP cables. So the CXL memories in the memory drawer can be bound to servers. Then the server sees its own memory, then form a logical node. So in the logical nodes all the apps will use that memory as a new memory.

This is two examples of the topologies. Let's say eight hosts connected to one single CXL memory box, then you create a memory sharing. It's memory pool. I should say it's a -- so we call shared memories, in fact it's memory pool, it's not shared memory. But memory could be assigned to one host, then returning back when it's no longer needed. And then in some of the cases if you want to have bigger memory clusters, you could have four hosts and four CXL memory chassis, then you create up to 24 terabyte memory box for this four hosts to use.

This is a real box. So we put a management CPU in it. So what does the management CPU do? It creates a host virtual CXL tree, so when the host sees it, it will see virtual devices from the host side. And then, of course, it will set up the CXL switch. For example, like configure port as upstream port or downstream port. Bind and unbind the memory from a single host. Or it will collect all the CXL memory information and aggregate, provide to the Fabric managers. Then on top of it, of course, we provide a RESTful and GUI. And you can go through the Ethernet to manage this mCPU. And the CXL switch is doing the device lookup table, DLUT tables, and of course transfer all the packets from the host to the device. The CDFP is running as a PCIe Gen5 x16s. And it's connected to the host and also to the memory module.

This is a software block. So we have a software block to do the chassis management, like fan management, power management. And then also the switch register configurations. Like mCPU is doing the register measurements. And then, of course, we also manage the memory devices to collect all the information from the CXL memory controllers and then aggregate as a CLI. Then we use the RESTful API to integrate with system orchestrations. So we got a system orchestration command saying, hey, which host you need what memory. And we follow their rules. And then there's something that when they want to remove the memory from the host, we will double confirm with them, hey, are you sure you want to remove this section of memory? Because otherwise, if you just move the wrong memory, the host will be dead.

This is a GUI. This is a host view. I use host view as an example. On the top bar, you see there's a reserved host memory address range. In this example, I use 8 terabytes to 9 terabytes as a reserved memory address for a single host. And the blue part is the memory chunk. It's on the CXL memory. So you see that memory chunk been mapping to a different memory, local memory address. On the right bottom side, you see there's all the memory chunk assigned to this specific host. In the middle, there's a variable memory chunk to be assigned to the host. So you can use this GUI. You can give ideas how we manage the memory mapping to the host.

This is a RESTful API. On the left side, you see all the API list, including like port settings, memory bindings, or CXL mapping, or you do the user management. All those kind of things is in the single units. I use CXL memory device query as an example. When you do the query, you see on the right screen, there's a list of all the different CXL memory modules information. So take the very first example. The first one is located in the bus number 4 and being assigned to host 3. And there is 128 gigabyte CXL memory. And the controller is coming from Montage. This is a Samsung 128 pre-production CXL memory module.

We set up the testing system by using Intel Sapphire Rapids servers. So we put one of the retimers on the CXL slots and go to the CDFP cable to the CXL switch. And then there's a Samsung 128 gigabyte CXL memory module being installed to the CXL switch. So then in the software side, we use the Ubuntu LTS 22.04 and Linux kernel 6.2 and Intel memory latency check 3.1. So this is the software environment.

The very first data that we got is that the direct attached CXL memory module is 260 nanoseconds. Switch attached is 500 nanoseconds. So again, it's a request coming from the CPU, go to the retimer, go to CDFP cables, go to the switch, go to CXL memory controller, go to the DDR, all the ways and back. I know it's bigger than people's expectations, but because all this retimer, switch, and CXL memory modules are in pre-production stage, so we do not really dig out where is latency coming from. We will retest it in Q2 next year. Everything will be in production units, and we will retest it.

The throughputs, if you over phy CPU phy rate, the direct attached and the switch attached, you will get 22 gigabytes per second. Yeah, I know it's less than 27 like everybody expects, but again, it's a pre-production one. And less than four threads, it's just 1% difference, so it's negligible. So from the bandwidth point of view, it's almost the same for the direct attached and the switch attached. We use O-read/O-true. This is O-read/O-true, the MLC test.

So we just do the CXL switch, it's a small portion of the total CMS ecosystem. So we need CPU help, we need OS help, we need orchestration people, we need a retimer, CXL memory module, and the cables. So please join to bring this developing solution to deployable and mature memory pooling solutions. We have the demos and lots of folks and also have innovation products in the CMS experience centers. So you can go there to check the solutions. Before ending my presentations, I want to thank you, Manoj, the CMS project lead, and all the participants for the CMS, because CMS project is a very great place to learn things and to talk about things. So there is a problem for me, because the weekly meeting is on a Friday midnight, Taipei time. It is very hard to keep a clear head for me. Normally it is beer time, right? Thank you very much.
