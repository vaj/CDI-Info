
Anyway, I'm happy to be here. I'm gonna talk about some work that we've been doing at Hewlett Packard Enterprise over the last, well, about six years now. And even before that with the machine project and HP Labs, we've been working on disaggregated memory systems and memory-driven computing. So I wanna talk a little bit about some of the work we've done more recently.

I have this paper with two colleagues, Sharad Singhal, who's here in the front row, has been on this program from the beginning, and then Clarete Crasta, who's not here today, but she's a big part of this work too. I'm gonna talk a little bit about why we started this program in the first place, how our fabric attached memory architecture works with the Slingshot fabric, and some proxy applications and work that we've done here and ideas for the future.

So obviously there's the big data problem. Everybody's seen some version of this graph where data is growing exponentially. And we have to have some way to not only store it, but analyze it and try to parse the useful stuff from the cat videos. And so that turns out to be a fairly difficult problem, especially with the sheer mass of data and real-time requirements on this data. It doesn't really do you much good if you're trying to predict tomorrow's weather, if your simulation takes a week to run. So there are some requirements on the performance of most of these analytics problems.

Also, we've had a kind of a revolution in high performance computing over the last few years. We're moving towards more heterogeneous architectures where instead of having massive numbers of compute nodes that all look alike, we have specialized functions in the network appearing like hardware specifically designed for running machine learning and developing models or making inferences. And even simulations that have been in the traditional domain and high-performance computing, such as genomics and CFD calculations, are being informed by AI these days and sped up sometimes by a lot. So you can increase the performance by decreasing the search space and converging simulations much faster with inference models. And then we have a heterogeneous workflow that can be complicated. For example, you can take a simulation that's informed by an inference model and then the simulation can go back and verify the inference. And if it turns out that the machine learning model is off by a lot, you can stop it and say, well, wait a minute, we got a few more data points. Let's retrain and then stop the job, retrain the inference model, and then go back and run it again. So the actual data flow is not linear in many cases. So one of the things that's happening in high-performance computing is we're gonna be moving away from this kind of batch job paradigm that we have now using things like Slurm, and we're gonna have to have workflows where we can have other things like Kubernetes and these AI workflows coexist in the network. So some of these things have enormous memory footprints. You can see the latest versions of the NVIDIA device, for example, this GH200 Grace Hopper module where they've increased the memory footprint. And the reason for that is these machine learning models have huge memory footprints. They're basically using the Grace chip as a memory controller because you can't get a lot of capacity on the HBM on the accelerator. So that's one way to solve it. What we're doing is attaching memory devices to the Slingshot fabric and trying to disaggregate memory that way. And so let me talk a little bit.

You've probably seen this slide a lot and this is the memory hierarchy, and it had this box in here for storage class memory, which was where Optane used to be. And so we now still have this gap, orders of magnitude difference in latency between DRAM and storage tiers. And if we really had something that would fill that gap well, we can speed up our workflows by a lot because we can store intermediate results and we can have a very, very large databases immediately accessible in that storage tier. So in the limit, if you take a device, say like the MI300A part from AMD where you've got eight HBM stacks on that device, this is a processor accelerator chip that's going into El Capitan and Livermore. You have about six terabytes per second of memory bandwidth on that device. So if you have two x8 CXL channels coming off of that, even for directly attached CXL devices, that's literally 1% increase in memory bandwidth. So you still have this enormous performance difference in that end, I mean, that's an extreme example, obviously, but the next generation of processors where they're going to 16 DDR channels with MRDIMMs, you still are going to have a tremendous difference between the memory bandwidth that's local and what you can get over a CXL channel. So we've developed an architecture here that was, we have a prototype that actually runs Optane in part of it. And then we are moving really towards figuring out how to use CXL devices and use a lot of CXL based memory servers to implement the segregated memory over the Slingshot network. I'll explain how that works in a bit.

So obviously, I talked a little bit about high bandwidth memory. I sit on the JEDEC 42.2 HBM committee. So that's one of my activities, but there's also an explosion in CXL devices, which you've probably seen at this conference. There's this big conversation now about CXL SSDs and there was talk about whether that makes performance sense over NVMe drives. I think there's some semantic advantages, whether you actually get improved IOPS or not, remains to be seen because we haven't really seen a production version of those devices yet, but it's interesting to us because of the memory semantics that can reduce the software overhead. But what you can do with CXL now will be nothing compared to what you can do when we have 3.1 devices. That's gonna be really exciting. So we can build memory fabrics in that case in CXL and actually in our systems, attach them to Slingshot and implement memory servers in our architecture.

This is an example here of bandwidth expansion. And what I'm showing here is Granite Rapids, Intel Granite Rapids, which has 12 DDR channels in the AP configuration. And so with CXL devices, this is showing eight devices x8 channels, I can actually get 40% bandwidth improvement here, but I do suffer latency penalties over CXL, which are not insignificant, but we can get more bandwidth and that's to be useful to feed more cores. And some of the core counts in this generation of processors are considerably more than you can get now. So you have to be able to feed the cores and get more bandwidth per core. This is one way to do it with CXL. So we'll see this at the node level in some cases, especially in the mainstream server business.

I'm gonna talk about the left side here. I'm not particularly fond of this picture, but we have a better one next. But the reason that we're pursuing scalable fabric attached memory, there are a number of them. Obviously I can increase the system memory capacity. If I can do that well, and I can store databases that are larger than the aggregate amount of DRAM in all my compute servers. I can also independently scale compute and memory, and I can disaggregate the failure domains. So if I have memory, if I have a redundant memory system, that's fabric attached, I can pull tricks like RAID 5 or striping and so forth. Where if I lose a module, I don't lose any data. And also if I lose compute nodes, I don't necessarily lose any data either, assuming I have the right assurances for resilience and logging and so forth. It's a pretty standard storage problem. But if you have that, then you can do something like the Machine system that we built at HPE, where we have a runtime that's fault tolerant. And you can have a seamless failover, and you don't have to do a checkpointing the way that you have to do it now in these high end systems. Let's see. Some of the challenges here, obviously this NUMA like cache consistency does not scale to tens or hundreds of thousands of nodes or cores that we have, even millions of cores in some of the systems that we're building in the leadership class HPC systems. So you have to manage that in software and you have to know, you have to have a mechanism for going which nodes are sharing data and who has what access rights, right access and so forth. And of course, on Slingshot, we don't have memory semantics the way that we had in the old Gen Z interconnect. So what we do now is use RDMA. And it actually turns out that's not, it doesn't hurt you that much. What we've had back when we had Gen Z before the Cray acquisition, we did a lot of work in this area and we really determined that our DMA engine gave us much better performance on these parallel applications than we really got doing individual load stores over the network. So while it was a nice idea, there's a lot of overhead associated to get a 64 byte cache line from point A to point B in a network and we're much better off doing that with a DMA transaction in general. So it really turns out that the movement to Slingshot interconnect did not hurt us much compared to the Gen Z network. So that was a happy result for us.

So we've developed this architecture at HPE called Golden Ticket. And it's kind of a fanciful name, but it's based on, we have a government sponsor for the program and they had all these names that were kind of based around Willy Wonka. So this is the Golden Ticket of Willy Wonka and other parts of the program were like View Paradise and Magic Lands and all that. But anyway, so that's the name. We've also had other names for this program because we kept changing it. And then we did the Cray acquisition. They also had a program that was similar to ours. So we merged them and changed the name again to Golden Ticket. And so we all had, instead of your project and our project, we've got the combined program now. And so we have been one team really for quite a few years, maybe three, four years now since the acquisition. So things are working pretty smoothly at HPE. And you've probably read about Frontier and El Capitan and some of the things that we're doing. So I think it's been very successful for us. But the Golden Ticket architecture implements fabric attached memory with memory nodes connected to Slingshot. And these memory nodes run a software stack to enable a system that we call OpenFAM. And this is available on GitHub. And so, you know, openfam.com/, or github.com/openfam. If you go there, you'll find our reference port that's publicly available. And that, we also have a version that we're tweaking that's optimized for Slingshot to use the special capabilities of the Cassini NIC and Rosetta switches. And so that at the moment is not public, but I anticipate that it will be at some point in time.

So we have built a prototype, which is in Houston. And this machine has 32 AMD Milan compute nodes that are dual socket. 128 cores each. And then we've got 20 memory nodes connected to the fabric. And half of them are AMD nodes that are just packed full of DRAM and SSDs. And the other half is a stack of ice like dual socket servers. And each one of those has eight terabytes of optane. So we've got more than 80 terabytes of DRAM in the system. We've got about 80 terabytes of optane in this machine. And it's been running now for about 18 months. And it serves our group of about 30 engineers. So it's pretty heavily used. And we've had a lot of time to wring it out. So I wanna talk a little bit about the software. That's the important thing.

We have the OpenFAM API, which was kind of based, it looks something like Shmem. So it has the messaging, kind of put, get paradigm that Shmem has. And we can do that to this shared memory, but it also provides a bunch of services. So there's obviously a security problem. You have to do provisioning, you have to do all sorts of bookkeeping, error handling and so forth. And the service, and we also have metadata for the data. But the basically the concept is that we implement regions and data structures in this shared memory that applications can share, collaborate with. So I will show you some of the examples of how we did that. For example, we ported the Arkouda library, which is running on our system. And Arkouda is actually a parallel implementation of NumPy. So the idea here is that a data analyst who doesn't know anything about programming one of these massively parallel clusters, can debug a program that uses NumPy on his kitchen table. And then he can jack into the supercomputer and have access to the big data and the accelerated data analytics NumPy functions just by importing Arkouda instead of importing the NumPy library. So it's seamless to him. And then also enables data analysts who don't know all the nuances of optimizing applications for parallel machines to use them and get the benefit of the accelerating.

So let's see, I think I talked about this. Yeah, we have an OpenFAM besides the get, put, scatter, gather operations. We have the ability to map data structures into the local address space. And then we can do the usual barrier operations and collectives, create and destroy regions. We can resize a region and we can allocate data items within that region. So there's a fundamental, you know, kind of Unix read, write, execute security model that's implemented now. And I don't know, I think it works. We might do something more sophisticated someday, but that's what we have at the moment. And then we have some other operations that allow us to optimize data movement in the machine. So we can order FAM to FAM copies, move stuff around. And then we have backup and restore operations. We also have a collective built in. So a lot of those things are implemented in the library.

This shows that basically we are maxing out the Slingshot network. I don't have a picture of the network in this slide deck, but basically what we have is we have four of the 64 port Rosetta two switches, which the ports are 200 gigabits each. And those are connected. First they're connected all to all. And then they're connected to the compute nodes and FAM nodes. And we also have a Lester file system that's attached to that network. So it's pretty close to 100% bisection bandwidth. So we have as many links going between the nodes, if you cleave them in half, we have as much bandwidth across the bisection as we have some total of all the injection. So we don't really expect contention on that particular design and was deliberately done that way just to see how fast our stack could drive this. And as you can see here with messages of 64 megabits, we're pretty much maxing out the Slingshot network and getting close to 25 gigabytes a second per port. So we can run these memory servers, which have two NICs a piece at 50 gigabytes per second each. And so that's what we're getting out of it.

We have a couple of proxies applications that we've done in addition to the Arkouda demo. And one of them is a radix sort that we've done. The other one is a sparse matrix vector multiplication. And so this is, I don't have the SPMV one on here, but this is an example of the sort algorithm that we implemented on FAM. So first we have data in FAM instead of this SSD. So the theory here is that I have two to 500 nanosecond latency to the memory. And then the network overhead over that adds probably a couple of hundred more. And then, but still the speed compared to, you know, 15 microsecond SSD, which is a fast SSD is really a huge benefit for these kinds of databases. So I can take the data and read it out of FAM instead of SSDs. And I just, I get the first order of big, just improving that one data movement operation for reading and writing is a huge benefit. And so then what we're doing here is we're doing, executing this out of FAM stage by stage instead of writing it back.

And so you can see that this hybrid application, we're really scaling it quite a bit better than just the plain vanilla SHMEM operation that's backed by SSDs. So we're getting speed ups in a 45 to 55% improvement on those operations on this proxy. Since I don't have the sparse matrix vector multiplication in this deck, I'll just speak to that a little bit. I think one of the big learnings from this experiment is that the data formats really matter. So if you look at the sparse matrix data format that's popular, the CSR format is widely used. It's difficult to look at the CSR format and figure out exactly where the particular piece of data that you're looking for is. So in other words, you have to do this translation between the CSR format and a pointer to when you're doing the sparse matrix multiplication, you've got a series of operations, which if it were not a sparse matrix, you'd know exactly where to get the operands. But you have to compute that in this case. So if you actually spend a little more memory not a lot, but enough to give a little more information about the row and column numbers for the data items that you have in the sparse matrix, you can make that calculation a lot easier. So one of the big things that we've taken away from the research is that the data formats matter. And so you really have to think about how you lay out your data and memory and how you're gonna optimize access to it rather than chasing pointers through a database with long latency memory.

So let me see here. Yes, so this is a diagram we've used a lot because this really...

Yes, sir.

Are the structures and fabric attached memory mainly read-only? Well, the answer to that is it depends. Some of those structures are databases, for example, where you do write operations. If you're doing a sparse matrix vector multiplication, you don't wanna write over that matrix immediately, but you've got intermediate results. So you have to compute what changes were made and then you have a result matrix. And so typically that's not the same matrix.

Yes, but if it's a database that's changing in real time, which is what this slide illustrates, if I have a lot of data coming in and some of our customers have tens of terabytes per hour of data coming in in real time that they have to parse and update these huge databases, I have to have a separate ingest path for data and pre-process it before I actually write it into this shared memory database. So what we show here is a generalized data flow for attacking this problem. So we have a set of ingest threads running on typically something like IO servers that are parsing incoming data, sorting wheat from chaff and writing the relevant results into the database. Then we have analysis threads that might be continually running in the back. This purple boxes represent your supercomputer back there. And then we have analysts that are doing queries and running operations, telling that supercomputer what to do via something like Arkouda and they are running interactively. So what we're doing with this paradigm is enabling interactive supercomputing, which is kind of a new thing, and it really depends on what the application is, how this is going to be used. It's very difficult to amortize your equipment costs or cloud costs or however you're doing this over a small number of data analysts. So you really wanna be able to serve a lot of people and have a lot of capacity for the computing piece, which is why the supercomputers have tended to be batch oriented and never really got out of that paradigm because the machines are so expensive, you have to run them 24/7 full out and schedule jobs to optimize their use and also schedule downtime in the event of failure. So this interactive analysis has to take place in parallel with the other uses of machine. So that makes it a little trickier, but this is the general data flow for the kind of problems that we're solving.

The question is, do we have to apply crash consistent techniques to the data structures? Well, that all happens behind the scenes, okay? What it looks like to the user with OpenFAM is data structures out in shared memory, okay? But what's happening on the memory server side is that those servers can be striped, they implement logging, you have to keep track of transactions in the event of a failure and you also have an independent fault domain. So you have to think of it almost like designing a storage system, where we got either a very reliable power or you've got resilient memory or non-volatile memory, but you still have to do the logging operations because when you recover, you wanna recover to a known state. So that all has to happen behind the scene. And so it's a little bit of a tough problem right now. I mean, we have a resilient runtime library that we run experimentally, which works, but I think if we're actually gonna get out there and replace MPI or Shmem or something like that, that's kind of a tall order at the moment. So right now it's best to move that recovery process and capability to the memory server domain itself. Because if you look at something like Shmem, Shmem has absolutely no provisions for that, right? And so it's not like some of the cloud applications where they know if a memory fails, they can, a node fails, they can recover from that and restart a job. With Shmem and MPI applications, you generally cannot do that very easily. So the mechanism, traditional mechanism has been check pointing. So, you know, you stop the machine, you write, you snapshot all the memory contents and then you restart. So something like, you know, some of these supercomputers have MTBFs in the single digit hours. When you get that many components, you know, 40 million components in a machine, something is always broken. It's like, you know, if your Hertz rent a car, you've got 50,000 cars for rent. Some of them are always gonna be in the shop and it's the same way with a machine that large. So you have to account for that. You have to be able to say, these guys, these nodes are out of service, these are being repaired and I'm gonna allocate the hardware that is working a little bit differently. And if I do have a failure, then I restart from a checkpoint and map out the bad nodes, assign that particular MPI rank to another place and then continue. And so that's typically the way those jobs work. So yeah, we have thought about that. I wouldn't say that we've solved it, our prototype, but it's designed to do that, yes.

Okay, so, and I guess along those lines, I should mention the archival storage because sometimes in some of these databases, people wanna know, well, okay, what changed today or what changed in the last hour? And you've got a huge database with petabytes of data, and you wanna know what changed or what's unusual. Snoop the internet, find the bad guys, for example. How do I look at traffic that has strange patterns or emanates from particular known bad IP addresses, bad actors on the internet, and analyze a large Tor network. We know the bad guys send something to the Tor network; it got pinged around and it popped up there at some random place and then attacks a bank or a government installation or a company. And so how do you find that? Well, that's the kind of analytics that we're looking at, trying to do on a massive scale, and it's a big problem.

Okay, so this is a slide that kind of illustrates what we did with Arkouda that I talked about before. So I won't go through this step by step, but it explains the operation of the Arkouda port that we did. We have a demonstration that actually runs up the New York taxicab database, which is kind of a popular experimental database for data analytics. So we have this database that has time and dates for all of these cab rides and starting and end points and duration and distance. And so you can look at this raw data and bring it in and we can do this in a HDFS format typically. And then ingest that data in parallel with doing the analytics. So I can say, I wanna know everybody who went to Lincoln Center, between the hours of six and 10 PM on Wednesday and it will tell you and give you some idea of the traffic. So that's our standing demonstration and it works quite well.

We're also looking at extending that paradigm to other Python libraries in the future, because we really think that this is a good idea for making supercomputer available and easy to use using the Python interface, as opposed to getting down and dirty with programming an MPI or SHMEM. Obviously, some people are willing to do that. And if you're in the oil and gas business, you might spend a lot of money on your particular codes, to look at the database from your seismic sensors and so forth. And it's worth the investment to do that. But for casual data analytics, that turns out to be a problem that doesn't scale because there just simply aren't enough people who know how to program these machines. And so enabling data analysts who just know Python and these libraries is actually a great idea. So I think here's some examples of how the performance of Arkouda scales versus the number of batches. And it's fairly linear in the system. And I think a lot of that has to do with the way that the network of our prototype was built because we designed it. I have this massive bisection bandwidth and so that we could clamp it down and do experiments on that basis also.

So where are we going in the future? Well, we don't have Optane anymore. So we've got to figure out what we're gonna do to fill in this storage class memory tier. So we have a project, obviously everybody's looking at CXL devices now and we're no exception. We are looking at substituting CXL memory for Optane. And it's kind of sad because the Donahue pass that was the version of Optane plan for Granite Rapids was supposed to be a CXL module. And we might take a moment of silence to lament the loss of that project, but nevertheless the problem remains. So if you look at memory technologies, there are technologies that work for storage class memory. And some of them are the like ferroelectric memories or the Carvite memory. Carbon nanotube type memories. I've seen these things, they actually do work. The problem that we have is that the memory industry, the DRAM and flash industry is dominated by a few extremely large players. And so if you got three guys in Silicon Valley that cook up some nutty idea, they can't exactly go to Samsung and say, "Well, we'd like to run some experimental memory chips through your fab." And the first thing they say is, "First of all, we're not gonna stop the fab for this because we're already running wafers for production. And secondly, you're not pouring that black poop in our machines." It's hard to get cooperation between these big vendors and the startups. So one of the things that we have done as far as the government lobbying goes is we've talked to the Department of Energy and the Department of Commerce about the CHIPS Act and said to them, "Well, we don't want this just to be a welfare program for these giant companies. We really want the government to understand that a lot of entrepreneurs need access to these state-of-the-art fabs and they need to be enabled because now the mass costs are so expensive and the equipment is so expensive that you can't start a chip company in a garage anymore, at least if you actually wanna make the chips because you have to have access to the backend processes too." So that's a work in progress and that's something that we're hoping to do. So I think where we're going with our fabric-attached memory work is we think that OpenFAM is pretty well poised to take on the CXL use cases because it already provides a lot of the things that you need in a user-level API. And I don't think that, you know, what we have now is basically MMAP, right? You can MMAP in a CXL device and processes could maybe coordinate swapping back and forth LUNs and CXL 2.0 and all that, but that's gonna be pretty crude. So you look at what's happening in CXL 3.1, we're gonna have fabrics that allow multi-level switching. So you can build very large memory subsystems out of CXL 3.1 and you're going to need the CXL fabric management pieces and firmware and drivers and all this stuff. And that's where the consortium tends to concentrate right now. But there isn't really a user-level API and OpenFAM actually would be a really good candidate for that because there's an existing code base and it works. We have it running in our lab. So I think that's one of the things that we're gonna be championing. The other thing is this idea of functions as a service, like the Arkouda model, where we can run this on systems that have a large memory and can support these big in-memory databases. That's a unique capability that we feel that we can offer. So I guess I'll open it to questions.

Yes, sir.

Yeah, we have two ways of handling that. And one is to have a backing store, right? That guarantees resilience. So we can write back to a storage tier. And right now the candidate for that is Daos because we're looking at Daos as a store and using the fabric attached memory as a first level tiering. So everybody's trying to design Optane out of Daos, frantically, and this seems like a good way to slot in that first tier of storage. But that's one way to do it as with backing store and then a failover of memory nodes. And that we can do. The other thing that we can do is we can stripe the systems. And we have in the prototype, we have some hardware that we haven't actually turned on yet but we have groups of five. So we can set the servers up in a RAID 5 configuration and do out of band communication between those servers with basically a Dolphin PCI direct connections between, you know, all the connections between the 10 sockets and the five server group. So that allows us to implement RAID 5 without interfering with slingshot traffic. And so that's another way to do it. But yes, the whole concept pretty much hinges on having a resilience memory tier. You can't, if you can't really depend on those contents staying there, you're not gonna put a database in that situation. So yes, resilience is a big requirement and something that's built into it.

Anyone else?
Yes, sir.

Do we have to optimize the applications to use fabric attached memory? I think that if you're going to do that directly, then the answer is yes, but there are some very low hanging fruit. For example, if I have a very large HPC system and I'm staging jobs, you know, what they do at NERSC, for example, is they'll collect the code and a snapshot of the user's file system, and they'll package that up and ship it to the machine where it reconstructs that file system in a fast storage tier that's temporary and then runs the job out of that. Now you could do that with fabric attached memory and stage the jobs that way. And you can pipeline the system. So I can have somebody running on the AI cluster, somebody running on the compute cluster, and job staging going on in fabric attached memory. For when this one completes, I fire up the next one and the speed up, the throughput that you get out of the machine from not having to wait for a Lustre file system or something in the back could be quite significant. Furthermore, if you look at what's happening with the workflows, I mean, there's kind of the first level thing is like slurm heterogeneous job steps, right? That will get you, that's kind of like the old IBM JCL. I know most of you aren't old enough to remember that, but you've had a job where it says, first, I'm gonna pile my program. And then here's my card deck of data. Then I'm gonna read that in, and then I'm gonna grind on it. And then I'm gonna write the data out to some database. So that's what the heterogeneous job steps will enable. And each job step has intermediates results. If I'm not writing those out to a file system, if I'm writing those out to a FAM tier, that's free, right? I can make that look like an object store or a file system, and it runs just orders of magnitude faster. So that's another thing that I can do with it that doesn't require any code changes at all. Now, if I have an application like the one I'm talking about where we're doing data analytics, yeah, data movement is the key. You have to expend a lot of energy in these systems, probably more than a third of it, just moving bits around and not doing any computation at all. So this is why you have these massive networks on a machine like Frontier or El Capitan, and they're multidimensional and have massive bandwidth and as low latency as you can get. So it's all about data movement, and that's how you can use FAM as a first-tier storage. And I'm thinking more and more that if you, one of the problems with this is people think about it as memory, the way they think about DRAM attached to the CPU. And so if you look at it that way, then it's not very good memory, right? 'Cause I've multiplied the latency by a factor of 10, and now that I don't have Optane, I'm using DRAM out there. So the cost per bit is the same roughly. So something's got to give there. And I don't think one of these other memory technologies is gonna be online for a few years now because all the Silicon guys have looked at the massive crater that Optane left, and something like $7 billion loss for Intel. And so they're not exactly eager to get into that business. So I think it's gonna take some time for the demand to get there. Also, there's gonna be revolutionary changes in what the processor memory speeds look like because DDR is running out of gas. They've pulled just about every trick that can possibly pull with MRDIMMs and 16 channels per processor and all this stuff. And there's only so far that you can go before you have to change what that looks like. So this memory tier is gonna change. I think I am running out of time, but thank you very much.

