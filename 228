
My topic is on CXL memory, how the CXL memory use cases can be leveraged on the Linux kernel system as such. So how the CXL memory limits can be expanded on Linux system. So about me, I'm a Linux kernel engineer and currently working as a CXL software architect at Micron. I'm basically driving the CXL software architecture for Micron's memory products, CXL-based memory products. Micron being the world leader in memory solutions as such. Basically, I've been involved in technical leadership and mentoring with the broad experience on Linux systems platforms.  Spanning the embedded systems as well as enterprise systems and open source software development.

 So the agenda is to understand what is CXL, what it brings on the table, why CXL and how CXL has evolved,  and the different value propositions that have been supported by CXL, particularly memory capacity expansion and memory bandwidth expansion.

So CXL is a Compute Express link standard. It's an emerging cache-coherent interconnect standard for processes, memory expansion and accelerators  that basically promises to revolutionize the way data center cost, performance and efficiency has been evolving. So why CXL? As we are aware, the servers, they are moving increasingly towards the heterogeneous computing architecture  with building accelerators, offloading the loads to particular accelerators as such from the CPU. And the memory cache coherency is one of the functionality which allows, facilitates the sharing of memory resources  between CPU and accelerators as such. So basically, they see CXL benefits by increasing the bandwidth and capacity, as well as efficiency,  lowering down the total cost of ownership as such.
 
So how that usually happens is to get to the facts, why CXL? Let's look at some of the facts as such. The differences that are seen, while the expanding data center requirements are being expanding as such. So particularly the memory demands have been growing in the data center application, like almost like 26% year to year. And considering that demand, the memory latency is improving at a slower pace, like one to one every two years,  compared to the processor speed, which is doubling every two years. And the memory is not being scalable as per the processor speed. And another fact is that due to this, there is an increase in the total cost for the data centers,  like almost 50% of the total cost goes into the memory. So how these problems can be solved of the increased demand for the memory as such.

So that's where CXL comes as the solution, which helps to elevate the above challenges  by providing an open source, open standard interconnect, providing the higher bandwidth  and low latency connectivity between the processors, accelerators, memory, storage and the other I/O devices. And so here, when you look at the challenges that are there. So it helps to solve those problems as such. And basically the way CXL enables the expansion of capacity and bandwidth  at access by providing the behavior like similar to the remote NUMA node access. Like the CXL memory is being considered as a separate NUMA node as such.

So we'll look into the basics of this protocol. That's where we'll get better clarity on this. So CXL standard specifies the three protocols, CXL.io, CXL.mem and CXL.cache. So CXL.io basically provides a functionality equivalent to the PCI interface,  and it has been used for configuration, DMA and interrupt handling between the device and the host. Whereas CXL.mem protocol enables hosts, such as a processor,  to access the device attached memory using the load and store semantics as such. And cache particularly specifies the semantics rules for accelerators  trying to access the memory directly connected to CPU.

So here in this case, based on these three protocols,  the CXL consortium defines the different classes of devices or different kinds of use cases. Particularly type 1, type 2 and type 3 devices. And these different types, they use the particular set of protocols that are,  like in case of type 1 devices, they support CXL.io and CXL.cache protocol. And when this combination is used, we can infer that type 1 devices do not contain memory,  which is available for the host consumption. So basically this combination works for accelerators such as smartNICs,  which typically don't have a memory, and via the CXL,  these devices can communicate with the host as such.

Type 2 devices are the devices that support all the three protocols,  where in which case, like if I can go on to the next diagram, that will help you understand better,  where the accelerator has the memory connected, high-speed memory as such, connected to its own,  as well as it can access the memory connected to the host memory as such. And another third type is the type 3 devices that supports the CXL.io and CXL.mem protocol. And basically they target towards the memory capacity and memory bandwidth expansion. So yeah, so this gives you the different use cases and the device types that are supported by the CXL.

So here what we are going to look at is, I'm going to try to target towards the two use cases. That is CXL capacity and bandwidth expansion as such. So these are the two important value propositions that CXL attached device brings to the table,  that is capacity expansion. And capacity expansion is leveraged using the memory tiering,  commonly using the kernel-level support built upon the NUMA topology abstraction in the interface. And the bandwidth expansion basically been used via the heterogeneous interleaving. Again, the support is incarnated through the software level as such. Now in case of CXL memory capacity expansion,  the configuration would be CXL directly attached tiering, or CXL memory directly attached as such. And another is like the CXL memory that can be at the back of the switch, or fabric manager as such. So as the CXL protocol has evolved, the functionality for CXL switch has been added into CXL 2.0 specification,  where multiple memory devices can be connected to the switch as such. And with the evolution of CXL 3.1x specification,  the more number of memory devices can be connected through the fabric manager as such. And again, the CXL attached memory tiering,  the memory can be managed by the application, or it can be managed by the kernel as such. So when it is managed by the application, then the application is aware of the CXL memory,  which is behind the NUMA topology. So in that case, the application can make use of this NUMA library to access the CXL memory as such. And based on the requirement, capacity requirement, it can modify the application. And another is like the application, unmodified applications can use the CXL memory as such. It does not have to know the underlying architecture. The kernel does the work of allocating the CXL memory as such.

So here in this case, to give an idea about how the CXL memory capacity bandwidth expansion happens,  let's take a look at the system configuration. I've taken this example from the paper published by Micron in collaboration with AMD,  where the configuration is the AMD Bergamo system with a dual-socket CPU. It has 128 cores, basically optimized for high-throughput workloads,  and it supports up to 12 memory channels, so allowing a total of 768 gigabytes of memory,  considering 64 GB as a per-memory channel. And based on the increased core count, that is 128, the capacity and bandwidth per core gets limited to 6 Gbps per core as such. Now if we add the CXL memory, like here in this example, 4 Micron CXL controllers with 256GB memory capacity,  in that case the system capacity and bandwidth increases to 14 Gbps per core as such. So basically, a system with a 64 GB memory module that increases the capacity of like 133%  compared to the native DRAM setup, and bandwidth expansion to 33% as such without CXL.

So what we are going to look at is memory tiering as a menu for CXL leveraging memory tiering used for CXL expansion, capacity expansion. So basically, memory tiering is a practice of dividing the physical memory into separate levels  according to its performance characteristics. And then allocating the memory in an optimal way as such. So originally, all RAMs that came from the DRAMs or DDR as such. Then as the memory innovation happened, we see the persistent memory, which is cheaper but slower,  then the HBM, that is high bandwidth memory, which is expensive and fast, and then we have the CXL memory. These all have different memory as the tiers to the memory system as such, like cache. If you look in order of for latency, cache is the fastest one. DRAM, CXL, persistent memory, and storage as such. So this memory tiering provides us additional capacity, and also it helps performance capacity trade-off to achieve the TCO gain. So here in this case, this CPU, it connected. The near memory is in the sense which is attached near to the CPU, that is the DDR memory as such. And the far memory can be the other tires that might be CXL or persistent memory, and eventually the storage. Which comes the access, that is storage, bigger capacity, but it comes with a huge latency.

So here to achieve continuing on the CPU capacity expansion with the memory tiering. This diagram that has been taken from the paper, from the TPP white paper that has the analysis of CXL memory tiering as such. So here we can see that the latency access for CXL, that sits between the main memory and the secondary storage, such as SSD as such. So to achieve optimal performance, the memory hierarchy focuses on storing the hot memory regions, that is the frequently used data,  much closer to the CPU, that is the main memory, and the infrequently used data, cold memory regions, onto the secondary storage,  which is typically larger but slower as such. And while doing this, moving the data between the main memory and storage  has increased performance penalty as such, because of the latency gap between the two. So there is an opportunity to use a new memory tier, which consists of higher capacity and comparatively less access latency,  comparable access latency to the main memory, that is the DDR as such. So CXL can be placed between the main memory and the storage media, and the performance can be optimized. The goal can be targeted in such a way that the hot pages remain in the main memory,  the warm pages or warm data goes into CXL memory, and the cold data, infrequently used data, that can be moved to the storage as such.
 
So here, the NUMA domain, particularly along with the memory tiering, has been used for expanding the memory capacity. So basically, the Linux community is working on making changes and improvements for improving the performance of applications  that are running across the NUMA domains. And to get to a NUMA understanding, it's a multiprocessor model,  in which the processor is connected to a dedicated memory, but can also access the memory that is connected to other processors in the systems,  with the multi-tier architecture. So from the hardware point of view, NUMA is a computer platform that compromises multiple components,  and in each of them, there can be zero or more CPUs, local memory, and/or I/O components as buses. And from the software perspective, Linux divides the system into hardware resources, into multiple software abstractions called as nodes,  and maps these nodes to physical cells of the hardware component platform. And these physical nodes, just as a physical node, the software nodes may contain zero or more CPUs, memory, and the I/O buses as such. And the other memories, that is, HBM and CXL, they can also be represented as the nodes, but they are like the CPU-less nodes as such.

So here, we look at the NUMA node memory layout. So here, in each of the nodes with memory, Linux constructs the independent memory management subsystems,  and in each node, it will have the multiple zones as such, which will have its own free list in use page list, user statics, and locks, all the things to maintain the different zones as such. And these zones, they can be like one or more, DMA, DMA2, normal zone, high memory, more. And the zone list specifies the zones, nodes to visit when a selected zone cannot satisfy the memory allocations as such.So this is the basic concept that works behind the memory tiering as such. 

So now let's look at how the page movement or demotion or promotion happens across this memory tiering as such. So when the pages are unused, they can be moved, demoted to slower tiers as such. So here, as we have seen that the CXL memory or the HBM memory, they can be constituted as a separate NUMA node, that CPU-less,  and they are being considered as a slower tier compared to the DDR as a first tier. So when the pages are not used, unused, one way is that they can be reclaimed, freed, or they can be moved to a slower tier, such as CXL as such. And basically, like page migration, through the page migration, page migration during the reclamation process allows the system to migrate the pages from the fast tiers to slower tiers when the fast tier is under memory pressure as such. So instead of totally moving the pages, the pages can be moved to the slower tiers, that is CXL tier as such. So basically, migrating the old pages onto other tiers, slower tiers, usually does not hurt the system performance, but the other way around, like when migrating the hard pages, that can cause system performance as such. So basically, all this functionality is driven by the memory management code that got merged through this TPP patches. Which basically identifies and places the hard-coded pages to the appropriate memory tiers. And here, this is the sysfs entry knob that has been used that enables or disables how the demoting of pages should be done during the reclaiming as such. And now, deciding when the pages should be promoted back to the DRAM or a closer node as such, that is based basically on the NUMA balancing policies as such. Which basically does the work of moving the task and its data closer to the main memory. And again, that has been governed through the sysfs entry NUMA balancing interface that enables or disables the NUMA balancing. And this NUMA balancing activity can be monitored by various parameters from this VMstat. We can get to know how the NUMA balancing happens as such.

 Okay, so let's look at how the memory management for this tiering happens as such. So the memory, tiered memory, it can be managed by the kernel or it can be managed by the applications as such. So when it has been managed by the kernel, the OS can map the file memory into the application address space. Application can execute from the pages from the file memory, that's CXL. But it can allocate the CXL memory, but it would be slower as compared to the DDR memory pages as such. And the kernel memory manager implement various policies for migrating the pages, hot and cold pages, between the tiers. When the memory, tiered memory, is managed by the application, in that case the application can access the memory as a memory map files as such. Which has basically been built on top of this DAX device support, direct access support as such. So here in this case, you see that these are the unmodified application directly using the CXL memory handled by the kernel. It might be unmodified or it might be NUMA aware. NUMA aware is when the underlying NUMA architecture is the application aware and based on that it is accessing the memory. Otherwise, if it is application managed, then it goes through the dev/dex interface or direct access file system interface. 

So here in this case, the memory capacity expansion happens through the memory tiers as such. So based on the configuration, the multiple tiers can be created. So the NUMA architecture leverages, feature has been leveraged to create the different nodes and memory tiers. And the memory page placement happens during the reclamation when the memory is the fast tier memory is falling short under the pressure. In that case, the pages would be moved to the slow tiers as such. And when they are needed, they'll be promoted back to the fast tier or the memory closer to the CPU as such. So that's how the memory expansion, capacity expansion happens. Now let's look at the memory, another feature that can be leveraged for memory bandwidth expansion, that is a memory interleaving as such. So to start with, like if you have more than one DDR controller on the device, then there can be two options. That is, each of the memory controllers can be used independently as such, or it can be used in such a way that two or more controllers can be interleaved together to present a single unified address space. So that's where this memory interleaving comes into picture. It basically participates the multiple memory controller appear as a single pool of memory and spreading out the pages across the set of the specified nodes as such. Okay, so in such a systems, we can make use of the additional bandwidth provided by the lower tiers, particularly for the bandwidth intensive applications.

So there are the various propositions for CXL interleaving, various solutions CXL heterogeneous interleaving as such, where the system address map is interleaved between the DDR memory and the CXL memory. So one is the hardware-based interleaving, which is interleaving taken care of by the hardware. It is easy to configure, but provides a fixed configuration as such and also not scalable for all kinds of workload. Also, the kernel is not involved in memory management. It is all taken care of by the hardware. Another option is hardware and software-based interleaving, where it is assisted by the hardware by associating the number of channels to different NUMA nodes as such. And then the kernel managing these different NUMA nodes for allocation. And another option is the software-based, completely software-based interleaving, where the kernel basically makes use of the NUMA interleaving infrastructure and manages the memory allocation.

So here we'll look at the second, that is hardware and software-based interleaving and then software-based interleaving examples. So again, taking an example from that white paper with the same system configuration AMD platform, which offers a wide configurability for the NUMA domains. So it basically supports the concept of NUMA node per socket as such. So the various combination here we can look at is the NPS 1, 2, and 4. So in case of configuration for NPS 1, where each socket is in a single NUMA node and with all the cores in that socket. And here in this case, with this configuration, there is one NUMA node for the memory connected to the CPU, with all the CPUs into that NUMA node, and another node that would be created for the CXL attached memory as such. And now in case of NPS 2, which happens where each socket is divided into two NUMA nodes with the six memory channels to each NUMA node. Basically this AMD processor has 12 memory channels. So with this NPS 2, it's been divided into two NUMA nodes, six memory channels for one NUMA node and another six channels for a second NUMA node. So basically with this configuration, we have two NUMA nodes for the memory channels and the third NUMA node corresponding to the CXL memory module as such. And in case of NPS 4 configuration, the socket partition into four NUMA nodes, with each domain getting the three memory channels and another fifth NUMA node that is created for the CXL memory module.
 
So here in this case, basically this is the way the configuration can look like, that these are the four NUMA nodes with each with three memory channels as such. And then there is a next another fifth NUMA node corresponding to the CXL memory module. So with this setup, it enables the software page level intervening so that the pages can be allocated for a specific workload to be distributed between the local DDR and the CXL NUMA nodes at this ratio. Like in case of NPS 1, it is one NUMA node and one DDR NUMA node and one CXL. So the allocation is one-to-one mapping 50/50 as such. So if a 100-page request comes, in that case 50 pages will come from the DDR memory and the 50 pages will come from the CXL node as such. In case of NPS 2, in that case it is two NUMA nodes and one CXL. So that is 66% from DDR and 34% comes from CXL NUMA node as such. Whereas in case of NPS 4, it is like 4 is to 1, that 80% pages come from DDR and 20% comes from CXL. So based on the application, whether it is a capacity, bandwidth intensity, the allocation can be formed as such. So the CXL provides a large amount of memory, but it comes with the latencies as such. So if the application is bandwidth sensitive in that case, more number of pages can be allocated from the DDR and then corresponding pages can be kept from the CXL memory as such. So the CXL memory helps in bandwidth expansion using this configuration software and hardware configuration interleaving between the DDR and the CXL memory. And here in this case, I take an example of a bandwidth sensitive workload such as Cloverleaf that can benefit from this configuration, NPS 4 kind of configuration,  where it helps to reduce the memory stalls that are happening because it is mainly using the memory spread across the DDR and the CXL NUMA node as such.

Let's look at the software interleaving. So here in this case, we'll look at the software interleaving that is basically weight-based interleaving policy. So this feature basically is based on memory tiering mechanism, but it is aimed at a different problem as such. Now here in this case, the weight-based interleaving, what basically it addresses is it tries to find a solution to what is the optimal original placement of pages,  means how many pages has to be allocated from a particular NUMA node as such. So here in this case, basically it addresses the practice of NUMA interleaving where the allocations are spread across a set of NUMA nodes  with the purpose of getting a consistent performance from any set of CPUs on the system. And NUMA interleaving spreads pages equally across the nodes as such, provided by the memory policies set by the application. So NUMA interleaving itself does not have the ability to decide how the allocation should happen, in what specific order as such,  how many pages should be allocated from a particular NUMA node as such. It uses a uniform policy as such. So with this weighted interleave policy, the feature, what it does is adds the concept of weight given to each NUMA tier as such. Where these weights determine how much pages or how strongly the allocation decision should be based towards each tier. And the higher the weights for a given tier, it means that more number of pages will be allocated from that node as such.
 
So the example here we can take is the tier weights that are controlled by the administrator commands. Here again, the system interface, here the tier 2, the weights are being assigned to tier 2, like here in this case,  is seen from CPU 0, the weight is assigned is 20 as such. So if we consider 100 page allocations, then this weight of 20 gives that 20 pages can be allocated,  should be allocated from this memory that is there in the tier 2 as such. Otherwise, by default, all tiers have a weight of 1 as such. So if there are 4 tiers into the system, then when an allocation happens, the interleaving will happen that  it will be divided equally among all the 4 tiers as such. So basically, these weights are used by the NUMA interleaving code when the allocation happens. So we have the ability of using the CXL memory, but as it comes with more delay or latency compared to the DDR memory,  so proper weights have to be aligned based on the application requirements as such. So this is a feature that recently got merged into 6.9 kernel as such. The early patches that was being contributed by Micron engineers.

So that is what we saw about the CXL memory being utilized for memory capacity expansion and bandwidth expansion. This support has been leveraged through the NUMA interleaving, any kernel as such. Now again, there is another proposition for CXL memory sharing and pooling,  where memory sharing is a type of memory that is accessible by all the processors in the system. And it allows each processor to read and write to this shared memory. And the benefits that it provides in terms of efficient communication between processors,  a simplified programming model, and low latency and lower power consumption. Memory pooling is a type of memory that is shared between processors, but managed by a specific memory controller. And when we talk of CXL memory pooling, the CXL 2.0 onwards specification supports the CXL switch. And probably where the memory, multiple logical nodes can be connected,  thereby giving the multiple memory devices can be connected, which can be pulled together,  forming a huge pool of memory, and that can be shared across multiple hosts as such. And it comes with the benefits of memory allocation efficiently and guaranteed access as such.

Okay, so here I wanted to talk a bit about the work that has been done by some of the Micron engineers on the,  one of the features that is being currently under development is the Famfs. That basically organizes the shared memory as the file system. So it's open source, been introduced last year in the LPC. So here in this case, the various CXL devices, they are being formed as a shared memory  and then exposed to the application as the shared file system as such. And this Famfs maps directly to shared memory and the application can directly use through a simple memory map interface as such. It's been available on GitHub. And basically the target is to focus on applications such as AI application where large data sets are being used,  which can be shared across the multiple applications.

Okay, so here basically my intention was to cover these two value propositions  and how those are supported currently in the kernel architecture as such. Actually, as the CXL standard itself is evolving and also the CXL development is evolving around that development as such. So this was more like what was currently present in the kernel. Along with that, there is a complete CXL driver stack, which basically takes care of the CXL protocols,  the CXL, particularly memory devices, which takes care of the CXL.io and CXL.memory,  interacting with the hardware, collecting that information from hardware,  exposing the memory as a system memory or as an index device to the applications as such. And based on the application, whether they are managing memory on their own or unmodified application,  the CXL memory can be leveraged as such. Okay, so that's all I have for CXL memory management. So to conclude, that CXL memory can provide a solution for increased memory, bandwidth, and capacity requirement as such. I'm taking help leveraging the functionality provided in the Linux kernel to do more in delivering memory-tiering functionality.

 And these are the references that have been used. Any questions?

 Do you think we need some new policy, considering CXL memory, or we can just consider it the same as the general NUMA memory node?

 Yeah, so it will be the same as a NUMA node, but it will be a far-off memory compared to the DDR NUMA node as such.

 It will be this CXL memory placed as a separate NUMA node.

 So we don't need a new special policy, especially for the CXL node?

 So the policies, they are being configured in the kernel as such. But if you have a specific requirement based on bandwidth and based on capacity,  that can be fine-tuned through the sysfs entries to distribute the load.
