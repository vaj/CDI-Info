
Good afternoon. Good morning. Good evening, depending on where you're at. Thank you for coming and sharing some time with us as we talk to you about a lot of the upcoming technologies and the way the consortiums are working together to bring new technologies into our industry. One of the things I need to tell you at the beginning is we're going to be recording this so that it can be available for people who couldn't be here or are attending other sessions at this time. Also, people may have heard that there's a letter of intent between CXL and Gen Z about bringing those two organizations together. This panel doesn't have any more information than what was available in the online stuff that's on CXL and Gen Z websites, so we can't answer a whole lot of questions there. If you've got those questions, we encourage you to talk to the president of Gen Z or the chairman of CXL. With that, let me introduce myself. My name is Kurtis Bowman. I work for AMD. I'm a member of both CXL and Gen Z and today we have a very exciting panel of local and remote people on our broadcast here and with that, let me have them start with the introductions and I will share the screen. Ahmad, we're going to start with you. 

And just doing an audio check. All right. Hello everyone and thank you for joining today. My name is Ahmad Danesh. I'm a senior product marketing manager at Microchip Technology and I'm representing the CXL consortium for today's discussion. I'll give a brief introduction to CXL before we move to the roundtable and questions. But please do visit us at our virtual SC 21 booth or if you're in person at SC 21, you can find us at the CXL consortium at booth 1607 and of course, you can always find more information at compute express link.org. Next slide. 

Yeah, I'm not able to advance the slide. So I'm looking for it's here. All right, haha. Three percent rule. There we go. 

Thanks. So CXL is what it is today because of the wide adoption we have across the ecosystem and significant contributions being made from leading CPU, cloud, OEM, memory, silicon, platform, and software providers, and the adoption and investments from over 165 member companies are making CXL a reality, and there's a number of live demos that we have available today. Big thank you to all the contributing members for making it. Next slide. 

CXL is focused on delivering the right features to address faster data processing, heterogeneous computing, memory capacity, and bandwidth expansion, and fundamentally enabling the tuning of compute, accelerator, and memory resources for targeted application workloads. CXL is an open industry standard that provides high performance cache coherent interconnect. And it does this by providing a coherent interface that leverages the PCIe physical layer. That makes it really easy to adopt, it provides a low latency protocol for load/store and memory and cache access. And it reduces a lot of complexity by enabling an asymmetric innovation of CPU and endpoints. We'll walk through some of the usage cases and some of the feature sets as well here. 

CXL has three mix-and-match sub-protocols. We have a CXL.io, .cache, and .memory or many of whom call just .mem for short and that enables three different device types. CXL.io, as you'll see in this diagram, is used for management, which is why it's needed for all the device types. .cache and .mem are high bandwidth, low latency protocols that enable faster data transfer so you can spend less time moving the data and more time on actual data processing. The .cache's purpose is to provide devices like accelerators or NICs access to the processor's memory. Whereas CXL.mem provides processors access to the device memory. So in addition to providing faster data transfer, CXL.io enables the host to expand more memory capacity and bandwidth for a variety of media types. 

Let me interrupt you just a second, Ahmad. Could we ask people that are online to please mute their lines unless they're actively talking? We're getting a lot of feedback from somebody. Thank you. Ahmad? 

Thanks Curtis. And being able to connect to this variety of media types, that's a great thing. That provides us some media independence that then helps us to actually grow the different types of memory that we want to connect to. Whether that's DRAM or new emerging technologies. Next slide. 

So a bit on CXL.io 2.0 which we introduced last November. We introduced switching to enable larger scale memory expansion and memory pooling. With memory pooling, what you can do is you can allocate memory resources dynamically to hosts. That allows us to optimize how the deployed memory is utilized, providing a lot of efficiency. It reduces stranded memory and allows us to meet the varying needs of different application workloads dynamically in real time. CXL.io also introduced support for persistent memory devices. This was really important so that we can now have a standard management of all memory types across all CXL-supported processors. And we also introduced enhanced security with link IDE to prevent snooping and replay attacks on the CXL bus. CXL.io is always continuing to evolve. It is also backward compatible with previous generations to ensure interoperability with previous devices and to ensure that the system is able to manage the memory of the host as well as maximizing your investments from previous generations. Thank you for your time. 

There is one question in the chat. 

We'll take questions at the end. Let's go ahead and then move to Eric and Jim. We'll have you guys introduce yourselves here in the room. 

Thanks Curtis. My name is Erich Hanke. I am the principal engineer of storage and memory products at IntelliProp. I have Jim Hull here with me. Jim is a senior principal software engineer at IntelliProp. We are members of the CXL consortium. We're also members of the Gen Z consortium. Today we're going to be representing the Gen Z consortium. Alright. 

Go ahead. So here I'm going to provide a quick introduction of the Gen Z consortium. The Gen Z consortium is a collection of over 60 companies who have come together to create a suite of specifications around memory-centric fabrics. The consortium was formed in 2016, and we're celebrating our five-year anniversary at this supercompute event. If you're interested in learning more about the specifications or interested in contributing, be sure to check out the website GenZconsortium.org and follow us on Twitter and LinkedIn. Next slide, I'll jump into a little bit of the background of the technology and give a brief introduction. 

So for those who may not be as familiar with the Gen Z specification, I'm going to jump into some of the key attributes. Gen Z is an open standard which defines low latency, high bandwidth, and scalable fabric solution. Which has built-in reliability, availability, and serviceability features such as multipathing and congestion management. It's been designed to support various PHY types including PCIe PHY and longer reach 802.3 PHY. And has current support for low latency 50 gig 802.3 PHY. On the demo floor downstairs, we're demonstrating fabric-attached memory through multiple layers of Gen Z switches. Using a mixture of copper and optical cables so be sure to check us out down there. The fabric is designed to be secure and flexible, allowing for harder enforced isolation between components. But sharing resources is enabled by Fabric Manager and we're showcasing some Fabric Manager software downstairs as well. It allows equal access to Gen Z attached memory from any compute element such as CPUs, GPUs, FPGAs, or other accelerator types. Since Gen Z is a true fabric, requests can flow through the fabric in any direction. Which enables built-in peer-to-peer communication and acceleration functionality that can take place in various fabric-attached components. Another thing in what we're talking about today is Gen Z enables growing the fabric and supports disaggregation from the ground up. So we're pretty excited to talk about these sort of capabilities and get some questions from the attendees both here physically and virtually. 

The next slide, I can talk about some of our collaboration in our industry relationships. We have a well-established relationship with our friends at SNIA. And SNIA now is the hub for form factor and connector specifications. Last year we completed an agreement with CXL, an MOU agreement with the goal of talking about bridging capabilities between the two specifications. And we have a demo of that running downstairs as well. Similarly, we're working with OFA to define a fabric management framework that can be used for Gen Z or other fabrics. We're working on demos of that as well. So lots of really cool and exciting stuff going on both here and on the virtual booths so be sure to check us out. 

Thanks Eric. Now let's move to Richelle to talk a little bit about SNIA. 

Thanks Curtis. So I'm Richelle Ahlvers. I work for Intel. I'm a storage technology enablement architect there. But I spend a lot of time working in standards organizations, one of which is SNIA and OFA that he just mentioned as well. I'm the vice chair of the board of directors for SNIA. I run the storage management initiative. And I'm also the primary author for the Swordfish management standard for storage. So a little bit about SNIA. We've been around for almost 25 years now. We have, we focus pretty much on anything storage, you know, data management, storage aspects, and I'll talk about that in a minute. We have 180 member companies with over 2,500 people as active contributors. Next. Okay. 

As I said, we cover all things storage, so this lovely little tree kind of talks about some of the areas that we're working in. Back one. Back one. Thank you. I have this to you. Thank you. Thank you. So Eric already mentioned a couple of areas that we work with. Some of the, have specific alliances on just in the upper right around physical storage. We have the SFF organization that's developed a whole lot of connector and form factor specs, including EDSFF. So moving into other areas, we have a focus on persistent memory. We have marketing and promotional activities as well as technical activities developing and supporting persistent memory as well as computational storage. We have a tremendous focus as well on network storage. Our network storage forum puts out a tremendous amount of educational material about, with vendor-neutral content providing information about using all different kinds of network storage technologies. Back over on the other side, you get to the storage management, the purple one down there with the little swordfish, that's me. And we do, we have had close to 20 years' worth of standardized management interfaces, the latest of which is Swordfish. And Swordfish, we partner with another group, DMTF, that develops Redfish for server management. Both of these were designed from the ground up to be scalable, support disaggregation, basically support next-generation manageability. And last but not least, we have work going on in both green, power efficiency management, metrics instrumentation, as well as a lot of work around security of our standards, our interfaces, as well as data protection and privacy. I mentioned briefly scalability and disaggregation, so I just kind of highlighted a few of the areas here, it's very pervasive throughout the organization. I will clearly talk a lot more about Swordfish than other things because that's my baby. But all of these areas in the boxes have work that they're doing in this area, which I wanted to highlight that since it was kind of one of the themes for today, just to get people moving a little bit into that direction. 

All right, thank you very much, Richelle. And now let me introduce a couple of end-users. These are folks who are going to give you an idea of when you put these together and you start to use them, what does it mean to you? And so let me start off with Bharath from Meta. 

Thank you, Curtis. Good morning, everyone. I'm Bharath Muthiah, I'm part of the infrastructure team at Meta. I lead the technical sourcing for various server and AI platform technologies. And from an end-user perspective, we really are very interested from a Meta perspective in terms of building out a truly open and interoperable ecosystem. That is hugely important for us in terms of both driving a rapid pace of innovation, as well as the best performance for TCO as we look forward to our evolving workloads. So in that capacity, we are really excited to be working with open standards-based organizations such as what was discussed earlier in this presentation in terms of jumpstarting the ecosystem. As an example, I will maybe take CXL to give you a sense of what are the types of end-user pain points it helps solve. When we build out large infrastructures at scale, over the past decade or so, it's pretty apparent that the core count growth has outpaced how much memory we are able to supply into the systems in terms of things like memory back. As we look forward, things like chiplets are allowing us to scale core count much more cost-effectively and this is further exacerbating the strain. While there are native memory architectures that do come and support in terms of growing memory channels and other types of memory integration, they do have implications on power and cost, which multiply quickly from an end-user perspective. So there is a strong need for heterogeneity in terms of how we build platforms going forward, so we can mix and match at scale for the specific demands of the workloads. This is where we are excited for instance with CXL where we are now able to drive a mixing and matching of different types of memory technologies on the same platform without necessarily growing the socket size too much which adds to platform cost. And as well as if we as a community come together and optimize around the software enabling and things like that needed for workloads. We really see a promising path from these open standard-based architectures in terms of how we drive performance efficiency at scale going forward. We can chat a little bit more about these perspectives in subsequent sections but now we'll maybe hand it off to Glenn to talk about it from a Microsoft perspective. 

Yeah, so hi folks. My name is Glenn Ward. I'm a senior director of technology development at Microsoft. And I like a few of the things that Bharath said, it sort of resonates with what we see at Microsoft and I'll say a couple of things. If I think about the trends that are taking hold in the industry, we do see core counts going up tremendously and the ability to attach memory to those core counts is increasingly challenging. That's really where CXL comes in for us. We're super excited about CXL for disaggregated memory scenarios. One of the things too that we've learned as a hyperscaler is that the customers can. Right. So anything that we can do to make customers happy and particularly with technology innovation that can give us a structural cost advantage. So having less expensive infrastructure is a big win for the customer because we can pass savings along to them. So things like being able to disaggregate memory for customers and attach large banks of memory and not have to make the sort of runtime decisions or provisioning decisions that we may have to make today. That's just a big win. We can do a lot more things in real time or dynamically. It starts to get us to that kind of promised land of fungibility that we've talked about for a few years. So I'm super excited about what's happening. You know, I think CXL for us has been at the forefront, but certainly we're involved and heavily consuming what's happening in the others. Really excited about the news that came out last week. My other position is co-chair of one of the task forces for CXL consortium. So we'll be talking about that more in the future, as Curtis said. Yeah, super excited. Thanks, everyone. 

All right. Well, thank you. As you see, we have a very distinguished and experienced group of panelists here. So let's just jump right into the roundtable and again remind people if you're not talking, please stay muted. It'll help things go. Let me start asking the panel the first question, which is kind of attachment. We've got these different ways to attach memory. We can attach them directly to the processor through DDR. We have CXL. We have Gen Z. What are the advantages and disadvantages maybe? Or why would I attach to one and not the other as I go forward? You know, I'll start with Eric since he's looking at me. 

Yeah, sure. So I think, as you mentioned, a local attach point is sort of direct connected to the CPU. And the ability to share that resource if it's not being utilized is limited or just not possible. So some of these new interconnects that are being introduced in CXL and Gen Z allows to be able to more efficiently share that resource, reducing costs between multiple nodes. There is going to be a little bit of a latency penalty as you sort of move away from the CPU. We haven't yet figured out how to beat the speed of light, for example. But the ability to utilize that memory in a heterogeneous way is going to become very important from a cost savings perspective. And then you can start to think about different use cases where you can have multiple nodes sharing that memory. So there can be collaboration between multiple nodes. So you can get some performance improvements that way as well. 

And Bharath, as you look at something like CXL and Gen Z, what do you see as memory you would have directly attached to a processor versus maybe some you'd put out on one of these interconnects? 

Sure, Curtis. I think I just want to kind of build on what Eric just mentioned, right? So there is no free lunch here. The flexibility you get from things like CXL do come with a latency penalty relative to native attached DDR memory, right? So the way we move this forward is like through hardware plus software co-design, right? In terms of making sure that the software layers are understanding of the platform topology, and they try and make use of had intelligence in terms of how we do near memory versus slightly farther memory. CXL, for instance, looks like a second NUMA socket without the CPU. And how can you now drive more intelligence? So maybe something which is a colder type of data ends up moving into the CXL memory, and whereas the more hotter data stays on the DDR. And how do you build this intelligence through the infrastructure layers, be it the kernel and above functions so that the applications can drive the benefits that we see? So it is going to be truly dependent on hardware plus software co-design that takes advantage of these technologies going forward. 

Excellent. Thank you. Ahmad, let me ask you, one of the things that I've heard, right, is you get kind of media independence by putting a standard interface between the processor and different types of memory. What's your point of view on the advantages or why customers might want that? 

Yeah, it's a really important, you know, Eric touched on memory pooling and Bharath there on the actual hardware and software, how to enable it. Media independence is where I feel one of the exciting doors that CXL opens for us here, allowing processors to expand to different types of memory. So by allowing users to not necessarily be tied to that one memory technology that a processor supports, we can now start thinking about memory tiering in the similar way that storage tiering became a big thing. So we can now optimize that system, you know, whether it's the performance you're trying to optimize or capacity or cost. And so you can now have a choice of memory depending on what that application requires, right? So if that requires a lot more bandwidth and or more capacity, you can now choose the lowest cost memory that fills that need. And so we'll see high performance applications still kind of predominantly being on DDR4, DDR5. If you're just looking for a lot of capacity, then we start seeing persistent memory and storage class memory being used. And you can now kind of fine-tune your system really depending on what that application needs. 

Excellent. Any other panelists want to chime in on this one? I picked up on Ahmad. He was talking about the persistent memory piece. So I would love to hear from Michelle because she's been in the middle of that. A lot of interest in persistent memory recently. What are your thoughts on the adoption rate, the benefits, overall uptake of why people are moving that direction? 

So what if you kind of step back and look at the bigger picture, what we're talking about here, what we're really talking about is allowing people to customize their configurations for the mix of applications they have in their environment, whether it's memory pooling or and, you know, price performance balance as well, whether it's memory pooling or aggregation or where is the bottleneck in your system? How do you optimize it as much as possible? And so that's honestly where a lot of the persistent memory comes in as well as looking at those specific workloads where you see, you know, I want a slightly different price performance ratio here. I can tolerate that. And I really need that that level of acceleration that you get from that that hybrid technology in there. So there's we're doing a bunch of things to help support that. We have about five hackathons a year. So anyone who wants to get in and start to play with the persistent memory environments, learn how it works. We also have a ton of educational materials coming out. It's one of those technologies has a really long lead time. So while we have had a couple of different vendors, we do expect to see a couple more showing up here. So there should be, you know, in a few years, you'll have multiple options out there in terms of, you know, how you can put your configuration together, not just be, you know, limited to a single vendor. So there's really there's really a lot going on there. 

That's excellent. Yeah, it reminds me of SNIA has been in storage for a long time. Storage started as a local device and then moved out into a shareable commodity. Do you see memory doing that same thing here in the next few years? 

Yes, I think what we're seeing and, you know, we kind of captured a little bit in the title of this talking about disaggregation. What we're really trying to do is allow people to have an easy and standardized way to, you know, right size every component of the system. And that includes, you know, memory, memory pooling on top of, you know, aggregators for specific workloads. One of the things that's working we're working on is in the SNIA side is computational storage. You can think of that as, you know, aggregators capabilities and accelerators. Sorry, I'm using the wrong word there. Accelerators for specific workloads and, you know, how to optimize that whole environment, put it together. The key to all of that is actually the manageability. If you don't really want to end up with all of these different pieces and parts and not be able to manage them consistently, which is where Redfish and Swordfish come in and why all the groups are working together on manageability. 

Makes a lot of sense. And then when when you introduced yourself, you kind of talked about some of the advantage of being able to disaggregate the memory. Do you see persistent memory being part of that disaggregation or are you thinking it's kind of standard memory. What use cases do you see as we look forward? 

Yeah, I do see persistent memory as part of what we disaggregate. I think it's for us the value prop is probably a bit less in the persistent quality and more that it's a pretty attractive tier of memory. It was something I was thinking about when Bharath and Ahmad were talking is it's just kind of crazy when you think about cloud computing is not shackled, but we're pretty hamstrung by legacy infrastructure, right, where fast memory and slow disks have dictated so much of what we do. And when Microsoft began our journey into the cloud, we started realizing that, you know, cloud-first applications were really born, you know, with a certain use case in mind like Twitter or Facebook, and you can actually see that materialize in the hardware design, right, which is something like a lot of tweets. Well that you know had a lot of, you know, performance demands and it's something very different than maybe hosting a generic VM, where hyperscaler may not know what the customer is doing in that VM right and that's part of why I talked about late binding at the beginning is that these technologies let us do a lot of customization at the very end of the pipeline, and that late binding quality is super attractive to us. Persistent memory, per se, is probably, it's a sweet spot for us, because not every application needs crazy fast memory. A lot of applications are happy, and would be extremely happy with memory performance that's maybe a little bit sub what they get with super fast memory, but then the costs can be passed along to them, or there's more scalability or resiliency to their app as a result of implementing something like persistent memory. So, yeah, it's kind of our, our take in a nutshell. 

Excellent. Varath, did you have anything you wanted to add to that? 

Yeah, I think in terms of optimizing for, like Glenn mentioned right in terms of optimizing for performance. It is definitely tending towards a not one size fits all kind of an infrastructure right even within the cloud we do need to tear it based on the specific use cases. And there is going to be specific optimizations we need to do at the current and other levels. And so while there is a lot of work happening within this consortium maybe the additional point I do want to make is, how do we kind of work together and jumpstart an ecosystem across the various technology providers, as well as the system providers so that as the specifications from these consortiums evolve at a specific clip, the technology providers and the system providers do match with the cadence, so that like if you take an inclusive perspective, you don't want to have a solution for instance in the 2023 timeframe that implements different points of CXL, like between 1.0 and 2.0 and different kinds of specifications which kind of leads into difficulty from an end-user perspective of scaling these into deployment. So we do really need to make sure that as the specifications evolve, we drive for some level of consistency with how the technology providers and system providers evolve. So it makes the process of qualifying and launching these new techs into mass production a little bit more seamless without a lot of work. 

Excellent. So I'd like to put Ahmad and Eric in a little bit of a challenge here. As you think about this relative to your interconnects, the CXL, Gen Z stuff, how do you see them either helping to realize the visions or are there things that need to be added to make that vision a reality? 

Well I think whenever you start to add resources to an interconnect, security and RAS features become incredibly important. So particularly as your node count or component count increases, the probability of failures increase and being able to still get to the data is very important. And so we think things like multipathing are very important. Being able to communicate events as they occur is very important and we're showcasing some of those capabilities in our demos downstairs. I do agree that there's a great capability of adding all of these resources, but you still need to manage them and you still need to handle errors as they come up. 

Okay. Ahmad, do you want to add to that for us? 

Yeah, I think Eric covered a lot of the key points there, especially when you talk about RAS and this term of blast radius, right, when we have a lot of memory that's sitting either pool being used by a number of hosts and this disaggregation model, or even aggregating the performance of multiple persistent memory devices behind a CXL switch that then you can have a single connect to the to the CPU. You're talking about a lot of memory at this point, and both have to make sure you have the right RAS features, but also the management of those devices, whether it's the device itself, the fabric devices such as the switches, or the actual media, and being able to actually communicate those errors correctly to the host, being able to have a fabric manager that can, you know, correctly manage this entire device and understanding where the errors have occurred, and being able to report that to the users. That's going to be really important and you know we've done a lot of work within CXL a lot of work with working with SNIA to really also reduce the investments and then also simplify the test infrastructure that needs to be placed in so having these standard based both device and fabric managers will really help that adoption of persistent memory and other emerging technologies. 

Excellent, excellent, thanks. So you guys both kind of mentioned the management. So let me jump into that and pull Jim into this conversation as well. Once we've got memory in multiple locations and we're starting to access it in different ways, the whole management of that starts to get a little more complicated than when it's directly attached to the processor. Jim, why don't you tell us a little bit about what you see the complications are, what works being done amongst the consortiums to help to overcome that challenge, and then what we should expect to see. 

Well, you're right that fabric management becomes one of the key features here. We have on the Gen Z side started with a prototype that only has basic features and we really need input from the wider community as to what new features to add to that. You know, basic crawl out of the fabric is obviously one thing. You have to identify what's there. You need standards to do that. You need to interact with the higher levels of the management stack with the OFA and those things through Redfish and there are proposals to have agents to do that. There's all kinds of features that these fabrics are probably going to have, like both on the wire and at rest encryption and you need to manage that. You have key management, all kinds of challenges that are to be faced and the fabric manager has to deal with all of that. 

Excellent. Rachelle, you've seen industry bodies work together. It's part of OFA, part of SNIA. Tell us what are we going to see? How does this work? Does it take years or can it be done in a short time? 

It can be. Well, that's a good question. How long is this stuff going to take? And one of the nice things that's been happening in the last few years is standards bodies are moving much more quickly. So Redfish and Swordfish, as an example, release two or three updates a year, sometimes four. It's incremental, incremental support. None of this waiting for a couple of years to even get the standard before you can actually then start working with it. The other pieces, as Jim was just describing all of these different aspects, there's so much more collaboration work happening now. I'll pick security, key management pieces. We know how to do that for other technologies. It's not a new problem. All you kind of have to do is say, hey, we need this and you can leverage it from someplace else. There's no wheel reinvention required. So, you know, the key management, certificate management are all being built into Redfish and Swordfish because we're doing it at a higher level in the system. So everything's there. That's also going to help us accelerate the work quite a bit because we don't need to come up with a Gen Z specific way or a CXL specific way. Here's the standard way we present it to the users. This actually works across your entire environment. Here's just the pieces you need to instrument. We're also doing that with security. SPDM, I won't tell you what it stands for because it's dumb, but this is a standardized model for attestation, runs at a low level, but it's being leveraged everywhere. And that's something that we can bring in as well. Security is very critical. But let's leverage all the pieces that we're using for both manageability and data path that are already defined in existing standards. 

Great. I have one more comment. So she was talking about acceleration of work. And I think another aspect of that is that a lot of this is happening in open source. You can go to GitHub. You can contribute. You can see the code. It's not, you know, some proprietary warehouse that you have to pay gazillions of dollars for. But it's all out there. 

Yeah, absolutely. One of the things that we're encouraging more of as well is partnering between open organizations and standards organizations so that the groups that are getting together to talk, like the Open Compute Platform is a great example. They're getting together to talk about the configurations they need. But they partner with the standards organizations to say, you know, so they're basically creating a recipe. And here's what we want. Here's, you know, here's the specifics, what we need for this specific configuration. And you do it through these standards developed by other organizations. So it becomes just a, you know, a big collaboration of partnership. We've all kind of alluded to some work that we're doing between fabrics in general. Gen Z is the first one engaging here and the OFA where we're just building an open source reference implementation for a management framework that should work across all types of fabrics. And we're feeding all of our learnings both up and down. So we're making sure that redfish and swordfish are fully populated with all the right properties and working in a standardized way across different fabric types and then we're feeding that information down to Gen Z as it's coming up to speed to say these are the things you need to be able to do. And helping to also get to prioritize that, you know, that functionality that should be implemented that Jim was just talking about. 

Excellent. And Barath and Glenn, let me put you on the spot a little bit. You know, as industry bodies do this kind of work, how easy or how hard is it to bring that into the work that you're doing to advance what you want to get done in your companies? 

Yeah, I can start if you like. I think it's pretty easy. I think one of the big benefits to Microsoft at least of participating in the initiatives that are discussed here is that it just launches everyone's comfort factor, everyone's contributions that we're making internally. All of a sudden we have a forum to do that in and we, you know, by virtue of having to, there's just an old, old industry phrase like eat your own dog food, but by virtue of having to consume the things that we're putting out there, we're making, you know, better internal product plans and more consumable kind of drops that we're putting into the standards bodies that we're working with. So I'd say the uptake is quite high. It's, Curtis, you and I work together, but one of the things that I always track is when we have trainings at the different consortiums, you can see it's a really good health monitor to see sort of the blip of, or the, you know, the stacked bar of attendees from my company and some of our peer companies. And when you see those things take off, you're like, yeah, we're cooking with gas. And you can really see the momentum of a standard and kind of the hot standard as it were take off because everyone you know in the industry is participating in it. And it's just moving super quickly and that just makes for a lot of excitement. We absolutely have that with what we're talking about today. So yeah, I'd say the consumability, Curtis, is super high. And participation levels across Microsoft, like all the groups, you know, that it takes to deliver cloud services and build cloud hardware, you know, 100% high engagement level. 

Thanks, Barath. How about yourself? 

Yeah, I think the active work lens said, I think the two things that we look for is the broadness of support from the ecosystem and the pace of iteration of the spec that I think was talked about earlier. We really do see the pace of the spec evolution picking up quite a bit, and that we feel comfortable that this will meet our demands, that things are improving perhaps on an annual-ish basis. For instance, I think Amar talked about how the CXL spec has evolved between 1.0, 2.0, 3.0, relatively big time, and also the CXL promise but also similarly in line. So the pace at which the spec is evolving and the openness of it really makes us feel comfortable about investing or software and putting efforts in terms of making these technologies successful. And so far from that perspective, it's been good. And similarly, it does require a lot of work with the ecosystem partners in terms of ensuring interoperability. So for instance, if you take something like a CXL memory expander, it does require various types of CPU architectures, different types of memory technologies, as well as the various expansion or home solutions, and then finally motivating the system providers to kind of get on this journey and build platforms and solutions. I think we have started this journey and it is starting to kick into momentum, but it is still in the long road in front of us. We really do need the entire community to work together in order to drive this forward, but I think we're off to a good start. 

Excellent. Alright, so one last question and maybe I'll start it with Eric on this one is down at the show floor. We've got a demo that spans across I think three booths. Tell us a little bit about that demo what it's showing off and, you know, have other people jump in and help out with what they're going to see when they get there. 

Sure. Yeah, so I think it's a pretty cool demo. We're stretching from the CXL consortium booth to the Gen Z consortium booth to the IntelliProp booth. We do have some wireless connection to the open standards pavilion, which is pretty nice. So really four, three and a half. 

Okay, I'll take that. 

Thanks. So, in a CXL booth, we're showing a CXL to Gen Z fabric adapter that has a multi-path connection optically 30 meters into the Gen Z consortium booth. We have a few different layers of switches in that booth, media boxes, which we then can configure using Fabric Manager software and present and allocate different resources to different nodes in the fabric. So it's pretty exciting demonstration of both proof of concept FPGA based implementations of multiple protocols and also the fabric management software that's running that we have codenamed Zephyr and then also a connection to the OFA open fabric management framework in the open standards pavilion booth. 

Excellent. I know, Jim, if you want to add anything, I know I've talked to you a couple times about it down there. 

Well, yeah, well, Curtis got pretty excited when we when we had a little graph of the topology. We did a crawl out of the Zephyr going discovering the Gen Z fabric and then we, for the first time ever, pulled the cable and saw the line change to red, the line representing the link going down and plugged it back in and it went green again and we were all cheered. 

Excellent. So, anybody who hasn't seen it, you know, walking, you know, walking a network probably isn't anything new, but walking a memory semantic fabric is new, it hasn't been done, it's really cool to watch. So I encourage you to go down there and see that. Now we come to the Q&A session. I think, pop through here. And if you wish to ask questions, is there a mic in the room or do they just have him have him stand up and ask a question. If you're, oh, certainly will. And if you're online, please send your question online and we'll catch it in the chat. So we got one question from Steve. Let's see. 

There's actually three different comments. Ah, okay. There are various points through the conversation. The first one, the, I wonder what latency is going to be, was actually when Ahmad was doing the presentation on CXL. So I don't know if there was a specific point in a, or specific configuration you're asking about latency or if there was a question about latency in general. Maybe one of the CXL guys can jump in on that one. 

Ahmad, do you want to take that, you know, what kind of latencies do we expect? 

Yeah, I'll cover a few different ones. So if we're talking about CXL direct attached with a memory controller using CXL.mem, you know, the targeted latencies that we're looking there is in the range of a NUMA. Right. So kind of what the industry has been used to in the NUMA. Those are the kind of the targeted latencies there. As we start going to CXL switches, that's where we're going to start seeing latencies increase. And that's where then we need to take a look at, well, what types of memory will sit behind a switch and where will that usage cases be? And can those applications tolerate that additional latency? We're certainly going to be above a NUMA when we get to CXL switching. Of course, we'll take a, we'll have to see what the ASIC providers provide and how low they can get that latency. But once you get to the behind the switch, then you have to take into account what types of memory should sit behind there, given the higher latency. Is high bandwidth DDR5 still the choice or perhaps persistent memory that is already higher latency, lower bandwidth? Perhaps that's the right decision that goes behind it and a combination of the two depending on that application. 

All right. Thank you. Next question comes in from Scott. It says, when considering a socket with different types of media attached and it gives HBM, DDR, persistent memory or disaggregated memory with remote persistent memory, how will programmers reason about the different characteristics of the various options? Will they be exposed to a new domains? Will they be transparent and treated as fast memory is closer? So how is that going to, how are programmers going to see this? Maybe Jim, see if you want to take that one. Because I'm the software guy here? Yeah, because you're the software guy here. Yeah, I'm going to pick on you. 

So sure. Certainly, we're just exploring that ourselves now. But what we're doing in the prototypes we have on the floor is that we put the remote memory across Gen Z in its own NUMA domain. And using the same kinds of slip tables and such that you would see in ACPI and a host, you can reason about the difference between the local latency and the remote latency. 

OK. And so you have this idea of near memory and then medium memory and far memory or what do you see happening there? 

I suspect that there will be, yes, will be several tiers that will have to be represented. And we need other other kinds of representation as well. So, you know, our ACPI has also got this table called HMAT, the heterogeneous memory, something or other. And so it can represent some of those other properties about persistence and things that were in the question. 

OK. Do we have latency numbers? There's some questions in here just kind of looking for a rough latency. So, you know, what do we have for latency numbers? 

So I can talk to that a little bit. So we have different latencies depending on different switch hops. What we're doing now is in FPGA implementation. So it's not specifically latency optimized. We think with ASIC silicon, it'll be substantially faster than what we have. But we have in FPGA through multiple hops on CXL hosts, we have sub 800 nanosecond type latency. Expectation is that will improve significantly as some of these pieces start to get hardened. 

OK. And then I understand from CXL they're kind of looking at roughly, you know, if you have CXL attached memory, it'd be like talking to the other socket in a two socket memory. 

Yeah. So the NUMA hop is sort of the target. So what we see in a multi socket server today, think about it as a NUMA hop to get to a CXL memory device. 

OK. Well, great. That's kind of the questions that we have online. 

I think there was one more in here that we can target to Glenn was, is there any significant new software targeting persistent memory from Microsoft? So I think I, yeah, Glenn, that's really just, you know, what are we looking at from an application layer up on adopting these technologies? 

Unfortunately, there is nothing I can comment on at this time. 

All right. So maybe not the most satisfactory answer, but you know, usually when somebody says that, that means they're thinking about it, if nothing else. So let me just get maybe from the end-user side of point of view. Glenn and Barath, what do you have advice for other end-users as they look into Gen Z, CXL and the Fabric Manager that's being developed there? Do you see this as something that you'd be interested in and able to implement in the near term or is this something that you think is many years out? 

You know, I can start. I think it probably depends on what you consider to be near term. I'd say it does meet my definition of near term that probably not probably, but within the next generation to at most, you will see offerings that are lighting up the technologies that we're talking about today. No question. And as evidence of that, just walk the booths, right? CXL booth, Gen Z booth, you know, lots of products and lots of announcements, you know, last week at OCP, this week at SuperCompute. So the wave is definitely there. You know, we're here, our companies, as a testament to our belief in the technology. And that really means uptake, like I spoke to, you know, earlier. Maybe a little bit of teething pains, but I'd say probably a year, a year-ish is the sweet spot. But for how fast these things are moving, I would caution people about, you know, a wait and see approach could translate to being left behind. So I would strongly recommend playing with the technologies, downloading, ensuring that, you know, your products are compliant if you're in the product business and/or you're using some of the latest sandboxes that we have among the hyperscalers so that you get a chance to kick the tires. 

And the only thing I would add is this is definitely a mindset change, right? I think if you're a software developer, one of the most important things that you love is local attached memory, right? That's kind of something that you are pretty passionate about. So I think changing from that into this paradigm requires a mindset change, and that requires to discuss the broader challenges of deploying in what we have done in the past at scale for the next 10 years and why that just doesn't scale. And as long as our software developers start to see that, then they embrace the challenge of doing different types of partitioning and driving intelligence into the infrastructure software layers in order to make use of these technologies. Definitely, I think this is going to be a pretty exciting ride for the next 10 years. And I think, like Glenn said, people do need to get started on it now, otherwise they'll significantly fall behind. So I think we do need to start moving now. 

Okay, and we got one last question in for Microsoft and Meta. How pervasive will disaggregated memory be? Will all the servers have this eventually, and how keen are you on needing this? 

I think the scale of deployment is maybe a question that we cannot answer, but in terms of the need for it, we definitely see the need for it, which is why we are investing in this technology and the standards bodies and working with the ecosystem partners. 

Okay. And I know we're getting close to the end here. We did have one other question. Is there a place to go play with this stuff? The answer is right now, no. Some of the first demos of CXL are on the show floor here. And so I encourage you to visit our booths at 1607, 1707, 1507. I think they're just all in a row, as we talked about. So really want you to get down there, take a look at what the industry standard bodies are doing, and particularly CXL and Gen Z and their booths are doing. And then finally, I'd like to thank all of our panelists locally. Jim and Eric from IntelliProp, Richelle from Intel, and then on the phone we have Ahmad talking about CXL from Microchip, and then Bharath from Meta and Glenn from Microsoft. So round of applause for them, please. Thank you very much. I hope to see you locally down on the show floor looking at all the cool demonstrations that are going on. 

And just one note, there's also in the, for any of the virtual participants, there is a whole set of online virtual video demos as well in the CXL Consortium online booths. 

Excellent. Thank you again. Everyone enjoy the rest of the show. Thank you.
