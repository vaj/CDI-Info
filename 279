YouTube:https://www.youtube.com/watch?v=zp2xJpuvCt8
Text:
All righty. Well, thank you, Frank, for hosting this event, inviting me to speak, and thanks to our guests for attending.My name is Reggie Reynolds. I'm the product marketing manager at NVIDIA for the network business unit.In my role, I collaborate with our data storage partners to bring high-performance storage solutions to AI data centers, leveraging NVIDIA's accelerated networking platforms.Today, I'll be discussing our networking solutions from the perspective of AI data centers, focusing on how they meet the stringent requirements of AI workloads.At NVIDIA, our primary goal is to deliver full-stack acceleration for AI data centers, and networking is a crucial component of this.While I won't be dwelling into our technology as a memory or fabric solution specifically, I hope to illustrate how our networking can be effectively applied in this area.

Now, from the networking perspective, this is how we define the data center.We break it down into three buckets, and we'll focus on networking solutions on two, AI factories and AI clouds.Now, AI factories are being used for large language model training, digital twins, and data science.AI factories require tremendously high computational power.And the AI supercomputers that power them are built to train and refine very large, complex foundational AI models at a very large scale.The ability for these systems to deliver huge performance at scale requires a specialised network comprised of NVLink for communication between the GPUs within the servers, and InfiniBand for communication between GPUs in separate servers. In contrast, AI clouds are hyperscale systems serving volumes of users hosting multi-tenants and running many less complex, small, low-scale jobs. The lesser demands for scale and performance of these systems are effectively served using Ethernet as their common network. For multi-tenant AI cloud environments, however, that cover a variety of workloads. NVIDIA now offers an Ethernet-based network called SpectrumX.

Now, in order to build new AI clouds to support these demanding workloads, a separate AI fabric is necessary. Now, AI workloads are unique and generate large network flows that lead to increased impact on network infrastructure. Now, running these workloads on traditional existing cloud Ethernet infrastructures introduce significant congestion, increased latency, and bandwidth unfairness. This causes performance loss and the inability to effectively utilize the system GPUs.

Now, Quantum InfiniBand and SpectrumX Ethernet are NVIDIA's network solutions dedicated to maximizing AI data center investments. Let's quickly look at Quantum InfiniBand first. Aimed at large-scale AI factories, Quantum InfiniBand is the highest performance network for AI. It targets deep learning, scientific computing, inference, LLMs, as well as HPC use cases. And it can provide impressive performance enhancements for your customers, including 32x more AI acceleration and up to 6.5x higher scalability improving ROI and overall investment.Now let's look at, just briefly, the SpectrumX Ethernet.Of course, we'll go over these in more detail later.Specifically built to accelerate generative AI clouds, SpectrumX brings high performance AI networking to Ethernet implementations over standard Ethernet protocol, boasting an impressive 95% effective bandwidth and 1.6x increase AI network performance over traditional Ethernet.

Now, these are our network platforms that are offered by us at NVIDIA. Each platform is designed to meet specific needs and provide high-performance solutions for computing and AI.Our upcoming Quantum X800 InfiniBand platform is designed for massive-scale AI.Our Bluefield 3 DPU networking powers modern data centers with robust computing power and integrated hardware, accelerators, and offloads for networking, storage, and security.And then Spectrum X Ethernet is our first Ethernet platform designed specifically to enhance AI.

Specialized Ethernet is essential for AI cloud environments, which are defined by their networks. We categorize these as East-West, or GPU-to-GPU connectivity, and North-South as server-to-client connectivity networks. Both use these networks to serve different purposes.The North-South network is optimized for connecting servers to clients, potentially across the globe.It handles numerous small TCP flows, which are easy to load-balance using ECMP. However, latency can vary significantly due to distances involving making outliers' latencies more critical than average latency.Long-tail completion times can severely impact AI performance, so minimizing these outliers is crucial.The East-West network is designed for high-speed loss-less communication between GPUs within a data center. It runs on UDP with RoCE, hardware base RDMA InfiniBand, and handling massive elephant flows that are more challenging to balance. This network must maintain low jitter and latency to ensure efficient GPU collaboration.AI applications are tightly coupled, Similar to high-performance computing, where components work closely together, this is unlike typical cloud environments, which are loosely coupled.In AI, the performance of the entire system can be hindered by the slowest component.Much like a group traveling together with their slowest member dictates the pace, ensuring all GPUs operate efficiently without the idle time is critical for optimal performance.

Now, let's take a deeper dive into InfiniBand.

Now, InfiniBand is the network of choice for data intensive simulation and analytic applications in HPC and supercomputing.It remains the most efficient way to move data using fundamental concepts of hardware, RDMA, delivering highest performance and return on investment.I mean, at any scale.By bringing additional network engines and capabilities closer to memory and offloading more tasks from the computer and onto the network, we now have a smarter, more capable, and most efficient network solution that can participate in application runtime.We have over 20 years of maturing the technology to meet the tremendous demands of extreme workloads on the largest supercomputers in the world.

Now, InfiniBand provides primarily two types of services.First, from a networking perspective, InfiniBand has the highest throughput.We currently- We're currently delivering 400 gigabits per second NDR, which we'll talk more about later.it is the lowest latency network on the- on the market, and InfiniBand comes with unique features such as quality of service, adaptive routing, and congestion control.InfiniBand, from the beginning, was architected to support native hardware RDMA to achieve highest performance by moving data between devices without CPU intervention.Now, the second set of services is the ability of InfiniBand to provide in-network computing.These are computing engines that sit inside the silicon on the InfiniBand network in both the network cards and the switches.These computing engines provide pre-configured programmable services for acceleration of I/O and offloading of services that normally run on servers.

It is important to note InfiniBand has been widely accepted in high performance computing and the supercomputing industry.This is a group that there is a group out there that tracks the fastest computers in the world, and they organize them in the list they call the top 500.As you can see in the graph, in terms of the number of top 500 systems, powered by InfiniBand, it stands head and shoulders above the others.

I'll briefly walk through the components of the InfiniBand infrastructure, starting with the ConnectX adapter card.These adapters support up to 400 gigabits per second in PCIe Gen 5.They are also RDMA optimized and can provide in-network and computing plus storage cloud and security offloads.The Bluefield Data Processing Unit, or DPU, provides a high performance programable network engine, the ConnectX chip coupled with ARM CPUs to enable the customization and optimization of both control and data operations.Along with the DPU, there is an NVIDIA software development kit, DOCA SDK, that enables developers to easily create high performance software-defined accelerated services, leveraging industry standard APIs.The Quantum InfiniBand switch delivers a complete chassis and fabric management solution to build cost effective and scalable switch fabrics ranging from small clusters up to thousands of nodes, reducing operational costs and infrastructure complexity.Advanced features such as adaptive routing, congestion control, and enhanced quality of service ensures the maximum network performance under all types of traffic conditions.The LinkX cables provide the lowest-end, lowest bit error rate in the industry, along with lower latency, lower power for superior connectivity.NVIDIA manufactures a complete line of cables and transceivers in order to provide the best performance in the end of the AI solution, networking solution.There is also the Metro 2 long-haul InfiniBand switch that can seamlessly connect your InfiniBand storage solution to data centers up to 40 kilometers apart.There's also the ability to provide simple integration to external Ethernet fabrics via the SkyWay Ethernet gateway.And then there's the Unified Fabric Manager, or UFM, that is an enterprise-class fabric manager that combines telemetry, analytics, and preventive troubleshooting at scale.

Now, just to go, we'll start to talk a little bit more about the Quantum 2 platform that we're currently delivering as our InfiniBand solution today.The Quantum 2 InfiniBand is our current product offering that delivers 400 gigabits per second in the end.The Quantum 2 QEM9700 switch has 64 ports of 400 gigabits per second, or 128 ports of 200 gigabits per second.This generation switch is using, delivers version 3 of SHARP, which stands for Scalable Hierarchical Aggregation and Reduction Protocol.It is used for small and large message data reductions and provides 32 times more AI acceleration, engines than the previous generation.The ConnectX 7 InfiniBand adapter delivers 400 gigabits per second single port, or 200 gigabits per second dual port bandwidth.And enhanced network offloads include hardware-based DMA and a 16-core data path accelerator.And then the Bluefield 3 InfiniBand DPU comes with all of the features of the ConnectX 7 plus 16 A78 ARM cores, offload, and accelerators to support security and storage services, and as stated before, a development SDK called DOCA.

With the Quantum 2 systems, you get twice the throughput, 32 times AI acceleration, and 6 and 1 half times the scalability, all with 36% lower power consumption compared to our previous generation, delivering unprecedented network and performance at any scale.

As these graphs illustrate, InfiniBand delivers the lowest latency when compared to RoCE over RDMA over converged Ethernet.The RDMA latency is 7% lower versus RoCE for direct connect between nodes without a switch.And the RDMA latency is 30% lower versus RoCE for node-to-node in the same way.with a single switch and deliver 60% higher BERT AI performance with four nodes.

The InfiniBand or NVIDIA InfiniBand will continue to lead in networking with our upcoming release of the XDR 800 gigabit per second technology, which was announced earlier this year at GTC with availability end of this year, early 2025.The NVIDIA Quantum X800 platform is the next generation of NVIDIA Quantum InfiniBand, which includes version 4 sharp, enhanced adaptive routing and telemetry-based congestion control and performance isolation capabilities.It delivers the highest performance with an impressive greater than two times performance improvement over Ethernet.The foundation of the Quantum X800 platform is the Quantum Q3400 switch and includes the new ConnectX 8 SuperNIC based on the next generation ConnectX architecture and delivers 800 gigabits per second in the end.Now, the Quantum X800 platforms are the first devices based on 200 gigabits per second and when compared with previous generations.The Quantum InfiniBand impacts five times higher scalability, connecting more than 10,000 GPUs in a two-level flat-tree topology and drives 4.4 teraflops of in-network computing.The Q3400 4U switch includes 144 800 gigabits per second ports, over 72 OSFP cages and a dedicated port for management.

SpectrumX is built on network innovations powered by the tight coupling of the Spectrum 4 Ethernet switch plus NVIDIA Bluefield 3 data processing unit.However, SpectrumX is still based on standards-based Ethernet and fully interoperates with any technology that communicates with Ethernet.When considering the total cost of ownership of the entire AI cloud and importance of the network, SpectrumX has an ROI larger than the cost of the network itself.To put it another way, an AI cloud based on traditional Ethernet could acquire its network for free, but it would still not match the ROI of SpectrumX.SpectrumX is tuned to deliver the highest performance with the state-of-the-art AI models and frameworks.

Now, SpectrumX delivers the nearly perfect bandwidth and extremely low latency needed for AI workloads.The key to this performance boost is the RoCE extensions that NVIDIA has introduced.Going beyond standard RoCE, these extensions include RoCE Adaptive Routing for dynamic load balancing and RoCE Congestion Control for performance isolation.These innovative extensions are unique to NVIDIA and require specialized and differentiated architecture of Spectrum 4 and Bluefield 3 DPUs, but they are still fully Ethernet compatible with applications and other elements of the data center infrastructure.

This technology relies on both Spectrum 4 and Bluefield 3 to work in tandem with SDKs and the software layered on top for full-stack optimization.Spectrum SDK and DOCA allow direct access and control of the network hardware itself with hardware telemetry data that provides real time insight while Magnum IO is a suite of technologies that introduce acceleration and offload from the network to the GPU and storage. At the software layer, NASs such as SONiC and Cumulus introduce the flexibility and operational efficiency needed for AI clouds, while tools such as NetQ, DOCA services, and NVIDIA Air deliver continuous monitoring, validation, automation, and simulation of the network.

NVIDIA has invested millions into building Israel-1, a cluster in Israel, and so it can be used as a testbed to benchmark and optimize generative AI with Spectrum X.Consisting of 256 servers with 2048 H100 GPUs, Israel-1 is the blueprint for generative AI clouds with peak AI performance of 8 exaflops.

The SN5600 is the first switch we introduced into the Spectrum 4 family, leveraging the OSFP form factor for either 64x800Gbps or 128x400Gbps.It delivers the performance and hardware innovation needed for Spectrum X deployments.The SN5600 also features hot-swappable redundant fans and power supplies.

The Bluefield III SuperNIC provides these best-in-class remote direct memory access storage equipment.remote direct memory access over converse Ethernet network connectivity between GPU servers at up to 400 gigabits per second, optimizing peak workload efficiency.For modern AI clouds, Bluefield 3 SuperNIC enables secure multi-tenancy while ensuring deterministic performance and performance isolation between tenant jobs.

Baseline optimizations are made to SpectrumX RoCE settings to ensure the network is equipped to handle the RDMA offloads.While the tuning buffer thresholds along with the switch and DPU are SuperNIC parameters, SpectrumX shows that not all RoCE is created equal.SpectrumX achieves 4.3x higher bandwidth and 2.2x lower latency.In conclusion, we offer the highest-performing overall network solutions in the world with our Quantum InfiniBand platforms and the highest-performing Ethernet network solution with our SpectrumX platform.Please consider these networking solutions in the future.Thank you very much for your time and enjoy the rest of the Memory Fabric Forum.
