YouTube:https://www.youtube.com/watch?v=pTsCL6-RSqw
Text:
So, my name is Gerry Fan. I'm a CEO and co-founder at XConn Technologies. And today I'd like to talk about the XConn CXL switch, which enables the composable data center infrastructure.

And so, the XConn CXL switch silicon is one to solve the following problem. And one of the main problems, as we know, is that as a server CPU, the core count increases over time. However, the DRAM, the bandwidth, and the capacity, for each core, as a matter of fact, is decreasing. And that becomes one of the system performance limiting factors. Because of the advance of the silicon process continue shrinking, and that makes it much easier to adding more cores into the CPU. And however, because of the HD RAM channels, which require a lot of pin count. So to increase the pin count for any of the package is getting more and more challenging. And also the cost associated with increasing this pin count become exponentially increased. So, however, the requirement for the memory has always been continued demanding because of those emerging applications such as in-memory database, computing, and any of these very computation-intensive of these genomics calculations, which is required the massive amount of memories. And so does the hyperscaler computing also require a lot of memories. And so that has become a big bottleneck, which is limiting the system performance and because lacking the capacity of the memory. So, in the CXL switch is imposed to provide a solution to solve this problem. 

And so the XConn solution is providing this emerging data center memory architecture. So as we can see that on the right side of the emerging the memory hierarchy, and we can see that typically you have a CPU on the top and right attached with CPU is a DRAM. Traditionally, and below that you could have some emerging memory technologies such as 3D cross point or ReRAM or PCM and other emerging memory technologies. And below that you have these SSDs for either the low latency SSD or regular NAND based SSD. And the access time for the DRAM typically, if you look at it from the hardware perspective, is about like 35 to 40 nanoseconds. And the access for the 3D cross point typically is 300 to 400 nanoseconds. And of course, for the SSD, it's much slower, that's typically is at hundreds of microseconds. So the DRAM obviously has the shortest access time. And then we can see that there's a big gap between the DRAM and 3D cross point, and especially the 3D cross point. And as we all know that it's become pretty difficult for customers to adopt. So in that case, we will see a huge gap between DRAM and SSDs. So the CXL DRAM will provide an ideal solution to fit into these two different types of memory, two different types of storage categories. One is for the non-persistent memory, such as DRAM, the other one is for persistent memory, which is SSD. And we as a CXL switch vendor, we provide the switch which can enable much shorter latency to access the DRAM. And as we know that the latency is one of the key factors for the system overall performance. And CXL technology, which is enabled such application based on its very short latency. And in addition to that, the CXL, the DRAM can be, the capacity wise can be increased profoundly by utilizing the XConn CXL switch. And to increase the capacity from hundreds of gigabytes to 16 or even higher, 32 terabytes with connection through our switch silicon. And with the shorter latency, along with the larger capacity, so that is going to give a profound boost to the overall system performance. And in the meantime, this solution will keep the TCO under check. Because again, the memory itself is contributing almost like half of the service, the overall cost. 
So for the modern, the composable data center infrastructure, and basically our solution has become the cornerstone and a key component, which is enabling the memory pooling and desegregation in the composable data center infrastructure. As you can see that our switch, silicon-based switch chassis is in the middle of the rack. And through our chassis and the compute chassis, which is, could be the GPU or could it be the high-performance, the data center server CPU. And on the other side, all the CXL-based memory card can be plugged into this memory chassis that formed into a composable memory system. And for any of the upgrade, either for compute chassis and the memory chassis can be very easily accomplished. And the connectivity between them can be using those cables called MCIO and to connect those chassis through our switch and to forming the composable data center, this type of memory desegregation. 

So for XConn, our solution, we offer a CXL 2.0 with PCIe Gen5 switch. And so we are offering the first CXL 2.0 switch. And our switch has a much higher capacity and to support the new, the large number of the servers and memory systems can be connected together. And we offer much lower latency that is enabled in the memory access. And our system architecture is optimized to reduce the power consumptions and compared with the existing, in a similar application, we have a much aggressive power consumption rate. And so our first generation of the switch is being taped out in February. And right now our engineering sample has been validated in our lab. 

So I'd like to talk about the CXL 2.0 switch. And basically the use case model for CXL 2.0 switch can be as simple as a memory expansion. And as we know that the 2.0 supports a single layer. And because of our large port count and we can adding the, we can connecting a large number of the CXL memory card and could expand the memory capacity tremendously. And especially for a type three memory device, and we can expand to 15 terabytes based on the current CXL memory card from those vendors. And also our chip, and we also enable to be able to work in with CXL 1.1 device, 1.1 server CPUs. 

And for another use case is more is in the memory polling. And because CXL 2.0 has the fabric manager and is able to support the multiple logical device versus a single logical device. So the multiple logical device combining with our CXL 2.0 switch, we are able to support the memory polling. And for example, on the picture on the right, the host H1, which is can be, can access multiple these devices and they can be assigned to one of the logical devices inside each CXL device. And that makes the multiple hosts able to share the same physical CXL memory device by allocating these hosts to his own logical device. So that makes the memory polling and become feasible and which is enabling this memory desegregation. And for the fabric manager is going to responsible to provision a when and how these logical devices get allocated to those hosts. So it's very much is on-demand based and that greatly improves the memory utilization and efficiency, which is the foundation of this memory polling concept. And to be able to improve the memory efficiency and reduce the cost of the overall system. And as a matter of fact, for XConn CXL switch, we are not only can dealing with the CXL 2.0, which is connecting to CXL 2.0 hosts. And we also can have our own proprietary scheme to be able to connect to the CXL 1.1 hosts on the market and to enable the similar, the memory polling and sharing scheme. 

And so this is our product roadmap. And right now we are in the engineering sample phase and we planning to have our production version of the CXL switch to be available the middle of next year and to line up with the CXL 1.1 CPU production and also CXL 2.0 servers available during that time. And continue to be the leader of the CXL switch and we're planning to have our CXL 3.X with a PCIe Gen 6 speed to be available in the middle of 2025. And for CXL 3.X, that's going to be open a much wider application, especially in the AI computing and in the GFAM, these type of applications and to enable more versatile applications for the GPU and CPU and also enable them to share the memory as well. And today, we are offering those reference chassis to enable our customers to be able to evaluate the CXL switch based system more quickly. So our reference chassis can basically be ordered and used by the system integrator or by the cloud service provider and to build their system to evaluate the advantages and the efficiency of the future system much more easily. 

Lastly, and I'd like to conclude my short introduction to our product. So if you have any questions, please feel free to send an email to me. My name is Gerry Fan and my email address is gerry.fan@XConn-tech.com. Thank you very much for listening to my introduction.
