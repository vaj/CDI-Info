
Hi, my name is Thomas—or Tom—Parnell. I'm a principal research scientist at IBM Research. I'm based in our lab in Zurich, in Switzerland. I've been with IBM Research for about 12 years now. I joined working on algorithms and error correction compression for IBM's data storage portfolio, and gradually, over this decade and a bit, moved in the direction of AI. For the last year—two years—I've been deeply embedded in the LLM inference work that we do within IBM, in particular, the work we do in open source. Today, I'm going to talk to you about vLLM, which is a software framework for high-performance serving of large language models using open-source technology.

Okay, so the agenda for this talk: I'm going to start with a brief overview of what vLLM is—kind of what it encompasses. I'll show some API examples of how it can be used and talk a bit about the community that's built up around vLLM. I'm then going to give you a kind of laundry list of features. I mean, there are so many features in vLLM that I don't have enough time to talk you through all of them. So, I'm going to kind of describe what's there. And then I'd like to focus on, you know, some trends that have been happening in LLM inference—like the way that people are using LLMs has been evolving a lot in the last few months. And I want to talk about how that affects vLLM and what kind of features are being developed to handle these new trends. I'm going to focus on one particular feature, which I think is very timely and interesting [way] called automated prefix caching. 

Okay, so what is vLLM, and like, why do we use it in IBM? Um, so yeah, IBM and Red Hat's flagship AI projects all use vLLM as the core inference and serving engine. So, that includes IBM Watsonx AI, as well as Red Hat OpenShift AI and RHEL AI for Red Hat Enterprise Linux AI. So, vLLM is, you know, quite a complex stack. So, I've shown this figure on the right—like the different components and layers of the framework. So, at the top, we have, like, an Open AI-compatible front end, which can handle requests from many concurrent users in parallel. Below that, there's a layer of scheduling and batching, which takes the requests from different users, batches them together before sending them down to the model. Below that, we have implementations of basically every large language model you could imagine—the number is really huge. And below that, there's also, you know, a very low-level GPU kernel implementation of different attention mechanisms, as well as other, more exotic kernels for LLM inference. But sitting alongside all of that, um, which is, you know, fundamental to the way that vLLM works, is a layer for managing the KV cache. Now, the KV cache stands for key-value cache, and this is, like, the state which we maintain between consecutive decoding iterations using an LLM. So, LLM inference is, like, an iterative process, right? We start by generating—we start by processing the prompt and generating a single token, and then we keep generating more tokens after that. Each of them depends on the previous ones, and in between each iteration, we have a state that's called the KV cache. It's very important to think about that state and manage it carefully, and that's what vLLM does really well. Um, vLLM is not the only framework for serving large language models. Um, there's quite a large number of other projects from NVIDIA, Hugging Face, and others. So, why did we, IBM, choose vLLM? Um, I think that the first and foremost reason is that vLLM has evolved into a really vibrant open-source community. So, it has more than 800 contributors now—many contributors from academia, but also from industry. So, lots of different companies are A) actively using vLLM in production, and B) contributing, you know, features and fixes back upstream. So, really, like, a large development community distributed across the industry. The second reason is it has really great performance. I actually initially wrote "state-of-the-art" on this slide, but I took it off because making a claim about state-of-the-art performance in this field is really very risky. Like, there's something new every day, pretty much. And vLLM has kind of become, like, the standard baseline for all, you know, research in this area. So, every day or every week, there's a paper saying, "We beat vLLM by X amount." That's not really what's important. What's important is that the vLLM community is very quick to, you know, like, get that new research that's happening and get it contributed back into the project. So, [it’s] kind of the focal point for research in this area. The third reason is vLLM has very good support for diverse hardware. I'm going to talk more about this later. My team has been involved in adding support to vLLM for IBM's own AI accelerator, IBM Spire, but also a whole bunch of other GPUs and custom accelerators are already supported. And finally, vLLM is part of the Linux Foundation—you know, this is very important for our long-term, like, strategic outlook in that no single entity controls the future of this project.

Okay, cool. So, how do you use vLLM? There's two main ways it can be used. They're quite different conceptually. The first one is as, like, a Python interface for doing offline batch inference. This is, like, something you can do on your laptop or on your development machine as a single user. vLLM is, like, a Python library. You can import it, you can instantiate this LLM object for a given model. You can call "generate" with some batch of prompts, and you get some text as output. This is what we call offline inference. It's a very valid use case. It's involved in things like synthetic data generation, as well as, more recently, it's used for, like, training. So, for things like reinforcement learning, where you need to generate lots of outputs from a model as part of the training process. I'm going to talk more about this later. So, this is a very important use case with the LLM. 

But maybe the one that's more commonly used in production is the second one that I show here. This is, like, when we say "serving LLMs," we mean this. So, we have a server and a client scenario. You can use "vLLM serve" to start an inference server, which exposes that model over various different endpoints. And then you can have multiple concurrent clients sending, you know, generation or completion requests to that endpoint at the same time. So, these are the two main use cases of vLLM. 

In terms of the community, yes, so it has—now, I mean, the project has something like 30,000–32,000 stars on GitHub, has, you know, a lot of commits every week. And you can see here some of the companies and universities contributing to the project. So, the vLLM actually started as a project from the Sky Computing Lab at UC Berkeley. Um, but you can see there are many contributors from AnyScale, AMD, Intel, Roblox, IBM—so my team and others are actively contributing to vLLM. And you also see a big—a lot of the contributions here come from a company called Neural Magic. So, Neural Magic—yeah, contributed a lot of features to vLLM, especially around quantization. Actually, I think either late last year or earlier this year—I can’t remember which—Neural Magic was acquired by Red Hat. So, they are now also my colleagues, and we work closely together on much of what you see here. 

Um, so, in terms of what features vLLM has, I've kind of organized this into three categories. And, like I said, I'm not going to go through all of this—I’d rather focus on something so I can describe it properly, and you take something away from this presentation. But just so you get a kind of sense of the, you know, breadth of this project. So, in terms of models that we support, of course, vLLM was initially designed with transformer-like LLMs based on the attention mechanism. LLaMA is one famous example. But, of course, there are many, many more. Then, we see a lot of, you know, use cases now involving LLMs with mixture-of-expert layers, like Mixtral. In IBM, we also train a lot of models using mixture of experts these days. There’s now, you know, multimodal LLMs, which combine language with images or other forms of multimodal data. We have state-space models—so these are things like Mamba, Bamber, Jamber. These are kind of emerging models from academia, which have very different characteristics to, like, attention-based models. They have to be handled in a different way but can be very promising in some domains. Finally, we can support, you know, embedding models and reward models on top of the rest. In terms of hardware, so, of course, many people use vLLM on NVIDIA GPUs—that was, like, what it was initially built for. But now AMD GPUs and Intel GPUs are well supported by the framework. It also has support for AI accelerators from different companies. So, it supports AWS Neuron—that’s, like, the Trainium and Inferentia chips. It supports TPUs from Google, the Gaudi accelerator from Intel, Ascend from Huawei, and, very recently, it now supports IBM Spire, which is the accelerator that we’re developing in IBM. In addition to the models and hardware, kind of the core, like, innovation in vLLM is in terms of performance optimizations. There is loads of stuff in vLLM—like, I’m not capturing it all here. Kind of, initially, when vLLM first came out, the main focus was around two optimizations, I would say. One is continuous batching, which is a unique way of managing requests from concurrent users when using autoregressive language models. This is really important for getting, like, maximal throughput from your expensive GPUs. And something called PagedAttention. PagedAttention was a really kind of innovative idea at the time, and it relates to how we effectively manage this KV cache in limited-memory accelerators like GPUs. So, then, like, since the kind of conception of vLLM, like, a lot of additional stuff has been added: chunked prefill, speculative decoding, different forms of parallelism—tensor parallelism, pipeline parallelism—with loads of optimized kernels. It supports PyTorch compile. There’s emerging stuff like disaggregated prefill. But yeah, what I want to talk to you more about today is something called automated prefix caching. Because I think, as I’m going to explain, this is a really exciting feature, and it really helps with a lot of the emerging workloads that we see people becoming interested in. So, with that, I’m going to switch gears a little bit and start talking about some of the trends in LLM inference and why automated prefix caching can help with that. 

So, the first trend I want to talk about is something that I’ve been working on for a long time. It’s something I’m sure people kind of are familiar with if they’re aware of the field, which is that we see exponential growth in the context length of language models—that’s, like, the maximum number of tokens in a sequence that they can handle. Yeah, when I first started working in this area, they were kind of around 8K—that was, like, the limit. Now we have models which can handle millions of tokens. This figure I pulled from a thread on Reddit—this shows, like, the closed-source language models, but you can find, you know, open-source models on the Hugging Face Hub with million-token context length today. So, this is not, you know, only a closed-source trend—these things exist in the open. And what does this mean for, like, LLM inference? There’s two things, really. One is that as the context length increases, the time to process the prompt—we sometimes call this the prefill time—grows quadratically. So, this affects, like, from a user perspective, this means that the time-to-first-token increases quadratically with the context length. The KV cache, which is the state we need to maintain between, like, inference iterations, also grows. But unlike the TTFT, it grows linearly with the context length. So, yeah, this allows us to make some kind of interesting trade-offs in the system design. Excuse me.

Okay, the second trend I’d like to talk about is agents. So, I think it’s sort of an overloaded term—maybe there’s lots of different definitions of agents. Yeah, if you Google it, you’ll find the code—there’s, like, a lot of conflicting information. I recently read through this course on Hugging Face, which I found pretty good. Their definition of an agent is taking an AI model and giving it the capability of building a system around it that allows that model to interact with its environment in order to achieve some user-defined objective. What this means is we ask the LLM—we give it a task—and we ask it to make a plan about how it should solve this task. So, in this example, if we want a coffee, we ask the LLM to make a plan about how to make that coffee. It’s going to go to the coffee machine—or rather, go to the coffee grinder, grind the beans, put the beans in the coffee machine, make the coffee, bring it to me at the laptop. And the second thing is then the LLM will create this plan, and then the system can take that plan and execute on it. So, what this means in practice is that we give the LLM the ability to call external tools—like, this can often be querying some API, could be searching the web. And this is really important because it gives the LLM the capability to ingest information that it didn’t see at training time. It can learn about the world beyond what it was trained on. And what we typically see in these agents is that the LLM makes a plan—this kind of thinking step—we then act on it. It then observes the result of those actions and then goes into another, like, thinking iteration. So, it’s kind of a loop of think, action, observation. And in each iteration of these, we’re calling the model with a longer and longer context. That’s why I want to link it back to the previous chart. So, again, agents create long context for LLMs, and it creates, like, multi-turn interactions with the model where each turn is building on the turn that came before. So, it’s, like, we’re concatenating information to the prompt each time we call a model.

The third trend I’d like to talk about is what people call test-time scaling. This is a huge area of research at the moment. I would say it kind of started as, you know, a scientific field of people trying to guess what OpenAI’s o1 is doing—which is maybe not the best way to do science, but whatever. I think people, before, like, the DeepSeek stuff came out, people thought there were three main ways that, you know, the o1 models might be behaving. One of them—I mean, all of them involve having some kind of verifier for the solution of the model. So, you can think of domains like solving mathematical problems where there is a correct answer. So, we have a mechanism for validating—did the model produce the right thing or not? And then, like, the way that people think that o1 could be working—this is, like, people guessing—one way was what we call rejection sampling, which is very simple. We just get the model, we give it the problem, and we ask it to generate a hundred—a thousand—different solutions to the problem. And we use the verifier to see, okay, are any of these correct? And it turns out this is very dumb, but it’s quite efficient—like, it allows us to create a great diversity of responses. And as we create more and more, the chance that one of them is correct increases pretty fast. The other way that people thought it might be working is what we call Monte Carlo tree search. It’s actually quite similar to generating a thousand—10,000—different responses. You try to do this kind of incrementally. So, you generate partial responses, you see which ones look good, you continue the ones that look promising, you discard the ones that don’t. That’s a bit more complex. And the third way—which turns out, it’s what DeepSeek was doing—is that we don’t actually generate multiple calls to the model at test time. Rather, we do it implicitly. So, we ask the model to think slowly—so we say, “Think step-by-step.” So, we give it more, like, computation at inference time. At training time, we used reinforcement learning to, uh, like, tell the model the best way to think slowly. Uh, now, like, yeah, DeepSeek published in a paper the way that they do this reinforcement learning using something called GRPO. I don’t want to go into the details of that—all I want to say is that it turns out that this algorithm actually involves, you know, sampling many times from the model for the same prompt. So, it’s, like, you take the same action, which is generating many trajectories through the model, and instead of doing it at inference time, you do it at training time. So, yeah, there’s, like, a common pattern across these, like, all of these test-time scaling methods, which is we have to generate many trajectories for the same prompt. And as a consequence of this—which is really kind of, um, I think resonating throughout the industry—is that, um, these trends mean that inference actually becomes a bottleneck at training time. So, the—the kind of cost of training is now dominated by inference when we’re training these kind of reasoning models. 

Um, so yeah, putting these trends together, right, the message I want to give is that prompts are getting longer. We see, like, quadratically increasing time-to-first-token, linearly increasing KV cache size. However, these prompts are increasingly reused. So, when we have agents, we have these, like, multi-turn interactions where each interaction contains part of the prompt from the previous interaction. And in test-time scaling approaches, we see the same prompt being used to generate multiple trajectories. Um, this is where a feature like automated prefix caching can really help. 

Um, so it’s quite—you know, the implementation details are quite complex, but the idea is very simple. So, um, rather than discard the KV cache after we’re done with a request or a sequence in vLLM, we keep all of the KV cache around—to the extent possible. And so, when a new prompt comes in, we say, “Okay, look—has, like, any part of the prefix of this prompt been seen before?” If it has, let’s just directly reuse it and forget about recomputing it from scratch. And in this example, right, if we start with a Prompt 1, which is “A”—we haven’t seen anything before, so we have to compute that. We have to, you know, compute attention on the whole sequence. But Prompt 2 then comes along, which has “A, B.” Because “A” is a prefix of this prompt, we get a cache hit, and we can reuse that KV cache directly without recomputing it. Then Prompt 3 comes along—again, now “A” and “B” are a common prefix. We get a hit on both of them—we can reuse that. Just to show you where this breaks is if a fourth prompt comes along where, like, the first part of the prompt is “D”—a sequence we haven’t seen before—this is going to break it. So, we can’t reuse “B” and “C” in this instance. We can only reuse it if, um, there’s, like, a string of prefixes that are shared. So, this is some internal benchmarking data that we’ve been doing. We see, you know—this is a relatively artificial workload—I just want to show that as the prompt length increases, right, as the context length gets longer, we see bigger and bigger benefits from this kind of technique. Because, of course, there’s more—like, we can save quadratic amounts of computation. So, the good news is this feature has been in vLLM for a little while, but it’s now very soon to be enabled by default in vLLM v1. Uh, this is really—I think—speaks to the importance of this feature for handling this kind of next generation of inference workloads.

This is my last slide before I conclude. I just want to mention that these workloads—like agents and test-time scaling—really, um, demand that we treat the KV cache as a first-class citizen in these systems. Like, it’s not an afterthought anymore. And the prefix caching is one step in that direction—the idea that you keep the KV cache around, you know, you persist it after each request so it can be shared across different requests. But we see a bunch of, like, emerging projects in open source that take this idea further. So, there’s a bunch of—three different projects, actually—which were open-sourced in the last few weeks, which all use vLLM. Um, and they all propose different ways to externalize that KV cache—so you can actually, like, offload the KV cache to some shared, um, like, persistent storage or database. You can share it across multiple vLLM instances. I don’t have time to go into details here—I just want to say that, you know, we see increasingly vLLM being used as a core component in these kind of complex distributed systems that—yes—think about KV cache sharing. They also think about many other things, like routing and autoscaling and things like that.

So, yeah, I conclude just with the kind of goal of the vLLM project, which is to build the fastest and easiest-to-use open-source LLM inference and serving engine. But it’s easy to try—you can “pip install” it and try the examples that I showed in the beginning. And yeah, if you’re interested in joining the development community, I provided some resources here. We have an active Slack workspace for developers, there’s documentation, there’s a bi-weekly office hours that Neural Magic runs—which is actually a really informative way to find out what’s happening in the community—and yeah, the blog and, of course, the GitHub too. And with that, I finish and [am] happy to take any questions you may have.
