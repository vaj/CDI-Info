
Hello, everyone, and welcome to Ceph Storage in a World of AI/ML Workloads.

This is Michael Hoard, your host and facilitator for today's discussion. I serve as chair for the SNIA Cloud Storage Technologies CST community. Today's conversation is with Kyle Bader, who is principal portfolio architect at IBM Storage. Hello, Kyle. How are you?

Hello, everybody.

Great! Next, we have Phil Williams, who is a product manager at Canonical. How are you, Phil?

Hi, Michael. Hi, everyone. I'm great. Thanks.

Awesome.

So before we get started, I want to mention that SNIA is a group of about 200 industry-leading organizations, comprising of 2,000 people—2,000 active contributing members—and 50,000 participating IT end users and storage experts worldwide. SNIA develops global standards and delivers education on all technologies related to data.

As one of the organizations within SNIA, the CST is dedicated to education on topics like Ceph, the Linux of storage today. AIOps, reactive to proactive. In September, at 4,000, the CST created and hosted the world's first Cloud object storage plugfest. We successfully established two outcomes. First, we created the plugin community around three days of cloud storage testing, and a birds of a feather session to collect industry feedback on the next steps. Based on demand among this new community, we kick-started the SNIA Cloud Object Storage Test Tools Group, which will work to create best practice test suites to help automate compatibility testing around popular cloud API protocols like AWS S3. Please go to this plugfest link to join this important effort.

Before we get started, let's take a look at the SNIA legal notice. This provides SNIA's copyright notice regarding use of the material. There are no warranties expressed or implied, so if you want to reference this material, please do so at your own risk. We can, you can download a copy of this presentation using the interface for this live webinar. The interface also allows you to submit questions during the talk and rate this presentation at the conclusion. We really appreciate your questions and feedback.

So, for today's discussion, Kyle and Phil will address AI workloads regarding training performance, checkpointing, and inference, as well as how Ceph storage meets the need of AI deployments and how to increase storage efficiency through acceleration and network scaling. Then, we will have an informal Q&A session, and we hope you in the audience will submit questions for our experts. If we're not able to cover all the questions during the session, we will answer your questions in a Q&A blog which will be posted after this webinar. So, let's get started, Phil, please take it away.

Thank you again, Michael, and again, hello everybody. So I think the important thing to do here is kind of just dive into this world of AI and what it actually means in terms of data. So I think I'm atypical AI system, and what's so kind of interesting to point out now, and you probably see this, reveal this because we go through the slides today, is that not all the different data sets use the same kind of access methods. So it's going to become kind of important as we get a little bit further into kind of the why stuff and why Ceph's involved in all of this stuff. So let's start with that workload, that the workload, our lifecycle. So where do we start? We start with inputs. We call that raw data. I guess that's kind of self-explanatory, but you can think of that as really as just like unstructured data, unstructured content, so video, audio, text, email, chat, telemetry, you know, like every IoT device on the planet uploading some kind of information to some kind of quote-unquote cloud somewhere. You know, this ends up in kind of like a data lake or, you know, just an object storage repository. So just imagine all of that, like, unstructured information that could potentially go into an AI workload. So we take all that raw data, what do we do with it? We kind of sort it, we have to organize it, we have to catalog it. So we take the raw data, we catalog it, and then we start using that kind of stuff for things like pattern matching. So imagine you're training a model, and that model is to recognize license plates on cars. So essentially, that catalog raw data is going to be like good examples of what a license plate or number plate looks like on a vehicle. You know, nice, clean, clear images that we've been able to sort of manually sort. We then take that nice organized data, and that's when we can start feeding it into a model, start training a model from that data. During that training process, it's very, very storage intensive. You're going to be reading a lot of this raw data and this training data to be able to, you know, start building that model out. But then also on the back end of that, once you have your model, you need to store that somewhere for future use as well. So that's going to typically be something like a shared file system somewhere. So that can then be reused in future during things like inference and in that kind of results phase. So there's a whole bunch of steps to get to this point where you have a model, you have nice data. You can then take an incoming stream of data. So in that results, like, portion of this, it feels like it's taken a lot of time to get there. You want to actually then be able to use that information for business decisions. Right. So say your license plate example, you want to do that real time for, you know, like a toll road, something like that. Or credit card transactions. You want to be able to detect fraud, those kinds of things. What's interesting there is that you might not even actually store the model in a centralized storage system. That might happen. It might actually be in something that's a bit more distributed. So you have low latency. So in this entire lifecycle, there's lots of different types of storage and use or different ways of accessing storage and use. So one of the things we want to highlight is kind of the flexibility of Ceph and being able to answer a whole bunch of those things. And then I think just finally, there is one area just to sort of think about is also this like ongoing iterative process. So we might train a model once, but actually in the future, we may adjust it. We may tweak it. We want to go back and retest it. So that training data, once we've kind of organized it, we want to like freeze that in time. We want to be able to use those same data sets to prove whether a model is like good, better, got worse after we've done any kind of adjustments to that. So this is where things like snapshots, object versioning kind of enters the picture.

So then what makes AI storage needs special? So let's take a look at a typical GPU. From a compute perspective, a single card peaks around four petaFLOPs. It doesn't really mean much to us storage folk. So let's kind of think about that from a bandwidth point of view. So to keep that thing highly utilized and running effectively, we need to feed it around five gigabytes per second. So that's a single card. This is kind of another like jump in computer architecture where we've introduced new components. It's kind of like a leap beyond the other components in the system. Kind of like what happened with, you know, moving from spinning media into NVMe where, you know, you are no longer waiting for the disk to provide you with data. You're now waiting for the CPU to kind of catch up with this super speedy storage medium. So let's start scaling this up. So I talked about that one GPU. So now if you think about a system that could have up to eight GPUs. So the typical recommendation there is that that's one GPU. So the typical recommendation there is that that single system now needs around 20 gigabytes a second just to keep all of the GPUs in that system highly utilized. It's not shown here, but there's another leap. So take, for example, NVIDIA. They have a super pod concept. That's actually up to 256 GPUs. It's like the base like entry model that scales further than that. So imagine 256 GPUs with this kind of demand. You're looking at something like a terabyte per second of read bandwidth. So that's a huge leap. So I think back to when I started in the storage industry and that was crazy talk. Now it doesn't feel all that. Well, it still feels kind of crazy, but it's, you know, it's the it's this kind of leap that has happened. The other thing to think about as well in these kind of nodes, there is a bunch of local NVMe typically. And the reason for doing that is a lot of training does involve a lot of rereads. So if you're able to have like a localized cache, that obviously alleviates some of the pressure on the shared storage system as well. So now that we have a kind of an initial quick understanding of some of the challenges, what actually happens when we want to create a model? So at this point, I'll hand over to Kyle. He's going to start talking us through Granite 13B, checkpointing and a bunch of other topics.

Thanks, Phil. So I wanted to talk a little bit about like the size of training corpuses. I wanted to start with LLMs because they're kind of the topic du jour. And so, as an example, here I have what I mean, they call the data pile that represents the training data that was used for the Granite 13B model. It gives you kind of a good gauge of size. So the actual raw, like the raw corpus before they've done anything to it, is just under 30 terabytes of data. They do some additional processing where they remove deduplications. They remove kind of like HTML markup from Common Crawl and profanity and all this other stuff. And then they get down to this relatively small, like around eight terabytes worth of data. And then they—that all gets tokenized. And that's what actually feeds the, feeds the GPUs during training. If you look at like the papers around GPT-3, the actual raw data bit, you know, they went through a similar process where the actual raw data was around 45 terabytes. And then that really got filtered down to about 570 gigabytes that was used for the training. And you can see that there is this correlation with the number of tokens and the actual size. There's a couple of links here for people that are kind of interested in learning more about those processes and the anatomy of these different sorts of data sets.

Checkpointing is a very key consideration in machine learning models. And it's kind of a really interesting area of active development. When you are checkpointing a model, you're basically saving a combination of the weights for the model and the optimizer states. So to estimate the size of a checkpoint really depends on the, you know, the precision of the weights and what optimizer is being done and the parameter count. So, like, for example, these kind of estimated checkpoint sizes are based on the use of the Adam optimizer, which uses about 12 bytes per parameter. And then we get to these different sizes based on the different scales of models. Right. So, you know, the we have the 13 billion Granite model here and then the 70 billion Llama, 175 billion GPT-3, Llama, you know, Llama three, the flagship, you know, 405 billion. And then those are all. So those those those four, those are all examples of large language models. And you can see, you know, the parameter sizes and the estimated checkpoint sizes. But I put in another one in there, which is the there's a Facebook paper in here and they talk about DLRM. Which is, in fact, a recommendation system model. And you can see that the number of parameters in that is significantly larger. And so it's going to have a significantly larger checkpoint. And to give people an idea of like what this means in terms of throughput is if you took like thirty-five gigabytes a second worth of throughput, how long would it take to do do these checkpoints? Right. And it's not perfect, too, because it depends on the data. The amount of data, level parallelization and the pipelining that they're using for training in terms of like how many streams are writing. So in some cases, maybe the storage system could do more IO if it had if there was more parallelization. But just kind of like to kind of just give a rough comparison, like when you're looking at these and you go, OK, you know, five seconds, 28 seconds, 70 seconds. Those are all, you know, kind of fairly reasonable as you get into the other ones. Those are starting to get bigger to be. Kind of bigger numbers. There was a really interesting article or paper done about like how long a checkpoint should take relative to the amount of compute. And I think they came up with a number of around 10 percent. In the case of the GPUs and the GPU clusters, you know, they represent a significant capital expenditure. And so the amount of time that you can minimize where you're not doing work because you're checkpointing is a lot. Of paramount importance. And so there's some interesting work where people are doing things like this Check and Run paper that was here where one of the things that they're doing is they're doing kind of differential checkpoints. There's also some recent things that have been done. So this reducing checkpointing times link on here discusses how there is a new asynchronous checkpoint mechanism in PyTorch. And what that effectively does is, you know, you have your forward propagation. Then you have your backward propagation. And then you have the optimization. And then at the point of the optimization, immediately after that, it destages the contents of GPU memory into CPU memory. And then they can start on the next. Basically, they can start on the next phase of the training or the next epoch of the training while asynchronously the checkpoint is dispatched down to storage. And that allows that checkpoint to continue to occur while the next kind of training epoch is progressing. And so as long as that checkpoint finishes before the next epoch gets to the point where it needs to do its checkpoint, then you can kind of take this time that it takes to do the checkpoint and hide it so that it's not kind of in the critical path anymore. And so, like, there's a lot of work, a lot of interesting stuff that's happening in terms of checkpointing. And it's something to look out for. But, yeah, there are some techniques now that help you kind of get storage out of the critical path. And you can perhaps get away with a little bit less crazy of a storage system.

Kyle, we have a question from the audience. I think you might have answered it already, but... The question is, what is the realistic checkpoint frequency? And if you could just touch on that.

Yeah, I think it depends, right? So it depends on the organization. I'm trying to think what it was. They wanted it to, yeah, so that they wanted to be about 10 points at the checkpoint time. There are different organizations that will, you know, do it every, you know, 10 rounds or every 100 rounds. Or in some cases, people do it, like, twice a day or something. I think the goal, though, is, like, if we can progress the state of the art when it comes to checkpointing and the storage systems that are absorbing the checkpoint to where you can do a checkpoint after every epoch, that is, of course, the, like, the goal goal. So, you know, you can get to the point where through methods like the asynchronous checkpointing, differential checkpoints, and having a sufficiently sized storage system to where you could checkpoint after every epoch. That's the gold standard, right? The reason why you want to checkpoint as frequently as possible without, like, compromising that, you know, how much time is spent for compute versus checkpointing is because especially as these GPU environments scale to, you know, thousands of GPUs, you know, the chance that you'll have a failure during of a GPU or some other component in the training cluster goes up. And so, like, as the training environment scales, you need to increase the checkpointing frequency.

OK, great. And then one other kind of high-level question. If you could briefly just address how cost-effective are GPUs in terms of performance and getting this job done?

How cost-effective are GPUs, um?

Yes, and you know, in comparison to other things, uh-huh. Go ahead. Like, if you were to use, uh, CPUs or things like that.

Oh, I mean, I think for any of the like, really, really intensive training, it’s going to be on GPUs It’s just more electrically efficient, right? Like the amount of things, the amount of um matrix multiplications that you can do in parallel is just, you just can’t, you know, you can’t do that.

Right, and it's also a timeliness—um, how critical is that workflow for production, and um, just the feasibility of uh, getting the work done um, on on a reasonable schedule? GPUs uh, would certainly be the top pick, so.

especially for training, yeah, there there are certainly uh, like, inference activities that could that you can that you can do with that you can do comfortably on CPU, especially if you use some of the the model quantization type things that change the precision of the models so that they’re in int8 or something, and then they can be more effectively, you know, you can more effectively run those sorts of inference things off of CPU, but you have to make sure that the you know, by by doing that quantization that the that the the the quality of the inference still meets the the uh requirements for the use case, right? So it might be you know, in some cases that it's fine, other cases you know, you might need the higher precision, and in which case you might uh, that that might push you towards using uh more accelerated hardware.

Great, thank you.

Um, so on this next slide, I wanted to um, kind of you know, LLMs, all or all the RAG, but I also wanted to make mention of recommendation systems because recommendation systems are perhaps some of the the the largest in terms of parameter count. We saw that on the previous slide with the you know, the one trillion parameter, and uh, most demanding in terms of checkpointing, but they also have far more data that they're trained off of. So when you look at LLMs and you look at that, you know, the Granite model training set or the the training data set for um, GPT-3, you know, they're they're measured in gigabytes or you know, a few terabytes. Whereas a recommendation system is often trained off of petabytes of data that is housed in uh, kind of like a data lake. And so in this example, you have an application that's generating events and features, right? So an event might be like the actions of a user on the webpage, so when they you know, add a product to the cart or something, and then features are you know, things about the the uh uh about the you know, user itself, and then these are kind of flowing in through some sort of streaming engine and um, and then there's kind of like online, right? There's like online inference that's going against you know, so there's there's online inferences going against these and the events and features and saying like, "Oh, when they click on this, then we're going to show them this other thing," that that that kind of thing going on. Um, but then these the these streams of events and features, they they kind of funnel into like a lake house, right? So you can think of it as kind of funneling and getting pulled off of like a Kafka topic and then putting into like iceberg tables within an object store, and then inside there, you know, there's there's there's kind of a separate decoupled pro uh process where people are going to be doing feature engineering. So they're going to be looking at these event tables, they're going to be looking at these feature tables, they're going to be looking at other tables that exist that kind of uh are perhaps like you know, snowflake schema things like, kind of think about like TPCDS and look at the actual orders that they've placed and you know, whether they were you know, in the store or re you know, online and you know, different things about their demographics and stuff, and then they create these features, and then those features actually get stored additionally into you know, as distinct feature tables within uh within that kind of data lake house, and then so so that's like one thing there's there's the feature engineering, but then there is when you go to do the training, there's something that's going to then extract and generate the different splits for the actual training workloads, and so often you'll be taking you know, taking and combining some of these different features and then making them available, and uh oftentimes you'll you know these splits will then be serialized into something like TF record or record lio, which are going to be uh far more efficient for the training engines to be able to ingest and read in, and then there's a there's like a whole uh whole science to like the sizes of the splits, but generally speaking, these are going to be relatively large like you know, TF record they tell you to be you know, um in the you know, megabytes to a couple of gigabytes or the range, which is which is great for realizing streaming bandwidth, um, but the actual number of splits will depend on like you know, whether you're using um like whether you're using tensor parallelism, data parallelism, pipeline parallelism, if you're doing some of the techniques like they talk about like in the g pipe character where they do like smaller batch sizes so that they can kind of shrink the bubble where GPUs are are not busy, but yeah, you can see that like there's this there's this whole process that they're doing, and they're doing a lot of research around recommendation systems and it's a lot of data, and when you you know, there's a lot of data coming in, and then when you start to do the feature engineering, they try and there's there's there's a tendency to try to preserve these these features because they'll be useful uh you know for training for training and developing different ways of of of doing inference, and in in this paper right here, the the link one here um is a is a paper by Facebook and they said that you know of and this is a couple years ago so maybe this is slightly changing, but these recommendation system models consume more than 80 percent of their machine learning inference and over half their training, so um these are these are very big workloads. LLMs are kind of the the the RAG these days, but um I mean recommendation systems right whether it's recommending products um recommending kind of new you know articles these these are these are all over industry and um and are a very demanding and interesting uh storage workload, and people have using have been using Ceph for data lakes and tabular data for a very long time, so it's a really really good use case for Ceph.

Yeah, we had another question, uh, why train with Ceph storage, or why train with Ceph storage instead of direct-attached NVMe storage that would speed up training by orders of magnitude? I think you mentioned that some of the local-attached in the nodes are for re-reads, and later in the presentation, I think you're going to talk about extremely large datasets in the tens of petabytes and higher. Um, any comments on local storage versus Ceph? Go ahead.

Yeah, I mean I think I think if we're looking at, you know, if I go back here right, and we're looking at these sorts of data set sizes and you're doing a significant amount of data data parallelization, like you can you know, you can you you can copy these data sets and do data parallelism across all, you know, pretty extensively, such that you know, you don't have to necessarily do the the raw training off of, off of the remote storage, right? and that will of course be faster, just because that's how physics works when you're dealing with these, you know, these these training of, like a recommendation system is going to be um, the amount of data that you would potentially need, depending on the size of your GPU system and how much data level parallelism you are um, you might not mean, you might not be able to fit at all on the um, on the local NVMe storage of the of the system, and so in that case, you want the you you want the remote storage, or if you want to have more flexibility because you're experimenting, you're you're experimenting with how how much data parallelism you're doing or how you're doing your pipelining, or something to to try to more maximally uh uh make use of the um, the the GPUs, and you know, kind of having to to to to readjust how the data, you know, how you're partitioning the data, and then pre-loading it onto all the local NVMe's maybe that you know, maybe that's uh maybe that's you know, troublesome right, so.

Thank you.

It's always a trade-off, right? That's engineering, right? Um


I also wanted to highlight, um, uh, kind of a, you know, we've done demos, and there's actually, um, like, GitHub links if somebody wants to set up this sort of thing, but, um, event-driven, event-driven inference, and this in these examples we did with, you know, X-ray imagery data where you, you know, upload X-ray imagery to the Ceph object store, it generates a notification, that notification can then be kind of, you know, then ends up in basically a topic and a Kafka topic that can then trigger kind of serverless processing. In this case, the serverless processing is actually applying inference to the image. Um, so in our demo, it takes the X-ray image and then it generates a pneumonia risk score, and then that pneumonia risk score is then stored as like a tag on the object, and then you could have additional notifications that you know, look that have conditions on these tags or something that that then, uh, you know, send off a notification to another topic that you know, alerts doctors or builds a list of imagery that you know, a clinician needs to look at or something like that, right? And it doesn't have to be limited of course to X-ray images or even medical type things, right? This could be anything, it could be like a p-cap and you've developed some sort of you know, way of doing inference against it that finds naughty packets or something, I don't know, um or or, you know, it you could do any number of things with this but it's just an example of when you're when you're injecting or when you're ingesting objects into the storage system, you can you can trigger kind of events and uh serverless processing of it on, uh, you know, so as these things are coming in you can then perform inference against the the incoming stuff and then make decisions and kind of chain off and do other workloads, and that's kind of an interesting use case that we've uh, that we've demonstrated and then, um, we've seen people put into practice as well.

Um, so, yeah, why why is storage important? Well, well, yeah, there's these datasets, there's all this data processing, um, both on the on the kind of analytical side, and then kind of creating the feature engineering, and then you know, actually creating splits and doing the training, and then doing inference against the data, you know, the data that is in the storage system as well. So you know, storage is a pretty critical part of a lot of different machine learning workflows, and these, the you know, we put these up here just to give kind of a taste of a few right, but there are a lot more that are involved, uh, retrieval augmented generation, and LLMs, and um, and it's a very fast and changing kind of environment, so it's it's rather exciting the intersection of you know, storage and uh, AI ML.

Great, so a question on: what are the changes we're seeing on upgrading storage hardware with AI? And is it only the GPUs or are there other specific hardware? And I know later in the uh presentation you'll address uh compression hardware, compression, and also network scaling, so any high-level thoughts on that?

Sure, um, so I always kind of look up, look keep my keep an eye out for what peers are doing in industry, and um, one of the things that I was looking into more recently to kind of you know, inspire thoughts about you know, Ceph hardware designs for um, for you know, satisfying these types of use cases was um, like Meta. So Meta, some— what you know, what Meta is doing from a storage perspective perspective for its model training. Um, if you if you read their you know, their blog posts and their papers, is one of the things that they're um, doing is they're using their Yosemite systems which are kind of like uh, it's like a open compute style system. Uh, multi uh, multi-node chassis, and then each of the things that they're using is like a single socket, it's kind of like a single socket machine, and it has six NVMes, and that's six NVMes with the the PCI generation that they're using in them is kind of like a perfect match to 200 gig of network throughput, and so you know the idea is basically to create this you know, to align line it so that the you know, you have a simple simple system that just has a single socket and then you have a number of NVMes that can saturate the anything, and then you use you know, they're using a scale out file system like Tectum, there are talks on on this and that that very is very similar resemblance to Ceph, and then you know, they've they've developed a fused kernel client in order to make it easy for the their um, AI ML kind of practitioners to you know, just use kind of standard um you know, POSIX interfaces in order to to to do the actual training and that sort of thing, so um, I don't know maybe that maybe maybe that helps answer the question a little bit, but, but yeah, I think I think the idea is just like you know, when you're when you're when you're picking the hardware right, you you want to you want to maximize the bandwidth, and so one way of doing that is just making sure that that the you know, the geometry in terms of like the the processing the amount the amount of processor the amount of SSDs and the amount of networking your provide is kind of completely streamlined to to to provide the maximum maximum bandwidth so that we can hit these uh or satisfy these very data intensive flows.

Thank you.

All right, yeah, go ahead, Bill, take away.

Yeah, thanks. All right, so I was just gonna say, let's um let's think about the sort of underlying fundamentals of a storage system and its design, right? So I'm sure these kind of four pillars are quite familiar to a lot of the audience, but it's just like, let's just go over them again and just kind of reiterate what they actually mean. So, first of all, you know, when you're building a storage system, it's all about balance, like you've got to balance the performance of it, how much data it can hold, how reliable the system is, and of course, the budget.  Um, so when we think about performance, it really is a case of what does the application need? Like, you can build the fastest storage system in the world, but if you're never actually going to access that data or you access it very infrequently, it just doesn't make any sense. So you kind of have to align the business value to actually the value of the data. Um, as we think about capacity, the only thing that we ever see now in the storage industry is, is never anyone giving back storage space; it's always "can I have more storage space?" The data sets are growing; everything's getting larger and larger and larger.  Um, so one of the key things when you're building a storage system is that long-term view, like what a storage system looks like today is not what it's going to look like in three, five, seven years time. Um, to have it so working with something that can handle that kind of scale, and then also in the rare scenario where people are willing to give capacity back or they actually delete data, also scale back as well. That's something that Ceph's really good at.  Um, reliability, like this is super interesting when you look at these very large systems, you can actually start tolerating quite a lot of loss. Um, you take, take like an erasure-coded back end, you can you can design it so that you can lose large sections of it as Kyle mentioned before, talking about Meta; they use a lot of erasure coding and actually power down sections of those storage systems to save energy. Um, so there are lots of of ways of kind of manipulating availability uh to reduce those long-term costs of storing petabytes and exabytes of data.  Um, and then finally, yeah, that cost side of things, it really is the sum of the three parts. Um, it is just finding that real balance between having the right performance, having enough capacity available, and making sure that the data is actually available when you want it. You know, it might not need to be available 24 hours a day; sometimes some data sets are just accessed like that. You know, you have to have to work out which one fits best.

So then why Ceph, and I guess this is kind of the point of interest today, as is why why would you use Ceph? Uh firstly for any kind of storage, but also for these kind of AI workloads? I kind of mentioned this before, but there's lots of different ways of accessing data, and that's one of the kind of wonderful things with Ceph. Um, it's it's it's multi-protocol capabilities that have been around for for years and years and continued to to evolve. Uh, so let's let's think about block storage. Uh, so historically, Ceph supported its own RBD protocol. Um, it's embedded or integrated into the Linux kernel, some of the virtualization solutions, uh like QEMU KVM, but also one thing that was kind of realized in in the wider Ceph community, um, was there also the block protocols, right? Uh, so historically, there was a there was a way of using iSCSI, um, that has now been deprecated because of course, the world's kind of moved on. Uh, so now there's an NVMe-oF fabrics gateway, uh, which can provide you with an NVMe endpoint, uh, just like any kind of proprietary storage system would. Um, for the file side of things, uh, Ceph has its own, uh, network file system called CephFS, super scalable, um, but then also again, there's there's kind of industry standards, uh, things like NFS, SMB, you know, that they've been around for years, um, they're kind of known, and you know, known quantities across the industry. Um, and also with those things, with NFS and SMB, with Ceph, we can get it, we can integrate with uh enterprise auth as well, so you can you know, have very fine, fine-grain control over over who has access to what in those shares. Um, and the other area in Ceph, and honestly, that this is where we see the largest like growth with with Ceph in the industry, uh, is a really round object. Um, Ceph supports the AWS S3 protocol. Um, it also includes some of the advanced um protocol features as well around, um, identity and access management, and the STS API, so that's the security token service, um, and that gives you very uh, tight-grained, and kind of time-limited access to individual buckets within within the object store. Um, you can also, um, provide an NFS share from a from an object storage bucket as well. Um, this is you know another way of using a traditional approach to access something that's more modern like object storage. You know, not all applications can speak object, uh, so this is like a another kind of entry point into this, uh, this new sort of scalable world, and I think kind of um, the way that I always like describe Ceph to anybody I I talk to is and I and if it's like, well the one thing you remember today, it really is the Swiss army knife of storage. It has an answer for all the things that you need to do, um, just from that one single tool or one single cluster.

A few moments ago, we did talk about some of the networking stuff, um, and I'll kind of dive into that, but if we think about, um, when you're building out a Ceph cluster, there's, there's from a hardware perspective, there's kind of three main areas that we tend to think about. So the systems themselves, they need to compute; they need some some working scratch space or memory. Uh, so those two two components are super important. Um, obviously, it's a software-defined storage system, so naturally, like most pieces of software, higher clock speeds do help. Uh, there are there are some parts of Ceph that are still kind of single-threaded, so higher clock speeds certainly help for some areas. Um, the more RAM in the system is is is great. You know, during kind of rebuild operations when something is broken or failed, and you're kind of rebuilding around it, you do see sort of increases in demand for additional memory, but then also, you can configure Ceph to have a a read cache as well, so depending on how you're accessing the system, that is uh, but you can use the um, the RAM in those systems as a read cache as well, just to kind of accelerate uh any any kind of re-reads that have not managed to be locally cached. Uh, in some of those GPU nodes, the network side of things, uh, Cal kind of mentioned this about not you know, having this really balanced environment, you know, not having a machine that's got tons and tons of local performance but like a 10 gig pipe, uh, that's not the kind of thing you want to do. It's it's gotten to the point now where you know, 100 gigs kind of everywhere, multiples of 100 gig, uh, kind of kind of everywhere. Um, so you know, finding that balance between the actual throughput within the system and what the network's capable of, and then kind of sizing those machines based on you know, network throughput, that's definitely the best way to go, and kind of the other thing to think about as well with Ceph, we have an internal network, we have an external network, so the internal network that's where we do all the replication, the protection of data, those kinds of things, um, and then the external network is obviously for for client access, um, so those two are typically very very segregated networks, we don't want those, there's two sets of traffic to kind of interfere with each other, and then finally, um, you know, Ceph is this this hardware-agnostic system, um, we can we can build systems that kind of capacity focus with, um, spinning disk, we can build like mid-range storage systems with, um, SAS‑attached SSDs, and now as we get into this kind of you know continued evolution of NVMe where we have large or high capacity NVMe devices as well as the super fast NVMe devices, we can use all of that different type of media uh within within a Ceph class depending on the kind of kind of needs of a particular pool or data set.

And then the one thing that I think I've personally mentioned a bunch of times already is this idea of scaling. Uh, so to kind of illustrate what happens with Ceph and how it scales, let's take this this sort of theoretical four-node uh cluster. So let's imagine, well I say imagine this is actually from from test data, so this is a cluster we could read 20 gigabytes per second from. Um, and now we've talked about this whole idea of as a set cluster, um the the performance continues to increase in kind of a linear fashion.

So let's double the size of the cluster. We've now got eight nodes instead of four. We're able to double, uh, the read performance from that cluster.

And we kind of continue with these steps, you know, we double again, we get to 80 a second, we double again, we're up to 160 a second. The one thing to remember, and we both of us have mentioned it now, about networking, this is really building a highly performing cluster is now like a networking planning challenge. You don't want to end up with a situation where the network itself actually becomes a bottleneck, and you've kind of got this, this, this fast hardware, or you know, any kind of any kind of hardware that's then bottlenecked is a really, you know, it's just poor planning and bad design, so you do really have to think, um, think about this this longer-term scaling, if you know, you know your data set is going to grow over time, and there's also a fantastic uh example, like here, I'm talking about scaling from tens into hundreds of gig a second, uh from one of our friendly organizations in the community, uh, Klyso, they have a great story on the ceph blog where they managed to scale a ceph cluster up to one terabyte per second, um, it's it's a fantastic read, took a lot of hard work from the team, um, so yeah, if you get the time, I want to find out more, definitely take a look at that.

And then in terms of accessibility to Ceph, um you know, Ceph has been around for an awfully long time now. Um, the last time I checked, the OpenStack user survey that the statistic was that something like 70% of production OpenStack environments used some form of Ceph for storage within, um, and that's not just for, you know, AI workload workloads; that's across all sorts of enterprise IT infrastructure. Um, within my organization, internally, we use Ceph as the backend storage within our internal cloud. Sum it's used across so many different organizations, thousands and thousands of different organizations across the world. Um, so if, if kind of some of the stuff you've heard so far is kind of interesting and you want to find out more, Ceph is of course fully open source. Um, Ceph.io is kind of the home page; it's where you can find everything about Ceph. Code is on GitHub; you can go and inspect it, download it, build it, you know, whatever you want to do with that, and then of course, the upstream docs are absolutely fantastic, in one explaining it and the other explaining it and all the different features and functions but also kind of how to get started as well, and there's loads of different ways of doing that. Um, from all of the downstream vendors, there's lots of different approaches as to how you might want to deploy a Ceph cluster, so definitely uh, kind of kind of do some research about what we all do and what we all provide. Of course, nothing comes uh, nothing comes without its challenges, and especially as you want to like you know, if you if you take you experiment with Ceph and then you want to to scale it over time, you know, you might run into kind of challenges along the way. There's a very active uh community mailing list; I saw one question earlier from from someone who's very active in in the community, so hello to you. Um, that's definitely one area where you can get help. Um, there's also uh, you know, this ecosystem of vendors and practitioners who can can help a provide um downstream code but then on downstream binaries, but also then also provide sort of the consulting services around the kind of understanding the best way of deploying the best way of scaling, um, and I just kind of want to reiterate, you know, this this idea that yes, AI has high bandwidth needs or high you know, high throughput needs of a storage system. I think the real like thing to think about is actually it's it's more about planning and design than than just um needing like a specialized storage system.

And then one last thing that I, I wanted to talk about, um, is really around this idea of storage efficiency. Um, so over the last couple of years, upstream Ceph has introduced, um, well, maybe not even the last couple of years, longer term than that, there's been features within Ceph to be able to compress data that's stored within the back end of a storage system, so whether that's data that's passed through, uh, the routers gateway, so that's the S3 endpoint, um, or is, is just stored, uh, in a BlueStore pool. So BlueStore, that is kind of the back end of a Ceph cluster, there are a couple of different areas where we can apply compression in that system. The two, the last couple of years portion comes into, uh, this kind of pluggable framework where, if you have a hardware accelerator, you can, um, you can then start to offload some of that that compression workload, uh, to help alleviate some of the the CPU overhead that you would see. So, if you take a look at these, these charts on the right here, you can see, uh, in the leftmost column, that's no compression for both read and write workloads, and then you can see in the in the middle there, uh, that's when, um, that data set has been compressed using, uh, the zlib or zlib, uh, compression library, and you can see there's a quite a significant decrease in performance, and this is obviously just relying on that, that you know the power of the local CPU. Then the right-hand side you can see where we've been able to use, uh, some hardware offload, um, and you can basically see that there's almost no impact, by using, uh, by using the hardware offload. So in this example here, this was Intel's QAT or QuickAssist Technology, so that's a feature that's available on-die, uh, in Intel's modern CPUs, historically, there are also options to do this on PCI cards, there are other options out in the market as well, um, and as, as kind of the interest kind of again comes back to adding in these kind of additional, um, hardware offloads, they can be integrated into Ceph, so it becomes quite an easy, easy thing to turn on, kind of reap the rewards of, um, of compression, and it's quite funny, like I've talked about this subject a few times recently, and it's quite funny you think of compression, it's kind of boring, but actually when you look at the the savings you can get, so take something like textual data, CSV data, we see that being compressed usually around like 25% in terms of like a reduction of raw space used, um, take raw video, raw videos, raw video is crazy like, um, with, um, with raw video, we see up to like a 60% space reduction obviously, that's not something that's already been run through a codec, uh, you know, once it, once you've run a raw video stream through a codec, you're not going to see any, any, uh, savings from compression, um, but for like, for, for that raw data, if you want to, that's that, the data set you want to keep, um, we see these these kind of huge savings, of course, what that means is you can build them a smaller cluster so that you don't have to go back and look at them as less upfront capex, but then also a smaller cluster means there's less machines in the datacenter, it's less power draw, and so the ongoing kind of opex, um, savings are quite good as well, uh, so I think at this point, we're gonna take a look at another use case, so I think back to you, Kyle.

Great, thank you, Kyle. Go ahead.

Oh yes, so this is just kind of a, kind of an example kind of example system that that that we have put together here, and um, and yeah, it's just like some conventional uh, dual socket configuration with you know, a little bit of memory, a couple hundred gig links, and some um, uh, some TLC, NVMe drives, and kind of giving you a a slight gauge on the amount of you know bandwidth that you that you might be able to see with it, and you know, as Phil kind of outlined the scaling, kind of the scaling uh slides side of things, you know, as you you know, as you demand more aggregate throughput you can kind of increase the quantity of this. When you're thinking about you know scaling for these workloads, um, as often you often want to try to understand what the ratio is of throughput to capacity, and then that'll help you determine like you know how many SSDs you put in the system, right? So if you have more of a capacity bias, um then you know, you could use something where you have like 24, you know, 24 TLC drives, and then like a 2U chassis, and you can use kind of the denser you know denser SSDs like 15, 30 or even like 61 terabytes these days. Um, if you're trying to you know, if you're doing just LoM training or something, you you might you know, pare that down to more like the meta configuration that I described earlier where it's like a single socket, some memory you know 200 gigs, and then like you know, six, six high-speed Gen five, and it'd be me or something like that.

Um, on the network working side, right? So this is this is something that we encounter a lot with people, on on where they go, 'Ah, you know, I I had some hardware that I you know, cobbled together, and I was running some stuff, and like, you know, I don't I don't like the performance that I'm getting, and really, like you know, when you're doing a distributed system like Ceph, a scale-out system, um, it's lifeblood is basically the network right so, um, and uh, one of the most common issues that we see is where there's uh, far too much oversubscription in the bisection of the the cluster, right? So, if you have a cluster that's distributed across multiple racks and um, like say you have you know, 20 ports or something of 100 gig going down to the servers, but then you only have, you know, 200 gig up links, now you've you've created this this this 10 to 1 oversubscription, very, very, very high, and so, when you know we're we're trying to decluster and place replicas or erasure‑coded chunks, you know, across hosts or across different racks, that network bisection, that that oversubscription is going to be the the the joke point for the cluster, and so you know, if you really want to build a high-performance Ceph cluster, you want that that oversubscription to be as low as possible, you want to be one to one or um, one to one, or you know, even in the most extreme cases like, right, definitely not 10 to 1. 10 was like, if you look back at like the GFS paper, right, the reason why they were doing the locality optimization for MapReduce was because they had these, you know, they were using archaic like switches that you know, they could only do like 12 to 1 over subscription or something. We're not in that world anymore, you can totally build non-blocking fabrics that have zero oversubscription, and um, even, and then I'll also add that like, on that on the training cluster side of things, they're doing things where they actually have like negative oversubscription, so like, if they fail, they don't have any oversubscription, right? So like that's how demanding these sorts of workloads are, and so you absolutely want to have um ample network connectivity, and you don't want to be choking out your your storage system or your train clusters, um by having an inadequate uh bisection.

All right, and yeah, if you guys want to learn more about Ceph, right there's there's a bunch of resources in the community. There's ceph.io, always a bunch of uh, really cool blog posts that are, uh, that are going on there. Uh, we have some upcoming events, actually. We just had the uh, a Ceph Day event in India, um, that was uh, earlier this month, um, in March. We will have one in San Jose hosted at, uh, Almond Research Facility, and then uh, we have the one planned for later this year in London, and of course, we have a kind of a yearly, yearly get-together, uh, technical conference called Cephalocon. This most recent one was hosted by CERN in Geneva, Switzerland, so if you're interested, the slides and recordings are online, and um, and yeah, of course, a wonderful SNIA. They have an education library, uh, there was a, a previous talk, uh, uh, given uh, by some uh, colleagues and friends from the Ceph community, and that's available as well.

Awesome, um, so any last takeaways? Uh, Phil, was that uh...

Yeah, I’ll dive in. Um, I think it’s really funny like, when, when I started working in the storage industry, like, it pains me to say almost 20 years ago, um, and it’s really funny even now the storage element in any kind of, of like infrastructure always seems to be kind of like hidden away, and it’s always a bit of an afterthought. But I think it’s becoming more apparent in this like high-cost scenario where GPUs are crazy expensive, you actually have to be able to feed them data, so there’s been this like awakening about the storage system that actually provides that data into these kind of environments, so it’s kind of, it’s great to see that. So, yeah, just to get the best TCO out of that massive investment, you also have to invest in the storage as well. Um, you know, I keep saying about Ceph scaling and the way that it is possible to piecemeal grow a Ceph storage cluster, you know, adding a few nodes at a time over a prolonged period of time. Those nodes don’t even actually have to, you know, match in configuration, of course, what you buy today is not what you’re going to buy in six months, it’s not going to buy in a year. So having that heterogeneity and the support for that within Ceph is fantastic. As Carl mentioned, the networking side of things, like it’s, it’s the interconnect has become like the Achilles’ heel at this point. You know, the planning of the network is is super, super important, um, and then you’re finally, I think, that the idea here of having this kind of ability to bring in uh different optimizations into Ceph, uh, so right now, you know, we there’s this support for Intel’s QAT, but then what’s coming next? Who’s going to bring you know some other hardware accelerator that can also bring in more optimizations in the future, and being able to kind of you know, um, take those into this this open source system and make use of those different types of hardware is fantastic.

Awesome, great! Uh, so right now is the QA session. We literally have three minutes, and so if we could do one, uh, question: there was a question about uh, the significance of I.T. administrator roles in the future, kind of um, professional growth, and I think your comments about planning and education—and this network scaling, etc.—would be critical, but any last comments? We've literally got two minutes now, so go ahead.

I think so, again, thinking back in my career, when when I started, there was this whole idea of you, you were like a storage professional, there was a virtualization professional. Um, I think what was seeing now is actually you have more, more the IT generalist, and that's possibly like a good thing because the tech has advanced to a point where you don't have to be super specialized. I would say that in the open source world, is a bit more complicated. Um, it's more like being a a true sysadmin rather than like a storage admin, um, but yeah, there's, as the way that um the tech has become easier to use, it's, it's easier to consume the the the long-term, like career growth there is actually being more of a generalist than being super or hyper focused in one area.

Yeah, go ahead.

I was going to agree, and, like, you know, one of the things that, that, like, I do from a storage practitioner perspective is, like, when we when there are kind of new and exciting workloads, one of the things I'll do is I'll, like, go and look at the body of research for, like, I did this for like analytics, where I go and I grab all the different seminal papers that talk about the different things in that kind of developed and I print them all out, I organize them chronologically, and then I just read through them all of them so that I know, like, I understand, like, the progression of why people are doing things, and why we are at the current, you know, why the current state of the art is the state of the art, right? Because, like, I, you know, people always say like, 'Oh, what is the best practice?' So, it's like, well, the thing about best practices is they're always just practice, right? We're always trying to get, like, figure out, figure out, what like the next best practices right, so it's like, you know, it's kind of it's always like a fluid thing, and if you do that, I think that that, you know, where we've been, where we are, I think is important, and then like, you know, I've been doing that more recently with like AI, so you know, going through getting all the kind of seminal research, putting it all out printed, a lot of it has all this math that I don't understand, but you know what I mean, but but but there are still things that you can, you know, you understand, you connect you understand about the distributed systems, and the the better you have understanding of the the workloads that you're catering to as a storage administrator, the better you'll be able to have conversations with, with, with your end-user users and really understand their needs and wants, and then how to construct the storage systems that are going to satisfy those needs.

Okay, great! I have uh 40 seconds or less. So, thank you very much to our presenters for the expertise and insight. Uh, this has been a fantastic discussion with uh, excellent information. And we will have a QA blog that will go through all of the elements of the questions. There were many questions that were answered directly through the chat, but we'll bring all those into the QA, and also thank you to the audience. Thank you for joining us. Please remember to rate this webinar, as this is very important to get your feedback, which helps us create better educational material. Also, please remember you can always download this presentation and many other items at our educational library, as well as follow us on Twitter. Thanks again to everybody, and have a great day! Thank you.
