YouTube:https://www.youtube.com/watch?v=0C4QiT4W6gY
Text:
Hello everyone and welcome to my session on using CXL with AI applications. My name is Steve Scargall. I'm a senior product manager at MemVerge. And you've heard a lot from our hardware partners as part of this memory fabric forum talking  about the lower level hardware and how that all works. I want to bring this up a level and kind of describe a little bit more about what we're  doing from the software side. And obviously AI is very prominent these days. So I want to describe what we're doing, at least some of the work we're doing with regards  to AI. And specifically for this talk, I want to focus on the database side. So vector databases and SQL databases.

So this is the agenda I put together for my session. I'll recap the memory expansion, form factors, you know, what they are, the two different  factors and how they fit into your server. I want to talk a little bit about the latency and bandwidth memory placement strategies. So these are the policies that we've implemented in our own MemVerge memory machine that allow  us to utilize the hardware that's available to us dependent on the application requirements. And then I'll show you some of the investigations that we've done. We've taken a deep dive into traditional RDBMS databases. So MySQL and vector databases, in this example, that'll be Weavey 8, show you the results  that we're getting from our memory placement strategy policies. And then I'll talk at the end about how you can better understand your own application  behavior. And I'll show you some of the tools and approaches that we take in this area.

So here we're kind of represented a standard 2U server, two socket, either Intel or AMD,  doesn't really matter. The DDR modules plug into the DDR slots, again, either one DIMM per channel or two DIMM per  channel dependent on the number of slots available to you and your requirements there. As far as the CXL is concerned, there are two predominant form factors. There's the add-in card that looks like a PCIe slot. It has the PCIe CEM form factor. And again, dependent on the vendor and the capabilities and features of that card, you  can plug in your own DDR into here. So the flexibility of these devices is that you can define the capacity based on the number  of DIMM slots that are available to you and the capacity of the DDR modules that you put  in here. We expect that many of the vendors will support either DDR4 or DDR5. So if you're moving up to the newer servers that have DDR5, you can recycle your old DDR4  and plop them into these add-in cards. Alternatively, if you want the highest bandwidth, go and purchase the DDR5 modules with these  types of devices. Another advantage here is that because they are a CEM factor, predominantly they fit  into a x16 slot, so you get maximum bandwidth out of these devices. The other form factor is the E3.S. It kind of looks like an NVMe drive, and you intended  to put these into the front of the servers as we're showing here, very much like you  would install front-loading SSDs. The disadvantage here is that they're typically only x8, so you've lost half the bandwidth  necessarily from the add-in cards, but you can usually fit more of them into the system,  so you can regain a lot of the bandwidth if you're able to fully populate a system here. The capacities are fixed, of course, because you can't install your own media into an E3.S,  so at least the ones that have been advertised by different vendors typically are in the  128, 256, and 512 gig ranges. So again, depending on the capacity and the number of slots available to you, you're able  to expand the memory of the server with this. Now I don't know if it's in, it may be down to the OEMs, but I haven't seen anybody that  allows you to do a mix of the two, right? So you're either going to be E3.S platform, or you're going to be an add-in card-based  platform. And that's really because if you go through the E3.S route, you're moving a lot of the  PCIe lanes to the front of the server, so the PCIe slots available at the back are typically  going to be for maybe a GPU or a network card or two, right, to allow you to connect to  your external networks and storage. So we do not expect to see servers supporting both add-in cards and E3.S memory modules.

So now we've described the hardware side of it, and again I want to bring it over to the  software side. So how do these hardware devices, these CXL memory devices, get exposed? Well, there are many different ways of doing it. I'm just going to describe one, and that is that we, in the BIOS, we configure the CXL  to be what we call special purpose memory. And that means that the BIOS maps the memory in to the 820 table, and it passes it through  to the Linux kernel. And the Linux kernel understands that this is now a software-reserved memory area, so  you get this contiguous chunk of memory, depending on how many devices you have. Now in that mode, the kernel will automatically create what we call a dev DAX or a device  DAX namespace type, and you'll get this entry in /dev called daxX.Y. It typically starts at 0.0 and goes up from there, depending on, again, how many devices  you have. And that's just a regular character device, right? So if your application wants to just memory map files, you can memory map this device  into your application address space and treat it as regular memory doing those store operations,  right? So the application itself, it then becomes the memory allocator. And whilst it is the best place to be doing this type of intelligent memory placement,  it does require that the applications be modified. Now if you don't want to modify your applications, we can convert this device DAX into a regular  NUMA node that we call system RAM in kernel terms. And you just run this command that I'm showing on the screen here. So the daxctl reconfig device changes mode from dev DAX to system RAM, pointed at the  device that you want. And then that memory appears to the operating system as a new NUMA node, but it does not  list any CPU. So for example, in the example we're showing here, node 0 and node 1 are our two sockets  on the server. They have locally attached DRAMs. That's the 64 gig that you're seeing here, plus the number of CPU cores. And then node 2 is our CXL device. So it just comes up as 128 gig of available memory. And that again is a NUMA node, right? So applications that are NUMA aware could potentially use this without modification. However, you can use tools such as numactl and other things, cgroups, to ensure that  your application can use both DRAM and CXL without modification.

So we've just described how the memory gets mapped into the operating system and made  available to the applications. But then how does the application really use that, right? So I'll first talk about the latency placement policy. And the intent here is that now we have this system with heterogeneous memory, DRAM being  the lowest latency memory tier, probably fairly high capacity, maybe up to a terabyte or two. In addition to the CXL that's available to you, again, dependent on the number of devices  and the capacity of those devices, you should have fairly high capacity of that as well. But the CXL will have slightly higher latency, right? So you don't want to treat these two things as equal because they're not. But what you want to be able to do is take advantage of the fact that you now have CXL  in the system. So the intent around latency tiering is that we identify the hot pages or hot memory. Those are the pages that are touched most frequently. And if DRAM becomes full or near full, you want to be able to use the capacity of CXL. So we want to identify the cold or cooler pages, those pages that are not touched as  frequently and move those to CXL. Now, the application is still going to be able to access them in CXL. It's still a load store operation. So nothing really changes there. It's just that it has slightly higher latency. But it's still better than going to disk, right? The alternative to this is that you would have to page memory in and out of disk or  even worse, have to go to your swap device, right? So this definitely has a significant advantage for applications. So our memory placement policy does this. It does it transparently to the application. So the applications continue running. And in the background, we're monitoring the temperature of the pages and moving them to  the appropriate tier. So we can do what we call demotion, so moving data from DRAM into the next level of memory  tier, which is CXL. And promotion, which is then if that cold page becomes hot, you would ideally want to  move that back into DRAM and maybe move some data in DRAM that's now cooler back into CXL. So you're able to move this data around. Now, the kernel itself also has a similar feature. It's called transparent page placement, TPP. And the feature is upstreamed by predominantly meta. It initially got upstreamed into the 5.16 kernel with demotion only, meaning that it  could only move data from DRAM to CXL. But in the 5.18 kernel and onwards, it can also do promotion. So now, again, it's doing similar things to us where we can move data between the tiers  dependent on the frequency of how often those pages are accessed.

So here are the kind of results that we got when comparing our own MemVerge memory machine  latency policy with default kernel TPP policy. As you can see, ours is the blue lines. We normalize these charts relative to the TPP results. So the transactions per second higher is better, of course. We do, for the most part, a much better job as we increase the number of clients along  the x-axis and the transactions per second as a percentage on the y-axis. Relative to that, we've got queries per second. So we're able to do more queries per second as the number of clients increase than what  we would call the ambient or the TPP implementation. Because of that, we're able to reduce the latency of each query by about 20 to 40%. That's the lower left chart here. And on the right-hand side, you can see that we actually use a little less CPU in doing  this work as well, again, relative to TPP. The bit towards the beginning of the chart that shows a much higher result or lower result,  depending which way you're looking at this, we think that that's mostly CPU cache related. So these newer CPUs from Intel and AMD have significantly higher L1, L2, and L3 caches. So the lower the number of clients available, that's a CPU caching artifact there.

 So the second memory placement policy that I want to talk about is bandwidth. And this is very similar to interleaving your DRAM or striping across drives for a RAID  set, for example. We're trying to maximize and utilize all of the available bandwidth to us from all of  the devices. Now, again, depending on how many DRAM modules you have in the system and how many CXL devices  you have in the system, whether they're using by 4, by 8, by 16 lanes, will determine the  characteristics and bandwidth of the CXL device. So what we want to be able to do is utilize the strategies of weighted interleaving, that  meaning that we probably want a heavier weight on DRAM and not as much weight on CXL when  we choose to move pages around. Similarly, we can also look at, say, random page selection. So if we know what that ratio of DRAM to CXL bandwidth is, we can just take a random selection  of pages from an application or many applications and decide to move that ratio down to CXL. The one that we're now looking at is intelligent page selection. So this is using heuristics of memory pages and not necessarily making an immediate decision,  but using some historical amount of information to decide which pages or page ranges should  be moved to DRAM or CXL. Now unlike the latency, we do actually end up with hot and cold data on DRAM and CXL  because again, this is a bandwidth plane of latency play. But using either streams or Intel MLC, you can determine the actual bandwidth and throughput  of your DRAM and CXL devices. So that helps us to calculate this ratio that we need to understand what weight we should  place on CXL and DRAM for our algorithms.

So I wanted to kind of show you an example using a bit of Napkin math here. We can use either streams or MLC to determine the real bandwidth throughput of our DRAM  and CXL devices. So the example I'm providing here is that we use our lab system that we used previously  for the results, which had eight DDR5-4800 DIMMs installed, one DIMM per channel for  maximum bandwidth. That gave us an approximate 300 gigabytes per second. Now if we install an add-in card that uses x16 lanes, that gave us an approximation  of 60 gigabytes per second. So the ratio between DRAM and CXL in this particular case is a 5 to 1 or 20%, right? So that gives you a rough starting point for determining what ratios you should be using  with the bandwidth policy.
 
So using our bandwidth policy in the memory machine, these are the results we got from  Weaviate. So Weaviate itself is a vector database, pretty popular out there in the AI world. We use the standard benchmarks that Weaviate, or benchmark tool I should say, that Weaviate  enters with. Now the benchmark tool itself has I think four different major tests. I think it's a SIFT, there's GloVe, DeepImage, and GIST. Each one is designed to test the database with different sizes, so different number  of objects, and a different level of number of dimensions in the vectors. GIST 960, which I pulled out here, has 1 million objects and a depth of 960 dimensions. So that's the GIST 960 Euclidean here. And here I'm showing the number of queries per second that we were able to achieve with  bandwidth. And this is using both DRAM plus CXL. You can see that we normalized the results to just DRAM only. So here we took the same system, we ran the benchmark in DRAM only, and then we did DRAM  plus CXL with our bandwidth policy. So this is the uplift that we got by using just a single CXL device. The charts here, the six bars, the first three bars represent the 90/10 ratio. So 90% of the data lived in DRAM, 10% lived in CXL. And then the three bars, the benchmarks limit, or have this variable called limit, that limits  the number of results that you get back per query. So the first bar is limit of one result, then it's 10 results, then it's 100 results. So you can see here that at least with the 10 and 100, the orange bar and the dark green  bar, we're getting a little over 7% here. The last three bars, the blue, the purple, and the green, are when we did a split of  80/20. So if you remember from the example, 80/20 was the actual theoretical limit of bandwidth  for DRAM and CXL. So the best optimal solutions fit somewhere between 90/10 and 80/20. Although having said that, we continue to refine our software and improve performance,  so we would expect these numbers to get better in the upcoming releases.

And then this is the same benchmark, same hardware, same task. This is just the P95 latency. So again, positive is better in this situation. So again, with the maximum returning 100 vectors per query, we're very close to 9% better in  terms of latency, which was reflected in the previous chart where we're able to do more  queries per second.

And then let's get on to understanding your application. This is where we can get down into the weeds a bit. The CPUs are very complicated these days, many cores, many ALUs, and a whole bunch of  other very intricate things that go on inside of the CPU to make sure that the CPU can handle  the workload that you're giving it. So these two diagrams are really just kind of what one shows a representation of a CPU,  what happens inside of it, and we call it front end and back end. And then the right hand side is kind of a decision tree as to, okay, how do I get from  running my application to understanding if this is a speculation problem? Is the CPU just speculating going and getting the wrong data or getting data that I'm never  using or is it what we call front end and back end? So is it more compute bound or is it more memory bound? And I'll put a link at the bottom of this slide here so you can kind of go read a little  bit more on this if you want to, but this gets down pretty deep into understanding what's  happening inside of your memory controllers, your CPUs, and other subsystems.

So the other tool that we commonly use is an open source tool from Intel called toplev. Works better on their CPUs obviously. By using this example, we're using our stream benchmark on the toplev and it shows that  we are memory bound. So in the center here it says back end under bar bound dot memory bound, right? And it's 20% in here. So we're spending quite a bit of time memory bound, which is what we expect for stream. It's a very complicated benchmark and it's doing a lot of vector lookup. So we do expect it to be memory bound in this situation. But again, you can run this against your application and see for yourself, is this more memory  bound or compute bound, et cetera. So just to give you an idea of another tool that's available.

So finally, here's a list of resources that you can go learn a bit more, engage with myself  and other industry partners and experts. So first there's the website, the memoryfabricform.com. You'll find a lot of resources on there. Our YouTube and SlideShare, where you'll find the recordings of this webinar plus the slides  and previous sessions and future sessions as well. We have a LinkedIn group, so you're welcome to go learn more there. And then finally, for more real time updates or asking questions, there's a Discord link  there. That's the invite to go and join our Discord server. A couple of different rooms in there. You're welcome to come ask questions or learn more. We're usually posting information around events like this as well. So this is more of a real time event. So I appreciate your time. Thank you so very much. I hope you have a good rest of your day and we'll catch you on the next one.
