
All right. Good afternoon, everyone. My name is Jungmin Choi, and I'm a memory system architect from SK Hynix. So today, as you can see here, I'm going to talk about some use cases and system benefits of the CXL-disaggregated memory system.

All right. So, here is our agenda for this presentation: First, I'm going to talk about the motivation, in particular, challenges in today's data center. Why should we focus on CXL-disaggregated memory systems? Then, I will introduce Niagara, our CXL-disaggregated memory prototype, and discuss some use cases that can mitigate the above challenges. I will end this presentation with an introduction to our future work.

All right. So, let's first talk about the system issues in this day. So, the volume of data grows exponentially. So, we need more cores, and it requires a continued increase in memory bandwidth and capacity. But, due to some reasons, like, you know, some implementation cost and some signal integrity issues, the gap between such a requirement and platform provisioning capability is growing. So, we basically need a new approach. And, you know, CXL can address these issues. And, CXL-based memory expansion is a decent solution for the issue. And also, you know, CXL can enable some memory-disaggregated systems beyond the memory expansion.

All right. So, from now on, I'm going to talk about some challenges in the data center. So, you know, the first challenge is that the memory utilization of the compute cluster varies—uh, time to time—uh, based on the working set size. But, uh, as illustrated—uh, on the left side—the inflexible nature of cloud hardware platforms—uh, leads to underutilized memory resources, resulting in memory stranding. So, you know, uh, memory is a very—uh, costly resource. So, the memory stranding due to the platform over-provisioning represents a significant resource waste. And also, there can be a temporary skew in memory usage causing the data spill, as you show on the right side. So, the storage swap can lead to system performance degradation.

All right, so this is a second challenge. So, the distributed computing system can be a powerful solution for large-scale AI applications, such as large language models and also deep learning recommendation models. But, it is very important to note that there can be a data transfer overhead between multiple loads by sharing some data. And also, storing that kind of data or shared object in the local memory of each node leads to some data duplication and also increased memory pressure. So, up to this point, we have looked at some issues in data centers. So, now let's explore the solution that can address these challenges.

All right, so we propose a disaggregated memory system that can support memory pooling and sharing capability. So, you know, memory pooling can mitigate some memory stranding and data spill issues by sharing disaggregated memory resources between multiple loads. And I'll explain about it later. So, this provides some memory pooling solutions that can dynamically allocate or de-allocate the disaggregated memory resources for each node. And also, memory sharing can reduce the data transfer overhead and also data duplication issues by sharing data or objects between the multiple loads.

So, this CXL-based memory system offers a significant benefit. But, you know, they face a fundamental challenge: the additional latency. So, we believe that implementing hotness tracking can help mitigate these issues. But, you know, the current tracking algorithm, like PBS, has some notable limitations. They can track only about half of the total memory traffic and also consume lots of CPU cycles. So, addressing these issues, we have developed monitoring logic that can track some hotness of data inside CXL memory. And then, this hotness information, you know, can be relayed to the operating system or applications, enabling users to migrate frequently accessed pages from, you know, the CXL memory to the local memory. So, depending on the workload pattern, we believe that this approach has the potential to enhance the system performance by effectively hiding the latency related to the CXL memory.

Okay, so we have built a Niagara hardware-software research platform for CXL disaggregated memory system exploration. Niagara is an FPGA-based CXL disaggregated memory prototype, and it is a 2U memory appliance. It can connect up to eight CXL host servers, and it supports up to four-channel DDR4. Its maximum capacity is one terabyte, and it can also support a dynamic capacity device described in the CXL spec 3.1 for memory pooling and sharing functionality, and a hotness monitoring unit described in the CXL spec 3.2 for hotness tracking. So, you can see the rack-scale system with our Niagara prototype on the right side.

Also, our Niagara provides a comprehensive hardware-software integrated solution for DCD and HMU. So, you can see here on the software side, for DCD, we have developed a DCD handler and memory usage monitor. I will explain again on the next page, but the memory usage monitor can track memory usage for each host and then report that data to the orchestrator. The orchestrator can effectively manage the memory allocation status by requesting memory addition or release to the device at the optimal time. And for HMU, we provide an HMU user library that can configure the HMU inside the CXL memory device. We are also in the process of designing the hotness monitoring engine. That engine will determine when to prompt the user to migrate some pages based on the hotness information, while the migration engine will execute the actual page migration from CXL memory to the local memory. On the hardware side, our solution includes a full memory controller for DCD, and both DCD and HMU capabilities, along with a full memory manager similar to the fabric manager. In summary, with our Niagara, we can provide a hardware-software integrated solution to our customers.

So, let's take a quick look at how each feature works. In the DCD mechanism, the memory usage monitor can trace the memory usage on the host servers and report the data to the orchestrator. If the memory usage reaches either a high or low watermark, then the orchestrator can request memory addition or release to the full memory manager. Our memory controller then actually allocates or deallocates the memory resources for the specific host dynamically, and then sends that result as an event to that host server.

In the HMU mechanism, the host first should configure the HMU settings to obtain optimal hotness information. The settings include tracking address range, unit size, epoch length, and hotness threshold. Then, our device, our HMU, can monitor the traffic accessing the tracking address range during the defined epoch length and count it based on the memory unit size. When the hotness counter based on the memory unit reaches the hotness threshold, our HMU can register that counter value to the hot list. Then, the host can access the hotness information and decide whether to initiate the page migration from CXL to the local memory.

All right, so this figure illustrates the DCD and HMU-enabled software infrastructure described in previous slides. So, in this system, we are evaluating the system performance with DCD and HMU use cases.

All right, so now let's look at our benchmark results. Actually, we collaborated with MemVerge to evaluate the system performance with these use cases. So, the first use case is memory pooling. As is shown on the left side, the Niagara can dynamically allocate or de-allocate the resources for each host. And on the right side, we evaluate the system performance using the CloudSuite benchmark. Notably, spilling 20% of data spilled to the Niagara resulted in a performance improvement of up to 2.5 times compared to spilling to NVMe. So, we demonstrated that even an FPGA-based memory system can effectively mitigate the performance loss. All right.

This is a second use case: the memory sharing. So, we have modified Ray, an open-source-based distributed computing framework for AI/ML, enabling multiple hosts to share the object within the CXL memory device. And this approach can effectively eliminate the data transfer overhead, resulting in performance improvement up to 5.9 times compared to the native Ray. So, these benchmark results prove that our DCD solution can effectively enhance the system performance.

All right. We believe that a key use case for HMU is HMSDK, the heterogeneous memory software development kit. And our other speaker, Hongyu Kim, will cover it in more detail in the following sessions. But the HMSDK is a software solution to support heterogeneous memory systems. So, in scenarios, including involving the capacity expansion in the middle of this slide through the CXL memory, the data—you know, the hot and cold page detection and migration—are effectively managed using the daemon, the data access monitor. But by applying HMU instead of a daemon, we can reduce the profiling overhead and also enhance the tracking accuracy through finer memory granularity in detecting hot and cold pages. So, this enhanced page migration decision based on the HMU information can lead to the system performance improvement.

All right. So, I'd like to invite you to visit our demo booth, the P14. In the booth, we are showcasing some dynamic memory allocation, de-allocation, as well as hotness tracking based on the request from multiple CXL host servers. So, in the DCD demo, four host servers are connected with Niagara. And then, if memory usage for a specific host reaches the high-water mark, the orchestrator requests memory addition to the Niagara. So, you can see that— you know, you can see that the allocated CXL memory capacity increases with the memory usage on the right-side graph. And, our HMU demo illustrates the heat map changes based on the HMU information for each benchmark, so that we can, you know, identify some migration candidate page candidates, from CXL to the local memory.

All right. So, finally, let me briefly outline our future work. And so, first, we will evaluate the system performance based on page migration using hot-list information. Another key focus will be some disaggregated memory server system architecture for AI applications. So, we aim to investigate the system benefit by, you know, applying the disaggregated memory to the GPU system for AI workload, such as LLM and DLRM. And we will also explore some evaluated functionality of disaggregated memory systems. So, the near-data processing can be one of our research candidates. And, you know, given that pre-do memory are connected to multiple hosts, ensuring the system reliability is critical. So, developing fault-tolerant memory systems will also be an important area of our research. All right. So, our works are aligned with OCP CMS. So, I'd like to invite you to join us in this effort. And above all, we really would like to do open collaboration with industry partners to enable the CXL hardware software ecosystem. All right. So, that's all I have for this presentation. And, thank you.

Thank you, Jungmin. I'm sorry we're not going to be able to take questions, but is it okay if I offer that you would answer questions either outside or at the back of the room?

Yeah, yeah.

Okay. Yeah, I've been told that we need to get back on schedule. So, Jungmin's available for questions if you have any. Thank you, Jungmin.
