YouTube:https://www.youtube.com/watch?v=rO4PdTAwLTY
Text:
Thank you, Frank, and welcome to the CXL Forum in Europe.

First, I'm going to give a couple of slides before I invite Antonio and Olivier to join me here. The data has been growing very fast, and much of it is led by AI and HPC. As you see here, with the EDA data, with bioscience genomics data, with AI model data, they have been growing exponentially. And not only the size is growing, but the speed that needs to be processed is also growing.

As a consequence, memory has been growing. For those of you who have been buying computers or buying servers, you will notice the memory is becoming a bigger and bigger percentage of the server cost. In fact, it's the most expensive part of a server. More than 50% of the server cost is being spent on memory. That means more and more memory is being bought and being deployed, and people just cannot get enough of it. And it needs a new architecture to allow more memory to be deployed more efficiently and to be used by these data-intensive applications.

And that's the fundamental driver for CXL, or Compute Express Link. This is a standard that has been developed over the last few years. There is a CXL consortium that consists of more than 200 companies that put together the standards with the first version published in 2019 and the version 3.0, just published less than a year ago, in 2022. And we have been organizing CXL Forum since August 2022 as well, with three of them took place already at the Flash Memory Summit, at the OCP Summit, and in Wall Street. And this is the first one in Europe. And Europe has a lot of innovation happening in HPC. In fact, we were talking to some people on the floor. We think there are actually more innovation in the HPC area happening in Europe more than any other places in the world. So it's great to be here and to be joined by the like-minded people. CXL Forum is really for the people who are interested in CXL to come together, to form this community, to exchange ideas, to network, and to collaborate so that we can bring these standards to the world, both in hardware, in software, and in the overall solution that would work. So at this point, maybe let me first invite Olivier to say a few words. He is from SiPearl, which stands for Silicon Pearl, who are making the processor for HPC in Europe. Exactly.

So you've discovered my DNA. OK, so I'm Olivier Desprez. I am a Principal Software Architect at SiPearl. So SiPearl is European in its DNA. As I said, and actually we are deeply involved into a European project related to HPC. And what we see is that as Europeans, we like to bet on standards. And we see CXL as a standard not only for disaggregation of memory, so that's the first wave to take your message, Frank, but also on the cache coherency. That's really key for us because we are trying to build some very efficient supercomputers. So Europe is currently taking CXL not only for memory, but also for cache coherency, and that's very important. So I will go into more detail during my presentation, but that's the message. Europe is really focusing on taking CXL at the moment and hopefully it will be a success.

And we not only have innovators like Olivier, who is a startup building this chip. They are growing really quickly. I was talking, they have hundreds of people, they're going to double by the end of the year, they're going to grow to a thousand in a couple of years. And hopefully it will be a new innovation that happens there. But there are also a lot of practitioners and users who are pushing the boundaries in HPC. I have here with us is Antonio from Barcelona Supercomputing Center. He is one of the earliest adopters to a lot of the cutting-edge technologies and he has been looking to CXL as well. Antonio.

Thank you. I would like to thank the organization for letting me talk today. To add to what Olivier said, because we are also partners with many of those projects at Barcelona Supercomputing Center, several of us are searching in different areas of supercomputing at the BSC. We are actually looking into different ways to leverage this. We are looking forward to get our hands on CXL technology. I agree it's certainly going to play an important role in the upcoming computers in Europe. As I said, many projects were already looking into this technology to be integrated and leverage not only memories but also to interconnect other devices. And just to conclude, I will be talking about two of my use cases where I'm really much looking forward to get my hands on CXL technology and to explore amazing things that we can enable with that.

Thank you, Olivier. And thank you, Antonio. Since the first time we did this at CXL Forum, it has become more real. It was just slides. But today, I think you will see from various companies, actually real hardware and real software that works. You'll see demos, you'll see actual proof of concepts showing both memory expansion, memory pooling, and memory sharing actually working in action. So it is becoming real.

But before going to those demos, let me first just give you a refresher on what CXL is, hopefully in the next five minutes. And then I'm going to give you a preview of some of those demos that you will see. So CXL started with version one. And what version one does is basically an extension of your memory where it specifies the CXL.io, CXL.cache, CXL.mem, where you can extend memory over this protocol on top of PCIe generation five and later.

And then CXL 2.0, extending that extension to pooling, which allows multiple hosts or multiple servers, here indicated by H1 through H4, connecting to the same memory pool. And this memory pool could consist of single logical devices or multiple logical devices. They can be in the form of a multi-port device or multi-port card, or they can be in the form of multiple devices interconnected over a switch, a CXL switch, to these hosts. And all these topologies will be supported. Actually you will see some of them today, both at this show as well as on the expo floor in the MemVerge booth.

And then CXL 3.0 is the latest specification that was published in August of 2022, and that extended the 2.0 specification to allow more than a single switch to be used. So you could have a cascading of switches that allow the scalability to increase beyond just four or eight hosts, where you can go to tens or hundreds or even thousands of servers connecting to the same pool of memory.

It also enabled pure peer-to-peer access. That means any processors, whether it is CPU, whether it's GPU, whether it's accelerators, can access any memory without going through the main CPU. So it can be just directly peer-to-peer access for more efficiency. And it also specifies cache coherency that's needed to allow memory sharing, allow multiple hosts to access the same memory region at the same time without creating inconsistency of data. And this is essentially a series of important specifications that mapped out the roadmap that this will become available.

And when you're talking about memory, people ask about performance, both bandwidth and latency, how does CXL memory will look compared to the DDR memory that we are used to in the computer today. And here are the specifications of the various PCIe generations and number of the lanes that your device will support and what are the bandwidths that it's going to support. CXL runs on PCIe generation 5 or later, so it's going to be on Gen 5 today, and it's going to be on Gen 6 in the near future. And depending on the type of devices you will see with Gen 5, if you have a full lane, it will have 32 gigabytes combining both ways of bandwidth going to your CXL memory device. If you have eight lanes, you can go to 64 gigabytes. And this is on par or similar to what you're going to get from your DDR devices. And you can certainly have more than one of them together to further increase the bandwidths. In terms of latency, it is similar to a memory that's connected to a CPU that's a one NUMA hop away. So in our measurement, it's somewhere between 200 to 300 nanoseconds latency when you need to access a CXL memory, which is slower than accessing the local DDR DRAM connecting to the same CPU, which is typically around 100 nanoseconds, but similar as accessing the CPU that's one hop away on NUMA. So it is not quite exactly as fast as DDR, but it's very close. And you can essentially run your application pretty effectively on the CXL memory.

And I discussed the timeline where it is becoming real. And here gives kind of a high level prediction or forecast of when we're going to see some of the hardware hitting the markets. The top three rows is indicating on the computing side, on the CPU and GPU, the processor unit side that we are seeing the first processors from Intel, from AMD, and from ARM, or starting to support CXL protocols. Where Intel is supporting CXL 1.1 type 2 devices, AMD supporting CXL 1.1 type 3 devices, and ARM is actually the most ahead supporting CXL 2.0 devices. In the next version of the CPU, they're going to have expanded their support. Both Intel and AMD will have support for 1.1 for all devices. AMD will start to support 2.0 in all devices. And ARM will continue to advance as well. On the memory device side, we are seeing more and more samples, engineering samples, that are available from the leading memory vendors. And they are here supporting the memory expansion use cases. But as you will see, we are also seeing the first development samples that are supporting the pooling and the sharing use cases. In fact, one of the most exciting news out of this event is that we think we can develop software innovations to bring in the timeline of some of the key features. For example, memory sharing is really the holy grail. A lot of people have been wanting to see memory sharing for a long time. And it's not available in the specification until 3.0 time frame. But through the software innovation, we think we can bring in the memory sharing timeline to the 2.0 time frame. So that through cache coherency implemented in software for certain semantics, we can allow certain applications to start enjoying accessing to the same region of memory at the same time with the right cache coherence. And we're going to go all into it in the first session in the afternoon. So the timeline is getting close. We are seeing some of the development system starting to occur this year. We think we can get a first system to the customers by the end of the year for them to test out. And the real production deployment can start next year for expansion, pooling, and some subsets of sharing use cases.

And the impact to the industry is going to be huge. And this is essentially going to make it possible for memory resources to be disaggregated and pooled like the other resources in the data center. Storage disaggregation happened more than 30 years ago when fiber channel came to the market. And storage can be disaggregated out of the computer and created in their own pool. And the technologies like SAN and NAS come to market. And there is creation of new use cases, creation of new technologies, both in hardware and software. There are various data services being implemented. And a new industry is born, and that is network storage. And this industry is over $100 billion in size, and that's impacting every data center in the world. And memory, until now, has been a subset of a server. It's a very expensive subset. It's more than 50% of the cost. But it's still tied down to the server. It cannot be disaggregated. It cannot be a resource that you can scale independently and you can manage independently. And all that's going to change with CXL. With CXL, the memory can be liberated from the server enclosure and to become its own first class citizen in a data center. It can be independently pooled, independently scaled, independently managed. It can be dynamically allocated to servers as needed. It can be used as a way for the data to be shared between servers at very high performance, very high bandwidth, and very low latency. We think this is going to lay the groundwork for a whole set of innovations in software, both in the applications as well as in the platforms that allow this to happen. And a new industry is going to be born because of it as well.

And I'm going to give some examples, and this relates to the work that MemVerge and many other companies are doing in collaboration to bring these innovations to the market. The first is in the area of composability. Composable infrastructure has been a hot topic in the last few years. And it's been largely revolving around composable storage, composable networking, composable GPUs, but not so much in composable memory. Because of the tightness, it's being coupled to the processors. And CXL is going to enable that decouplement and that composability to happen. Today, while the other stuff could be shared among the servers, DRAM within each server. But what's emerging with CXL is that we could have a pool of memory that's indicated by the red boxes that can be shared across the servers. And it's going to start at a rack scale, where all the servers on the same rack can be connected to that pool of devices, either through multi-port devices, through some kind of topology, or through a switch that can interconnect them all. And we are seeing the first innovators, both of the multi-port CXL devices and of the switch devices. And we're going to hear from some of them today, such as Xconn, who's going to present. And then in the future, we're going to see this to expand to a data center scale, where more than one rack could be interconnected between those switches. And any of the servers could potentially access to any of the memories across the data center. And this will be further enabled by people who are working on photonics technologies, which have a different kind of wire that can go to 50 to 100 meters to allow servers to talk to a memory that's a distance away. And it's going to enable additional use cases that are not possible today.

And then the first very interesting solution that we are building with our partners is a service we call the Elastic Memory Service. And you may also hear about it as Project Endless Memory. And the fundamental objective of this project is to eliminate the out-of-memory errors. For those of us who work with computing, with HPC in particular, but any computing, out-of-memory or OOM error is one of the most common errors that we see. And it can cause crashes sometimes if your application doesn't handle it well. And at the very least, it can slow your application down by a couple order of magnitude. It basically falls through a cliff once you run out of memory. And that's a very severe bottleneck for many of the applications.

And CXL enables a solution that we can basically create endless memory, where it is achieved by having a common memory pool connected to multiple hosts. And you could have various applications running on the hosts. It could be in the various virtual machines, could be in different containers, it could be running bare metal on that machine. And the memory demand for those applications could change over time. In steady state, there might be enough memory just in the local memory to handle those applications. And once the demand of those applications increase in memory, you run into those OOM situations. And with memory pooling and with the right software being written, we can detect before it runs out of memory. We'll set a threshold. It's getting close to running out of memory, so we would dynamically and automatically allocate more memory to that node so that there's enough memory that it doesn't run out. And when the demand decreases, we can also detect it. There could be another threshold being set that we can return the memory to the pool. So it's a dynamic process to manage the memory capacities so that any of the hosts never run out of memory as long as there is enough memory totally for this cluster. And this solution essentially could eliminate the OOM issues if there's overall enough memory resource for all of the hosts.

And so we are in Germany, so it's like we drink a lot of beer here. And it's like having bottomless beer where it never runs out. If you're getting empty, there'll be more refill. And similarly with memory, with this EMS service, it never ran out. When you're getting low in free memory, you'll get more. So that's a bottomless memory case.

And the benefits is self-evident. It can really have more efficient use of your resources. It's always that effect when you have pooling happen. It can decrease downtime by reducing those crashes. It can increase performance, and it can optimize overall for your infrastructure.

And we are living in a time where sustainability is becoming increasingly important. And it does have a major impact by more efficient usage through pooling to the environment as well, that you can use this memory to have more efficient use of resources. You could essentially, potentially power off things when they are not used and power on when they are being used. And we can always right-size the amount of resources that's needed. By doing this, we can reduce the carbon footprint and optimize on the energy usage for the data center.

And to enable endless memory, hardware is a foundation, but software is a necessity. That we need to build the right software component to enable the auto-monitoring and detection of the memory usage on each of the hosts. And we need to have a module that we call Composer that talks to each of the software we call memory machine on each of the hosts and can manage this dynamic allocation and deallocation intelligently. There could be advanced algorithms that we can predict such usage and creating more intelligent memory pooling solution through this all as well. And MemVerge is working on this layer of software with our hardware partners to enable this intelligent and automatic balancing of memory resources across the pool. And you will hear more about it from Greg right after this. And he will also show you a real demo on this as well.

So with endless memory, what we hope is that we will no longer see the OOM errors anymore. So that's very exciting.

But I do have another thing to share, the second thing. And that is shared memory. So as you know, besides OOM is a big problem, with any distributed applications, whenever it needs to go to I/O, whenever it needs to go on a network to talk to a node next door, or whenever it needs to go to storage to persist something on SSD, the application becomes slower. That's why people buy a lot of memory so that they can keep things in memory. It doesn't go into I/O. But if you are distributed, you have to talk to the other node, and you have to share some data with the other node, and there's no way going around I/O. So we kind of stuck. And that's a major I/O bottleneck that people call an I/O wall. And with CXL, for the first time, we are seeing actually an alternative for nodes to communicate with each other. And this alternative is through shared memory. And so for the first time, node A and node B could talk to each other, not over network, but by talking to the same shared memory. And this is possible today by multiple processes on the same host using shared memory as IPC, as inter-process communication. But it's not possible across nodes. But because of CXL, it is possible. And on the hardware specification, it's going to be possible by the 3.0 time frame, which is at least two or three years away before we see the first hardware. But through software implementation, we can demonstrate it on 2.0. And this is one of the demos that you will see this afternoon. Seeing that CXL memory works actually on hardware samples that are available today.

So the way we are doing it is through a project we called Project Gismo. Gismo, for those of you who watch movies, it's a nice spirit before it turns into gremlins. And it's also a nice tool. But in this case, it stands for G-I-S-M-O. It's a global, I/O-free, shared memory object. It's essentially an in-memory object store that runs across multiple nodes. And it allows one application on multiple nodes to access the same memory and to be able to use that memory to communicate and to share data across each other. And this, we believe, is the most efficient way data can be shared across multiple nodes. We have been working with multiple applications, including AI, including databases, to validate that this is useful for them. This is a very simple API. This allows the creation, it allows the read, it allows the destruction of these objects. But even such simple APIs can already be very useful for those applications.

And this afternoon, we're going to go into the full details of it. We think because of this shared memory architecture, now memory becomes your network. And now memory becomes your new storage. And this is the promise of this new memory-centric computing model that's enabled by CXL and where I/O can be eliminated for many of the use cases as well.

And so this afternoon, we are—I think this is the first session in the afternoon where we're going to go into details on these use cases. And we're going to see a real demo of Gismo at work.

So that's it. That's the introduction. And just repeat what Frank said. CXL is really-- it's not a meeting. It's a community. So please scan and join. And hopefully, we will share information. We'll collaborate as part of this community. Thank you for being here. And now let me introduce Greg, who is our lead engineer, who's been working on Project Endless Memory. And he's going to tell you all about it. Thank you.
