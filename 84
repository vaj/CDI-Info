
Good morning, everybody. Just a quick introduction. My name is Mahesh Wagh. I'm a senior fellow at AMD, and I drive the Epic server system architecture. And on the consortium side, I'm the co-chair of the CXL technical task force. We'll talk about, basically, in this session, all of the enhancements that we've done in the specification towards advancing coherent connectivity. So we'll start with just the basics of CXL, and then it's more of a rapid fire session, go all the way from CXL 1.0 to 3.0. But the key thing that-- what I would say, the way to connect it to what Charles was talking about before is, if you look at that market opportunity, it requires you all of these capabilities, and a big part of that was specification. So we'll talk about all of the things that we're doing in the specification to make sure that you have all of the needs, that all of the mechanisms that you need to address that particular market.

There are a lot of slides in here, so I'm going to just skip through the first few, and then touch on some of the key aspects of it. So CXL basically starts with very, very fundamental capabilities, which build on what exists in the ecosystem from just this data speeds perspective. From a connectivity, it relies on PCI Express as a very basic foundation for electrical connectivity. So that enables you to get the high-speed connectivity based on PCI Express, and then you build up on top of that the new protocols that give you this new enhanced capability for coherency. And those protocols are basically, essentially, three protocols, cxl.io, that's PCI Express, and then you add cxl.cache and cxl.mem that extend cache coherency and memory semantics to these devices. And the key mechanisms that we were looking at there were, how do you build these protocols where they're efficient in terms of their accesses? So when you look at CXL, it provides you this fine-grain ability to interleave between these different protocols, so at a 64-byte granularity. So that way you get the performance as you're simultaneously driving cxl.mem.cache protocols over that particular link. And the goal, as we were defining CXL, is that you want to build this technology so that you're going to enable the use cases that are coming in the future. You start with memory expansion. You build on top of that to enable memory disaggregation, memory pooling, and then get into driving large-scale memory systems, memory-oriented accelerator systems. So that's sort of the premise.

And it starts with CXL 1.1. Just from a quick consortium perspective, pretty much the board of directors are here. If you look into what CXL has done from an ecosystem perspective, you're bringing all of the compute companies, all of the memory companies, all of the accelerator companies, and pretty much all of the PCI Express ecosystem all together to drive this open standard for high-speed communications. And that's a pretty good place to be. Consortium is growing with more than 250 member companies. From a specification perspective, in August '22, we defined the CXL 3.0 spec. 3.1 spec is on its way pretty soon. So a good steady cadence of spec delivery from the consortium perspective.

So if you look into just CXL approach, as I was talking about, you take the PCI Express physical layer and then on top of that, you provide the ability to multiplex between these three different protocols-- I/O, memory, and cache.

What that enables is really these three fundamental use cases from a device-type perspective. You have caching devices that want to participate, but want to participate using the .cache protocol. So those would be your type 1 devices. A good example for that would be an accelerator such as a NIC that's taking advantage of the .cache properties. On the very right is you're making use of CXL.io protocol, and you're using CXL.mem, and now you're building memory capabilities for those devices. These would be examples of memory expansion devices. Either you're enabling them for memory bandwidth expansion use cases, memory capacity expansion, or you're bringing a new class of memory connected to the processor. So that would be the memory buffers or memory expanders. And then the devices in the middle are you have CXL.cache and mem. Think about this as an accelerator that has a large amount of memory. You want to map that memory as part of system memory and participate in the coherency protocol between both the host processor and the accelerator. So that's what we call type 2 devices. So then if you look at it, the capabilities that we enable with CXL 1.0 is direct attached devices. Either they are accelerators or they are memory expansion devices.

So based on that, then when you get into CXL 2.0, we start to look at, OK, now that you've defined these devices, how do you provide some scale to these devices? And what are the new features that you bring in in the CXL 2.0 specification? So we start with this concept of you want to be able to support memory pooling. You take memory as a resource. You partition that memory into multiple segments, multiple partitions. And you assign that given partition to a given host image so you can pool that memory between multiple hosts. Just as an example in here, those colors represent host images. That's the host system memory. And if you take that device as an example, device D2, it's shown in two different colors. So it's mapping some portion of its memory to host 1. And it's mapping some portion of its memory to host 3 as an example. And the specification provides you this ability for the device to advertise its capabilities that it can partition and then support all of the mechanisms that you need to map that particular memory to different host images through a CXL switch.

The other capability that it brings is the ability to fan out. So either you can use the CXL 2.0 switch to get to doing memory pooling, or you're just looking at expanding the memory capability and get the fan out capability. So it moves CXL from direct attach now to being connected behind a CXL 2.0 switch. And this is an example of a single level switching. It enables that memory expansion capability that I was talking about.

In addition to that, a couple of other things that CXL 2.0 brought in that wasn't there in 1.1 was the devices in CXL 1.1 were visible as root complex integrated endpoints, which means if you're running a legacy PCI bus driver, that link is not visible to that bus driver. You need to have a CXL OS aware driver if you want to manage the link. With CXL 2.0, that particular link is now visible as a PCI Express link, so that a standard legacy bus driver can manage that link. And the other thing that it enables is the ability to manage hotplug. So today, there isn't a support for hotplug of a root complex integrated endpoint. With CXL 2.0, you make use of all of the PCI Express hotplug management mechanisms to manage the CXL devices. In 2.0, the type 1, type 2 devices still assigned to one host. There were some limitations that were then addressed in the CXL 3.0 specification. But in any given hierarchy, until CXL 2.0, you could only support one type 1 or type 2 device below a virtual hierarchy. I touched on the memory pooling aspects. Couple of other things that were addressed in CXL 2.0 was now providing the ability for persistent memory use cases. This is where a device can support persistent capabilities, but you need mechanisms to notify the device of impending power failures. So CXL 2.0 specification provided the hooks for the system to communicate with the device about these impending power failures so that the device can take care of the persistency for that given memory. It started to define the Fabric Manager and API interfaces so that if you have a switch and you have to manage a pool resource, you need the ability to understand the device's capabilities and then allocate those pools to a given host. So you needed certain standard capabilities to be defined. So Fabric Manager APIs were defined. So now you have a very common framework through which you can manage these resources. And finally, looking at security aspects of it, what CXL 2.0 defined was now the ability to provide integrity and data protection for requests as they pass through over the link. And this is to protect from threats, from a security threats perspective. So you provide the ability to do link-to-link encryption with CXL 2.0. So as you can see from 1.0, we're incrementally building these capabilities, both from enhanced capabilities from a device perspective, providing you the scaling to get to switching, adding some more capabilities to bring new types of devices, like persistent memory devices, and then provide the scaling with the switches, and then finally address some of the very key security aspects that you need to enable memory technology.

So when we were done defining 2.0, there were still a lot of use cases, a lot of interest in going beyond what was defined in CXL 2.0. And those usually start with you need more higher bandwidth, you need high-performance accelerators to be connected, provide more scaling for those accelerators, and you're also looking at more efficient peer-to-peer resource sharing. So there were a lot of use cases where when a request comes in, and if you're trying to target an accelerator memory, you have to bounce that request off the processor. So there were requests to support direct peer-to-peer so that a request coming in from a NIC, you can directly go to an accelerator or to system memory. So those were some of the capabilities. To address that, what CXL 3.0 defined was add more fabric capabilities. This is provide switching and scaling beyond just a single-level switch. So those capabilities were added. As we were looking in this particular time frame, even in CXL 2.0, some of the pooling aspects were addressed by the notion of multi-headed devices. And you could define it. So it was what we had looked at it and said, well, if the ecosystem is going towards multi-headed devices, why don't we define that particular use case through the specifications so that we know it's part of the standards? So multi-headed and fabric-attached devices were defined. What came along with that was this enhanced fabric management. So you're looking at a scale for the fabric now to grow from a switch to support up from a capability perspective up to 1,000 nodes, realistically maybe about 100 nodes, is what you can support using CXL 3.0 capabilities. Part of it was the composable disaggregated infrastructure. So this is where you're looking at supporting memory pooling. What do you want to build? The capability of being able to dynamically pool these devices. So we needed certain capabilities that would allow you to dynamically size your pool. So dynamic capacity devices were defined as part of an ECN through the 3.0 specification that allows you to now support these devices, which can dynamically manage their memory, both size as well as capabilities. And then it supported multi-level switching. And then the key part about it was looking at certain-- enhancing some of the symmetric capabilities. So when we look at what we have with CXL 2.0, it had the ability to support coherency using the bias flip flows. There were a lot of interest in supporting a more scalable solution. And also, one of the things that I talked about to support peer-to-peer. And in that case, you're looking at multi-host sharing of multi-host accesses to a given memory. So in that case, if you're trying to solve those problems, you need the ability to provide-- a better scaling solution would be when you provide a snoop filter capability down into the device. So that was enhanced in CXL 3.0 to provide what is called a back-end validate. So the device can manage coherency from the device perspective. And those capabilities were added. And finally, from the bandwidth perspective, when we looked at CXL 3.0, it's increasing the bandwidth by adopting all of the advances that were done with PCI Express going to the 6.0 specification. So just extend the capabilities to run on that particular physical layer, and you get all of the benefits of the ecosystem to get more bandwidth. And that comes with-- there was a lot of effort looked into. The key aspect of what we continue to drive for CXL is node-level properties. So when we look at transitioning to CXL 3.0, we looked at all of the FLIT formats that need to be supported and make sure that we're addressing the solution that doesn't incur added latency. So we looked at it, and all of the work that was done from the specification was to make sure that we're not adding more latency to CXL 2.0 as these new formats get defined. And finally, the backwards compatibility part of it, which is we really, really try to make sure that all of the investments that people make into the prior generations continue to be supported. So there is a lot of focus on maintaining full backwards compatibility going all the way from CXL 2.0 to CXL 1.1. So in interest of time, that's all I had for today. Thank you all. So this was just laying where we started with CXL 1.0, 2.0, and then 3.0, building all incremental capabilities. This work will continue from the consortium perspective. We're looking at new and interesting use cases that we will drive for CXL 4.0. So thank you.

Thank you very much. Mahesh, before you sit down, I just wanted to give a special thanks to AMD. We started this initiative a year ago, and the first people we called were Mahesh and Sid, and they said yes. And that was crucial for us getting this off the ground. So special thanks to Mahesh and AMD.

No, I mean, it's again, as what Charles was saying, nobody can do this alone. We need the ecosystem. And together, we can really make it happen.
