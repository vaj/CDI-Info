
Hello, everyone. I am Anil Godbole, Senior Data Center Product Planning and Marketing Manager at Intel. I'm going to present today about boosting AI workload performance using CPUs and CXL memory attached to the CPUs. Before we begin, I want to thank MemVerge for giving me this opportunity to show why CPUs can still matter in the AI world today.

I want to start by pointing out a simple fact: there is too much pressure on CPU core counts to increase, and CPU manufacturers like Intel are, of course, keeping up with that. But the simple reason is that modern workloads have very big memory footprints, and these workloads cannot be satisfied by simply adding more memory. In the end, the demand for memory capacity and bandwidth in a server arises simply because there are multiple cores simultaneously crunching the workload. And today, of course, we will focus on the AI workloads.

To help increase the memory capacity, as we know, man invented CXL. As the graphic in the center shows, CXL allows one to augment the CPU's memory capacity beyond what it can do with its DRAM channels alone. Here, we explain the value proposition of CXL memory. The first use case, on the left box, is, of course, one everyone is familiar with—everyone knows a case where we want to add more memory beyond what DRAM can allow us. And those are all the different workloads which can benefit. The second box shows how CXL memory can also be used—besides adding capacity, it can also be used to increase total memory bandwidth. The third box shows how CXL memory can be used to reduce overall memory TCO. We are not going to focus so much on that, but I'll refer to it every now and then. And today, we'll just focus on the second box. We will first explore how bandwidth is expanded using a technique called address interleaving. And later, we will see how one can put this bandwidth to good AI use.

Before we get fully underway, I want to show a quick slide to demonstrate that Intel remains fully committed to supporting CXL going ahead. With the recent launch of the Xeon 6700/6500 series of processors, which are aimed primarily at enterprises, we now have completed the launch of all our sixth-generation family of Xeons, previously codenamed Granite Rapids. This family offers broad support for CXL in the sense that all CPU SKUs in this family offer CXL support.

Now, here's a slide which shows the four possible configurations which the user can deploy using CXL memory. There are actually only two configurations. But on the left side, I'm showing what I call hardware control modes, meaning no OS or any middleware is needed here. It's simply the CPU BIOS which can configure this mode. And those two modes are for memory capacity expansion and memory bandwidth expansion. On the right, we are showing the same modes, but now using software control techniques. So, this is where we use an OS or middleware, like Memory Machine from MemVerge, to do this steering. But today, we will focus on the bandwidth expansion modes, which are shown below. Of course, in these modes, the capacity is also increased, as I pointed out earlier. But some special configuration is done, like interleaved memory addressing, to improve the bandwidth.

We will begin with a slide to show the concept of address interleaving. It shows the Intel hardware-based, hardware-control-based hetero-interleaving mode. I just want to explain how this method is used to increase the memory bandwidth. Actually, this method to increase the memory bandwidth by address interleaving is something we have been using on RAID SSDs for all these years. So, rather than store all the data for a given workload in a single DRAM channel, you can stripe that data across in chunks, across multiple channels. So now, when that data access takes place, it can be accessed eight times faster if a CPU has eight DRAM channels. Or it can be 12 times faster if it's a different processor, like the Granite Rapids or the Xeon 6900 series of processors. And then, when you add CXL memory to the mix, one can increase this interleaved ratio even more. So, a typical rule of thumb is a x16 CXL channel has an equivalent bandwidth of two DDR5 channels. So, if we employ two CXL channels out of the four available, effectively, we are adding four more DRAM channels. That's why we are showing that with DRAM alone, it's either 8-way or 12-way. But when CXL is added to the mix, we end up with either 12-way or 16-way.

Here's an example of how an AI-related workload—rather, an AI workload which is related to bone age assessment—We have performed this on our EMR system, Emerald Rapids, which uses the fifth-generation Xeons. This is one of the few AI inference workloads where there happens to be a good mix of reads and writes. 

Now, let's look at the other approach to increase memory bandwidth, and that is the software-based approach. This slide explains that. This software technique, or this software capability, is now available through the Linux kernel, beginning with version 6.9. I want to acknowledge that our host MemVerge, along with big contributions from Micron and Hynix, contributed this capability to Linux. Here, the memory physical pages are interleaved, or the whole virtual address mapped into physical pages by the Linux space table is interleaved between the main DRAM channels and the CXL channels. For simplicity, the graphic is showing only two nodes: one for main memory, which is the DRAM channels, and one for all CXL channels. But in practice, one can extend that to multiple NUMA nodes when DRAM channels are further split into sub-NUMA nodes, like some of the SNC modes we have on the Intel processors—sub-NUMA clustering modes. The granularity of interleaving here is 4 kilobytes, which is a Linux space size. This is in contrast to the hardware approach I showed earlier, which can do as low as 256 bytes. So, some workloads may actually benefit because now the data needed can be retrieved much faster, since you're only getting 256 bytes from all the channels. You don’t have to wait for the whole 4K to come in. But the more important benefit—which is the ability to adjust the allocation ratio between the main memory NUMA node and the CXL node. The ratio of interleave, which I call M:N in the graphic, is chosen based on the bandwidth ratio between the two memories of the NUMA nodes. The bandwidth of one node—let's say one node—is lower due to a read-read access pattern, as we know, right? A DRAM channel has the highest bandwidth if you are doing all reads or all writes. But once you start doing a read-write mix, then the bandwidth kind of goes down. The same thing can happen with CXL. When you do a read-write mix—which is actually the opposite, because CXL is a bidirectional channel—read and write, one benefits if you are doing both reads and writes in the workload. But for read-intensive workloads, the CXL bandwidth becomes lower. So, it’s no longer the case that one CXL channel is equivalent to two DDR5 channels. Anyway, whatever it is, right? The M:N ratio selection can be used to offset this. So, when the main memory bandwidth is higher, you choose M to be bigger than N, and vice versa. There can also be a TCO play here. So, imagine—let’s say we had to use all 128-gigabyte DIMMs only using CPU memory, right? Before, let’s say there was no CXL to meet the workload demand. The workload needs so much footprint—let’s say it needs a one-terabyte footprint. Now, one can do the same thing between the main memory. By using CXL, one can now spread the memory between the DRAM and the CXL using, say, less expensive DIMMs, like 64-gigabyte DIMMs. So, as you can see, the total cost of the memory, right—the cost can now go down—but you also get a bandwidth boost.

Now, one might wonder why the higher-latency CXL memory does not come in the way of performance when you interleave it with faster memory like DDR5. This is the question people ask me the most. But the slide here shows why the net latency of a memory access is actually reduced when you do this address interleaving technique. As the blue curve shows, which is the DRAM-only curve, when multiple cores start doing memory accesses, the memory controller becomes a bottleneck and starts queuing up the transactions. That shoots up the net latency to the requesting core because some core is waiting on the transaction, but others are in the queue—not because your DRAM got any slower, but simply because there’s so much traffic congestion. But when CXL channels are added to the mix by using interleaving, there are more ways to spread this traffic across the additional CXL channels, which wouldn’t be there if only DRAM was used. That’s why we see the flattening of the latency curve, as shown by the orange curve. So, yes, eventually even that will saturate if you keep increasing the bandwidth. But at least you get a reprieve—from going, in this graphic, from 400 gigabytes to 600 gigabytes—the latency is still kept in check. Now, for the remaining part of my presentation, I’m going to cite a few examples of AI workloads which have benefited from the memory interleaving techniques.

I just want to acknowledge the various contributions Intel has worked with—so many IHVs. They have worked with us, partnered with us over the last few months at different shows to show off all the AI performance boosts we can get using CPUs. That was their memory. Keep in mind, there were no GPUs used for all these demos here—all these examples I’m showing. I won’t have time to go through all the material, but I will leave all these slides here for reference. But I’ll start by acknowledging the work we did with Micron. At the last Supercomputing show in November, we were showing this RAG database acceleration demo.

A quick slide on what that RAG database is: it’s called FAISS. I believe it was contributed by Meta, but it has found good traction with a few cloud service providers. Not only do they use it for AI RAG acceleration, they can also use it for search optimization. 

We were able to reduce this RAG database access time by up to 23% by using the memory interleaving technique.

Next, I’m showing an example of a workload done by Intel’s own internal performance team. This time, we are showing the Redis Vector database. Now, Redis—keep in mind—is an in-memory database. It can be used to store all kinds of information. So, when we use the Redis Vector database, the database is quite sparse compared to a Redis document database. At such times, the high bandwidth which we get with memory interleaving really helps. So, once again, this is a popular one people use for RAG databases, and we were able to show a performance boost of up to 27–28%. We had used our IHV Astera Labs’ CXL memory for this.

Here’s a contribution from our other IHV partner, namely Samsung. They showed off how yet another open-source RAG database, called Milvus, can be accelerated using this software memory with interleaving.

They tried out many different ratios, and as you can see on the graphic side, it looks like a 3:1 ratio was the best and provided a 17% boost. 

Now, moving a little bit away from the RAG database acceleration, here’s an example of yet another Intel-Micron collaboration, where we actually ran the LLaMA 2—version 2 LLaMA LLM—on the CPU. As you know, in an LLM inference, the subsequent token generation is memory bandwidth-intensive. By using the address interleaving technique, we did just that. We also deployed the onboard Intel AMX accelerator to do the fast matrix multiplication. So, once the parameter data was looked up with high bandwidth from the memories, AMX helped to crunch the thing and make the LLM inference go faster. Here’s the 23% performance boost, which we were able to show.

Last but not least, we have the example of in-memory caching database acceleration, which was contributed by our IHV partner, Hynix. They showed off the popular CacheLib in-memory database getting a boost with CXL memory. Now, yes, this is not quite an AI workload, but since I acknowledged so many other IHVs, I also want to acknowledge Hynix here. So, that’s another area—like just some of the in-memory databases can also be accelerated with this technique. Of course, needless to say, HPC workloads also benefit.

I think that pretty much brings me to the end of my talk. So, I’ll summarize: going ahead, CPUs will play a big role in the AI revolution. There are many workloads—like RAG databases and even small LLM inferencing—where CPUs can do a job more economically and just good enough without needing a GPU. Modern CPUs offer many features—like, besides the CXL capability, things like accelerators—which can further help with efficient execution of AI workloads. So, I’ll finish with a call to action: if interested, people should check out the Intel Xeon family—the platform—or sixth-generation family. Also, all the CXL memories I showed from our IHVs—if you are interested in looking at how to boost AI performance. Thanks for your attention, and thanks, MemVerge.
