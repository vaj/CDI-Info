
Hello everyone. My name is Gada Badeer. Thank you for joining this session. Today over the next 30 minutes, Anil and I will be covering an important topic, the  Grand Teton PCE errors and error containment. How many of you have heard about Grand Teton? Well, that's good. See familiar faces. So today our main focus is to discuss how to develop a RAS solution to mitigate uncorrectable  errors in Grand Teton. RAS solutions are essential for the robustness of hardware systems in a complex environment. And today we want to share our achievements with you.

We'll start with an overview of an AI ML cluster. We'll zoom in at the architecture level of Grand Teton. And then we'll examine the I/O hierarchy and the PCIE devices within the server. Our primary focus today in this presentation centers around fault domains. We will dive into the various types of errors and when they occur. Moving forward, we will talk about error handling. We'll take a closer look on two specific examples, real world examples. Anil will walk us through the session. And then we'll wrap up with a summary of key takeaways and conclude with a call to action.

In this slide you will find an image of an AI ML cluster. This is basically a collection of RAS, collectively known as an AI ML cluster. Before diving into the details of the cluster, maybe you're wondering why do we even need  a cluster? Have you ever wondered how technologies like chat GPT work? Or perhaps if you use Instagram, have you noticed how remarkably ads are personalized  to your interest? In order to answer these interesting questions, we need to look at the AI ML cluster. This is like the cutting edge technology that is powered by these racks. In this slide you will find an image, the left side is an image of the components of  the cluster. This is an assembly of numerous GPUs that are organized into compute nodes. These GPUs are connected through high bandwidth network interface fabric to facilitate the  communication among the GPUs. We also have storage components to enhance the overall capabilities of the cluster. Now that you know how the cluster looks like at a high level, let's dive into the architecture  of one node.

This is a platform view of Grand Teton. The Grand Teton is our latest hardware system based on GPUs for inference and training workloads. On the left side you see an image of a Grand Teton rack, which has two systems. On the right side, this is a high level block diagram of Grand Teton. Grand Teton is built of three different trays. The first tray is the CPU for data ingestion and control path. Second tray is the IOX for IO expansion and fan out. The third tray is the GPU for compute. On the first tray, which is the head node, we have two CPUs and two front end NICs. Those are connected through the PCIe links to the IOX, which is used for fan out. The IOX tray has four PCIe Gen 5 switches. These switches are connected to 16 SSDs and 8 NICs. The NICs are connected to a back end switch, and that's how we form a cluster. All these devices connect to the GPU tray, which has 8 GPUs. Those are all connected through acceleration interconnection fabric. In order to set the stage to talk about error handling, we will dive into the IO hierarchy  of a slice of the system.

The previous slide showed a simplified overview of the high level block diagram. However, when we take a closer look, the PCIe 3 is pretty deep. On the left side, you see a slice of the system with CPU 0 connected to three downstream ports,  root ports. One is connected to the front end NIC, and two root ports are connected to PCIe Gen 5  switch. Beneath the switch, we have SSDs, NICs, and GPUs. As you can see, the PCIe 3 is pretty deep. When we talk about error handling, we need to take into consideration that if a device  is flagging an error, it will propagate all the way from endpoint to root port, going  through all these devices. So let's move on to talk about all the fault domains.

So our main focus is the fault domains. We will go over the different types of fault domains and how we handle those for error  propagation and handling. The first category is the bit errors. Those are corrected errors, such as receiver error or bad TLP. These errors are corrected by hardware and handled by error reporting and LCRC, link  retraining, and recovery. The second category are the bus faults and protocol errors. Those are basically the uncorrectable errors. Those are classified into fatal and nonfatal. An example of nonfatal is unsupported requests and completion timeout. Those are handled by error reporting. In some cases, we have poisoning mechanism to prevent the propagation of the error. The second classification is the fatal, just like data link protocol error and surprise  link down. These errors, whenever they occur, they cause a system halt. Those are handled by error reporting and DPC. In both cases of uncorrectable, nonfatal, and fatal, we have implemented a DPC mechanism. DPC stands for downstream port containment, and the DPC is actually an additional layer  of containment that we have to handle these errors. The DPC is actually bridging the gap between fatal and nonfatal errors and making the differences  less pronounced. Any errors that do not fall under the first or second category, which is what is defined  in the PCIe specification, falls under the third one, which is the internal error. Those are uncorrectable error, which are detected and managed through the vendor-specific error  detection and reporting and ECRC. Let's move on to talk about how the PCIe protocol is handling these errors.

So the PCIe protocol have extensive coverage for these errors across all the three layers. The PCIe protocol has three layers, physical layer, data link layer, and transaction layer. At the physical layer, we have prevention mechanism for encoding and data scrambling. While at the data link layer, the primary focus is on data integrity, where we handle  through TLP sequencing and LCRC. At the transaction layer, we have mechanisms to handle flow control and poisoning. So while the PCIe specification has extensive coverage for these types of errors through  all the layers, there are still other errors which are not addressed here. Those are the uncorrectable errors. With that, I'll hand off to Anil to address these uncorrectable errors in the next upcoming  slides.

Thank you, Gada. Let me just give you a little more clarity and add something what Gada mentioned already. And actually, before that, let me ask you one question. How many of you have actually worked on PCIe errors, like design, experience, quite a bit  of hands? Based on your experience, let's say you have 100 machines in your data center. How many times you will see correctable errors, PCIe correctable errors, or uncorrected errors? Any thoughts? Let's say in a given day or given year. Will you see errors in every machine? So let me give you some insight based on our own experience learning. We typically see 10% of the machines. Again, I'm just quoting number, but that can vary from day to day. But roughly 10% of the machines, we will see at least one correctable error in a day. 10%. And when it comes to uncorrected errors, it's not a huge number. It's all of magnitude lower. So it's roughly, we can say 1%. So if you have 100 machines, in one day, we will see at least one machine experiencing  uncorrected error. It's not a huge number percentage-wise. But if you look at the scale, when you look at the edge scale, and we have thousands and  thousands of machines, the uncorrected error numbers per day increases. And the challenge is with uncorrected error, first of all, correctable error. Correctables are correctable, right? So there is not a huge impact to the machine operation in terms of functional operation  of the machine. But with uncorrected error, machine typically would crash because it's uncorrected error. Hardware could not correct. So as the fleet size increases, the impact of uncorrected error is experienced and huge,  even though percentage-wise, it's only 1%. So therefore, we need to think through and design a system so that we can handle these  uncorrected errors more elegantly and gracefully, rather than crashing the machine. Now good thing is in the spec, PCI spec, there is a feature defined called DPC, downstream  port containment. It's like really magic. Uncorrected error turns into a correctable event. So we looked at it, and we actually have incorporated this in our Grand Teton machine. So let me kind of talk about that in more detail.

And I'll give you a few examples. So first example is a PCI-- so first of all, on the right-hand side, what I'm showing is  a very simplified view of the whole stack. So at the bottom is devices. So the NVMe drives, as Gautam mentioned, in a typical Grand Teton system, we've got GPUs,  NVMe drives, NICs. So these are kind of main type of devices. And then the switch, the purple color box is a switch. So there's a downstream port, upstream port. And then the CPU, the control plane. CPU itself has a root port, and then the memory and the cores and everything. On top of that, now the software stack and firmware stacks comes in. So platform-specific error handler basically is nothing but a SMM handler. It's a firmware. PMC is out-of-band module that manages all the error reporting and logging. Then on top of that is operating system. The operating system, in this context, the key thing is the green box, the AER and DPC  handler. AER is advanced error reporting, and the DPC is downstream port containment. And of course, on top of that is our own DC tooling. And left of OS is the device driver. The device driver is the one that handles and manages the devices, either NVMe drive  or GPU or NIC. So now if you look at this example, let's say there's an uncorrected error detected  by the switch at its downstream port. So the link between the device and the switch is where there's an error detected. And it is uncorrected error. If the DPC is not enabled, guess what? This will become a fatal event. Right? Machine will crash. With the DPC, the magic happens is that first of all, the containment happens. That means the link goes down. There is no-- this is actually at the transition level. The transition at which the uncorrected error is detected, that transition itself will not  flow above beyond the switch, means to the CPU root port. Or if it is going down, then it won't go down to the device. And then the link will go down. So that's where the whole containment happens. And then now the signaling will happen. Because the error is detected, that whole signaling will flow from switch to the CPU  and then onward. So in this case, the example is a surprise link down. Surprise link down is an event that causes link to go down. And when that happens, the DPC gets triggered, number two step. Number three step is the signal called error correct signal. Because now even though it's an uncorrected error, but it is signal as a correctable error. That's the magic. So that error corrected message comes all the way to CPU root port. And then the root port is going to pass it-- trigger something called SMI. Or an SMI handler comes in from there. And it's going to look at the cause of this event and see, oh, there's a DPC event happened  here. Let me collect all the data and send the message to the OS. Because the device is now gone. Because the link is down. So we need to make sure the device driver is notified and is unbounded, unbinding, or  removed. Otherwise, the device driver doesn't know and then keep pinging. And then it's going to cause all kinds of other problems. That's where the crash happens eventually. So device driver gets notified to the OS AER DPC handler. And unbinding happens there. And in addition to that, the firmware is going to also send a message to BMC to log the event. Because we want to make sure that this event is logged in a persistent store. And BMC is used in our fleet as a persistent store for the-- within the platform for any  kind of error. Today, we use cell log. We would like to use the OCP-defined CPER format also. So the cell log is captured in the BMC. And now once the OS gets the notification and the device is unbinded, now the next thing  is we need to recover. Because link is down because the DPC event. So the OS is going to now clear the DPC status, which is the step five. And that will trigger link retaining. And also a hard plug event. Because now the device is visible again. So that hard plug event or hard plug driver is going to get a message. And the driver will again be reloaded. And once the driver is reloaded, then the whole device is visible. And the rest of the stack is initialized. So this is how, in the case of a surprise link down, the containment is achieved. And then after that, the whole system recovers. So this is a good example where the whole DPC flow worked. And uncorrected error is contained. And the system downtime is not experienced. There is no crash. Now life is not that easy. This is a good example. This is a simple example. Let's take a look at the more complex example.
 
Now in this case, an uncorrected error is detected at the CPU root port. And of course, the same thing will happen here. And in this example-- so the example here is the NVMe drive. And NVMe drive firmware got corrupted. So any time there is a firmware corrupted, if somebody access the drive, guess what? There will be a company timeout. Because there is no response from the drive. So the root port is going to detect a completion timeout. Now completion timeout is an uncorrected error. So root port is going to trigger DPC. Link will go down. Link-- at this time, the link between the CPU and the switch upstream port is the one  that goes down. Now what happens then? When that link goes down, any devices behind the switch-- because there are multiple devices. We saw the block diagram. There are multiple GPUs, NVMe drives, NICs. They all will not be visible to the surface stack. So even though there was only one failure on NVMe drive, one NVMe drive, but it appears  like, oh, lots of devices are gone. So now in this case, what happens is because there are multiple devices that are involved,  and the OS gets the notification, OS will try to unbind all those device drivers. The challenge we faced in our design as we-- through the whole journey of enabling this  feature, not every device vendor and device support this hot plug concept. So some of the device drivers, we were not able to unbind. And when that happens, then link will never recover. So the whole recovery flow fails. When the recovery flow fails, that means from the user point of view, the application point  of view, all we'll see is, oh, a bunch of devices are gone. That means the failure could be any other device. And based on our tooling and the infrastructure design in such a way that is going to trigger  remediation for all the devices, especially the GPUs. Now GPUs were not part of this failure, but still it'll result into GPU swap. So that's not a very good experience. So to overcome this-- so in this example, by the way, for the firmware corruption, we  figured out a solution, at least to find out, narrow down the failure. That is, this was the NV drive firmware. NV drive failure, and based on our heuristics and learning that most of the time is the  firmware, we kind of make the correlation and then came up with our-- what we call through  the repair brain and all that-- a solution to take the right action. But there's still some gaps here, because a side effect of that is it will also trigger  GPU replacement. And we had to wing through that. But rather than doing that, we actually made it a little more different, redesigned the  whole DPC flow.

So what we did, we still maintained the DPC on the switch downstream port. But the CPU root port, we disabled DPC. Now in addition to DPC, there is also something called root port I/O error reporting, RPP  I/O, which covers the error which are not covered through the AER. For example, if the device detects a unsupported request, UR, then of course, device will log  UR event, but also it will send the completion with UR. Now that is not covered through AER. AER and the root port will not log that event. To cover that, the RPP I/O concept is introduced, which actually rolled up into the DPC, and  just we call it E-DPC, E-enhanced DPC, which is basically some of these additional error  types. Those error types are very useful for us. So what we did was we enabled RPP I/O at the root port to get those signals. Because any kind of error signals are very useful for us, especially when we are operating  in such a large fleet, fully autonomous, as much autonomous as possible. Any kind of error signals are so useful for us, every breadcrumb. So what we have done is we have enabled RPP I/O so that we can get those errors also,  in addition to the AER errors. And most of those errors we have-- currently we are going with a fatal event. That means all the errors will run fatal event. At least we want to crash the machine rather than-- so there's containment on the root  port when an unconfirmed error is detected. Basically our containment strategy is to crash the machine. Now it kind of worked for us so far, because we have the nice containment on the device  side on the switch-to-automation port. And the probability of errors on the CPU root port, of course, is lower. So we can still manage that. It's not the perfect solution, but it has been working for us.
 
So what are the key learnings from this whole experience of DPC? So first of all, DPC is required. We learn, because if you don't enable that, guess what? If you don't enable it, if there's a completion timeout, what happens? If there is some-- let's say we have a read from the flash, and there's a completion timeout. The driver who requested or the application requested the read data will get something,  which we call FF, or all ones. And if the driver is not resilient enough, then it will consume the data happily. And that can cause all kinds of data corruption problems. So we obviously cannot manage or cannot tolerate such kind of failures or events or situations. So containment is very important. We have to enable it. So that's something we learned, and we have enabled it. Recovery may not be possible based on our learning, especially when we are designing  these complex machines with multiple vendors. If I had to design everything from scratch, I would have designed it right from ground  up by talking to each vendor, hey, do you support that? And make sure your driver supports the hot plug, and then design it. But that's not the luxury we have. We have to pick the components, because we are working on this system design in such  an early phase. Not every device vendor supports the hot plug. So recovery may not be possible. But that's OK also, because we have managed so far. And the third thing is, DPC is a complex feature. As you saw that, there is hardware involved, firmware involved, OS, driver, even applications  also. So validating this whole flow is always a challenge. So make sure you think through as you're designing, and system level validation, how do we do  it, and what are all the tools required, is something that's important, and keeping that  end-to-end flow in mind.
 
So call to action. First of all, when it comes to the PCIe fault domain, as Gaila was mentioning, there are  correctable errors, uncorrectable errors. The good thing is that they are corrected, but still, there are a lot of correctable  errors happening. That's not good either. Some action needs to be taken. And we have implemented quite a sophisticated logic to collect every correctable error possible,  and then compute the error rates. And then based on our own heuristics, we have developed a pretty sophisticated solution  to determine at what error rate we need to take an action. So that so far has been working. We are hopeful there is always room to improve that. And then DPC, and there is another feature called ECRC, end-to-end CRC. We have enabled that also. So basically consider all the features available when you're designing such complex machines. And so far, we are using PCIe Gen 5 and all that. Down the road, PCIe Gen 6. The higher the speed, a lot more signal integrity challenges are we going to face. And therefore, this is not the end. This is just the beginning. Yesterday, we learned in the keynote from Meta, so far we are just 1% done. We are just beginning. So life is going to be even more interesting. So think through, consider all the features available so that we can minimize the blast  radius as an impact of this correctable and uncorrected both. Progress in developing diagnostics, remediation, repair solutions for precise through identification. Now in the OCP, hardware fault management, we are actively working on these kind of problems  and collectively thinking through what kind of solutions we require, what are the key  requirements from the vendors, from the software developers. So feel free to come and join and contribute and learn, of course. So join OCP hardware fault management subproject. And for any other questions specific to this presentation, feel free to reach out to either  myself or Garda.
 
And I think we have some time. Yeah. We have some time for questions. I do have one question. So for your DPC policy, do you allow mix of devices where not all are capable of doing  ADPC or you require all the devices to be?

Yeah, so if you look at the spec, right, DPC is really the switch, downstream port or root  port. So device really doesn't really have anything specific about DPC. So devices, if you talk to any device vendor, they'll say, yeah, we support it. Because from the hardware level, there's no specific requirement for the devices. The driver is the one.

 Yeah, that's where I was going to go.

Oh, OK. So yeah. So we have that, right? So we have the devices where some devices do not support hot plug. And actually, the DPC really is the hot plug. Hot plug support is what we need there. Right.
 
So I think your thing is not only that you need the whole hot plug infrastructure support.

Exactly. Yeah. All right. Thank you. Yeah. Any other question? Go ahead.
 
I'm Derek from Google. We have very similar problems with the scale and obviously with validation as well. That's something that you mentioned is very important. Obviously, it's really hard to-- it's hard to just-- you don't want to wait for this  to just show up in the fleet in order to validate whether your flow is working correctly. So I'm curious, what are your approaches to doing the validation? Is it a mix of simulation, injection? What kind of approaches have you guys considered?
 
Yeah. It's a difficult problem, right? Because in order to test this whole flow, injecting error is very important. And unfortunately, the error injection, if you look at the industry-defined solutions,  there's really only one standard, which is EINJ. And EINJ really is defined for the root port. So from a device point of view, we have to work with each vendor. So what we have done is we have worked with each vendor and looked at their own solution,  right, to inject errors, either in-band or out-of-band, and then integrate into our own  validation flow, and then test it out. It's not perfect. But the best we can do, that's what we have done. And then we keep pushing vendors to provide us the error injection capabilities that are  uniform across vendors and discoverable, easy to use, not like we have to load a bunch of  software drivers to enable that. So it's a difficult problem. We have somehow we managed it. Of course, we leverage ODMs a lot. And with the ODMs, they have the whole lab environment. And so, for example, there's a Keysight error injection card, which is designed in the industry. Keysight is the company. So they have set up a system where they've installed that card. And now that card can create different kind of failures. So that's one scheme we have worked with ODM to deploy that. So this is an area that there is still a lot of room to improve.

 Thank you.

 Yeah.
 
Francesco from META. In terms of error injection, out of a failed experiment, we actually know that we might  be able to introduce error using the lane margining feature when an independent sampler  is not employed. And that might be a good way to introduce errors at the hardware level, pretty much. You also mentioned that in DPC, we can avoid the possibility of reading an FFF all the  way to the application layer. Does DPC guarantee that we will not see that? Or does it happen afterward? How does that work?

 Good, good. Good question. So DPC feature, the way it is defined by spec, doesn't provide that clarity. DPC's main goal is to make sure machine doesn't crash and contain. How do we handle it all at once? All of them will still happen, even after in the DPC. In the example we looked at, CTO event, there will be all ones. So now the thing is, if the driver is DPC aware and hot plug aware, then the likelihood  the driver only supports how to handle all ones, exception handler. But that's one of the fundamental requirements for hot plug. So for example, NVMe drives, most of the vendors supports how to handle all ones. And we have no problem with that. We test it, we use it pretty successfully. Other devices, I won't give the name of any other vendor, but there are a lot of other  vendors where they don't have that support. So we still have that challenge.

 Yep.

 Last question.

Sure. My question is around, if you see these devices which are tend to fail because of a lot of  these correctable errors, have you looked at doing a secondary burst reset or a function  level reset, or even toggling the burst to ensure the device is back up and it retrains  the link so that it will have a better lane margin or something? I mean, at runtime, a lot of these things happen, right? So that internally, there would be a DPC event, but it would be not catastrophic, right? So by doing this kind of runtime resets, hot resets, right, could eventually have a better  signal strength and not eventually cause the system to crash.

Yeah. So let me first clarify something. Correctable errors not necessarily turn into uncorrected error, unless there is a really  physical link issue and all the retries fails, right? Then it becomes a surprising link down, right? So correctable errors, there are lots of correctable errors happening due to some stuck lane kind  of thing, as Francesco was talking about, right? In those cases, secondary burst reset would be useful, right? Do we do that today? I don't know. Can we do it? Yes, we can. Right? Now, I do know that in case when our fleet is monitoring the correctable errors and detects  the rate exceeding some other threshold, we do take an action of resetting the machine  first before we take any other repair action and see if the problem still persists. So we can't do the whole reset, but secondary burst reset would be a very useful thing and  we should definitely look at it. I'm pretty sure we may be doing it also, because there are so many different kind of machines  we have deployed. Not every machine probably will do secondary burst reset, but some machines may be doing  that. And uncorrected error, there is no secondary burst reset. I mean, we don't have opportunity to do secondary burst reset, right?

So that's not really the scope for this. So, I guess, like, a lot of correctable errors are leading to an uncorrected error.

 Yeah.
 
So that's a surprising down, right?

So before we get surprising down, yes. Before we get down. Yes.

Okay. Thank you. All right. I think we are out of time. Let's thank Anil and Gaurav, our speakers. Okay. We have one hour break and then we'll resume at 12.30. Thanks everyone.
