
Hi, we're here to talk to you about designing and optimizing CXL configurations. I'm Andy Banta, I'm the storage janitor at Magnition IO. I've been doing storage development for about 30 years now, but ironically enough, this is only my second year at the Storage Developer Conference. I'm joined here with Grant Mackey.

I'm Grant, I'm the CTO for Jack Rabbit Labs. I'm Grant Mackey, I'm the CTO for Jack Rabbit Labs. We're an open source company for CXL and memory fabrics. I've done stuff at big companies, and I've done stuff at labs. That's me, thanks.

And I'm Craig Carlson, a storage and networking architect at AMD. I've been in the storage industry for the last 25 years, working on things like fiber channel and other storage networking technologies.

And so I'm going to talk a little bit about CXL and data center challenges. So you've probably all heard about the scaling challenges going on with data centers today, especially with the increasing core counts. Cores can be a good thing, but they also can be a bad thing when it comes to scaling your memory. For traditional applications like virtualization and such, each core can require a minimum amount of memory to be effective. And if you start increasing these cores, not only do you start putting a burden on the system to be able to handle all this memory, but you also start increasing your cost. And sometimes that cost can increase non-linearly because as you increase your memory, you may be getting into denser memories, which may cost more. So this really does create, for just traditional data centers, this really does create a problem. And if you enter AI computations into the mix, then with some of the memory requirements for AI, this problem becomes almost unsolvable in some cases because the amount of memory required can sometimes outstrip the amount of memory that the system can handle.

So I'm sure you've all heard about CXL and high bandwidth, low latency, cache coherent memory. It uses flexible processor report that can accommodate both PCIe and the CXL protocols, based on the base PCI Express 5.0. And it also has different protocol layers in it that allow for IO cache coherency memory access. And it allows for creating disaggregated memory—not only in tiers, but in pools as well, which is something which is very powerful for some of the memory constraints that are happening in data centers today.

The tiers allow you to create different levels of memory. And the strength in this is that some of this memory, because requirements are not as close to the processor, may not need to be as fast. So it could be cheaper memory. You can build bigger memory pools, which allow you to share the memory between different systems. Therefore, also potentially reducing your memory cost because you're not committing all the memory to one system, which may not use it all. You can pool it and make it composable so that the memory can be placed where it's needed. Of course, your downsides to this type of architecture are that the farther away you get from the processor, the more latency you have. So these tiered memories have higher latency, which can result in slower performance. Different applications have different reactions to this type of performance. If you saw the talk earlier this morning, you could see some applications don't have much reaction to it. Some other applications slow down a lot. You also need the software infrastructure to manage this memory. And traditional operating systems understand the difference between storage, traditional storage (SSDs, hard drives), and main memory, but they don't necessarily understand the memory tiering idea. So there also needs to be a software infrastructure as well to make this work correctly.

So, a little bit of what we're talking about is that traditional architectures are hitting a limit. And this is something that is causing a data center a lot of headache. We call it the memory wall. Not only are memories increasing to the point where the systems can't hold them, they're also increasing to the point where they're getting very expensive. So, the tiered memory architecture allows some methods in some ways of relieving some of that. And so I think it's a good idea to have that.  And I think it's a good idea to have that. And I think that architecture allows some methods in some ways of relieving some of those issues. Of course, at the cost of some other issues in the software and the execution. One of the things that's very good for CXL in the solution space is that CXL is an open-source technology. There's a robust, uh, ecosystem to develop it, you know, there so you have standardization, interoperability, collaboration across many, many different companies and many different providers, and it provides an end-to-end solution which is open and brings all players to the table.

No CXL presentation is complete without a timeline. So, the CXL timeline: version 1.0 was released in 2019. Version 2.0 was released in 2020, and that's probably what you're going to start seeing coming out in hardware platforms today, just based on the development time for building out hardware. And version 3.0 was released in 2022, and in the next two years, you'll probably start seeing some of the hardware platforms coming out with that.

So, we'll talk a little bit about, I think, the software side of this—the stack that makes these devices more interesting for endpoint applications—and how you can start playing around with some of those things while hardware availability is a bit of a challenge.

So I think people have seen this, right? But essentially you have this single-node resource constraint, and so people are talking about direct attached memory, and that's now, and that helps to alleviate some problems with things that just need better than NVMe performance, right? And as we move forward in the technology, you'll look at, okay, well now I have pools of memory because I have a switch, and I can do some mild composability. Or I can hawk around 20 gigs there and 15 gigs there so that applications don't oom, and that's nice. And then further, I'll get into this point where I have shared memory pools, and those are interesting things and prospects for applications and users, but with all of these things, it brings further complexity to the software stack that I don't think has been addressed at this point, simply because people are trying to existence-proof the hardware and the driver stack and the Linux kernel so that these devices can be tested and hardened. But we haven't really thought about like, how do you put Kubernetes on this, right? Those are things that haven't been talked about.

So I think that needs to be done in an open-source way, and that's great for us because that's what we do, right? But when you have these new types of technologies that come out that are somewhat nascent still, you have this propensity to build a lot of proprietary solutions because it allows you to move fast. And there's nothing wrong with that because you want to show the coolness of these technologies. But historically, proprietary solutions haven't played well with each other just because they all sort of went in generally the same direction, but not closely enough. And so, to take advantage of CXL and composable memory systems as quickly as possible, an open-source solution, an open-source approach is probably the better way to get everybody headed in the right direction because it's going to touch everything, right? This technology is going to impact all of these layers of the stack in different ways.

These are a few things that we've identified, and this is an eye chart. If you can find the spelling mistake, I'll give you a dollar. These are the things that we think are sort of nascent right now that need some work, that we're taking a look at. So CXL switches are brand new, and the idea of them having any sort of smarts beyond just routing traffic is sort of fresh. The interposer layer between resource schedulers and CXL fabrics is nascent. Application integration plugins, these sorts of things, need a lot of work. And because the hardware is sort of hard to come by, any sort of development on it can be challenging. Without, like, an emulation or simulation framework.

For this reason, right? They don't want to know how it works. They just want to use it, because Kubernetes and Spark and all these other things are good at scheduling resources. They're not good at speaking network protocols and memory fabric protocols and storage protocols, right? Paid to lead, not to read.

So this is one thing, how we think it can work, right? For Kubernetes, for that matter. So the stuff in TEAL are things that we're working on right now, most of which have been open source at this point. You can check out our GitHub. They're all Apache V2. And all of these different components, in their various states of completeness, are becoming complete. We developed on QEMU and the CXL emulation devices that they have on QEMU, because, you know, it's just, it's difficult to get ahold of switches and different resources that can speak, like 2.0 and DCD and stuff like that for CXL. So the thought pattern here is that, you know, you have controller nodes, you've got your orchestrator servers, you've got your worker nodes, and then you have your fabric resources. And there's a bunch of different ways that these things can communicate with each other, but the point is, is that these are familiar pieces of the stack for the Linux community and for the device maintainers, so that all of the libraries build on top of each other in a way that's meaningful to bring value to these larger resource schedulers that are deployed in these very large data center environments, right?

So, this is one of the few things, this is one of the things that we've put together. It's been a little old now, but I mean, it's still relevant. So, this is it's two things. It's the Jack, which is a CLI orchestrator for CXL Fabric, and then the CSE, which is what we call the CXL switch emulator, which at this point should probably be renamed, but it allows you to speak FMAPI compliant commands to something that behaves like a switch in QEMU, or outside of QEMU, which is why we called it an emulator to start. It'll allow you to either plug virtual QEMU devices into it, or real CXL devices into it, and speak Fabric compliant stuff to compose your memory pools and carve off resources, VCSs, stuff like that. This is all available now.

And this is what we're working on now. Again, all in QEMU, wherein you have the different pods, the different VMs, and they're all, for necessity, with QEMU, and how it exposes virtual devices. It's nested QEMU instances so that they can all see the type three devices, and they can all be scheduled and stuff like that. It takes some liberties with memory safety out of necessity so that all the devices can populate across all the different VMs that are nested in the QEMU VM, which is not ideal, but it allows you to existence-proof these different software stacks, and harden how the communication between these different stacks works, and like... Online blocks out of offline blocks create VCSs, tear down VCSs, associate devices with everything, in a way that you can then obfuscate those mechanical processes for a CXL Fabric to a broader resource scheduler. The goal here, like Andy will talk more about performance tuning and applications, stuff like that, for these types of workloads. For us, the goal here is existence-proof, and making sure that everything communicates in a way that when hardware does exist, these libraries should just plug in. Should, in quotes, because that's what you do, right?

I'm not gonna subject you to a YouTube video, but we have some videos on our channel which talk about how you can use these tools and what they're doing, and if you had the hardware, it would work on your hardware, right?

This is a wall of text that you can read later. That's our QR code to reach out to us, but these are all the different resources that are kinda out there right now. So, yeah.

So, I'm here to talk to you about how we can actually try out CXL before you can go out and buy it. So, let's go back to some basics first. NUMA came around with the idea that you would have multiple different banks of memory, so that it's non-uniform memory access. That's the whole point of it. You would have directly connected DRAM to your CPU, and you might have a partner CPU that has DRAM connected to it. CXL brings up the idea that you could, when the two CPUs communicate with each other, the access to their, the two different memory banks, is not going to be the same speed. Andrew and Heiner talked about this earlier with their simulations. CXL brings up the idea that you can have locally attached CXL, and this could be inside the box, or this could be outside the box, and what Grant and Jack Rabbit are talking about is the idea that you can have a switch outside the box that has CXL connected to it as well. And when we get to the idea of CXL 3.0, you could potentially have multiple hosts, connected to that switch. So, if you look at this, we end up with one, two, three, four, five, six, seven, and contention, all working on these various different NUMA nodes that are available to you. This means that you have not just one or two tiers of memory, but you potentially have seven in this example, and this is not a complete example, this is just an example that I threw together for you to look at, but there's, there's going to be more than just two tiers of memory that you need to work with, and you need to start thinking about CXL memory in terms of more than just one or two tiers.

So, you look at this diagram and you say, 'Wouldn't it be cool to try this?' Wouldn't it be fun to be able to put together a CXL system and see how performance works on this thing? Eh, not so much. Can't do that yet. Not all of these CXL components exist. This is part of the reason Grant and Jack Rabbit are working on a simulated CXL switch. This is why there's a handful of other people, Adam out here, working on simulated CXL devices. This is why none of these devices don't exist yet, but it would be nice if we could actually go out and try it.

So, if you think about test-driving a car, there are certain limitations that you, to a test drive, a car, you typically are going to a dealership in a city, and you're not going to be able to take it out for a long drive. You're not going to be able to load it up like you're going on a trip, and you're probably not going to have a chance to go out and toss it around a country road to see how it performs. A CXL test drive should be better.

You should be able to try out different workloads on your CXL test drive. You should be able to try different capacity planning, different data placement, different interconnect speeds, and it should be something that's adaptable over time. You shouldn't be stuck with one setup. You should be able to try things that are different. And what you really want, is something that's predictable, deterministic—sampling or examples of what the CXL is going to do. And when you think about it, this is actually what engineering is: engineering something so you understand how it's going to work when you're done with it.

And in reality, engineering means simulation. If we look at electrical engineering, they typically do loads of simulations before they even, before they even think about putting, laying down silicon for anything. Chemical engineering is largely the same way. Some of it is figuring out how to do molecules properly. An awful lot of it is actually finding the resources that are being used. Mechanical engineering does all sorts of stress tests, you know, figuring out how strong metals are, figuring out how much power you're going to need to do something. This is all done in simulation before somebody even attempts to build a car. Civil engineering, along the same lines. And bioengineering, there's the idea that you're figuring out how DNA fits together. You're figuring out how medicines work best by rearranging molecules. All of these engineerings use simulations before they even attempt to do anything. That's because it's cheaper, faster, and easier than actually building it and saying, 'Oh, that didn't work, let's start over.' Isn't it about time that software engineering and system design, it's about time that we took the same approach? I was talking to a company named Enfabrica last week, and they said that they were, in some of their AI modeling that they were doing, that the highest GPU usage that they saw in running any of the models was 30 to 35%. And they said, once they started researching this, they looked at it, and the best software design out there would have a theoretical, would theorize it, would theoretically be able to drive a GPU at 60 to 65%. So this is saying that the software is inadequate of taking advantage of the full capacity of the processors. So if we started engineering software to take advantage of the full capabilities of a processor, you know, we might be able to make use of all the silicon that's out there.

So let's talk about how you would build a CXL test drive. Magnition, the company that I'm consulting with, has built a large-scale simulation framework. So this is a discrete behavioral simulation. It's a framework that can be distributed. It can run loads of different components, and you can build whatever components you want to fit into this thing. And the components are typically very simple to put together, and you can interconnect them the way you want. And the whole point of this is to build out a simulated system where you can test not just the correctness of it, but performance and the repeatable performance. It gives you deterministic results every time. So the way that we're approaching this with CXL is we want to run various different workloads in it. And for, you know, one of the possible workloads that you could run in this is a trace replayer, and this is something that was talked about at the UCSC talk earlier today, where they were running traces against it. But one of the problems with running traces against it is that you've stuck yourself with the exact sequence of events that were in that trace. You cannot vary the sequence of events based on the timing of the various different things that happen. You can also run artificial or synthetic workloads where you come up with something saying, this is an algorithm that kinda is sort of like this other thing. But I think the most valuable thing is, you know, this is an algorithm that kinda is sort of like this other thing. But I think the most valuable thing is, you know, I have a workload and I want to see how it's gonna work in a CXL environment. So let's be able to, let's run a genuine workload. And the Magnition framework allows you to run a genuine workload and capture the load and stores of any given application and be able to feed them into a simulated memory subsystem. And that memory subsystem would be potentially connected to local DRAM and a swap device. And, if we're going to be simulating CXL environments, it could have local CXL. And since we are talking about switched environments with Jack Rabbit, et cetera, it could be connected to a switch and that switch could have far CXL associated with it. And I think we need to start thinking about the difference between CXL just being far or CXL being like a switch far away. Because these are two different configurations of how you would do it.

So the way that we do this is, in our framework, we build small modules that simulate very specific things. So, this could be a simulator for a memory subsystem. It could be a simulator for DRAM. These simulators can be as simple or as complex as you need them to be. This is a very simple one because it's just for an example. You could say, 'DRAM typically has this type of delay.' So, let's say the DRAM has this type of delay plus or minus a small amount. But you could also build into it contention. You could build into it various other different factors that you might have in a memory subsystem or an SSD. We have a very complete SSD simulator that simulates the complete working of an SSD down to the internal queuing, the trash/garbage collection that goes on in an SSD. These components can be as simple or as complex as you need them to be to make your simulation as correct or as loose as you need it to be.

And once you've built this, we do have a way of graphically representing this. So this is what a representation of a CXL example was put together. And I don't expect you to read this. The idea is that you can graphically see the components and the interconnects between the components, and you can drill down on them. You can click on any one of those, and you would see the components inside of that, the subcomponents that are in there. And then we have a configuration file that goes through and says, 'Here is the exact configuration I want to run.'

And this is the CXL configuration file we used for our CXL simulation.

A different example would be a content delivery network. And I brought this one up simply to show you that the exact same configuration file, could be used to simulate very different environments. And if you notice, there are some variables in here. What the Magnition framework is going to do is it's going to run through each one of the variables that are there and give you a simulation of all of these different points. And when you multiply these out, lots of times you will end up with millions of examples. And that is exactly what the Magnition framework is going to do. It's going to run all of these different examples and plot out the results for you and give you any nature of graphical views that you want. You can put together your own graphs through a Jupyter notebook, or you can use some of the pre-programmed ones we have. What I'm going to show you today are going to be some very simple graphs. If you want to see the depth of what we do, I am doing a content delivery talk on Wednesday where I go into the details of a research project that an intern did for us and show you the wide, wide range of results.

But the reason that I want to talk about this for CXL is something that Craig pointed out, and that was brought up in the SDXI talk right before this, that current processors have hundreds of cores and hundreds of PCI channels. And you can put together clusters of numerous different nodes. And this means that just about any application—application you have is never going to be compute bound. There's almost no way that you can have a compute bound application these days with the number of cores that are available, the number of GPUs that are available. As I mentioned in the other example, the way you end up being GPU bound is your software isn't capable of taking advantage of more of the GPUs that are what are there. So everything I'm going to talk about here is nothing is compute bound. It's all going to be memory bound.

So here's a couple of experiments where we put together three different applications that use roughly the same amount of memory, but use them in very different ways. The way that they allocate, deallocate, read and write those memories, that memory is very different. And we hooked it up to a memory subsystem. And just for a first-off experiment, we said, okay, there's DRAM and swap attached to it. And any one of these applications will fit into deep RAM just fine, but no two will. So the first experiment we ran was to just run one application and see what our performance was. And then we ran two applications to see what our performance was. And then we ran all three of the applications to see what our performance was.

And again, very simplistically, I realize the numbers are very small here, but I'll tell you. The green bars show the total run time of the entire experiment, and the blue bars show average memory latency. And you can see with one application where we're using entirely DRAM, the average latency is undetectable. You can see as we add applications, each one of these—all the applications—take longer to run. And I forgot to mention this: each one of the applications had roughly the same amount of run time. So if you ran the application start to finish, the run time would have been the same. And here you can see as we add more applications, the run time goes up. And the top of the blue bar says that we're taking almost 200 microseconds for dual memory access when we're swapping. And part of this is that swap, swap isn't even like tiered CXLs. Swap, if you have a memory miss, the process goes to sleep waiting for the memory subsystem to satisfy that request. So it's not a matter of just a slow memory request. It's a matter of the process sleeps waiting for memory access to be satisfied.

So let's talk about the second experiment. We have the same basic idea: we have the three applications, we have the memory subsystem, but we're going to talk a little bit more about this. We're going to stick in local CXL, and we're going to stick in CXL attached off a switch. But in this case, swap doesn't matter; the swap is attached, but we should never be touching swap. In this case, we're going to treat all three of these memory regions as monolithic. This is just one big memory pool that the system has. So we run the same experiment again.

And we go forward instead of backward. And we can see that we run one application, and again, the scales are very different than the previous one. You can see that we're talking maximum about 200 nanoseconds of latency on memory accesses here. But as we keep adding applications, all three applications take longer. And this really doesn't make sense because, as I mentioned, we are not CPU bound. This is not a CPU resource issue, but as we keep adding applications, the memory gets spread across more, very different types of memory, and we don't have consistent type runtime on our applications.

And this drives performance engineers nuts. If you can't get consistent runtimes out of your applications, the performance engineers are going to be tearing their hair out, saying, 'What's wrong? Why do I not get the same results time after time?' And Linux has this cool utility that allows you to say, 'This process should be allocated this memory from these NUMA nodes.'

So we rerun our experiment, but instead of having one big monolithic flat of memory, we assign the local DDR to application one, and we assign the local CXL to application two, and we assign the far CXL to application three.

Then we run the results again, and you can see all three of the applications run in the exact same amount of time, the way we would expect them to. The latency, whether we run one, two, or three, is consistent across each one of the runs.

So for comparison's sake, on the left, you have what we did in experiment two, where you can see that each one of the applications takes longer to run as we add the applications, and the memory latency increases for each one of the applications as well. When we assign the various different CXL regions appropriately, we end up with very consistent run times, and the scales aren't the same. I apologize for that. Here on the right side, all the applications run in about 50 seconds. And on the left side, all three of the applications, when you're running them all together, take 80 seconds to run. The latency is about the same. But it's a matter of you get very consistent results, and this keeps performance engineers happy.

So what I really wanted to get to here was that you can model CXL systems, and you can try out the various different pieces you have. You can just say, "This is a generic piece of CXL equipment, or you can do something like Samsung has done, and you can say, "I have elaborately modeled a CXL device. You can do the same thing with switches. You can simply say, "A switch is a multiplexer, or you can do something like Jack Rabbit has done, and accurately model a CXL switch. And these are things that can be plugged into the Magnition framework. This is a nice concept, the idea that you can actually run your own applications against these, rather than just running a benchmark or running a trace to get an idea of how it's going to work. You can go through the configuration files and say, "These are the things I want to vary. And if they're not getting you the results you want, you can run different simulations. And vary those as well. Keep in mind that these simulations are running on logical time. They're not running on real time. So they actually can run a lot faster than in real time, because anything that would have been dead time in a real application can just be ignored. We don't even, that doesn't exist. And you can use this to take a genuine test drive of a multi-tiered CXL system and see how it's going to perform. And see how you can tune it and optimize it for your work environment.

So let's talk about what we talked about. Let's reiterate what we talked about today. I think Craig kind of summed this up. There's an awful lot of flexibility with CXL, but it also, with the multiple tiers, presents a lot of complexity. There's an awful lot of innovation going on, especially in places like AI, AMD, and various other companies as well. There are companies attempting to put together what Grant refers to as middleware, and there's a lot of innovation there. And the more people who join in, the better it will be. And you can actually try various applications and figure out how to get a large-scale CXL system running before you can buy the hardware. So, thanks for your time. Any questions?

It's really, some of the slides you showed aren't in the package we downloaded.

Softball question. It says some of the slides that are in this presentation aren't what's in the downloadable. And yes, that's because the three of us actually finished up the slides this morning. And I will make sure that all of the slides get uploaded to the portal.

Thank you.

Adam, what a surprise! Adam, you have a question?

So, now the three of you have come together and worked on this, right? And you're putting these pieces together. What do you take away as the way to drive CXL? You know that they're saying there are limits here, right? Is there some configuration that you think is like, 'Hey, we see this here,' right? Have you come to something?

Yeah, and the question is, do we know of a configuration that makes the most sense? And the answer is, I certainly don't. I don't know if these guys do. I don't know if your microphones are on. But I think it's more imaginative. It's a matter of we need somebody to come to us and say, 'I have this application. Can you help us come up with the best CXL configuration for it?' And that is the proper way to do it because I would much rather have the question posed that way than saying, 'We've built this CXL system. How do my applications run on it?' Because that's doing it wrong. You want to have the applications, figure out what you're trying to do, and design the CXL system to work for it rather than the other way around.

So, what's your question? What's the killer app question? Kind of.

The answer is all is AI.

There you go. We're done. Good night, everybody. Yeah. Well, I mean, so this is interesting, right? Because there's some folks that say, like, 'I don't see the point of CXL.' And there's other folks that are like, 'I see CXL being useful for memory pooling.' And then there's other folks that are like, 'Okay, well, I have actual random walk applications and a little bit of coherency would be nice, right?' So I think that it comes down to what's your core application and does it need local DDR latencies? If yes. If yes, then probably not this. But, you know, if you're bandwidth bound because you have core stranding, well, that's not the same thing. But basically, like, if you have too many cores and not enough memory bandwidth, this might be fine, right? If you would love to give your InfiniBand network a break, this might be a good idea. And I think just as the hardware ecosystem matures, because we have to move at the speed of silicon, more people will start playing around with small-scale hardware instances. And I think we're just hoping that folks start playing around with emulation and simulation first so that they have a better idea of what they want to buy when the hardware is available to buy.

One follow-up, maybe that's for SNIA. It would be a good fit, right? Like, you know, of some application that does work well, you go through this exercise using all of your tools, and then you publish your results somewhere that somebody can go back and look at them and feel more comfortable, like, 'Hey, I can relate to this.' Yeah, like, white papers might be very useful.

Yeah, so the follow-up was, 'Why isn't there a white paper?' 'What have you been doing?' And, no, I do think that's a good idea. So, having, and I think there are a few efforts, and this goes back to, like, the fragmentation thing that I was talking about, is that folks are trying to put together, like, 'What should the testbeds look like?' 'What should the workloads that sort of scope the multifaceted Venn diagram of where these different points in a CXL ecosystem make sense?' I think that a few organizations are working on that white paper. I'm not sure how it fits in SNIA at this moment, but it's not a bad place for it. Yeah.

More questions? And if you're intrigued with the talk about the simulation piece, I do recommend that you come to my session on Wednesday morning, where I spend a lot more time talking about how the simulation actually works instead of how to apply it to CXL, because it goes into a lot more detail, you know, this is exactly the way that we put together the simulation, and these are the exact results we got from it. And here's the various different things that we changed to see the results we got. 

Having been at FMS a few weeks ago, the majority of the talks focused on power. And so far, what I've seen with the SPC so far this morning is more performance-focused, a little less emphasis on power. How would applying a power sensitivity affect your modeling?

So, the question is that this was very performance-oriented instead of being very power-oriented in terms of power consumption-oriented. How would this apply? And you can aim for whatever; you can optimize these simulations for whatever factor you want to optimize them for. So, if you're trying to optimize them for speed, which is what I was talking about here, that's certainly one of the things that you can look at. But you can also—the output just as easily could be a power output. You could say, 'The results I'm looking for is how much power consumed.' In that case, your models would have to—your behavioral models would have to model the amount of power that's consumed, and you would have to know that correctly, and it could certainly be one of the outputs.

So when you were showing the models, those models could easily be extended to say, 'This particular interface is so many picajoules per bit.'

Exactly, yeah.

That could be rolled into the same model.

Right, right. And an awful lot of these models, I mean, what I was talking about, the model pieces were pieces that, honestly, were just thrown together in the past week. So the Magnition framework has been built long over time. Okay, but the various different pieces that we used for the CXL simulation were all built over the past week. Well, actually, not this past week, but the week before. You know, I had one engineer at Magnition who went down and said, 'Okay, yes, I can model this CXL thing for you in a week, and I can give you results.' And that's exactly what he gave me.

What was the follow-up question?

What was the question you were answering?

I'm sorry, it's, I thought I... I thought I said that the question was power consumption versus performance. I was, I talked about performance here, and the question was, can you also model power consumption?

My follow-up question was that these models could be extended.

Yes, and the question, the question was, could these models be extended? And my answer is, yes, the models can be extended to do whatever you want them to do. And thank you very much for your time. It looks like we are out of time. Thank you.
