
All right, so welcome everybody.Greg Selman, Principal Architect at AMD.

Hi, I'm Damian, Technical Lead from Meta.

My name is Dirk Blevins.I'm a Senior Platform Architect and Principal Engineer at Intel within our Networking Management Group.

All right, and today we're going to touch on shared infrastructure and what we're doing around that in DCMHS.If anybody was here last year, we touched on this.Just getting started, it was an idea last year.There was a session we called Multi-Host Modular Systems, if anybody remembers that.And a lot of that presentation presented shared infrastructure as we thought about the idea of a hotel.So, think about a hotel.You got a whole bunch of rooms, right?Your individual rooms.We thought about those as your compute nodes or something like that.And then there are a number of things in a hotel that we would like to share.You got a shared pool, gym, that kind of thing.And so, when we thought about shared infrastructure, we kind of had that in mind.And then how are we going to communicate with those, right?You need some interoperability or interconnect, if you will.Thought of that as like a set of elevators or something is what connects your compute nodes to your shared elements, that kind of thing.So, like I said, we kicked that off last year with this Multi-Host Modular System.Out of that, we've spent the last, I guess it has been a year, defining what we've called now shared infrastructure.So, we've added this work stream under the DCMHS umbrella.And really a goal to take that vision of a hotel and turn it into an architecture that you can build upon, right?So, I think there was a question just the last couple of minutes about how can we expect to have systems that are expandable, multiple IO connectivity, that kind of stuff.And I think, if he's still here, maybe not, but I think he would have been really pleased to hear about the shared infrastructure here.So, next page.

So, as we get into what it is, like I said, the first thing we wanted to talk about was how do all of these pieces communicate with one another.So, we have what we've called the high-speed IO, and within that, we are connecting things through PCIe and CXL.I think it just makes sense with where we're at today, what's out there, what's coming.The goal of M-SIF was not to reinvent the wheel.It was to take advantage of some things that are already out there, but then help to expand people's thinking on how these things will plug together and make them interoperable, first and foremost, as part of DCMHS.So, what you see here is what we call our high-speed IO section, and we've defined some data slices, if you will.So, we think that through PCIe, CXL, we've defined a x16 link.We're going to support some bifurcation of that link, and really, that's what we expect the transport to be between the different nodes and the shared elements.Dirk, talk about management.

Sure.So much like with the high-speed IO, we also are trying to define the communication for how to configure, control, and essentially manage the different elements that would be attached within the system.In doing this, one of the considerations was, what do we choose?In all fairness, there's a lot of variation, and this has been a lot of the conversation that we've had in the work stream of what's the right interface?Is it two-wire, I2C, I3C, USB, Ethernet?There's a lot of different ways to implement these solutions.So as we started looking at these things, what we tried to do is we tried to say, what are people currently using, and how are they currently implementing these various management links?Now, the reality is, when you're dealing with a shared infrastructure system, you've got to keep your signals down to a reasonable number, or you're going to have wires and connectivity everywhere.So essentially, what we ended up deciding is that we wanted to make sure that everything was discoverable, was configurable.We're going to have the ability to configure the data slices to be essentially bifurcated.As Greg was saying, we have the ability to deal with directionality.So things can be configured as host ports, root ports, or endpoints dynamically.But there'll be some rules behind that.But coming back to the management aspects, whenever we look at these particular things, what we wanted to do with the redundancy was to make sure that we didn't have any bent pin scenarios.We were able to have redundant interconnects on everything that we would define.We have the two-wire is going to be a main piece of implementation to bring the system up.So you're dealing with I2C, I3C to bring that up.Then we have this notion of some higher speed communication and management interfaces.Some people like to use USB.Other people have looked at Ethernet.So we've been looking at those particular scenarios and providing means by which the customers essentially can implement whatever they need to implement, or consumers of the product is a better way to put it.Then we need to have some reliability aspects in of, do you have a cable attached to know when you need to start discovery?When do you need to do things of that nature?Force throttle was introduced because we felt like some immediate need to be able to force power reduction in a system would be required.We were worried about latency elements there.And then we have the possibility of some various management flexible IO solutions that can be implemented depending upon the connector size and implementation.

So this is an example of one of the things that we've been looking at from a perspective of the solution to be able to map out a ExaMax connector.So when we're looking at this particular connector, you can see that we chose this solution and we were able to be able to take the first two columns in this scenario, map all of our management signals in those two columns.In addition to that, we went from there for the remaining columns and we were able to basically map the data signals in those remaining pieces.So this is an example mapping.We're looking at mapping other systems.We worked with the Amphenol team to be able to pull these together.But we believe this is a way that we can integrate a management interconnect essentially with a data slice into one signal, one connector basically.So in addition to this, if you want to have more data slices to produce scalability, we basically have the ability to be able to add different ExaMax connectors with essentially less columns in order to be able to expand this.So if you needed a x16 or you needed a x32 or something more, you have the ability to be able to do this.And that's the way we dealt with scalability in these scenarios.

So with that said, I'm not going to touch on this real long, but one of the things that we're looking at is shared power, shared cooling, and how far that we take the definition.So when we look at these particular things, we're trying to - everybody feels like there's a lot of innovation in these areas.So we want to be careful about being too prescriptive, but as we're diving into it, we're realizing there's certain elements that are going to have to be defined.When you think about placement of where these connectors may have to live on an HPM, when you think about - and I'm referring to power connectors in that scenario - when you think about if you have liquid cooling, where they're going to live at in relationship to the HPMs.These are things that often want to be similar and consistent.So we're working through those particular things, but we're trying not to be too prescriptive to allow innovation and variation in systems.

I'm Damien.Now I'm going to walk through some of the implement examples that the group have discussed and how we boil this down to what you have seen that Dirk and Greg have talked about.So at a very far left, this is actually an Intel example of what Intel thinks the shared infrastructure should look like and what it would look like.You can see there are some blades, there's a shared power supply, there's a shared NIC.The fan is not shared in this manner.When you move out to the middle, on the top of the middle, there is actually a Dell example of what the vision looks like.The shared fan, single chassis manager, shared elements and core elements, slots can be core elements, it can be a shared element, configurables.Those are great.And when Meta comes to tell, "Hey, the green color one is actually Meta." And then we have been doing shared infrastructure for four generations, Yosemite V1 to V4.I've just now met and talked about we have a Yosemite V4 presentation later on in the afternoon and we also have a Yosemite V4 in a Meta booth if you would like to take a look.Our vision is from shared fan, shared NIC, and Yosemite V4 also allows for shared elements as well.So when we talk about it, it's very similar.And for this, we share a lot of our learning from V2, what we did, what learning we have, so we no longer do it again.All those learnings we talk in the CLA forum that share our learning.Of course, it's good to learn from your own mistakes, but it's even better if you learn from others' mistakes so you no longer do it again.And then as for the one on the right, the one on the horizontal and vertical, the gray box, it's actually AMD's idea.You can see all our group came up with different ideas and, "Hey, how it looks like." And all these get boiled down to a few things that we see and boil it down to those interfaces that you see that Greg and Dirk have been mentioning about that you see in the previous slides.

And as for our timeline, we have shared the Rev 0.3 early in the year, and now we just shared the 0.5 on the M6 spec, which is more complete.It's available today in our wiki.And do take a look and give us the feedback that what do you think?What are the good things?What are the improvement areas?What is missing if you want to implement your own ones?And through that, your feedback, we'll put into the 0.75, and the intent is to release the 0.75 by end of the year.It is also our goal to think this is good enough to start a design based on 0.75.And of course, sometimes when we do design, when you start a design, you say, "Oh, there's one thing I really need, but it's not there." So we do have reserve for future use pins in there we can add in.The idea is between 0.75 to 1.0 is actually very little changes.And through your feedback, we will have them available targeting Q2 next year, the final S spec.And next, I'll pass back to Greg to close.

So next steps.I think you've heard it from a lot of the DCMHS talks this morning is get involved.We would love the feedback, particularly with M-SIF.As Damian mentioned, we're going to try to get to a 0.75 by the end of the year.So we would like to be confident that what we have as far as pinouts and what we've thought about in terms of use cases make sense.We're a relatively large group of 10, but the more the merrier, I would say.So that's first and foremost.If you've just joined the room, check out the rest of the DCMHS specifications on the wiki.And closing comments, Dirk?

Yeah.So I think I kind of somewhat want to echo what Greg said.It's really important that we get this feedback fairly quickly.So if you're seeing things, and this is a really complex scenario because there's been a lot of deviations in the way these things have been implemented, as I explained.So really, please, we ask of you to read the specs, provide us feedback, and we do take that very seriously.I don't know, Damian, if you want to provide a closing?

Yeah.And I know shared infrastructure is not something very common in the server industry.Meta has been doing it for some time.But what we actually see, this is like, if you talk about power efficiency, you talk about cooling efficiency, and meta is very frugal with power, and also cooling.If a turbine engine is here, it's very low.So this is something that we've seen to really get efficient.This is one of the ways, just like what Greg had mentioned, like the hotel.There are certain things we don't want to share them.So this is a way of how we do it in the electrical area and in the service.

We want to open it up to questions for a couple minutes.We have time.We have time for questions.Here we go.

Hey, Dirk.One of the questions in the last session was around CXL attached memory and PMM.Can you guys maybe talk about that for a minute?

Who wants to take it?

I'm moving it.So yeah.So I think if you look at our high-speed I/O definition, I think you'll see that it matches up very well with that.We would consider that a shared element.As long as you take to heart the signal definitions that we've defined there, I don't see why it is not-- can we scroll back a second?I don't know why we couldn't consider some of these different areas, these light blue boxes or these dark blue boxes, to be shared elements as PMMs.

I think another key point there is we're participating in these work streams.So you're seeing us wanting to drive compatibility between what we're doing in DCMHS versus what we're doing in PMM and what we're doing in various other things.So I think that we really want to see compatibility.We don't want to see any deviations in the way you configure these things, regardless if it's in the chassis or in a shared instance in one of these different racks.

So do you envision any sort of unique challenges around interoperability for shared infrastructure versus single node interoperability challenges?And if so, do you have anything in mind that maybe we need to think about as we start building out interoperability specifications and other design aspects?

I'll take that one to start with.I don't care.

I'll take the first shot there.So I think that certainly as we start to look, we've been thinking about how we start to define clear register definitions, clear ways that these things can interoperate and work together in a very well-defined fashion.We don't want to get too detailed, but we have to do enough.And it was kind of like I think you were talking in some of this fashion.We're kind of going down the same path and looking at this is the only way we're going to make this work.You're going to have to have a common way that you bring a system up, the way that it's done, the way that you access it, you determine, you discover, for lack of a better word, what your system's capable of, and then how you configure that in a common uniform way as well.I don't know if you guys want to add to that.

I think maybe just the one thing I'll add is that I don't think we have an expectation that it's going to be perfect out of the gate.I think there's some, particularly in the area of connectors, certain people may choose different connectors.I think I believe that if you're going to do a shared infrastructure platform to start, it may be something that's entirely under one roof.Use Meta for an example.And maybe it takes one or two iterations for it to kind of see who wins.

Yeah.Interoperability, like the group that kind of done the OCP-NIC, it's also pretty much in the same MHS.So if you know what to be implemented and what it needs to be like, as just now we do have, like Tim mentioned about plug and code.So there are some other ways, a strategy to be compatible.Because we're going to take a couple of years to get there.But as I've seen in OCP-NIC, even though it's a simpler subsystem, it's possible to have a part in play.

I think that's all the time we have.All right, real quick, CLA.

So to enable this interoperability, could we all work on a reference system?

Say that again.Who wants a reference system?Who's going to donate a platform?

So right now, we don't have anybody that is working.So the question was, do we have anybody that's working in OCP on a reference system to prove out M-SIF?Was that kind of the question that I'm aware of?We don't have anybody that is doing it currently.So that's a question to the group.You guys are all out here.If you're interested in doing something like that, please get in touch with us, and we'll have the conversation.

All right.Thank you, Dirk.Thank you, Greg.Thank you, Damian.
