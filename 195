YouTube:https://www.youtube.com/watch?v=qlveMGLpokA
Text:
Well, thank you so much for having me, Frank, and the team that put this together. I'll be talking about breaking through the memory wall. Again, I'm Michael Ocampo from  Estera Labs. I'm Senior Product Manager of our ecosystem and our cloud-scale interop lab.

 The agenda I'll cover today is how we're breaking through the memory wall, a few of the use cases  of where we see CXL being a good fit, and OCP designs where CXL could be optimized. And we'll get into some ecosystem collaboration that is happening now  and we're a part of, and we'll go through some of the calls to action.

So, if you take a look at the compute on the bottom left side of this slide,  this is over the last 20 years. What I'd like the viewers to see is how much compute has improved  over the last 20 years, so about 60,000 times, versus DRAM in the green line, the bandwidth is  only increased by about 100. And so that is what defines the memory wall from our view. And basically, we've tried to chip away at this with previous technologies. The industry has been  challenged with this for years, we've been plagued with it. And so the previous solutions have just  been limited in scale in terms of memory bandwidth and capacity. And there's also been significant  latency challenges where the Delta is much different than local memory. Software applications  are developed around servers with local memory, and so SLAs and applications are built around that. So any latency, Delta, or deviation can really throw things off for the end user. And on the data center side, it's also challenging to deploy big memory systems with proprietary  configurations, unless there's mass scale adoption and development that happens. And also on the software integration side, if the technology is all there, and it's all  well adopted and there's standards, but if it's not orchestrated well, and there's a complex way  to provision servers, then it's not going to be adopted and very applicable for popular applications.

So we're trying to solve these things with our approach here. So you can see on the  diagram on the bottom right, we're actually showing 12 memory channels with two LEOs. So  if you just think about one socket, we've got eight memory channels that are local,  and then we can attach our add-in cards powered by our LEO CXL memory controllers. And those  controllers have a x16 link, four DIMMs supporting up to DDR5-5600. And so we're increasing memory  bandwidth and capacity by 50%. And under load, we're actually able to reduce latency by 25%. And we're also using standard DIMMs. So this is JEDEC compliant. This is allowing hyperscalers to  have a flexible supply chain and really take control of their cost structure. And because  we're able to work with ecosystem partners, we're able to seamlessly expand memory. And a good  example of that is with our hardware interleaving technology with Intel fifth-gen Xeon scalable  processors. 

So some of the use cases where we see that this is applicable are two primary camps. It's your standard SQL databases on your left-hand side, and there's some new applications  with this whole momentum around Gen AI and AI services. We do see a fit where CXL can be used. Just to walk through this briefly, what we're talking about is for a SQL database on the  left-hand side, we're able to show how CXL can accelerate or provide time to insights  very quickly about what's happening right now or what has happened. And so there's industry  benchmarks I'll walk through in the next slide to show that. And then there's other use cases that  I'll go into more detail. But essentially, what we're showing on the right-hand side is a vector  database that is caching relevant images. And so Gen AI can have a hybrid approach to provide  semantic cache to serve up AI and supplement data models.

So if we look at the MySQL database acceleration here, we're able to improve  transactions per second by 150%. And so on the bottom left is the actual configuration that we  tested. So we have DRAM only in green, so that's 128 gigabytes, and then DRAM plus CXL in blue. And so we're simulating 1,000 clients with a Persona Labs script. And you can see over time,  we're measuring kind of the delta of the peak performance of the two different configurations. And then you can also see we improved CPU utilization by 15%. And there's actually  headroom to spare here. So if we actually had more devices, such as our Leo memory controller,  we can see the throughput increase even more. And on the right-hand side is OLAP benchmark. So  we're using TPC-H test suite. And the configuration is a little bit different. You can see the DRAM is  512 gigabytes. And then the DRAM plus the CXL is the same local memory plus our CXL controllers. So that's an additional 256 gigabytes of memory. This time, though, we are using our hardware  interleaving mode. And so no software configuration needs to be done. There's nothing really special  about it. All the configuration is actually done by the BIOS. And so what ends up happening is  the system will only see one NUMA node, and that really improves performance. You can see here what it meant for this benchmark is that we're cutting query times in half,  and that can be very significant for some database admins that may be analyzing large pieces of data  that can take an hour and a half. And so cutting that time in half is more time for family or for  other things. And yeah, just so there's different ways, there's different modes to actually use CXL. Also want to call out that we did win an award at Flash Memory Summit last year for the solution  on the left where we partnered with MemVerge as well as Supermicro to  show what we can do with our collaboration.

So CXL has broad applicability. You can see that there's some applications on screen now. On the left is a new data point that we haven't shared before. This is essentially an AI caching  simulation, which is popularly used for semantic search or recommendation engines. And so  what this represents is relative performance of a computer vision system,  identifying scenes with two configurations. Of course, it's the same configuration we were just  talking about with hardware interleaving. So what's essentially happening is a multi-dimensional  model was cached with scenes sharing membership and semantic categories. And that could be like  mountains or forests or streets. So the caching service allowed the model to recall images more  efficiently with additional memory bandwidth. Some of the other benchmarks on screen on the right  we've shared before, it resonated with the HPC community last year at SC. So some of the  use cases here like CFD and EDA, we do see a very good fit here. So we can improve performance by  up to 50% with our interleaving mode between CXL as well as the local DRAM.

So what does this actually look like? So Leo is our chip for the CXL memory controller,  and it's in add-in cards. And so what do servers look like without CXL? Well,  typically there are two systems, like the ones that you see on the left. And without CXL,  you can have up to 48 DIMMs with two sockets. And the challenge for in-memory databases is  there's just not enough memory to really serve up information as fast as possible because the  data sets are just getting so large. And so by procuring two servers, you may be buying things  that are not necessarily needed for your in-memory database. So two systems means you're buying two  backplanes, two power supplies, two chassis, two sets of drives that may be over-provisioned. Whereas on the right, we could just use one box that supports eight double-width cards,  which allows hyperscalers to add the standard DIMM they're using on the motherboard and just  place them onto the add-in cards that are called Aurora A1000. So the advantage here is we're  simply adding the ability to add more DIMMs. And so that would help to improve the TCL equation. And this is not just for your CAPEX because you're not having to procure so many servers,  but you're also improving the OPEX based on the results I showed earlier for the CPU utilization  improving by adding more memory. So in terms of memory capacity, we're increasing  by 2.33 with these two configurations being compared. Or you can say we're increasing  memory bandwidth by 1.66 per socket. 

But not every single system supports a double-width card. So what's interesting is the OCP community has really aligned on some system architectures that  I think is very relevant for today's topic, which is geared around system builders. And so the alignment is around the DCMHS specification, specifically for modular  shared infrastructure. And so META actually has a design that is based off of this specification. It's called Yosemite V4, and that's what I'm showing on the screen here on the top right. Some of the motherboard designs that not just for META but can be based off of MDNO,  which I'll talk more about in the next slide, but that gives the expansion options for both  PCIe and CXL. Now you can see here on the top right that this chassis has the ability to add  eight trays or eight nodes. Not all of them need to be compute. It could be just a bunch of memory,  for example. I think most people are familiar with just a bunch of flash or just a bunch of GPUs,  and so that could be added or expanded from the HPM to a sidecar.

It's essentially disaggregated resources. So same thing can be done with JBOM. So here on the screen  I've done my best to kind of illustrate what that would look like. And so we're disaggregating  and attaching memory through an MXIO cabling or backplane solution. So that helps to reduce TCO  as you're not necessarily needing to procure more host processors to support additional memory. And the community has also aligned on different connectors, which I'll get more into  on the next slide, but one of the nice things about some of the new connectors,  it can support higher power. So there's an auxiliary connector that's providing  additional 400 watts, which is not necessarily needed for just adding more DIMMs.

So there are challenges with this design, and so some of those challenges are signal integrity,  not just for the card itself but for the DIMMs. We've done a lot of interoperability testing  between our controller and the DIMMs, and for sure this is a challenge for system builders. There's also link bifurcation. So if you're using a cabling solution and you're doing by 16,  maybe you want to bifurcate a 2x8, there's got to be mechanisms to do that. And there's also  link and performance that need to be reviewed and tuned. And so we're helping to do that with our  ARIES retimer technologies, which is a complementary solution for the CXL LEO memory controller. So you can imagine this channel loss going from the CPU to MXIO through a backplane and then  through a host interface board and then connected to MXIO connectors on the far right. There's going to be some challenges. So we believe the retimer is an essential  tool for the toolkit to successfully deploy this at scale.

 And so here's a deeper and more detailed look of what it could potentially look like. So you can  see here on the bottom left is the MDNO density optimized host processor module. And so what we  see working with hyperscalers is this new connector, which is classified as MXIO based on the  SFFTA 1037 SNIA specification. And so that allows for high density nodes to have a coplanar connection  with the motherboard. And so that is a huge value proposition because you can now have access to  DIMMs just like you would on the motherboard, but without having to put the system on a crash  cart or pull a node out of the enclosure to simply replace a DIMM. So yeah, this is a very popular  solution that hyperscalers are looking at for low latency memory expansion. And so we see this being  used with 2 dBc or even 1 dBc. And so the x16 controller is a very powerful solution to provide  a lot of bandwidth and low latency because it's so close to the motherboard. Another option if folks  don't want to have cabling, there is an edge connector, which is defined by this SFFTA 1037  SNIA based on the SFFTA 1002. So this is very similar to the OCP 3.0 NIC card. So this is a  very elegant design, and it's not just the OCP community. Almost every OEM has some kind of  blade-like architecture where this could be the perfect solution to expand memory. But of course,  the cabling solution is still great. Not everyone has a blade system. The cabling solution allows for  a little bit more flexibility where the cards may be on the side, on the back, or on the bottom,  or top of the motherboard. So for the system builders, this creates a lot of optionality  just based on the connectors. But what's essential are the key silicon chips that's going to allow  this to happen at scale. 

So it's kind of the same thing in a more simple view as far as what we're  trying to unlock here is our memory controller allowing to expand memory, whether it's directly  onto the motherboard through a CEM connector or a SFFTA 1002 compliant solution. 

And then  we're also allowing for short reach connection. And so for these, depending on where the cable is  routed and how far away it is from the motherboard and the actual channel loss, you may need a  retimer. And so this is going to be essential for enabling just a bunch of memory through a back  plane or a longer MXIO cable solution. 

And at DesignCon, we actually just released or launched  a new product. It is our smart cable modules, which is enabling active PCIe CXL cabling up to  seven meters. So it's unheard of length for a copper cable. But this is really going to unlock  new potential, new architectures for things like shared pooling and shared or just basically pool  memory, which can really help to augment how AI models are trained. And also going back to the AI  inferencing example and how AI services can actually benefit from this cache. So here is just  another view of what this could potentially look like. 

Here on the bottom left, you can see kind of  the more traditional design in the kind of the Yosemite-based platform where you have the HPM  with a local memory attached. And in the middle, you can see kind of like a host interface board,  which have memory expansion. And then on the right, you can now have longer reach for multi-rack  cluster for a memory fabric to a larger memory appliance.

So you might be asking, well, if I have all this far reach capabilities and re-timers in the  channel, how does that impact my latency and performance? Well, based on some of the testing  that we've done in our lab, it's pretty minimal impact. There is definitely some latency impact,  but it's less than 10% even with two re-timers. So you can see the configuration on the bottom. We did use a fifth-gen Intel Xeon scalable processor. All the memory was DDR5 5600. We're just using standard Ubuntu, and we use Intel MLC, memory latency checker, to basically stress  and measure the latency of the memory with these three different configs.

And so all this is great, but it really takes a village. It takes the whole ecosystem to really  mobilize and deploy CXL at scale. And so we're working closely with not just the memory vendors,  but of course the OS vendors. We want to make sure that the memory and the devices are discoverable,  and memory can be allocated seamlessly. And we're working, of course, on the DIMM's ability  and performance, and we feel that we're there. We've done a number of interop bulletins with  all the different DIMM capacities and speeds that we support, and we have more in the pipeline as  far as some of the OS developments that are happening to release new features. And we'll continue to work with other ecosystem partners, hardware, as well as independent  software vendors to unlock more potential and manage resources more seamlessly and tune things,  whether we're using hardware interleaving or software tiering solutions. Of course,  MemVerge has their own memory tiering solution. There's also other open source solutions. And so  we all need to work together to exercise those different options to provide the best solutions  for hyperscalers and enterprise as well. And so the other thing that we're doing is we announced  our COSMOS solution, which stands for Connectivity System Management and Optimization Software. And what this ultimately is doing is providing link, fleet, and RAS management information. And so this actually is based off our SDK, and it's something that can really empower  fleet management and provide deeper insights. And so this is going to operate  in BMC of the system, and so that will enable Redfish support. And again, that's another area  of opportunity for the ecosystem to work together and align on to manage this infrastructure at  scale. And of course, we'll continue to work on the embedded software to really meet all the  compliance and requirements for CXL 2.0 RAS. We have those features, and we're excited to share  that with the industry. And we're providing a lot of telemetry as well, whether you're looking for  CXL bandwidth or even the PCIe infrastructure. We can look at every single lane to see  what is the recovery rate, what is the link speed. And so we can diagnose that on the fly. So this is something deeper that we haven't seen in the industry. So this is our software-defined  IC architecture. And all these fundamental building blocks are essential for the software  integration as well as orchestration for things like Kubernetes and infrastructure as code. And so a lot of the site reliability managers we want to work with to build out the Ansible  playbooks, the Terraform. And that's really going to provide a better experience for managing CXL  at scale. 

So calls to action here, definitely visit our page if you want to learn more about  our Leo CXL Memory Controller. OCP has a lot of information on their website. There's a really  good spec for CXL tiered memory expander by Facebook and Google. Check that out. And there's,  of course, the DCMHS MSIF base specification that I talked about that are hyperlinked here. And yeah, get engaged. Definitely join the CMS group or the Linux collaboration or reach out  to me directly. We'll also be at the NVIDIA GTC. So come check us out there and stay up to date  with our PCIe and CXL bulletins at hysterialabs.com/interop. That's my presentation. I'm happy to take questions.
