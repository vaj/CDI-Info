
In this session, we'll explore MemVerge's cutting-edge GPU-as-a-Service solution to transform enterprise AI operations. We'll learn how MemVerge AI Platform maximizes the GPU utilization, streamlines workload management, and ensures uninterrupted operations through innovative features like dynamic GPU surfing. We'll dive into key use cases from training large language models to enterprise-scale AI deployment, and I'll demonstrate how our solution can benefit various stakeholders, from platform engineers to data scientists and decision makers. We'll discover how our platform optimizes costs while maintaining data security and sovereignty. Hello, my name is Steve Scargall. I'm the Director of Product Management for AI/ML and CXL at MemVerge. Let's go ahead and dive in.

I want to ask the audience a general question about where you are in the AI journey. I suspect the vast majority of you are in the first three steps. So, either you haven't started yet, you're in the early stages of research and coming up to speed with a plethora of options available to you, you've done the research and are now in the pilot and experimentation stage, trying to decide which solutions are right for you and your use cases, or, for the smaller majority, you've probably already made that decision and have a roadmap to ramp up gen AI in your business. And then, finally, for the very small contingent, you may already have launched some gen AI solutions into production and are executing the roadmap to scale up to those deployments or bring new ones to market.

Our KPMG Gen AI survey from July of 2024 captured the perspectives from over 200 C-suite and senior business leaders representing organizations with an annual revenue of a billion or more. And what it showed was 16% of the organizations think they already have a highly equipped and capable organization with the skills they need. 78% had moderately equipped organizations, and 55% were already investing in upskilling the existing employees for GenAI. And then, finally, most of them, 61%, were hiring new talent, which is an increase over the previous year. So, we can see that there's definitely a requirement and a need for enterprises to get ramped up here, ready for building out their roadmaps and deploying solutions to production.

And from the same survey, people were asked whether or not they were going to build versus buy. And you can see here that 12% thought they were going to build in-house, probably because they have very specific requirements that they couldn't find external solutions to. 29% of the respondents said they were going to do a mixture of using external sources that could be open-source software or buying and then customizing that to their own needs, and then 50% were either going to buy or lease from external vendors.

Another question was all about data sovereignty and security, which is paramount for enterprise customers that are laser-focused on maintaining their data security. So, because of this, many enterprises want to bring their generic solutions on-prem to guarantee data sovereignty, security, and quality. So, the industry has a lot of work to do to handle regulatory requirements that are still being defined, but you can see that this is a necessity for a lot of customers that don't want data to be leaked out onto the internet or they don't want data to be used for the foundational models that can then be used for future training.

Now, a different study from Weights and Biases focused more on the GPU utilization. So, their study asked enterprise customers that were using GPUs about the distribution and utilization of those GPUs. So, this chart shows the GPU utilization on the x-axis and the percentage of the respondents on the y-axis. So, we just take a data point, for example: approximately 60 percent of the respondents declared that their GPUs were 45% or less utilized, which is a significant waste given how expensive these things are, and you want to be able to utilize a lot of that resource for the workloads that you want to throw at it. And this is, again, one of the key focuses for MemVerge AI.

So, enterprises that are bringing up GPUs in their own data centers have to have a way of managing that fleet and handle the workloads. Now, of course, there's plenty of solutions for managing data centers; that's not anything that we're particularly interested in. But there's a disconnect between how applications, particularly gen AI applications, are going to work and utilize that resource that is being built out in data centers, and that's where we come in with our MemVerge AI platform, with a blue box kind of sitting below the applications and above the data centers. And we see ourselves as an infrastructure platform software, making it very simple for platform admins to manage their fleet of GPUs and for application owners to ensure that the resources that are available within the data center—and we kind of see that as a pool of resources that they're available to those applications when the applications need them.

So, if we take a quick look at the high-level view of what MemVerge AI is, it's designed particularly for enterprises to stand up the GPU clusters within the data centers and then allow the deployment of applications on top of that. So, we're looking at inference servicing, fine-tuning services, and, for at least the LLM pieces, the persistent context. You know, our DNA and memory allow us to focus in these areas and deliver some innovative solutions on top of the data centers that provide us with the underlying CPU, memory, and GPUs.

So, let me do a deeper dive onto the blue blocks that you just saw in the previous slide here. So, again, we're kind of focusing at the moment on the GPU-as-a-Service and the lower layers. And for the IT organizations, we have our Tech Checkpoint technology that we've had for many years that allows us to take transparent checkpoints of applications on, modified applications, to clarify. And more recently, we've added the GPUs into that technology as well. So, now an application that uses CPU and GPU to do the work, we can now take a consistent checkpoint of the entire application and then restore it. So, we're... in addition to that, we're also able to use reservation and bursting, and I'll get into that in a little bit. But between the ability of pooling of resources at the data center level and using our technology just sitting on top of that, we can deliver the reservations and allow users to consume more resources that they necessarily haven't reserved up front without penalty. We have the priority queuing, so we can prioritize workloads for different projects and departments and ensure that high-priority workloads always get onto GPU in lieu of low priorities. We can also do checkpointing and bump off the lower-priority jobs that might be currently running on the hardware and then put them back once the high-priority jobs have completed. We support NVIDIA's Megan time-slicing solutions, and I'll talk a little bit about some of the other ones that we've got going on there in a minute. But then, on the Observability piece, again, for the IT platform guys: the utilization optimization of CPU and GPU. We provide telemetry into the CPU, GPU, memory utilization because we manage all of the workloads. And, again, going back to the reservation and bursting piece, you can set up your own spot market internally, meaning that, you know, if department A has some resources that they're not using, but department B's consumed all of their resources, department B can use the resources that are sitting idle for department A. And then, you know, the tracking and telemetry allows us to then generate a bill for department B that will then pay department A at the end of the month, whenever is appropriate. So, over on the user side, we have the GPU-centric job scheduler. We allow GPU surfing, and, again, I'll showcase what that means in a moment. But, fundamentally, that's using our checkpoint technology and allowing us, with the priority queuing and job scheduling, to move workloads around the environment to where they're needed without necessarily having to kill them and start them over again. We support Jupyter notebooks and VS Code for the user workspaces, and we'll have an app and model directory as well where you can store all of those fine-tuned LoRA adapters in the future. On the infrastructure side, we focus initially on Kubernetes, but we also have roadmap items that get us into LSF and Slurm and maybe even bare metal as well. So, on the cloud side, because we do sit above the hardware, we're pretty much agnostic to running on-premise or in the cloud. So, we can showcase our software running in AWS and Google Cloud, Azure, and many others besides.

So, where does MemVerge AI help in the productivity for enterprises? Uh, we first of all enhance the resource efficiency. So, again, we're utilizing the pooling capabilities of data centers—the CPUs, the memory, and the GPUs—so we're efficiently maximizing those expensive resources that we ideally don't want just sitting idle for long periods of time. So, in doing that, we can put more jobs into the environment that improves the time to result for the owners of those applications that need to run. And, again, with GPU surfing, we can have uninterrupted operations, meaning we can move these workloads around the environment dependent on what's happening at that moment in time. Again, we're using the suspend and resume technology for minimal downtime, where we have an intuitive UI and command line that allows us to control all of that stuff. And then, we support Kubernetes today, but I just talked about, you know, the on-prem and the public cloud and other schedulers as well; they're part of our roadmap. Again, for those enterprise customers that are keen on security, because you're running now in your own environment, you have maintenance of your data, so it's not leaking out into the internet.

So, some of the key use cases that we're focusing on right now—of course, you know, everybody's talking about gen AI and model training and inferencing—so we have solutions for that. And, dependent on which model type you're training for—if it's a general model or, like, a GPT, for example, or you're into generating images or videos or maybe even speech recognition—we can support all of them. And then, at the enterprise-scale deployment side, we've focused on NVIDIA right now, but AMD is coming very shortly here, probably the next release. And then, multi-department resource allocation is our ability to allow, again, kind of departments to share resources rather than being siloed. So, we're getting better utilization of the underlying physical resources whilst maintaining, you know, ownership, so to speak, of those resources.

So, I just want to kind of focus on the personas of who we're interested in and who we're targeting right now. So, I mentioned before, you know, the platform engineering teams—kind of the teams that are responsible for the hardware and orchestration layers and that type of stuff. We've got the data scientists and the developers and the ML ops engineers that are kind of their customers that are writing code, deploying it, and managing the CI/CD processes and then leveraging any APIs that they have, both internally and externally, to allow for that fast model deployment and accurate execution into production. And then, of course, the decision makers, right? The people that have to make the decision about who has access to what resource and what resources are needed to be bought. So, any of the CTOs, CIOs, data center managers, etc.

So, again, you know, the optimization of GPUs in a data center is going to be pretty paramount. You know, these things are expensive; we want to make sure that we're using them optimally for the workloads that the users are assigning to us and scheduling. So, again, the strategic resource allocation of the workloads dependent on the priority and the criticality of those jobs as they come in is one of our goals. And we want to reduce the idle time and improve the utilization of those GPUs overall. So, being able to, you know, move workloads around dependent on the requirements—you know, if we have a high-priority job that comes in that needs a lot of GPUs, we should be able to move any existing running jobs that have, you know, lower priority off that host or hosts onto somewhere else, allowing the high-priority jobs to come in. So, that's the priority piece. And then, we have extensive telemetry in our software that allows you to monitor all of this—GPU utilization, the CPU, and other components as well—so that we can demonstrate we're doing a good job with our software.

So, let's dive in a little bit about how, you know, people will actually start to use MemVerge AI. So, one of the core concepts is that we align to organizations, that being that, you know, a company—could be MemVerge, for example—has many different departments in it, and each department has multiple projects that are going on at the same time. So, this example just shows, you know, that a company has an engineering, a test, W80, and a production environment department, and each one of those could be working on different parts of the same project or different projects altogether. But, fundamentally, this scales to enterprise-class customers, so you can have as many departments and projects as you desire underneath your corporation.

So, the way that we manage hardware is at the project level. So, you can take a server that has some CPU, DRAM, and GPU, and then we group those servers that have the same components or the same manufacturer of CPU and GPU together, and we call those a node group. These are our logical abstractions. And then, we can create project resource pools. So, in this example, I have two projects—a knowledge base and a chatbot—and I have three resource pools here: the A100 group, the H100 group, and the L40S group. So, the dotted line shows the resource allocation. So, in this case, the knowledge base has exclusive access to the A100 group, the chatbot has exclusive access to the L40S group, but they can both share the H100 group. So, if they didn't have enough resources or high-priority jobs come in and they want to be able to use specific H100s, both teams—the projects—could definitely do that.

And, again, this is kind of a demonstration of the bursting, borrowing, and GPU surfing. So, for example, we have three projects—Project A, B, and C—color-coded here. So, Project C has four GPUs, but he's only using two of them. Project A has four GPUs, is using two for its own workloads, but you can see that Project B had four GPUs and it exceeded that allotment. But because Project A had those GPUs available, we're able to run workloads from Project B onto Project A. And if Project A had some more workloads that came in that needed the GPUs that they had reserved, then we would bump off the workloads for Project B and either move them over to Project C if it had resources, or, if it didn't have resources, those jobs would go back into the queue, and they would be resumed at some future point once some GPUs are available. But this demonstrates the pooling capabilities, right? So, we're not siloing GPUs to a particular project, although they may have purchased them, but you can see now that this is just a giant pool, and we can move workloads around as required.

So, how does GPU sharing work? Well, we've got a few options here. NVIDIA has their MIG—multi-instance GPUs—that allows us to logically partition a physical GPU into up to seven virtual GPUs, each one having isolated SMs—streaming multiprocessors—of the compute side on GPU and some amount of high-bandwidth memory. The other approach is that you can give workloads access to the entire GPU but only for a very short period of time, and that's what we call time slicing. So, here you have four workloads all kind of jumping on and off the GPU dependent on, you know, what they need to do at any given time. And then, one thing that we've introduced from the MemVerge side is fractional GPU. So, rather than time slicing, assuming that we can bin-pack these workloads onto a single GPU, we can do that very easily. So, this example—the same workloads from the time-slicing example—now you can see that, you know, given some amount of memory requirements, we can bin-pack them onto the same physical GPU and allow them to run continuously until they, you know, no longer need GPU.

So, let's dive into a quick bit of demonstration here. I'll walk you through some screenshots and some videos. 

So, I talked about the priority scheduling and preemption using our checkpoint technologies. So, whenever a project is assigned that priority—high, medium, low—projects with that higher priority will interrupt anything that's currently executing out there that has a lower priority. So, in the example here, we start on the left where I start—you know, I've already started some three projects in my low-priority group for engineering. The blue hexagon implies that that job is currently running on GPU and CPU, and then I've got two jobs backed up behind it. And then, I'm starting a job in my high-priority group. So, once that job is started and gets to the point where it needs to be entered into the cluster, at that point, our checkpoint technology comes in. It checkpoints the currently running low-priority job, puts that to the back of the queue, and then we start our high-priority job because we've freed up those resources to go and do that.

So, this is a short video for demonstrating how this works. So, you can see here the high- and low-priority projects there, and the GPU reservations, any CPU and memory reservations that we have, and how many workloads that we have total, and then currently executing. So, the hexagons I showed in the previous slide are here, and they're interactive, so I can hover over them and show more information. This particular demo is using Kubeflow right now; we've since replaced this with our own user interface to make it a little bit more streamlined. But here, we'll go ahead and create a new Jupyter notebook using that high-priority project. We can assign, you know, CPU and RAM to this. We've chosen our notebook image that has NVIDIA CUDA and PyTorch available, and then, towards the end, we can enable our checkpoint technology. So, once we've done that, we can start that notebook, and, again, it's going to get queued up. We're going to look at the resources in the backend cluster, and what you'll notice soon here is that not only is our high-priority job pending, that that's us kind of waiting for those resources to become available. And now, you can see that the high-priority job has started, right? So, it's now running on that cluster, and we've terminated—or suspended, really—the workload that was running. So, I can connect into this workload and start my Jupyter notebook. Here, I can run a very simple training algorithm here that just runs for 10 epochs, just to keep the demonstration short. But you can see that this will complete after 10 epochs; each epoch has just a little over 1800 steps to it, so it completes pretty quickly. And now that that training job is completed, I can kind of stop it manually here. This would be automated in an enterprise environment, of course, but being a Jupyter notebook that's interactive, we manually do this. And then, once I destroy that workload, there—now you can see it's gone. And after a few moments, you'll see that one of the workloads that was queued in the low-priority queue now can be entered back into the cluster, and there you go—it goes blue, showing that it's now running, and he's doing its workload. So, that's a very brief demonstration of how prioritization and checkpointing works in our environment.

Here's a screenshot of our dashboard. So, we show a lot of information here, like I was talking about before, but you can kind of take away from this that the amount of data for the CPU, the GPU—you can break down by project. We show how many workloads there are. We've got heat maps for GPU, so you can see over a 24-hour period how utilized your GPUs were. 

We talked about node groups—our logical abstraction of grouping of same servers that have the same CPU and GPU resources. So, here you can see that we have the default node group and an engineering node group, 

and that the node groups are made up of nodes. And so, this is a very simple cluster with just a single management control plane for the cluster and a GPU worker node as well. 

We have multiple departments here, so 'corporate' is the one that you get out of the box. I've created some—engineering, a project management, a test dev, and a UAT department—and you can see how many projects are assigned to each one of those, the node groups, and when they were created.

Each project allows you to see detailed information around the resources that have been allocated or reserved and how many workloads are running inside of each project. 

Each project allows you to create not only the project but also assign it to a particular department, set the priority. You can choose the node groups, so if we had more than one node group here, you'd see that in the list. You can select the node groups you want based on the resources that that node group has, and then you can prioritize the execution of that. So, again, if we go back to our previous example of H100, A100, and L40, if I had those node groups available, I could say I always want to use the A100s first, and if I can't use A100s because they're all busy, then go check the H100. And if they're all busy, then go check the L40 group. So, that's the way this works. And then, I can reserve some amount of GPU, CPU, and memory as well in the boxes, so I guaranteed some resources for my department and my project.

The departments—again, this is looking at the billing side. So, you can see now that me and the project management didn't have any resources available, so I had to borrow some GPUs from my engineering department. And in doing so, my jobs executed successfully, but that cost me a little over ten dollars to go do that.

So, let's look more closely at the checkpoint restore. So, this is a quick demonstration of how that works for an application that has no in-built checkpoint capabilities. So, very similar job to what you saw previously. We have the workload running currently, so it's training in the bottom side. And in Jupyter Notebook, you can see it's moving forward. This one I kept at 10,000 epochs, so we won't let it go all the way to the end. But if I suspend that job, you can see now that it stopped at epoch 4.33—oh, sorry, step 4.33 of 1875, and the epoch is 2.96. So, again, my code doesn't have any checkpointing in there, so ordinarily this would have to restart from the beginning. But because we took a checkpoint, we can now restore that back to a running state. And you'll see, again, rather than restart from the beginning, starting from epoch 0, the job actually continues from where it left off without skipping a beat. So, we don't lose anything in terms of epoch or step. And then, once the UI catches up, there you go. You can kind of see that that job was paused for around 34 seconds and then continues in each epoch now. Now runs to completion, and this job will continue until it stops.

So, that's a whirlwind tour of MemVerge AI. If you'd like to learn more, feel free to reach out. We have a pioneer program. You can access it using that QR code. You can reach out to me directly or go to memverge.com and find out more information. So, this is how you can get access to MemVerge AI. We're happy to answer any questions. We can set you up with a sandbox environment so you can go play with it yourself and learn a little bit more. And, yeah, we'd love to continue that discussion with you. So, with that, I thank you very much for your time. I greatly appreciate it. And, yeah, we'll hand it back to Frank for the next session.
